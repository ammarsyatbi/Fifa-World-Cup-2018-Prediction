{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "\n",
    "'''\n",
    "filename\n",
    "fifa_2018_results.csv\n",
    "fifa_2018_results_updated_20180708.csv\n",
    "fifa_2018_stats.csv\n",
    "past_results.csv\n",
    "players_scores.csv\n",
    "sofifa.csv\n",
    "'''\n",
    "\n",
    "#Preparing data\n",
    "data = pd.read_csv('dataset/fifa_2018_results_updated_20180708.csv')\n",
    "data.head()\n",
    "\n",
    "data_home = data.groupby('home_team', as_index = False).agg({\"home_result\": np.sum})\n",
    "data_away = data.groupby('away_team', as_index = False).agg({\"away_result\": np.sum})\n",
    "\n",
    "data = data_home.rename( columns = {'home_team' : 'team'})\n",
    "data['away_result'] = data_away['away_result']\n",
    "data['total_score'] = data['away_result'] + data['home_result'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 24) (64,)\n",
      "(8, 24) (8,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>attack</th>\n",
       "      <th>midfield</th>\n",
       "      <th>defense</th>\n",
       "      <th>bu_speed</th>\n",
       "      <th>bu_passing</th>\n",
       "      <th>cc_crossing</th>\n",
       "      <th>cc_passing</th>\n",
       "      <th>cc_shooting</th>\n",
       "      <th>aggression</th>\n",
       "      <th>...</th>\n",
       "      <th>midfield_opponent</th>\n",
       "      <th>defense_opponent</th>\n",
       "      <th>bu_speed_opponent</th>\n",
       "      <th>bu_passing_opponent</th>\n",
       "      <th>cc_crossing_opponent</th>\n",
       "      <th>cc_passing_opponent</th>\n",
       "      <th>cc_shooting_opponent</th>\n",
       "      <th>aggression_opponent</th>\n",
       "      <th>pressure_opponent</th>\n",
       "      <th>avg_age_opponent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>82</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>74</td>\n",
       "      <td>59</td>\n",
       "      <td>44</td>\n",
       "      <td>74</td>\n",
       "      <td>39</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>26.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>74</td>\n",
       "      <td>59</td>\n",
       "      <td>44</td>\n",
       "      <td>74</td>\n",
       "      <td>39</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>82</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>25.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  attack  midfield  defense  bu_speed  bu_passing  cc_crossing  \\\n",
       "0       84      83        85       82        35          30           53   \n",
       "1       78      78        78       74        59          44           74   \n",
       "\n",
       "   cc_passing  cc_shooting  aggression        ...         midfield_opponent  \\\n",
       "0          24           35          47        ...                        78   \n",
       "1          39           61          61        ...                        85   \n",
       "\n",
       "   defense_opponent  bu_speed_opponent  bu_passing_opponent  \\\n",
       "0                74                 59                   44   \n",
       "1                82                 35                   30   \n",
       "\n",
       "   cc_crossing_opponent  cc_passing_opponent  cc_shooting_opponent  \\\n",
       "0                    74                   39                    61   \n",
       "1                    53                   24                    35   \n",
       "\n",
       "   aggression_opponent  pressure_opponent  avg_age_opponent  \n",
       "0                   61                 61             26.64  \n",
       "1                   47                 47             25.65  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Data\n",
    "sofifa = pd.read_csv('dataset/sofifa1.csv') #only the latest stat\n",
    "sofifa.rename(columns = {'countrystats' : 'team'} , inplace = True)\n",
    "sofifa.head()\n",
    "\n",
    "data_new = pd.read_csv('dataset/fifa_2018_results_updated_20180708.csv')\n",
    "data1 = data_new.join(sofifa.set_index('team'), on = 'home_team')\n",
    "data1 = data1.join(sofifa.add_suffix('_opponent').set_index('team_opponent'), on = 'away_team')\n",
    "\n",
    "data1 = data1.drop(columns=['game','away_team','away_result','home_penalty','away_penalty','lat','long','stadium' ,'country','city','datestats','bu_positioning','cc_positioning','datestats_opponent','bu_positioning_opponent','cc_positioning_opponent'])\n",
    "data1 = data1.rename( columns = {'home_team': 'team','home_result' : 'result'})\n",
    "\n",
    "data2 = data_new.join(sofifa.set_index('team'), on = 'away_team')\n",
    "data2 = data2.join(sofifa.add_suffix('_opponent').set_index('team_opponent'), on = 'home_team')\n",
    "\n",
    "data2 = data2.drop(columns=['game','home_team','home_result','home_penalty','away_penalty','lat','long','stadium' ,'country','city','datestats','bu_positioning','cc_positioning','datestats_opponent','bu_positioning_opponent','cc_positioning_opponent'])\n",
    "data2 = data2.rename( columns = {'away_team': 'team','away_result' : 'result'})\n",
    "\n",
    "pd.options.display.max_rows = 200\n",
    "data_new = data1.append(data2 , ignore_index = True)\n",
    "data_new = data_new.dropna()\n",
    "#Feature Transformation\n",
    "\n",
    "# split train and test\n",
    "y = data_new['result']\n",
    "X = data_new.drop(columns = ['date','team','result'])\n",
    "X = normalize(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "#Prediction Data\n",
    "fr = sofifa[sofifa['team'] == 'France'].drop(columns = ['datestats','bu_positioning','cc_positioning']).reset_index(drop=True)\n",
    "cr = sofifa[sofifa['team'] == 'Croatia'].drop(columns = ['datestats','bu_positioning','cc_positioning']).reset_index(drop=True)\n",
    "\n",
    "data_predict = fr.join( cr.drop(columns = 'team').add_suffix('_opponent'))\n",
    "data_predict = data_predict.append(cr.join(fr.drop(columns = 'team').add_suffix('_opponent')), ignore_index = True)\n",
    "team = data_predict['team']\n",
    "data_predict.drop(columns ='team', inplace = True)\n",
    "data_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = np.array(data['team'])\n",
    "home_s = np.array(data['home_result'])\n",
    "away_s = np.array(data['away_result'])\n",
    "total_s = np.array(data['total_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x864 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['team'] = data['team'].astype('category')\n",
    "index = data['team'].cat.codes\n",
    "width = 0.3\n",
    "\n",
    "fig , (ax ,axt) = plt.subplots(2, figsize=(17,12))\n",
    "fig.subplots_adjust(hspace=.5)\n",
    "\n",
    "home = ax.bar(index, home_s, width, label = 'Home Score', color = '#1abc9c')\n",
    "away = ax.bar(index +width, away_s, width, label = 'Away Score', color ='#34495e')\n",
    "\n",
    "\n",
    "def autolabel(bar):\n",
    "    for b in bar:\n",
    "        height = b.get_height()\n",
    "        ax.text(b.get_x() + b.get_width()/2, height+0.05, height, ha='center', va='bottom')\n",
    "    \n",
    "ax.set_xlim(left = min(index)-0.5 , right = max(index) + 1 )\n",
    "ax.set_ylim(top = ax.get_ylim()[1] + 1)\n",
    "ax.set_xticks(index + width) \n",
    "ax.set_xticklabels(team,  rotation='vertical')\n",
    "ax.set_title('Score \\nHome vs Away')\n",
    "ax.set_xlabel('Team')\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend()\n",
    "\n",
    "autolabel(away)\n",
    "autolabel(home)\n",
    "\n",
    "\n",
    "total = axt.bar(data['team'], data['total_score'], label = 'Total Score' , color = '#c0392b')\n",
    "axt.set_ylim(top = axt.get_ylim()[1] + 1)\n",
    "axt.set_xticks(data['team']) \n",
    "axt.set_xticklabels(data['team'],  rotation='vertical')\n",
    "axt.set_title('Total Score')\n",
    "axt.set_xlabel('Team')\n",
    "axt.set_ylabel('Score')\n",
    "axt.legend()\n",
    "\n",
    "for b in total:\n",
    "    height = b.get_height()\n",
    "    axt.text(b.get_x() +b.get_width()/2, height+0.05, height, ha='center', va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9MAAAGGCAYAAAB40dIoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X20pVddH/DvrxkoSQYGMHiFhDpqKUhJLeYu3xA7AyhgqOhqK1CkorJGqyJiWhutFqVagxoKK6VqBJRKZGpTMJQgIMLISws6AWQSAiVCIBNCJhAYmBiNg7t/POfWy517c8+emfOc+/L5rHXWmfs8+zz7d/bZ9yTf+7xVay0AAADA9P7OvAsAAACAzUaYBgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKCTMA0A20RVvbKqWlWdN6f+f2HS/zcvW7ZjsuzN86hpWR1zHRsANh9hGmCbmQSGu3s8c941sraqOrzi87qrqj5dVe+vqv9WVf+squ45o77fUVXHZ7HtWVstyAPAqdgx7wIAmJufX2P5+0atgpP1n5N8LsMfxu+T5GFJ/lmSZyT5v1X19NbawRWv+bdJfiHJJ8csdJkXJXllko/Nqf+7M++xAWCTEaYBtqnW2s/NuwZOyQtba4eXL6iq+yb5xSQ/nORNVfX1rbUPL61vrd2S5JZxy/xbrbVPJfnUvPq/O/MeGwA2H4d5A7CmZeeR/r2qek5VHaqqO1ee31pVT6yqP5gcbvxXVfXnVfXLVXWfNbb7+Kp6Z1XdUVW3V9VrquofrHbealU9brLsZ9bY1uGqumGNdU+vqgNV9dmq+suq+kBV/fTKw6CXn7dbVQ+oqpdW1Scn7+XaqvpXdzNGT6iq11XVkUn7m6rq96vqMZP1T5ps+/I1Xn+vyRjceqqHZ7fWPtta+5Ekv5vkfkl+aUVfq54XXFXfWVVvWfaeb56M2w9N1v/9qmpJHpXkjBWHmb952XYOV9UNVbWrql5UVR+rqr9e+uzWO9S6qs6rqiuq6rbJPDtYVU9Zpd2zJtv5nlXWnXAOdlUdTvLvJz++fVntx5e1WfOc6ap6alW9vao+N6nr/VX171b7vJaNwdlVdWlVfXwyph+uqn9TVbXaewdg87FnGoBpvCTJNyd5fZKrk9y1tKKqnp/kZ5N8Osn/SnJbkq/JcNjsE6vqm1prn1/W/ikZwt5fJfnvGQ6r/ZYk/yfJB05XwVX1iiT/KsnHk1yZ5GiSb8qw5/YxVfX41toXVrzs/pM6/iLJ7yU5M8l3J3lFVX2htXbFij5+MclPJ/l8kt9PcjjJgzKM1b9M8pYMY3ZjkqdV1UXLx2LiuzME3xe01u7K6fH8Sf/fUVU7W2vH1mpYVT+c4fO9JclrM+w5/tIMn+H3Jvn1JLdnOC3g+5OcN9n+ko+s2OS9khzIcOj5GzKMzY1T1PwlSf73pP+XZxiT706yv6oe1Fr7z1NsYy0vTPKdSR6d5LcyzIkk+Zv1XlhVv5xhLt+W4RD1O5JcmOSSJN82mUcrzyO/Z5I3ZxjH1yf5QpLvSvIrSf5uhjkIwGbXWvPw8PDw2EaPJG3y+LlVHs9c0faVk7Y3JfnyVbb1rZP1b0+ya8W6Z03W/cqyZfdJ8pkMYfyRK9pftqy285Ytf9xk2c+s8X4OJ7lhjb5/L8m9Vqz7j5N1P7Js2Y5lff9GkjOWrTs/Qxh6/4rtfPuk/Q1JHrRiXSU5d9nPF0/a/tAq9b8jQ6j7qik/v8Mrx2iNdp+YtHv0Kp/n8vH9syR3JjlnlW2cs0qtx6eo7Y1Jzlpl/S9M1n/zGmP/u0lq2bqvSvLZDH94+fJly5c+3+9ZpY+l7b15vb7XmOvLx+bRk2U3JvnSFX28frLuJ9cYg/+V5Mxly78swznuty+fXx4eHh4em/fhMG+A7et5qzyeuUbbS1prq1006scmz89qrR1dvqK19tIk1yZ5+rLF35Xkvkl+p7X23hXb+g8Z9mKeDs/JENif1Vr7yxXrfj5DQHv6Ca9KjiW5qC3bY91aO5TkXUkeUVVnLmv77Mnzc1trn1i+kTa4edmil03q+cHl7arq4RkOnX5za+3Pp31zU1qq6QFTtP3ryeOLtOEc55PxE621v+h8zfEkF7fW2rL+/zzJf8mwp/eEQ7pH8P2T5+e31o4sq+t4kosyhOZnrfHaZ7fW7lz2mk9mCNj3S/KQ2ZQLwJgc5g2wTbXWes7d/JM1ln9jhr2GT1vjVNAdSR5YVbsmYftrJ8v/eJV6PlNV788QLk9aVd07ySOS3JrkJ9ao6y+TfPUqyz/UVj8k+qYMh4jfN8Ne3CT5hgx7lN+4Xk2ttduq6sok/7Kqvq61tjSeS+H619fbxklYeuPtblslVyR5QZIPVNV/z/DZvPMUgvQdrbXrTuJ1H22tfXyV5QcynO/8yJOs51Qszde3rFzRWru+qm5J8pBVDqX/dGvtxlW2d9Pk+X6nt0wA5kGYBmAaa90u6P4ZQtvz1nn9zgznLO+a/HxrZz897j95XlinrtXul/zZddqesWzZriS3tenPc/6vGc5j/sEkf1JV98pwG6tPZjhX+XR70OT5trtr1Fr75ao6kuRfJ/nxJM9N0qrqrUn+bWvtPZ39rvXZnuzrlubErjXWz9JSn2td5fuWDOO8K8NRDUt65hEAm5TDvAGYxlp7Nz+XIVDWOo+lQ56XDgVfWGN7X7bKsqWLRK31B+CVIWupjz9dp6Z7rLG9aR1Ncs60V+Burb0zyfuTPLWqduVvLzz2snbiBaxOSVU9LMNY/nWSlYfTr1bbb7fWvj7DRcCelOEiXXuTvLGqvqSz+/X2hK9lvTmx/DSCu5sT9z3J/lez1Odq8zJJHriiHQDbiDANwKl4V5IHVNVDp2y/tJfzn6xcUVX3S/KPVnnNZybPD17lNQ/LsNf7/2utfTbJh5KcX8N9l2flXRn+O/r4jtf8WpKzMpz/uy9DKPzN019a/sPk+fdba3dM+6LW2mdaa1e31n4gye8kOSfDlcmXfCFJzej2Tl9RVSd8xkn2TJ6X/1FgzTmRZHGN7S+dB9+zV3ipzz0rV0zm/AOTfHiNUwMA2OKEaQBOxQsnzy+tqgeuXFlVO6vq65ctek2GvXjPqKqV58A+P8m9V+njAxkOof2uqjpn2bbPSvLiu6nrXkleNtkLvLKu+6/Sf6/Llvpa472fu8prXpnhIms/leHc8DescWG3kzK5v/NLkjwtw1Wjf3qK1zyhqnasWFYZbuuUDLcJW/LpDP/vcMK9mE+DHUlesDyoV9VXJfnRDHvYl9+W7GCGPeBPX35RuMle9EvW2P6nJ89/r6Oml0+ef3b5HvrJeF2a4RSHl3VsD4AtxDnTAJy01tqbqupnMtxu6sNV9QdJPpphb/HuDHug35rh0OG01j5XVT+U4RZI75xc8GrpPtNfneHWS9+8oo+/qqrLMgTQ91XVa5LcI8Me4Y9llXNtW2uXV9UFGfb+/pOqelOGewvfP8lXZrjl0W9mCGon+95fX1WXZLjt1Qerauk+0182eQ9vz4orPbfWjlXV7yT54cmi3zjZ/jNcXO1zGcLtvZM8LMP7OjvJB5M8vbV2wxTbuTLJ56vqHRluAXXGZDuLGS4899Zlbf8owxXZf7+q3pDhYmwfbSvuv32S3pdh3K6ZfF73z3Ao/K4MVwe/calha+2mqtqf4Y8G753Mu10Zblf2xxnukb3SWzIE8BdU1ddkOK/5b1pr/2mtglprb6uqFyb5iSTXTS4i9xcZ7jP98ElfL1zr9QBsbcI0AKektfaLVfX2DLfJelSSJ2fY+3w4w1Wqr1jRfn9VfSbDochPyXBl7bcl+b7Jsi8K0xM/k+SOJD+Q4QJen5xs9/lJPrxGXT9YVVdP2n9rhnNpP50hgP9yhr3Ep6S19lNV9c4Mt8n6pxmC7JEMIXSt7b88Q5g+nOTqU+j+uZPn4xn2dt+c5NVJrkry2tbaCbe6WsNPJvm2JBdkCIl/mSFU/2SSX1txPvdvZDi0+imT9TsyBOzTEaY/nWEMX5Dhc753kusy3Kd8/yrtvz/DBcCemmE8P5Yh2L44Qwj/Iq21a6vq+zIE4x/JcOTCF5KsGaYnr7uoqt4z6eN7M/wh54YMe/1f2DHOAGwxtex2jgAwV1X1ygz3f35wa+3wvOuZhap6Voa94j/XWvv5edcDAJwc50wDwEgm59o+N8ldSS6fczkAwClwmDcAzFhVPTrDeeGPyXCu7Ytaa2vduxgA2ASEaQCYvccn+fcZrrD96xkupgYAbGLOmQYAAIBOzpkGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOi0YxYbPeecc9ru3btnsenT7o477sjZZ5897zLYJsw3xmbOMTZzjrGZc4zNnNv6rrnmmk+11h6wXruZhOndu3fn4MGDs9j0aXfgwIHs2bNn3mWwTZhvjM2cY2zmHGMz5xibObf1VdXHpmnnMG8AAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKDTVGG6qp5bVddV1bVV9aqqutesCwMAAICNat0wXVXnJvmxJIuttUckOSPJU2ddGAAAAGxU0x7mvSPJmVW1I8lZST4xu5IAAABgY1s3TLfWbk7yq0k+nuSWJEdba2+adWEAAACwUVVr7e4bVN0vyf9M8pQkn03yP5Jc2Vp75Yp2+5LsS5KFhYUL9u/fP5OCT7djx45l586d8y7jtDt089HR+jr/3F2j9bXZbdX5xsZlzjE2c46xmXOMzZzb+vbu3XtNa21xvXY7ptjW45J8tLV2W5JU1auTfFOSLwrTrbXLk1yeJIuLi23Pnj29Nc/FgQMHsllq7fHMi68era8bn75ntL42u60639i4zDnGZs4xNnOOsZlzLJnmnOmPJ/mGqjqrqirJY5NcP9uyAAAAYOOa5pzpdye5Msl7khyavObyGdcFAAAAG9Y0h3mntfa8JM+bcS0AAACwKUx7aywAAABgQpgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6rRumq+qhVfW+ZY/PVdWPj1EcAAAAbEQ71mvQWvtQkn+cJFV1RpKbk7xmxnUBAADAhtV7mPdjk/x5a+1jsygGAAAANoNqrU3fuOrlSd7TWvsvq6zbl2RfkiwsLFywf//+01bkLB25/WhuvXOcvs4/d9c4HSU5dPPR0foa831tdseOHcvOnTvnXQbbiDnH2Mw5xmbOMTZzbuvbu3fvNa21xfXaTR2mq+qeST6R5B+21m69u7aLi4vt4MGDU2133i674qpcemjdo91PixsvuXCUfpJk98VXj9bXmO9rsztw4ED27Nkz7zLYRsw5xmbOMTZzjrGZc1tfVU0VpnsO835ihr3SdxukAQAAYKvrCdNPS/KqWRUCAAAAm8VUYbqqzkryrUlePdtyAAAAYOOb6mTh1tpfJPmSGdcCAAAAm0LvrbEAAABg2xOmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQaaowXVX3raorq+qDVXV9VX3jrAsDAACAjWrHlO1enOQNrbV/XlX3THLWDGsCAACADW3dMF1V90nyLUmemSSttbuS3DXbsgAAAGDjmuYw769McluS36qq91bVS6vq7BnXBQAAABtWtdbuvkHVYpJ3JXlUa+3dVfXiJJ9rrf3sinb7kuxLkoWFhQv2798/o5JPryO3H82td47T1/nn7hqnoySHbj46Wl9jvq+xzGr8Fs7MCfNtK44fG8exY8eyc+fOeZfBNmLOMTZzjrGZc1vf3r17r2mtLa7Xbpow/WVJ3tVa2z35+dFJLm6tXbjWaxYXF9vBgwf7Kp6Ty664KpcemvbU8VNz4yVrDtlpt/viq0fra8z3NZZZjd9F5x8/Yb5txfFj4zhw4ED27Nkz7zLYRsw5xmbOMTZzbuurqqnC9LqHebfWPpnkpqp66GTRY5N84BTrAwAAgE1r2l2yz05yxeRK3h9J8n2zKwkAAAA2tqnCdGvtfUnW3c0NAAAA28E0V/MGAAAAlhGmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ2EaQAAAOgkTAMAAEAnYRoAAAA6CdMAAADQacc0jarqxiSfT/KFJMdba4uzLAoAAAA2sqnC9MTe1tqnZlYJAAAAbBIO8wYAAIBO04bpluRNVXVNVe2bZUEAAACw0VVrbf1GVQ9qrX2iqr40yR8meXZr7W0r2uxLsi9JFhYWLti/f/8s6j3tjtx+NLfeOU5f55+7a5yOkhy6+ehofY35vsYyq/FbODMnzLetOH5sHMeOHcvOnTvnXQbbiDnH2Mw5xmbObX179+69ZprrhE0Vpr/oBVU/l+RYa+1X12qzuLjYDh482LXdebnsiqty6aGeU8dP3o2XXDhKP0my++KrR+trzPc1llmN30XnHz9hvm3F8WPjOHDgQPbs2TPvMthGzDnGZs4xNnNu66uqqcL0uod5V9XZVXXvpX8n+bYk1556iQAAALA5TbNLdiHJa6pqqf3vttbeMNOqAAAAYANbN0y31j6S5GtGqAUAAAA2BbfGAgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0GnqMF1VZ1TVe6vqdbMsCAAAADa6nj3Tz0ly/awKAQAAgM1iqjBdVecluTDJS2dbDgAAAGx81Vpbv1HVlUl+Kcm9k/yb1tqTVmmzL8m+JFlYWLhg//79p7nU2Thy+9Hceuc4fZ1/7q5xOkpy6Oajo/U15vsay6zGb+HMnDDftuL4JebgRnHs2LHs3Llz3mWwjZhzjM2cY2zm3Na3d+/ea1pri+u127Feg6p6UpIjrbVrqmrPWu1aa5cnuTxJFhcX2549azbdUC674qpcemjdYTgtbnz6nlH6SZJnXnz1aH2N+b7GMqvxu+j84yfMt604fok5uFEcOHAgm+X7mK3BnGNs5hxjM+dYMs1h3o9K8h1VdWOS/UkeU1WvnGlVAAAAsIGtG6Zbaz/VWjuvtbY7yVOTvKW19j0zrwwAAAA2KPeZBgAAgE5dJwu31g4kOTCTSgAAAGCTsGcaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADotG6Yrqp7VdWfVNWfVdV1VfXzYxQGAAAAG9WOKdr8VZLHtNaOVdU9kryjqv6gtfauGdcGAAAAG9K6Ybq11pIcm/x4j8mjzbIoAAAA2MimOme6qs6oqvclOZLkD1tr755tWQAAALBx1bDjecrGVfdN8pokz26tXbti3b4k+5JkYWHhgv3795/OOmfmyO1Hc+ud4/R1/rm7xukoyaGbj47W15jvayyzGr+FM3PCfNuK45eYgxvFsWPHsnPnznmXwSbW+7u82vfctPwuczJ8zzE2c27r27t37zWttcX12nWF6SSpqucluaO19qtrtVlcXGwHDx7s2u68XHbFVbn00DSnjp+6Gy+5cJR+kmT3xVeP1teY72sssxq/i84/fsJ824rjl5iDG8WBAweyZ8+eeZfBJtb7u7za99y0/C5zMnzPMTZzbuurqqnC9DRX837AZI90qurMJI9L8sFTLxEAAAA2p2n+dPzAJK+oqjMyhO/fa629brZlAQAAwMY1zdW835/kkSPUAgAAAJvCVFfzBgAAAP6WMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7CNAAAAHQSpgEAAKCTMA0AAACdhGkAAADoJEwDAABAJ2EaAAAAOgnTAAAA0EmYBgAAgE7rhumqenBVvbWqrq+q66rqOWMUBgAAABvVjinaHE9yUWvtPVV17yTXVNUfttY+MOPaAAAAYENad890a+2W1tp7Jv/+fJLrk5w768IAAABgo+o6Z7qqdid5ZJJ3z6IYAAAA2AyqtTZdw6qdSf44yS+21l69yvp9SfYlycLCwgX79+8/nXXOzJHbj+bWO8fp6/xzd43TUZJDNx8dra8x39dYZjV+C2fmhPm2FccvMQc3imPHjmXnzp3zLoNNrPd3ebXvuWn5XeZk+J5jbObc1rd3795rWmuL67WbKkxX1T2SvC7JG1trL1yv/eLiYjt48OBUhc7bZVdclUsPTXPq+Km78ZILR+knSXZffPVofY35vsYyq/G76PzjJ8y3rTh+iTm4URw4cCB79uyZdxlsYr2/y6t9z03L7zInw/ccYzPntr6qmipMT3M170rysiTXTxOkAQAAYKub5pzpRyV5RpLHVNX7Jo9vn3FdAAAAsGGtexxWa+0dSWqEWgAAAGBT6LqaNwAAACBMAwAAQDdhGgAAADoJ0wAAANBJmAYAAIBOwjQAAAB0EqYBAACgkzANAAAAnYRpAAAA6CRMAwAAQCdhGgAAADoJ0wAAANBJmAYAAIBOwjQAAAB0EqYBAACgkzANAAAAnYRpAAAA6CRMAwAAQCdhGgAAADoJ0wAAANBJmAYAAIBOwjQAAAB0EqYBAACgkzANAAAAndYN01X18qo6UlXXjlEQAAAAbHTT7Jn+7SRPmHEdAAAAsGmsG6Zba29LcvsItQAAAMCmUK219RtV7U7yutbaI+6mzb4k+5JkYWHhgv3795+mEmfryO1Hc+ud4/R1/rm7xukoyaGbj47W15jvayyzGr+FM3PCfNuK45eYg6fD6RjD1ebcarbqGHLqeufhtHNuNVt1Hvo+nK1jx45l586d8y6DLW757/GpfM9NQ2aYv717917TWltcr91pC9PLLS4utoMHD07TdO4uu+KqXHpoxyh93XjJhaP0kyS7L756tL7GfF9jmdX4XXT+8RPm21Ycv8QcPB1OxxiuNudWs1XHkFPXOw+nnXOr2arz0PfhbB04cCB79uyZdxlscct/j0/le24aMsP8VdVUYdrVvAEAAKCTMA0AAACdprk11quS/J8kD62qw1X1A7MvCwAAADaudQ/2b609bYxCAAAAYLNwmDcAAAB0EqYBAACgkzANAAAAnYRpAAAA6CRMAwAAQCdhGgAAADoJ0wAAANBJmAYAAIBOwjQAAAB0EqYBAACgkzANAAAAnYRpAAAA6CRMAwAAQCdhGgAAADoJ0wAAANBJmAYAAIBOwjQAAAB0EqYBAACgkzANAAAAnYRpAAAA6CRMAwAAQCdhGgAAADoJ0wAAANBpqjBdVU+oqg9V1Q1VdfGsiwIAAICNbN0wXVVnJHlJkicmeXiSp1XVw2ddGAAAAGxU0+yZ/rokN7TWPtJauyvJ/iRPnm1ZAAAAsHFNE6bPTXLTsp8PT5YBAADAtlSttbtvUPUvkjy+tfasyc/PSPJ1rbVnr2i3L8m+yY8PTfKh01/uTJyT5FPzLoJtw3xjbOYcYzPnGJs5x9jMua3vy1trD1iv0Y4pNnQ4yYOX/Xxekk+sbNRauzzJ5VOXt0FU1cHW2uK862B7MN8YmznH2Mw5xmbOMTZzjiXTHOb9p0keUlVfUVX3TPLUJK+dbVkAAACwca27Z7q1dryqfjTJG5OckeTlrbXrZl4ZAAAAbFDTHOad1trrk7x+xrXMy6Y7NJ1NzXxjbOYcYzPnGJs5x9jMOZJMcQEyAAAA4ItNc840AAAAsMy2DdNV9YSq+lBV3VBVF8+7Hra2qnpwVb21qq6vquuq6jnzrontoarOqKr3VtXr5l0LW19V3beqrqyqD06+775x3jWxdVXVcyf/Tb22ql5VVfead01sLVX18qo6UlXXLlt2/6r6w6r68OT5fvOskfnalmG6qs5I8pIkT0zy8CRPq6qHz7cqtrjjSS5qrX11km9I8iPmHCN5TpLr510E28aLk7yhtfYsx1vRAAACyklEQVSwJF8Tc48Zqapzk/xYksXW2iMyXCT3qfOtii3ot5M8YcWyi5P8UWvtIUn+aPIz29S2DNNJvi7JDa21j7TW7kqyP8mT51wTW1hr7ZbW2nsm//58hv/BPHe+VbHVVdV5SS5M8tJ518LWV1X3SfItSV6WJK21u1prn51vVWxxO5KcWVU7kpyV5BNzroctprX2tiS3r1j85CSvmPz7FUm+c9Si2FC2a5g+N8lNy34+HMGGkVTV7iSPTPLu+VbCNvCiJD+Z5G/mXQjbwlcmuS3Jb01OLXhpVZ0976LYmlprNyf51SQfT3JLkqOttTfNtyq2iYXW2i3JsLMkyZfOuR7maLuG6VplmcuaM3NVtTPJ/0zy4621z827HrauqnpSkiOttWvmXQvbxo4kX5vk11prj0xyRxz+yIxMzlN9cpKvSPKgJGdX1ffMtypgu9muYfpwkgcv+/m8ODSIGauqe2QI0le01l4973rY8h6V5Duq6sYMp7I8pqpeOd+S2OIOJzncWls66ubKDOEaZuFxST7aWruttfbXSV6d5JvmXBPbw61V9cAkmTwfmXM9zNF2DdN/muQhVfUVVXXPDBeseO2ca2ILq6rKcB7h9a21F867Hra+1tpPtdbOa63tzvAd95bWmr02zExr7ZNJbqqqh04WPTbJB+ZYElvbx5N8Q1WdNflv7GPjgneM47VJvnfy7+9NctUca2HOdsy7gHlorR2vqh9N8sYMV398eWvtujmXxdb2qCTPSHKoqt43WfbTrbXXz7EmgNPt2UmumPyh+iNJvm/O9bBFtdbeXVVXJnlPhjtmvDfJ5fOtiq2mql6VZE+Sc6rqcJLnJbkkye9V1Q9k+KPOv5hfhcxbteZUYQAAAOixXQ/zBgAAgJMmTAMAAEAnYRoAAAA6CdMAAADQSZgGAACATsI0AAAAdBKmAQAAoJMwDQAAAJ3+HzHprk8t603gAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1224x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x864 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Frequency Distribution \n",
    "sorted_home = np.sort(home_s)\n",
    "sorted_away = np.sort(away_s)\n",
    "sorted_total = np.sort(total_s)\n",
    "\n",
    "plt.figure(figsize=(17,5))\n",
    "plt.hist(sorted_home, bins = home_s.size)\n",
    "plt.title('\\nFrequency Distribution\\n\\n', fontsize= 20)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "fig , (fq1,fq2,fq3) = plt.subplots(3, figsize=(17,12))\n",
    "\n",
    "def freq_dist(fq,dat):    \n",
    "    fit = stats.norm.pdf(dat, np.mean(dat), np.std(dat)) \n",
    "    fq.plot(dat,fit,'-o' , color = '#e67e22')\n",
    "    fq.hist(dat, density=True , color = '#2ecc71')\n",
    "     \n",
    "freq_dist(fq1,sorted_home)\n",
    "freq_dist(fq2,sorted_away)\n",
    "freq_dist(fq3,sorted_total)\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\n",
      " Home : 2.5625\n",
      " Away : 2.34375\n",
      " Total : 4.90625\n",
      "Mode\n",
      " Home : 11\n",
      " Away : 7\n",
      " Total : 14\n"
     ]
    }
   ],
   "source": [
    "#Descriptive Analysis\n",
    "\n",
    "#Mean | not skewed\n",
    "home_mean = home_s.sum()/ home_s.size\n",
    "away_mean = away_s.sum()/ away_s.size\n",
    "total_mean = total_s.sum()/ total_s.size\n",
    "print('Mean')\n",
    "print(' Home : ' + str(home_mean))\n",
    "print(' Away : ' + str(away_mean))\n",
    "print(' Total : ' + str(total_mean))\n",
    "\n",
    "#Median| skewed\n",
    "\n",
    "#Mode\n",
    "#Best measure of central tendency (nominal)\n",
    "print('Mode')\n",
    "print(' Home : ' + str(np.nanmax(home_s)))\n",
    "print(' Away : ' + str(np.nanmax(away_s)))\n",
    "print(' Total : ' + str(np.nanmax(total_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Analyze Data Condition for model selection\n",
    "result = np.array( data_new['result'])\n",
    "overall = np.array(data_new['overall'])\n",
    "overall_op = np.array(data_new['overall_opponent'])\n",
    "\n",
    "for i in range (0,result.size):\n",
    "    plt.scatter(overall[i],result[i], marker ='o', c ='b')\n",
    "\n",
    "for i in range (0,result.size):\n",
    "    plt.scatter(overall_op[i],result[i] , marker = 'x', c= 'r')\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "def PCA_Transformer(PCA_data):    \n",
    "    PCA_data = np.array(PCA_data)\n",
    "    pca =PCA(n_components =24)\n",
    "    pca.fit(PCA_data)\n",
    "    #pca.fit_transform(PCA_data)\n",
    "    X = pca.transform(PCA_data)\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print(pca.singular_values_)  \n",
    "    print(X.size)\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.76300047e-01 2.55820634e-01 1.09871020e-01 6.84516157e-02\n",
      " 6.75439110e-02 4.62106305e-02 4.27132420e-02 3.42813106e-02\n",
      " 3.13977825e-02 2.24505010e-02 1.87189897e-02 1.11271697e-02\n",
      " 4.94614905e-03 2.74790330e-03 2.68435897e-03 1.62113845e-03\n",
      " 1.20629263e-03 6.22120000e-04 4.66096390e-04 4.09215548e-04\n",
      " 1.55807303e-04 1.46851626e-04 5.97211627e-05 4.74923286e-05]\n",
      "[0.66061725 0.63566342 0.41658249 0.32881475 0.32662734 0.27016581\n",
      " 0.25974112 0.23269558 0.22269421 0.1883096  0.17194939 0.13257199\n",
      " 0.08838794 0.065881   0.0651148  0.05060221 0.04365015 0.03134704\n",
      " 0.02713297 0.02542351 0.01568749 0.01522996 0.00971233 0.00866106]\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "X_train = PCA_Transformer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.03286198\n",
      "Iteration 2, loss = 2.03233142\n",
      "Iteration 3, loss = 2.03180125\n",
      "Iteration 4, loss = 2.03127161\n",
      "Iteration 5, loss = 2.03074257\n",
      "Iteration 6, loss = 2.03021428\n",
      "Iteration 7, loss = 2.02968680\n",
      "Iteration 8, loss = 2.02915980\n",
      "Iteration 9, loss = 2.02863328\n",
      "Iteration 10, loss = 2.02810732\n",
      "Iteration 11, loss = 2.02758191\n",
      "Iteration 12, loss = 2.02705708\n",
      "Iteration 13, loss = 2.02653276\n",
      "Iteration 14, loss = 2.02600900\n",
      "Iteration 15, loss = 2.02548585\n",
      "Iteration 16, loss = 2.02496320\n",
      "Iteration 17, loss = 2.02444126\n",
      "Iteration 18, loss = 2.02391998\n",
      "Iteration 19, loss = 2.02339930\n",
      "Iteration 20, loss = 2.02287925\n",
      "Iteration 21, loss = 2.02235982\n",
      "Iteration 22, loss = 2.02184101\n",
      "Iteration 23, loss = 2.02132284\n",
      "Iteration 24, loss = 2.02080542\n",
      "Iteration 25, loss = 2.02028872\n",
      "Iteration 26, loss = 2.01977270\n",
      "Iteration 27, loss = 2.01925734\n",
      "Iteration 28, loss = 2.01874264\n",
      "Iteration 29, loss = 2.01822874\n",
      "Iteration 30, loss = 2.01771554\n",
      "Iteration 31, loss = 2.01720317\n",
      "Iteration 32, loss = 2.01669153\n",
      "Iteration 33, loss = 2.01618056\n",
      "Iteration 34, loss = 2.01567024\n",
      "Iteration 35, loss = 2.01516058\n",
      "Iteration 36, loss = 2.01465159\n",
      "Iteration 37, loss = 2.01414325\n",
      "Iteration 38, loss = 2.01363575\n",
      "Iteration 39, loss = 2.01312890\n",
      "Iteration 40, loss = 2.01262242\n",
      "Iteration 41, loss = 2.01211650\n",
      "Iteration 42, loss = 2.01161121\n",
      "Iteration 43, loss = 2.01110667\n",
      "Iteration 44, loss = 2.01060288\n",
      "Iteration 45, loss = 2.01009961\n",
      "Iteration 46, loss = 2.00959692\n",
      "Iteration 47, loss = 2.00909486\n",
      "Iteration 48, loss = 2.00859342\n",
      "Iteration 49, loss = 2.00809263\n",
      "Iteration 50, loss = 2.00759259\n",
      "Iteration 51, loss = 2.00709318\n",
      "Iteration 52, loss = 2.00659444\n",
      "Iteration 53, loss = 2.00609635\n",
      "Iteration 54, loss = 2.00559885\n",
      "Iteration 55, loss = 2.00510187\n",
      "Iteration 56, loss = 2.00460533\n",
      "Iteration 57, loss = 2.00410937\n",
      "Iteration 58, loss = 2.00361399\n",
      "Iteration 59, loss = 2.00311922\n",
      "Iteration 60, loss = 2.00262503\n",
      "Iteration 61, loss = 2.00213136\n",
      "Iteration 62, loss = 2.00163817\n",
      "Iteration 63, loss = 2.00114562\n",
      "Iteration 64, loss = 2.00065362\n",
      "Iteration 65, loss = 2.00016209\n",
      "Iteration 66, loss = 1.99967089\n",
      "Iteration 67, loss = 1.99918014\n",
      "Iteration 68, loss = 1.99868993\n",
      "Iteration 69, loss = 1.99820026\n",
      "Iteration 70, loss = 1.99771088\n",
      "Iteration 71, loss = 1.99722196\n",
      "Iteration 72, loss = 1.99673356\n",
      "Iteration 73, loss = 1.99624563\n",
      "Iteration 74, loss = 1.99575818\n",
      "Iteration 75, loss = 1.99527120\n",
      "Iteration 76, loss = 1.99478470\n",
      "Iteration 77, loss = 1.99429878\n",
      "Iteration 78, loss = 1.99381339\n",
      "Iteration 79, loss = 1.99332853\n",
      "Iteration 80, loss = 1.99284417\n",
      "Iteration 81, loss = 1.99236029\n",
      "Iteration 82, loss = 1.99187708\n",
      "Iteration 83, loss = 1.99139438\n",
      "Iteration 84, loss = 1.99091207\n",
      "Iteration 85, loss = 1.99043025\n",
      "Iteration 86, loss = 1.98994902\n",
      "Iteration 87, loss = 1.98946813\n",
      "Iteration 88, loss = 1.98898766\n",
      "Iteration 89, loss = 1.98850753\n",
      "Iteration 90, loss = 1.98802792\n",
      "Iteration 91, loss = 1.98754871\n",
      "Iteration 92, loss = 1.98706997\n",
      "Iteration 93, loss = 1.98659147\n",
      "Iteration 94, loss = 1.98611326\n",
      "Iteration 95, loss = 1.98563559\n",
      "Iteration 96, loss = 1.98515836\n",
      "Iteration 97, loss = 1.98468155\n",
      "Iteration 98, loss = 1.98420513\n",
      "Iteration 99, loss = 1.98372902\n",
      "Iteration 100, loss = 1.98325343\n",
      "Iteration 101, loss = 1.98277828\n",
      "Iteration 102, loss = 1.98230356\n",
      "Iteration 103, loss = 1.98182926\n",
      "Iteration 104, loss = 1.98135536\n",
      "Iteration 105, loss = 1.98088204\n",
      "Iteration 106, loss = 1.98040919\n",
      "Iteration 107, loss = 1.97993679\n",
      "Iteration 108, loss = 1.97946455\n",
      "Iteration 109, loss = 1.97899268\n",
      "Iteration 110, loss = 1.97852110\n",
      "Iteration 111, loss = 1.97804996\n",
      "Iteration 112, loss = 1.97757918\n",
      "Iteration 113, loss = 1.97710881\n",
      "Iteration 114, loss = 1.97663884\n",
      "Iteration 115, loss = 1.97616922\n",
      "Iteration 116, loss = 1.97569897\n",
      "Iteration 117, loss = 1.97522806\n",
      "Iteration 118, loss = 1.97476247\n",
      "Iteration 119, loss = 1.97430180\n",
      "Iteration 120, loss = 1.97384204\n",
      "Iteration 121, loss = 1.97338147\n",
      "Iteration 122, loss = 1.97292156\n",
      "Iteration 123, loss = 1.97246228\n",
      "Iteration 124, loss = 1.97201308\n",
      "Iteration 125, loss = 1.97156824\n",
      "Iteration 126, loss = 1.97112058\n",
      "Iteration 127, loss = 1.97068129\n",
      "Iteration 128, loss = 1.97025182\n",
      "Iteration 129, loss = 1.96983091\n",
      "Iteration 130, loss = 1.96941648\n",
      "Iteration 131, loss = 1.96901871\n",
      "Iteration 132, loss = 1.96862689\n",
      "Iteration 133, loss = 1.96824593\n",
      "Iteration 134, loss = 1.96787324\n",
      "Iteration 135, loss = 1.96751816\n",
      "Iteration 136, loss = 1.96717454\n",
      "Iteration 137, loss = 1.96683417\n",
      "Iteration 138, loss = 1.96649671\n",
      "Iteration 139, loss = 1.96616949\n",
      "Iteration 140, loss = 1.96585458\n",
      "Iteration 141, loss = 1.96555241\n",
      "Iteration 142, loss = 1.96526056\n",
      "Iteration 143, loss = 1.96497723\n",
      "Iteration 144, loss = 1.96469958\n",
      "Iteration 145, loss = 1.96442788\n",
      "Iteration 146, loss = 1.96416286\n",
      "Iteration 147, loss = 1.96390807\n",
      "Iteration 148, loss = 1.96366570\n",
      "Iteration 149, loss = 1.96343215\n",
      "Iteration 150, loss = 1.96320688\n",
      "Iteration 151, loss = 1.96298897\n",
      "Iteration 152, loss = 1.96277903\n",
      "Iteration 153, loss = 1.96257725\n",
      "Iteration 154, loss = 1.96238237\n",
      "Iteration 155, loss = 1.96218993\n",
      "Iteration 156, loss = 1.96199883\n",
      "Iteration 157, loss = 1.96181636\n",
      "Iteration 158, loss = 1.96164092\n",
      "Iteration 159, loss = 1.96147142\n",
      "Iteration 160, loss = 1.96130675\n",
      "Iteration 161, loss = 1.96114285\n",
      "Iteration 162, loss = 1.96097981\n",
      "Iteration 163, loss = 1.96081760\n",
      "Iteration 164, loss = 1.96065572\n",
      "Iteration 165, loss = 1.96049486\n",
      "Iteration 166, loss = 1.96033593\n",
      "Iteration 167, loss = 1.96018130\n",
      "Iteration 168, loss = 1.96002748\n",
      "Iteration 169, loss = 1.95987444\n",
      "Iteration 170, loss = 1.95972176\n",
      "Iteration 171, loss = 1.95956961\n",
      "Iteration 172, loss = 1.95941801\n",
      "Iteration 173, loss = 1.95926693\n",
      "Iteration 174, loss = 1.95911634\n",
      "Iteration 175, loss = 1.95896621\n",
      "Iteration 176, loss = 1.95882167\n",
      "Iteration 177, loss = 1.95867856\n",
      "Iteration 178, loss = 1.95853616\n",
      "Iteration 179, loss = 1.95839682\n",
      "Iteration 180, loss = 1.95826568\n",
      "Iteration 181, loss = 1.95814248\n",
      "Iteration 182, loss = 1.95802318\n",
      "Iteration 183, loss = 1.95790682\n",
      "Iteration 184, loss = 1.95779599\n",
      "Iteration 185, loss = 1.95768923\n",
      "Iteration 186, loss = 1.95758504\n",
      "Iteration 187, loss = 1.95748172\n",
      "Iteration 188, loss = 1.95737917\n",
      "Iteration 189, loss = 1.95727732\n",
      "Iteration 190, loss = 1.95717610\n",
      "Iteration 191, loss = 1.95707582\n",
      "Iteration 192, loss = 1.95697593\n",
      "Iteration 193, loss = 1.95687980\n",
      "Iteration 194, loss = 1.95678448\n",
      "Iteration 195, loss = 1.95668952\n",
      "Iteration 196, loss = 1.95659685\n",
      "Iteration 197, loss = 1.95650741\n",
      "Iteration 198, loss = 1.95641914\n",
      "Iteration 199, loss = 1.95633126\n",
      "Iteration 200, loss = 1.95624360\n",
      "Iteration 201, loss = 1.95615605\n",
      "Iteration 202, loss = 1.95606878\n",
      "Iteration 203, loss = 1.95598350\n",
      "Iteration 204, loss = 1.95589838\n",
      "Iteration 205, loss = 1.95581340\n",
      "Iteration 206, loss = 1.95572855\n",
      "Iteration 207, loss = 1.95564380\n",
      "Iteration 208, loss = 1.95555917\n",
      "Iteration 209, loss = 1.95547462\n",
      "Iteration 210, loss = 1.95539017\n",
      "Iteration 211, loss = 1.95530579\n",
      "Iteration 212, loss = 1.95522148\n",
      "Iteration 213, loss = 1.95513723\n",
      "Iteration 214, loss = 1.95505304\n",
      "Iteration 215, loss = 1.95496894\n",
      "Iteration 216, loss = 1.95488495\n",
      "Iteration 217, loss = 1.95480100\n",
      "Iteration 218, loss = 1.95471709\n",
      "Iteration 219, loss = 1.95463320\n",
      "Iteration 220, loss = 1.95454935\n",
      "Iteration 221, loss = 1.95446552\n",
      "Iteration 222, loss = 1.95438268\n",
      "Iteration 223, loss = 1.95430022\n",
      "Iteration 224, loss = 1.95421776\n",
      "Iteration 225, loss = 1.95413532\n",
      "Iteration 226, loss = 1.95405289\n",
      "Iteration 227, loss = 1.95397046\n",
      "Iteration 228, loss = 1.95388805\n",
      "Iteration 229, loss = 1.95380564\n",
      "Iteration 230, loss = 1.95372324\n",
      "Iteration 231, loss = 1.95364085\n",
      "Iteration 232, loss = 1.95355847\n",
      "Iteration 233, loss = 1.95347609\n",
      "Iteration 234, loss = 1.95339372\n",
      "Iteration 235, loss = 1.95331136\n",
      "Iteration 236, loss = 1.95322935\n",
      "Iteration 237, loss = 1.95314765\n",
      "Iteration 238, loss = 1.95306592\n",
      "Iteration 239, loss = 1.95298417\n",
      "Iteration 240, loss = 1.95290238\n",
      "Iteration 241, loss = 1.95282058\n",
      "Iteration 242, loss = 1.95273876\n",
      "Iteration 243, loss = 1.95265692\n",
      "Iteration 244, loss = 1.95257506\n",
      "Iteration 245, loss = 1.95249320\n",
      "Iteration 246, loss = 1.95241133\n",
      "Iteration 247, loss = 1.95232944\n",
      "Iteration 248, loss = 1.95224755\n",
      "Iteration 249, loss = 1.95216566\n",
      "Iteration 250, loss = 1.95208380\n",
      "Iteration 251, loss = 1.95200196\n",
      "Iteration 252, loss = 1.95192011\n",
      "Iteration 253, loss = 1.95183827\n",
      "Iteration 254, loss = 1.95175643\n",
      "Iteration 255, loss = 1.95167460\n",
      "Iteration 256, loss = 1.95159277\n",
      "Iteration 257, loss = 1.95151094\n",
      "Iteration 258, loss = 1.95142915\n",
      "Iteration 259, loss = 1.95134768\n",
      "Iteration 260, loss = 1.95126613\n",
      "Iteration 261, loss = 1.95118453\n",
      "Iteration 262, loss = 1.95110288\n",
      "Iteration 263, loss = 1.95102118\n",
      "Iteration 264, loss = 1.95093944\n",
      "Iteration 265, loss = 1.95085772\n",
      "Iteration 266, loss = 1.95077619\n",
      "Iteration 267, loss = 1.95069464\n",
      "Iteration 268, loss = 1.95061308\n",
      "Iteration 269, loss = 1.95053151\n",
      "Iteration 270, loss = 1.95044992\n",
      "Iteration 271, loss = 1.95036832\n",
      "Iteration 272, loss = 1.95028672\n",
      "Iteration 273, loss = 1.95020525\n",
      "Iteration 274, loss = 1.95012377\n",
      "Iteration 275, loss = 1.95004224\n",
      "Iteration 276, loss = 1.94996067\n",
      "Iteration 277, loss = 1.94987913\n",
      "Iteration 278, loss = 1.94979768\n",
      "Iteration 279, loss = 1.94971621\n",
      "Iteration 280, loss = 1.94963473\n",
      "Iteration 281, loss = 1.94955324\n",
      "Iteration 282, loss = 1.94947174\n",
      "Iteration 283, loss = 1.94939035\n",
      "Iteration 284, loss = 1.94930894\n",
      "Iteration 285, loss = 1.94922748\n",
      "Iteration 286, loss = 1.94914597\n",
      "Iteration 287, loss = 1.94906458\n",
      "Iteration 288, loss = 1.94898318\n",
      "Iteration 289, loss = 1.94890176\n",
      "Iteration 290, loss = 1.94882043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 291, loss = 1.94873905\n",
      "Iteration 292, loss = 1.94865763\n",
      "Iteration 293, loss = 1.94857632\n",
      "Iteration 294, loss = 1.94849500\n",
      "Iteration 295, loss = 1.94841367\n",
      "Iteration 296, loss = 1.94833233\n",
      "Iteration 297, loss = 1.94825097\n",
      "Iteration 298, loss = 1.94816960\n",
      "Iteration 299, loss = 1.94808835\n",
      "Iteration 300, loss = 1.94800709\n",
      "Iteration 301, loss = 1.94792578\n",
      "Iteration 302, loss = 1.94784444\n",
      "Iteration 303, loss = 1.94776327\n",
      "Iteration 304, loss = 1.94768211\n",
      "Iteration 305, loss = 1.94760094\n",
      "Iteration 306, loss = 1.94751976\n",
      "Iteration 307, loss = 1.94743858\n",
      "Iteration 308, loss = 1.94735740\n",
      "Iteration 309, loss = 1.94727622\n",
      "Iteration 310, loss = 1.94719504\n",
      "Iteration 311, loss = 1.94711386\n",
      "Iteration 312, loss = 1.94703281\n",
      "Iteration 313, loss = 1.94695174\n",
      "Iteration 314, loss = 1.94687060\n",
      "Iteration 315, loss = 1.94678941\n",
      "Iteration 316, loss = 1.94670835\n",
      "Iteration 317, loss = 1.94662729\n",
      "Iteration 318, loss = 1.94654622\n",
      "Iteration 319, loss = 1.94646515\n",
      "Iteration 320, loss = 1.94638408\n",
      "Iteration 321, loss = 1.94630305\n",
      "Iteration 322, loss = 1.94622202\n",
      "Iteration 323, loss = 1.94614105\n",
      "Iteration 324, loss = 1.94606000\n",
      "Iteration 325, loss = 1.94597904\n",
      "Iteration 326, loss = 1.94589808\n",
      "Iteration 327, loss = 1.94581719\n",
      "Iteration 328, loss = 1.94573627\n",
      "Iteration 329, loss = 1.94565526\n",
      "Iteration 330, loss = 1.94557434\n",
      "Iteration 331, loss = 1.94549341\n",
      "Iteration 332, loss = 1.94541248\n",
      "Iteration 333, loss = 1.94533170\n",
      "Iteration 334, loss = 1.94525084\n",
      "Iteration 335, loss = 1.94516991\n",
      "Iteration 336, loss = 1.94508898\n",
      "Iteration 337, loss = 1.94500816\n",
      "Iteration 338, loss = 1.94492733\n",
      "Iteration 339, loss = 1.94484654\n",
      "Iteration 340, loss = 1.94476567\n",
      "Iteration 341, loss = 1.94468485\n",
      "Iteration 342, loss = 1.94460402\n",
      "Iteration 343, loss = 1.94452325\n",
      "Iteration 344, loss = 1.94444247\n",
      "Iteration 345, loss = 1.94436165\n",
      "Iteration 346, loss = 1.94428095\n",
      "Iteration 347, loss = 1.94420017\n",
      "Iteration 348, loss = 1.94411943\n",
      "Iteration 349, loss = 1.94403869\n",
      "Iteration 350, loss = 1.94395798\n",
      "Iteration 351, loss = 1.94387726\n",
      "Iteration 352, loss = 1.94379657\n",
      "Iteration 353, loss = 1.94371587\n",
      "Iteration 354, loss = 1.94363518\n",
      "Iteration 355, loss = 1.94355450\n",
      "Iteration 356, loss = 1.94347386\n",
      "Iteration 357, loss = 1.94339320\n",
      "Iteration 358, loss = 1.94331257\n",
      "Iteration 359, loss = 1.94323193\n",
      "Iteration 360, loss = 1.94315137\n",
      "Iteration 361, loss = 1.94307072\n",
      "Iteration 362, loss = 1.94299012\n",
      "Iteration 363, loss = 1.94290955\n",
      "Iteration 364, loss = 1.94282900\n",
      "Iteration 365, loss = 1.94274841\n",
      "Iteration 366, loss = 1.94266784\n",
      "Iteration 367, loss = 1.94258727\n",
      "Iteration 368, loss = 1.94250672\n",
      "Iteration 369, loss = 1.94242619\n",
      "Iteration 370, loss = 1.94234568\n",
      "Iteration 371, loss = 1.94226518\n",
      "Iteration 372, loss = 1.94218473\n",
      "Iteration 373, loss = 1.94210419\n",
      "Iteration 374, loss = 1.94202371\n",
      "Iteration 375, loss = 1.94194326\n",
      "Iteration 376, loss = 1.94186279\n",
      "Iteration 377, loss = 1.94178236\n",
      "Iteration 378, loss = 1.94170194\n",
      "Iteration 379, loss = 1.94162151\n",
      "Iteration 380, loss = 1.94154107\n",
      "Iteration 381, loss = 1.94146074\n",
      "Iteration 382, loss = 1.94138027\n",
      "Iteration 383, loss = 1.94129989\n",
      "Iteration 384, loss = 1.94121954\n",
      "Iteration 385, loss = 1.94113920\n",
      "Iteration 386, loss = 1.94105887\n",
      "Iteration 387, loss = 1.94097855\n",
      "Iteration 388, loss = 1.94089821\n",
      "Iteration 389, loss = 1.94081790\n",
      "Iteration 390, loss = 1.94073757\n",
      "Iteration 391, loss = 1.94065727\n",
      "Iteration 392, loss = 1.94057700\n",
      "Iteration 393, loss = 1.94049669\n",
      "Iteration 394, loss = 1.94041641\n",
      "Iteration 395, loss = 1.94033614\n",
      "Iteration 396, loss = 1.94025589\n",
      "Iteration 397, loss = 1.94017564\n",
      "Iteration 398, loss = 1.94009538\n",
      "Iteration 399, loss = 1.94001515\n",
      "Iteration 400, loss = 1.93993500\n",
      "Iteration 401, loss = 1.93985477\n",
      "Iteration 402, loss = 1.93977457\n",
      "Iteration 403, loss = 1.93969440\n",
      "Iteration 404, loss = 1.93961427\n",
      "Iteration 405, loss = 1.93953413\n",
      "Iteration 406, loss = 1.93945399\n",
      "Iteration 407, loss = 1.93937387\n",
      "Iteration 408, loss = 1.93929372\n",
      "Iteration 409, loss = 1.93921361\n",
      "Iteration 410, loss = 1.93913353\n",
      "Iteration 411, loss = 1.93905344\n",
      "Iteration 412, loss = 1.93897335\n",
      "Iteration 413, loss = 1.93889325\n",
      "Iteration 414, loss = 1.93881317\n",
      "Iteration 415, loss = 1.93873309\n",
      "Iteration 416, loss = 1.93865316\n",
      "Iteration 417, loss = 1.93857310\n",
      "Iteration 418, loss = 1.93849312\n",
      "Iteration 419, loss = 1.93841314\n",
      "Iteration 420, loss = 1.93833315\n",
      "Iteration 421, loss = 1.93825315\n",
      "Iteration 422, loss = 1.93817315\n",
      "Iteration 423, loss = 1.93809322\n",
      "Iteration 424, loss = 1.93801329\n",
      "Iteration 425, loss = 1.93793328\n",
      "Iteration 426, loss = 1.93785339\n",
      "Iteration 427, loss = 1.93777350\n",
      "Iteration 428, loss = 1.93769360\n",
      "Iteration 429, loss = 1.93761369\n",
      "Iteration 430, loss = 1.93753377\n",
      "Iteration 431, loss = 1.93745385\n",
      "Iteration 432, loss = 1.93737392\n",
      "Iteration 433, loss = 1.93729423\n",
      "Iteration 434, loss = 1.93721435\n",
      "Iteration 435, loss = 1.93713439\n",
      "Iteration 436, loss = 1.93705461\n",
      "Iteration 437, loss = 1.93697482\n",
      "Iteration 438, loss = 1.93689502\n",
      "Iteration 439, loss = 1.93681521\n",
      "Iteration 440, loss = 1.93673543\n",
      "Iteration 441, loss = 1.93665566\n",
      "Iteration 442, loss = 1.93657590\n",
      "Iteration 443, loss = 1.93649616\n",
      "Iteration 444, loss = 1.93641641\n",
      "Iteration 445, loss = 1.93633666\n",
      "Iteration 446, loss = 1.93625689\n",
      "Iteration 447, loss = 1.93617726\n",
      "Iteration 448, loss = 1.93609759\n",
      "Iteration 449, loss = 1.93601778\n",
      "Iteration 450, loss = 1.93593814\n",
      "Iteration 451, loss = 1.93585848\n",
      "Iteration 452, loss = 1.93577881\n",
      "Iteration 453, loss = 1.93569914\n",
      "Iteration 454, loss = 1.93561958\n",
      "Iteration 455, loss = 1.93553995\n",
      "Iteration 456, loss = 1.93546035\n",
      "Iteration 457, loss = 1.93538078\n",
      "Iteration 458, loss = 1.93530120\n",
      "Iteration 459, loss = 1.93522161\n",
      "Iteration 460, loss = 1.93514205\n",
      "Iteration 461, loss = 1.93506249\n",
      "Iteration 462, loss = 1.93498305\n",
      "Iteration 463, loss = 1.93490357\n",
      "Iteration 464, loss = 1.93482398\n",
      "Iteration 465, loss = 1.93474454\n",
      "Iteration 466, loss = 1.93466509\n",
      "Iteration 467, loss = 1.93458563\n",
      "Iteration 468, loss = 1.93450617\n",
      "Iteration 469, loss = 1.93442678\n",
      "Iteration 470, loss = 1.93434728\n",
      "Iteration 471, loss = 1.93426785\n",
      "Iteration 472, loss = 1.93418843\n",
      "Iteration 473, loss = 1.93410903\n",
      "Iteration 474, loss = 1.93402964\n",
      "Iteration 475, loss = 1.93395035\n",
      "Iteration 476, loss = 1.93387093\n",
      "Iteration 477, loss = 1.93379161\n",
      "Iteration 478, loss = 1.93371228\n",
      "Iteration 479, loss = 1.93363294\n",
      "Iteration 480, loss = 1.93355361\n",
      "Iteration 481, loss = 1.93347427\n",
      "Iteration 482, loss = 1.93339497\n",
      "Iteration 483, loss = 1.93331574\n",
      "Iteration 484, loss = 1.93323641\n",
      "Iteration 485, loss = 1.93315714\n",
      "Iteration 486, loss = 1.93307792\n",
      "Iteration 487, loss = 1.93299864\n",
      "Iteration 488, loss = 1.93291941\n",
      "Iteration 489, loss = 1.93284021\n",
      "Iteration 490, loss = 1.93276101\n",
      "Iteration 491, loss = 1.93268185\n",
      "Iteration 492, loss = 1.93260263\n",
      "Iteration 493, loss = 1.93252357\n",
      "Iteration 494, loss = 1.93244463\n",
      "Iteration 495, loss = 1.93236553\n",
      "Iteration 496, loss = 1.93228630\n",
      "Iteration 497, loss = 1.93220742\n",
      "Iteration 498, loss = 1.93212851\n",
      "Iteration 499, loss = 1.93204963\n",
      "Iteration 500, loss = 1.93197071\n",
      "Iteration 501, loss = 1.93189175\n",
      "Iteration 502, loss = 1.93181283\n",
      "Iteration 503, loss = 1.93173384\n",
      "Iteration 504, loss = 1.93165489\n",
      "Iteration 505, loss = 1.93157594\n",
      "Iteration 506, loss = 1.93149696\n",
      "Iteration 507, loss = 1.93141822\n",
      "Iteration 508, loss = 1.93133942\n",
      "Iteration 509, loss = 1.93126049\n",
      "Iteration 510, loss = 1.93118147\n",
      "Iteration 511, loss = 1.93110264\n",
      "Iteration 512, loss = 1.93102390\n",
      "Iteration 513, loss = 1.93094512\n",
      "Iteration 514, loss = 1.93086636\n",
      "Iteration 515, loss = 1.93078755\n",
      "Iteration 516, loss = 1.93070883\n",
      "Iteration 517, loss = 1.93062998\n",
      "Iteration 518, loss = 1.93055118\n",
      "Iteration 519, loss = 1.93047236\n",
      "Iteration 520, loss = 1.93039353\n",
      "Iteration 521, loss = 1.93031499\n",
      "Iteration 522, loss = 1.93023651\n",
      "Iteration 523, loss = 1.93015760\n",
      "Iteration 524, loss = 1.93007875\n",
      "Iteration 525, loss = 1.93000021\n",
      "Iteration 526, loss = 1.92992165\n",
      "Iteration 527, loss = 1.92984304\n",
      "Iteration 528, loss = 1.92976439\n",
      "Iteration 529, loss = 1.92968578\n",
      "Iteration 530, loss = 1.92960713\n",
      "Iteration 531, loss = 1.92952845\n",
      "Iteration 532, loss = 1.92944980\n",
      "Iteration 533, loss = 1.92937112\n",
      "Iteration 534, loss = 1.92929243\n",
      "Iteration 535, loss = 1.92921408\n",
      "Iteration 536, loss = 1.92913557\n",
      "Iteration 537, loss = 1.92905683\n",
      "Iteration 538, loss = 1.92897832\n",
      "Iteration 539, loss = 1.92889990\n",
      "Iteration 540, loss = 1.92882145\n",
      "Iteration 541, loss = 1.92874298\n",
      "Iteration 542, loss = 1.92866450\n",
      "Iteration 543, loss = 1.92858599\n",
      "Iteration 544, loss = 1.92850746\n",
      "Iteration 545, loss = 1.92842894\n",
      "Iteration 546, loss = 1.92835042\n",
      "Iteration 547, loss = 1.92827206\n",
      "Iteration 548, loss = 1.92819370\n",
      "Iteration 549, loss = 1.92811519\n",
      "Iteration 550, loss = 1.92803681\n",
      "Iteration 551, loss = 1.92795850\n",
      "Iteration 552, loss = 1.92788014\n",
      "Iteration 553, loss = 1.92780175\n",
      "Iteration 554, loss = 1.92772333\n",
      "Iteration 555, loss = 1.92764493\n",
      "Iteration 556, loss = 1.92756682\n",
      "Iteration 557, loss = 1.92748842\n",
      "Iteration 558, loss = 1.92741003\n",
      "Iteration 559, loss = 1.92733183\n",
      "Iteration 560, loss = 1.92725360\n",
      "Iteration 561, loss = 1.92717534\n",
      "Iteration 562, loss = 1.92709706\n",
      "Iteration 563, loss = 1.92701876\n",
      "Iteration 564, loss = 1.92694056\n",
      "Iteration 565, loss = 1.92686233\n",
      "Iteration 566, loss = 1.92678401\n",
      "Iteration 567, loss = 1.92670589\n",
      "Iteration 568, loss = 1.92662761\n",
      "Iteration 569, loss = 1.92654941\n",
      "Iteration 570, loss = 1.92647125\n",
      "Iteration 571, loss = 1.92639304\n",
      "Iteration 572, loss = 1.92631500\n",
      "Iteration 573, loss = 1.92623687\n",
      "Iteration 574, loss = 1.92615885\n",
      "Iteration 575, loss = 1.92608086\n",
      "Iteration 576, loss = 1.92600281\n",
      "Iteration 577, loss = 1.92592473\n",
      "Iteration 578, loss = 1.92584666\n",
      "Iteration 579, loss = 1.92576854\n",
      "Iteration 580, loss = 1.92569046\n",
      "Iteration 581, loss = 1.92561239\n",
      "Iteration 582, loss = 1.92553440\n",
      "Iteration 583, loss = 1.92545641\n",
      "Iteration 584, loss = 1.92537843\n",
      "Iteration 585, loss = 1.92530041\n",
      "Iteration 586, loss = 1.92522236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 587, loss = 1.92514444\n",
      "Iteration 588, loss = 1.92506643\n",
      "Iteration 589, loss = 1.92498843\n",
      "Iteration 590, loss = 1.92491061\n",
      "Iteration 591, loss = 1.92483269\n",
      "Iteration 592, loss = 1.92475476\n",
      "Iteration 593, loss = 1.92467683\n",
      "Iteration 594, loss = 1.92459898\n",
      "Iteration 595, loss = 1.92452106\n",
      "Iteration 596, loss = 1.92444320\n",
      "Iteration 597, loss = 1.92436532\n",
      "Iteration 598, loss = 1.92428750\n",
      "Iteration 599, loss = 1.92420970\n",
      "Iteration 600, loss = 1.92413187\n",
      "Iteration 601, loss = 1.92405397\n",
      "Iteration 602, loss = 1.92397622\n",
      "Iteration 603, loss = 1.92389839\n",
      "Iteration 604, loss = 1.92382046\n",
      "Iteration 605, loss = 1.92374272\n",
      "Iteration 606, loss = 1.92366497\n",
      "Iteration 607, loss = 1.92358713\n",
      "Iteration 608, loss = 1.92350928\n",
      "Iteration 609, loss = 1.92343159\n",
      "Iteration 610, loss = 1.92335371\n",
      "Iteration 611, loss = 1.92327600\n",
      "Iteration 612, loss = 1.92319823\n",
      "Iteration 613, loss = 1.92312041\n",
      "Iteration 614, loss = 1.92304306\n",
      "Iteration 615, loss = 1.92296511\n",
      "Iteration 616, loss = 1.92288727\n",
      "Iteration 617, loss = 1.92280961\n",
      "Iteration 618, loss = 1.92273187\n",
      "Iteration 619, loss = 1.92265428\n",
      "Iteration 620, loss = 1.92257656\n",
      "Iteration 621, loss = 1.92249876\n",
      "Iteration 622, loss = 1.92242114\n",
      "Iteration 623, loss = 1.92234369\n",
      "Iteration 624, loss = 1.92226594\n",
      "Iteration 625, loss = 1.92218846\n",
      "Iteration 626, loss = 1.92211088\n",
      "Iteration 627, loss = 1.92203335\n",
      "Iteration 628, loss = 1.92195577\n",
      "Iteration 629, loss = 1.92187811\n",
      "Iteration 630, loss = 1.92180071\n",
      "Iteration 631, loss = 1.92172324\n",
      "Iteration 632, loss = 1.92164569\n",
      "Iteration 633, loss = 1.92156806\n",
      "Iteration 634, loss = 1.92149041\n",
      "Iteration 635, loss = 1.92141304\n",
      "Iteration 636, loss = 1.92133553\n",
      "Iteration 637, loss = 1.92125815\n",
      "Iteration 638, loss = 1.92118071\n",
      "Iteration 639, loss = 1.92110315\n",
      "Iteration 640, loss = 1.92102577\n",
      "Iteration 641, loss = 1.92094831\n",
      "Iteration 642, loss = 1.92087107\n",
      "Iteration 643, loss = 1.92079372\n",
      "Iteration 644, loss = 1.92071632\n",
      "Iteration 645, loss = 1.92063882\n",
      "Iteration 646, loss = 1.92056152\n",
      "Iteration 647, loss = 1.92048422\n",
      "Iteration 648, loss = 1.92040679\n",
      "Iteration 649, loss = 1.92032951\n",
      "Iteration 650, loss = 1.92025224\n",
      "Iteration 651, loss = 1.92017485\n",
      "Iteration 652, loss = 1.92009752\n",
      "Iteration 653, loss = 1.92002034\n",
      "Iteration 654, loss = 1.91994292\n",
      "Iteration 655, loss = 1.91986581\n",
      "Iteration 656, loss = 1.91978880\n",
      "Iteration 657, loss = 1.91971153\n",
      "Iteration 658, loss = 1.91963439\n",
      "Iteration 659, loss = 1.91955716\n",
      "Iteration 660, loss = 1.91947986\n",
      "Iteration 661, loss = 1.91940254\n",
      "Iteration 662, loss = 1.91932543\n",
      "Iteration 663, loss = 1.91924818\n",
      "Iteration 664, loss = 1.91917091\n",
      "Iteration 665, loss = 1.91909386\n",
      "Iteration 666, loss = 1.91901677\n",
      "Iteration 667, loss = 1.91893958\n",
      "Iteration 668, loss = 1.91886243\n",
      "Iteration 669, loss = 1.91878531\n",
      "Iteration 670, loss = 1.91870835\n",
      "Iteration 671, loss = 1.91863128\n",
      "Iteration 672, loss = 1.91855416\n",
      "Iteration 673, loss = 1.91847712\n",
      "Iteration 674, loss = 1.91840001\n",
      "Iteration 675, loss = 1.91832317\n",
      "Iteration 676, loss = 1.91824611\n",
      "Iteration 677, loss = 1.91816892\n",
      "Iteration 678, loss = 1.91809195\n",
      "Iteration 679, loss = 1.91801499\n",
      "Iteration 680, loss = 1.91793794\n",
      "Iteration 681, loss = 1.91786101\n",
      "Iteration 682, loss = 1.91778412\n",
      "Iteration 683, loss = 1.91770717\n",
      "Iteration 684, loss = 1.91763025\n",
      "Iteration 685, loss = 1.91755330\n",
      "Iteration 686, loss = 1.91747632\n",
      "Iteration 687, loss = 1.91739939\n",
      "Iteration 688, loss = 1.91732264\n",
      "Iteration 689, loss = 1.91724589\n",
      "Iteration 690, loss = 1.91716881\n",
      "Iteration 691, loss = 1.91709207\n",
      "Iteration 692, loss = 1.91701530\n",
      "Iteration 693, loss = 1.91693844\n",
      "Iteration 694, loss = 1.91686153\n",
      "Iteration 695, loss = 1.91678457\n",
      "Iteration 696, loss = 1.91670787\n",
      "Iteration 697, loss = 1.91663104\n",
      "Iteration 698, loss = 1.91655435\n",
      "Iteration 699, loss = 1.91647759\n",
      "Iteration 700, loss = 1.91640074\n",
      "Iteration 701, loss = 1.91632421\n",
      "Iteration 702, loss = 1.91624741\n",
      "Iteration 703, loss = 1.91617050\n",
      "Iteration 704, loss = 1.91609382\n",
      "Iteration 705, loss = 1.91601725\n",
      "Iteration 706, loss = 1.91594059\n",
      "Iteration 707, loss = 1.91586376\n",
      "Iteration 708, loss = 1.91578711\n",
      "Iteration 709, loss = 1.91571040\n",
      "Iteration 710, loss = 1.91563390\n",
      "Iteration 711, loss = 1.91555722\n",
      "Iteration 712, loss = 1.91548049\n",
      "Iteration 713, loss = 1.91540397\n",
      "Iteration 714, loss = 1.91532743\n",
      "Iteration 715, loss = 1.91525082\n",
      "Iteration 716, loss = 1.91517424\n",
      "Iteration 717, loss = 1.91509772\n",
      "Iteration 718, loss = 1.91502101\n",
      "Iteration 719, loss = 1.91494441\n",
      "Iteration 720, loss = 1.91486811\n",
      "Iteration 721, loss = 1.91479158\n",
      "Iteration 722, loss = 1.91471502\n",
      "Iteration 723, loss = 1.91463845\n",
      "Iteration 724, loss = 1.91456199\n",
      "Iteration 725, loss = 1.91448545\n",
      "Iteration 726, loss = 1.91440890\n",
      "Iteration 727, loss = 1.91433242\n",
      "Iteration 728, loss = 1.91425597\n",
      "Iteration 729, loss = 1.91417949\n",
      "Iteration 730, loss = 1.91410316\n",
      "Iteration 731, loss = 1.91402673\n",
      "Iteration 732, loss = 1.91395031\n",
      "Iteration 733, loss = 1.91387393\n",
      "Iteration 734, loss = 1.91379754\n",
      "Iteration 735, loss = 1.91372122\n",
      "Iteration 736, loss = 1.91364484\n",
      "Iteration 737, loss = 1.91356853\n",
      "Iteration 738, loss = 1.91349225\n",
      "Iteration 739, loss = 1.91341577\n",
      "Iteration 740, loss = 1.91333958\n",
      "Iteration 741, loss = 1.91326339\n",
      "Iteration 742, loss = 1.91318710\n",
      "Iteration 743, loss = 1.91311074\n",
      "Iteration 744, loss = 1.91303449\n",
      "Iteration 745, loss = 1.91295814\n",
      "Iteration 746, loss = 1.91288185\n",
      "Iteration 747, loss = 1.91280560\n",
      "Iteration 748, loss = 1.91272934\n",
      "Iteration 749, loss = 1.91265311\n",
      "Iteration 750, loss = 1.91257689\n",
      "Iteration 751, loss = 1.91250068\n",
      "Iteration 752, loss = 1.91242458\n",
      "Iteration 753, loss = 1.91234835\n",
      "Iteration 754, loss = 1.91227225\n",
      "Iteration 755, loss = 1.91219615\n",
      "Iteration 756, loss = 1.91212006\n",
      "Iteration 757, loss = 1.91204391\n",
      "Iteration 758, loss = 1.91196764\n",
      "Iteration 759, loss = 1.91189158\n",
      "Iteration 760, loss = 1.91181548\n",
      "Iteration 761, loss = 1.91173931\n",
      "Iteration 762, loss = 1.91166328\n",
      "Iteration 763, loss = 1.91158718\n",
      "Iteration 764, loss = 1.91151110\n",
      "Iteration 765, loss = 1.91143510\n",
      "Iteration 766, loss = 1.91135902\n",
      "Iteration 767, loss = 1.91128299\n",
      "Iteration 768, loss = 1.91120702\n",
      "Iteration 769, loss = 1.91113116\n",
      "Iteration 770, loss = 1.91105527\n",
      "Iteration 771, loss = 1.91097929\n",
      "Iteration 772, loss = 1.91090320\n",
      "Iteration 773, loss = 1.91082735\n",
      "Iteration 774, loss = 1.91075154\n",
      "Iteration 775, loss = 1.91067539\n",
      "Iteration 776, loss = 1.91059950\n",
      "Iteration 777, loss = 1.91052378\n",
      "Iteration 778, loss = 1.91044793\n",
      "Iteration 779, loss = 1.91037199\n",
      "Iteration 780, loss = 1.91029610\n",
      "Iteration 781, loss = 1.91022022\n",
      "Iteration 782, loss = 1.91014447\n",
      "Iteration 783, loss = 1.91006842\n",
      "Iteration 784, loss = 1.90999259\n",
      "Iteration 785, loss = 1.90991690\n",
      "Iteration 786, loss = 1.90984110\n",
      "Iteration 787, loss = 1.90976517\n",
      "Iteration 788, loss = 1.90968946\n",
      "Iteration 789, loss = 1.90961390\n",
      "Iteration 790, loss = 1.90953800\n",
      "Iteration 791, loss = 1.90946192\n",
      "Iteration 792, loss = 1.90938626\n",
      "Iteration 793, loss = 1.90931051\n",
      "Iteration 794, loss = 1.90923479\n",
      "Iteration 795, loss = 1.90915903\n",
      "Iteration 796, loss = 1.90908319\n",
      "Iteration 797, loss = 1.90900768\n",
      "Iteration 798, loss = 1.90893192\n",
      "Iteration 799, loss = 1.90885624\n",
      "Iteration 800, loss = 1.90878060\n",
      "Iteration 801, loss = 1.90870498\n",
      "Iteration 802, loss = 1.90862935\n",
      "Iteration 803, loss = 1.90855362\n",
      "Iteration 804, loss = 1.90847802\n",
      "Iteration 805, loss = 1.90840234\n",
      "Iteration 806, loss = 1.90832697\n",
      "Iteration 807, loss = 1.90825118\n",
      "Iteration 808, loss = 1.90817563\n",
      "Iteration 809, loss = 1.90810003\n",
      "Iteration 810, loss = 1.90802478\n",
      "Iteration 811, loss = 1.90794904\n",
      "Iteration 812, loss = 1.90787367\n",
      "Iteration 813, loss = 1.90779830\n",
      "Iteration 814, loss = 1.90772282\n",
      "Iteration 815, loss = 1.90764721\n",
      "Iteration 816, loss = 1.90757151\n",
      "Iteration 817, loss = 1.90749602\n",
      "Iteration 818, loss = 1.90742078\n",
      "Iteration 819, loss = 1.90734502\n",
      "Iteration 820, loss = 1.90726969\n",
      "Iteration 821, loss = 1.90719436\n",
      "Iteration 822, loss = 1.90711892\n",
      "Iteration 823, loss = 1.90704345\n",
      "Iteration 824, loss = 1.90696796\n",
      "Iteration 825, loss = 1.90689258\n",
      "Iteration 826, loss = 1.90681736\n",
      "Iteration 827, loss = 1.90674189\n",
      "Iteration 828, loss = 1.90666651\n",
      "Iteration 829, loss = 1.90659132\n",
      "Iteration 830, loss = 1.90651608\n",
      "Iteration 831, loss = 1.90644073\n",
      "Iteration 832, loss = 1.90636528\n",
      "Iteration 833, loss = 1.90628996\n",
      "Iteration 834, loss = 1.90621469\n",
      "Iteration 835, loss = 1.90613925\n",
      "Iteration 836, loss = 1.90606409\n",
      "Iteration 837, loss = 1.90598886\n",
      "Iteration 838, loss = 1.90591365\n",
      "Iteration 839, loss = 1.90583837\n",
      "Iteration 840, loss = 1.90576312\n",
      "Iteration 841, loss = 1.90568776\n",
      "Iteration 842, loss = 1.90561256\n",
      "Iteration 843, loss = 1.90553742\n",
      "Iteration 844, loss = 1.90546228\n",
      "Iteration 845, loss = 1.90538717\n",
      "Iteration 846, loss = 1.90531202\n",
      "Iteration 847, loss = 1.90523696\n",
      "Iteration 848, loss = 1.90516180\n",
      "Iteration 849, loss = 1.90508654\n",
      "Iteration 850, loss = 1.90501138\n",
      "Iteration 851, loss = 1.90493643\n",
      "Iteration 852, loss = 1.90486130\n",
      "Iteration 853, loss = 1.90478600\n",
      "Iteration 854, loss = 1.90471085\n",
      "Iteration 855, loss = 1.90463624\n",
      "Iteration 856, loss = 1.90456120\n",
      "Iteration 857, loss = 1.90448581\n",
      "Iteration 858, loss = 1.90441096\n",
      "Iteration 859, loss = 1.90433601\n",
      "Iteration 860, loss = 1.90426092\n",
      "Iteration 861, loss = 1.90418577\n",
      "Iteration 862, loss = 1.90411086\n",
      "Iteration 863, loss = 1.90403572\n",
      "Iteration 864, loss = 1.90396089\n",
      "Iteration 865, loss = 1.90388597\n",
      "Iteration 866, loss = 1.90381093\n",
      "Iteration 867, loss = 1.90373581\n",
      "Iteration 868, loss = 1.90366095\n",
      "Iteration 869, loss = 1.90358587\n",
      "Iteration 870, loss = 1.90351100\n",
      "Iteration 871, loss = 1.90343614\n",
      "Iteration 872, loss = 1.90336125\n",
      "Iteration 873, loss = 1.90328624\n",
      "Iteration 874, loss = 1.90321163\n",
      "Iteration 875, loss = 1.90313672\n",
      "Iteration 876, loss = 1.90306166\n",
      "Iteration 877, loss = 1.90298694\n",
      "Iteration 878, loss = 1.90291209\n",
      "Iteration 879, loss = 1.90283719\n",
      "Iteration 880, loss = 1.90276232\n",
      "Iteration 881, loss = 1.90268751\n",
      "Iteration 882, loss = 1.90261263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 883, loss = 1.90253777\n",
      "Iteration 884, loss = 1.90246302\n",
      "Iteration 885, loss = 1.90238825\n",
      "Iteration 886, loss = 1.90231354\n",
      "Iteration 887, loss = 1.90223875\n",
      "Iteration 888, loss = 1.90216418\n",
      "Iteration 889, loss = 1.90208959\n",
      "Iteration 890, loss = 1.90201461\n",
      "Iteration 891, loss = 1.90194009\n",
      "Iteration 892, loss = 1.90186548\n",
      "Iteration 893, loss = 1.90179071\n",
      "Iteration 894, loss = 1.90171582\n",
      "Iteration 895, loss = 1.90164111\n",
      "Iteration 896, loss = 1.90156662\n",
      "Iteration 897, loss = 1.90149186\n",
      "Iteration 898, loss = 1.90141722\n",
      "Iteration 899, loss = 1.90134265\n",
      "Iteration 900, loss = 1.90126797\n",
      "Iteration 901, loss = 1.90119321\n",
      "Iteration 902, loss = 1.90111857\n",
      "Iteration 903, loss = 1.90104410\n",
      "Iteration 904, loss = 1.90096939\n",
      "Iteration 905, loss = 1.90089488\n",
      "Iteration 906, loss = 1.90082024\n",
      "Iteration 907, loss = 1.90074590\n",
      "Iteration 908, loss = 1.90067122\n",
      "Iteration 909, loss = 1.90059660\n",
      "Iteration 910, loss = 1.90052228\n",
      "Iteration 911, loss = 1.90044787\n",
      "Iteration 912, loss = 1.90037336\n",
      "Iteration 913, loss = 1.90029871\n",
      "Iteration 914, loss = 1.90022419\n",
      "Iteration 915, loss = 1.90014974\n",
      "Iteration 916, loss = 1.90007538\n",
      "Iteration 917, loss = 1.90000083\n",
      "Iteration 918, loss = 1.89992653\n",
      "Iteration 919, loss = 1.89985226\n",
      "Iteration 920, loss = 1.89977777\n",
      "Iteration 921, loss = 1.89970320\n",
      "Iteration 922, loss = 1.89962902\n",
      "Iteration 923, loss = 1.89955471\n",
      "Iteration 924, loss = 1.89948029\n",
      "Iteration 925, loss = 1.89940569\n",
      "Iteration 926, loss = 1.89933126\n",
      "Iteration 927, loss = 1.89925698\n",
      "Iteration 928, loss = 1.89918261\n",
      "Iteration 929, loss = 1.89910843\n",
      "Iteration 930, loss = 1.89903418\n",
      "Iteration 931, loss = 1.89895978\n",
      "Iteration 932, loss = 1.89888530\n",
      "Iteration 933, loss = 1.89881100\n",
      "Iteration 934, loss = 1.89873682\n",
      "Iteration 935, loss = 1.89866257\n",
      "Iteration 936, loss = 1.89858830\n",
      "Iteration 937, loss = 1.89851401\n",
      "Iteration 938, loss = 1.89843995\n",
      "Iteration 939, loss = 1.89836560\n",
      "Iteration 940, loss = 1.89829145\n",
      "Iteration 941, loss = 1.89821732\n",
      "Iteration 942, loss = 1.89814305\n",
      "Iteration 943, loss = 1.89806893\n",
      "Iteration 944, loss = 1.89799491\n",
      "Iteration 945, loss = 1.89792051\n",
      "Iteration 946, loss = 1.89784635\n",
      "Iteration 947, loss = 1.89777237\n",
      "Iteration 948, loss = 1.89769831\n",
      "Iteration 949, loss = 1.89762411\n",
      "Iteration 950, loss = 1.89754987\n",
      "Iteration 951, loss = 1.89747589\n",
      "Iteration 952, loss = 1.89740190\n",
      "Iteration 953, loss = 1.89732779\n",
      "Iteration 954, loss = 1.89725365\n",
      "Iteration 955, loss = 1.89717951\n",
      "Iteration 956, loss = 1.89710557\n",
      "Iteration 957, loss = 1.89703150\n",
      "Iteration 958, loss = 1.89695757\n",
      "Iteration 959, loss = 1.89688354\n",
      "Iteration 960, loss = 1.89680939\n",
      "Iteration 961, loss = 1.89673527\n",
      "Iteration 962, loss = 1.89666147\n",
      "Iteration 963, loss = 1.89658755\n",
      "Iteration 964, loss = 1.89651350\n",
      "Iteration 965, loss = 1.89643970\n",
      "Iteration 966, loss = 1.89636579\n",
      "Iteration 967, loss = 1.89629172\n",
      "Iteration 968, loss = 1.89621766\n",
      "Iteration 969, loss = 1.89614386\n",
      "Iteration 970, loss = 1.89606993\n",
      "Iteration 971, loss = 1.89599612\n",
      "Iteration 972, loss = 1.89592224\n",
      "Iteration 973, loss = 1.89584821\n",
      "Iteration 974, loss = 1.89577440\n",
      "Iteration 975, loss = 1.89570050\n",
      "Iteration 976, loss = 1.89562684\n",
      "Iteration 977, loss = 1.89555306\n",
      "Iteration 978, loss = 1.89547916\n",
      "Iteration 979, loss = 1.89540532\n",
      "Iteration 980, loss = 1.89533154\n",
      "Iteration 981, loss = 1.89525759\n",
      "Iteration 982, loss = 1.89518397\n",
      "Iteration 983, loss = 1.89511017\n",
      "Iteration 984, loss = 1.89503625\n",
      "Iteration 985, loss = 1.89496254\n",
      "Iteration 986, loss = 1.89488886\n",
      "Iteration 987, loss = 1.89481515\n",
      "Iteration 988, loss = 1.89474144\n",
      "Iteration 989, loss = 1.89466791\n",
      "Iteration 990, loss = 1.89459433\n",
      "Iteration 991, loss = 1.89452064\n",
      "Iteration 992, loss = 1.89444686\n",
      "Iteration 993, loss = 1.89437324\n",
      "Iteration 994, loss = 1.89429961\n",
      "Iteration 995, loss = 1.89422597\n",
      "Iteration 996, loss = 1.89415236\n",
      "Iteration 997, loss = 1.89407864\n",
      "Iteration 998, loss = 1.89400506\n",
      "Iteration 999, loss = 1.89393146\n",
      "Iteration 1000, loss = 1.89385787\n",
      "Iteration 1001, loss = 1.89378439\n",
      "Iteration 1002, loss = 1.89371083\n",
      "Iteration 1003, loss = 1.89363725\n",
      "Iteration 1004, loss = 1.89356377\n",
      "Iteration 1005, loss = 1.89349019\n",
      "Iteration 1006, loss = 1.89341676\n",
      "Iteration 1007, loss = 1.89334316\n",
      "Iteration 1008, loss = 1.89326986\n",
      "Iteration 1009, loss = 1.89319647\n",
      "Iteration 1010, loss = 1.89312291\n",
      "Iteration 1011, loss = 1.89304938\n",
      "Iteration 1012, loss = 1.89297620\n",
      "Iteration 1013, loss = 1.89290249\n",
      "Iteration 1014, loss = 1.89282899\n",
      "Iteration 1015, loss = 1.89275560\n",
      "Iteration 1016, loss = 1.89268226\n",
      "Iteration 1017, loss = 1.89260876\n",
      "Iteration 1018, loss = 1.89253549\n",
      "Iteration 1019, loss = 1.89246208\n",
      "Iteration 1020, loss = 1.89238867\n",
      "Iteration 1021, loss = 1.89231534\n",
      "Iteration 1022, loss = 1.89224190\n",
      "Iteration 1023, loss = 1.89216860\n",
      "Iteration 1024, loss = 1.89209525\n",
      "Iteration 1025, loss = 1.89202199\n",
      "Iteration 1026, loss = 1.89194864\n",
      "Iteration 1027, loss = 1.89187542\n",
      "Iteration 1028, loss = 1.89180209\n",
      "Iteration 1029, loss = 1.89172880\n",
      "Iteration 1030, loss = 1.89165557\n",
      "Iteration 1031, loss = 1.89158223\n",
      "Iteration 1032, loss = 1.89150906\n",
      "Iteration 1033, loss = 1.89143584\n",
      "Iteration 1034, loss = 1.89136258\n",
      "Iteration 1035, loss = 1.89128944\n",
      "Iteration 1036, loss = 1.89121615\n",
      "Iteration 1037, loss = 1.89114303\n",
      "Iteration 1038, loss = 1.89106968\n",
      "Iteration 1039, loss = 1.89099666\n",
      "Iteration 1040, loss = 1.89092364\n",
      "Iteration 1041, loss = 1.89085048\n",
      "Iteration 1042, loss = 1.89077728\n",
      "Iteration 1043, loss = 1.89070397\n",
      "Iteration 1044, loss = 1.89063094\n",
      "Iteration 1045, loss = 1.89055791\n",
      "Iteration 1046, loss = 1.89048485\n",
      "Iteration 1047, loss = 1.89041140\n",
      "Iteration 1048, loss = 1.89033850\n",
      "Iteration 1049, loss = 1.89026562\n",
      "Iteration 1050, loss = 1.89019259\n",
      "Iteration 1051, loss = 1.89011950\n",
      "Iteration 1052, loss = 1.89004645\n",
      "Iteration 1053, loss = 1.88997332\n",
      "Iteration 1054, loss = 1.88990012\n",
      "Iteration 1055, loss = 1.88982705\n",
      "Iteration 1056, loss = 1.88975415\n",
      "Iteration 1057, loss = 1.88968114\n",
      "Iteration 1058, loss = 1.88960818\n",
      "Iteration 1059, loss = 1.88953528\n",
      "Iteration 1060, loss = 1.88946222\n",
      "Iteration 1061, loss = 1.88938940\n",
      "Iteration 1062, loss = 1.88931641\n",
      "Iteration 1063, loss = 1.88924329\n",
      "Iteration 1064, loss = 1.88917054\n",
      "Iteration 1065, loss = 1.88909765\n",
      "Iteration 1066, loss = 1.88902470\n",
      "Iteration 1067, loss = 1.88895196\n",
      "Iteration 1068, loss = 1.88887896\n",
      "Iteration 1069, loss = 1.88880588\n",
      "Iteration 1070, loss = 1.88873294\n",
      "Iteration 1071, loss = 1.88866037\n",
      "Iteration 1072, loss = 1.88858750\n",
      "Iteration 1073, loss = 1.88851463\n",
      "Iteration 1074, loss = 1.88844199\n",
      "Iteration 1075, loss = 1.88836916\n",
      "Iteration 1076, loss = 1.88829628\n",
      "Iteration 1077, loss = 1.88822356\n",
      "Iteration 1078, loss = 1.88815067\n",
      "Iteration 1079, loss = 1.88807812\n",
      "Iteration 1080, loss = 1.88800543\n",
      "Iteration 1081, loss = 1.88793255\n",
      "Iteration 1082, loss = 1.88785997\n",
      "Iteration 1083, loss = 1.88778742\n",
      "Iteration 1084, loss = 1.88771444\n",
      "Iteration 1085, loss = 1.88764206\n",
      "Iteration 1086, loss = 1.88756960\n",
      "Iteration 1087, loss = 1.88749692\n",
      "Iteration 1088, loss = 1.88742417\n",
      "Iteration 1089, loss = 1.88735158\n",
      "Iteration 1090, loss = 1.88727883\n",
      "Iteration 1091, loss = 1.88720617\n",
      "Iteration 1092, loss = 1.88713323\n",
      "Iteration 1093, loss = 1.88706082\n",
      "Iteration 1094, loss = 1.88698834\n",
      "Iteration 1095, loss = 1.88691570\n",
      "Iteration 1096, loss = 1.88684326\n",
      "Iteration 1097, loss = 1.88677067\n",
      "Iteration 1098, loss = 1.88669805\n",
      "Iteration 1099, loss = 1.88662557\n",
      "Iteration 1100, loss = 1.88655297\n",
      "Iteration 1101, loss = 1.88648046\n",
      "Iteration 1102, loss = 1.88640787\n",
      "Iteration 1103, loss = 1.88633530\n",
      "Iteration 1104, loss = 1.88626273\n",
      "Iteration 1105, loss = 1.88619034\n",
      "Iteration 1106, loss = 1.88611793\n",
      "Iteration 1107, loss = 1.88604555\n",
      "Iteration 1108, loss = 1.88597309\n",
      "Iteration 1109, loss = 1.88590070\n",
      "Iteration 1110, loss = 1.88582820\n",
      "Iteration 1111, loss = 1.88575576\n",
      "Iteration 1112, loss = 1.88568332\n",
      "Iteration 1113, loss = 1.88561097\n",
      "Iteration 1114, loss = 1.88553865\n",
      "Iteration 1115, loss = 1.88546624\n",
      "Iteration 1116, loss = 1.88539367\n",
      "Iteration 1117, loss = 1.88532137\n",
      "Iteration 1118, loss = 1.88524893\n",
      "Iteration 1119, loss = 1.88517685\n",
      "Iteration 1120, loss = 1.88510431\n",
      "Iteration 1121, loss = 1.88503204\n",
      "Iteration 1122, loss = 1.88495974\n",
      "Iteration 1123, loss = 1.88488733\n",
      "Iteration 1124, loss = 1.88481514\n",
      "Iteration 1125, loss = 1.88474303\n",
      "Iteration 1126, loss = 1.88467075\n",
      "Iteration 1127, loss = 1.88459838\n",
      "Iteration 1128, loss = 1.88452607\n",
      "Iteration 1129, loss = 1.88445388\n",
      "Iteration 1130, loss = 1.88438157\n",
      "Iteration 1131, loss = 1.88430950\n",
      "Iteration 1132, loss = 1.88423713\n",
      "Iteration 1133, loss = 1.88416500\n",
      "Iteration 1134, loss = 1.88409282\n",
      "Iteration 1135, loss = 1.88402058\n",
      "Iteration 1136, loss = 1.88394858\n",
      "Iteration 1137, loss = 1.88387635\n",
      "Iteration 1138, loss = 1.88380420\n",
      "Iteration 1139, loss = 1.88373210\n",
      "Iteration 1140, loss = 1.88365985\n",
      "Iteration 1141, loss = 1.88358781\n",
      "Iteration 1142, loss = 1.88351575\n",
      "Iteration 1143, loss = 1.88344366\n",
      "Iteration 1144, loss = 1.88337154\n",
      "Iteration 1145, loss = 1.88329953\n",
      "Iteration 1146, loss = 1.88322730\n",
      "Iteration 1147, loss = 1.88315514\n",
      "Iteration 1148, loss = 1.88308317\n",
      "Iteration 1149, loss = 1.88301119\n",
      "Iteration 1150, loss = 1.88293921\n",
      "Iteration 1151, loss = 1.88286712\n",
      "Iteration 1152, loss = 1.88279518\n",
      "Iteration 1153, loss = 1.88272306\n",
      "Iteration 1154, loss = 1.88265118\n",
      "Iteration 1155, loss = 1.88257915\n",
      "Iteration 1156, loss = 1.88250713\n",
      "Iteration 1157, loss = 1.88243538\n",
      "Iteration 1158, loss = 1.88236307\n",
      "Iteration 1159, loss = 1.88229131\n",
      "Iteration 1160, loss = 1.88221957\n",
      "Iteration 1161, loss = 1.88214787\n",
      "Iteration 1162, loss = 1.88207601\n",
      "Iteration 1163, loss = 1.88200399\n",
      "Iteration 1164, loss = 1.88193182\n",
      "Iteration 1165, loss = 1.88185960\n",
      "Iteration 1166, loss = 1.88178780\n",
      "Iteration 1167, loss = 1.88171600\n",
      "Iteration 1168, loss = 1.88164416\n",
      "Iteration 1169, loss = 1.88157226\n",
      "Iteration 1170, loss = 1.88150040\n",
      "Iteration 1171, loss = 1.88142847\n",
      "Iteration 1172, loss = 1.88135664\n",
      "Iteration 1173, loss = 1.88128481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1174, loss = 1.88121302\n",
      "Iteration 1175, loss = 1.88114118\n",
      "Iteration 1176, loss = 1.88106942\n",
      "Iteration 1177, loss = 1.88099768\n",
      "Iteration 1178, loss = 1.88092595\n",
      "Iteration 1179, loss = 1.88085414\n",
      "Iteration 1180, loss = 1.88078249\n",
      "Iteration 1181, loss = 1.88071067\n",
      "Iteration 1182, loss = 1.88063908\n",
      "Iteration 1183, loss = 1.88056733\n",
      "Iteration 1184, loss = 1.88049552\n",
      "Iteration 1185, loss = 1.88042391\n",
      "Iteration 1186, loss = 1.88035228\n",
      "Iteration 1187, loss = 1.88028085\n",
      "Iteration 1188, loss = 1.88020883\n",
      "Iteration 1189, loss = 1.88013730\n",
      "Iteration 1190, loss = 1.88006557\n",
      "Iteration 1191, loss = 1.87999387\n",
      "Iteration 1192, loss = 1.87992240\n",
      "Iteration 1193, loss = 1.87985053\n",
      "Iteration 1194, loss = 1.87977896\n",
      "Iteration 1195, loss = 1.87970728\n",
      "Iteration 1196, loss = 1.87963591\n",
      "Iteration 1197, loss = 1.87956421\n",
      "Iteration 1198, loss = 1.87949261\n",
      "Iteration 1199, loss = 1.87942111\n",
      "Iteration 1200, loss = 1.87934951\n",
      "Iteration 1201, loss = 1.87927802\n",
      "Iteration 1202, loss = 1.87920642\n",
      "Iteration 1203, loss = 1.87913503\n",
      "Iteration 1204, loss = 1.87906334\n",
      "Iteration 1205, loss = 1.87899201\n",
      "Iteration 1206, loss = 1.87892065\n",
      "Iteration 1207, loss = 1.87884906\n",
      "Iteration 1208, loss = 1.87877761\n",
      "Iteration 1209, loss = 1.87870605\n",
      "Iteration 1210, loss = 1.87863444\n",
      "Iteration 1211, loss = 1.87856305\n",
      "Iteration 1212, loss = 1.87849152\n",
      "Iteration 1213, loss = 1.87842016\n",
      "Iteration 1214, loss = 1.87834891\n",
      "Iteration 1215, loss = 1.87827726\n",
      "Iteration 1216, loss = 1.87820589\n",
      "Iteration 1217, loss = 1.87813451\n",
      "Iteration 1218, loss = 1.87806322\n",
      "Iteration 1219, loss = 1.87799184\n",
      "Iteration 1220, loss = 1.87792046\n",
      "Iteration 1221, loss = 1.87784915\n",
      "Iteration 1222, loss = 1.87777770\n",
      "Iteration 1223, loss = 1.87770641\n",
      "Iteration 1224, loss = 1.87763484\n",
      "Iteration 1225, loss = 1.87756367\n",
      "Iteration 1226, loss = 1.87749241\n",
      "Iteration 1227, loss = 1.87742107\n",
      "Iteration 1228, loss = 1.87734957\n",
      "Iteration 1229, loss = 1.87727835\n",
      "Iteration 1230, loss = 1.87720699\n",
      "Iteration 1231, loss = 1.87713591\n",
      "Iteration 1232, loss = 1.87706470\n",
      "Iteration 1233, loss = 1.87699359\n",
      "Iteration 1234, loss = 1.87692229\n",
      "Iteration 1235, loss = 1.87685079\n",
      "Iteration 1236, loss = 1.87677967\n",
      "Iteration 1237, loss = 1.87670852\n",
      "Iteration 1238, loss = 1.87663733\n",
      "Iteration 1239, loss = 1.87656594\n",
      "Iteration 1240, loss = 1.87649488\n",
      "Iteration 1241, loss = 1.87642366\n",
      "Iteration 1242, loss = 1.87635240\n",
      "Iteration 1243, loss = 1.87628122\n",
      "Iteration 1244, loss = 1.87621009\n",
      "Iteration 1245, loss = 1.87613901\n",
      "Iteration 1246, loss = 1.87606783\n",
      "Iteration 1247, loss = 1.87599682\n",
      "Iteration 1248, loss = 1.87592572\n",
      "Iteration 1249, loss = 1.87585452\n",
      "Iteration 1250, loss = 1.87578355\n",
      "Iteration 1251, loss = 1.87571243\n",
      "Iteration 1252, loss = 1.87564138\n",
      "Iteration 1253, loss = 1.87557034\n",
      "Iteration 1254, loss = 1.87549924\n",
      "Iteration 1255, loss = 1.87542815\n",
      "Iteration 1256, loss = 1.87535718\n",
      "Iteration 1257, loss = 1.87528627\n",
      "Iteration 1258, loss = 1.87521537\n",
      "Iteration 1259, loss = 1.87514433\n",
      "Iteration 1260, loss = 1.87507314\n",
      "Iteration 1261, loss = 1.87500225\n",
      "Iteration 1262, loss = 1.87493109\n",
      "Iteration 1263, loss = 1.87486021\n",
      "Iteration 1264, loss = 1.87478939\n",
      "Iteration 1265, loss = 1.87471833\n",
      "Iteration 1266, loss = 1.87464742\n",
      "Iteration 1267, loss = 1.87457651\n",
      "Iteration 1268, loss = 1.87450550\n",
      "Iteration 1269, loss = 1.87443455\n",
      "Iteration 1270, loss = 1.87436353\n",
      "Iteration 1271, loss = 1.87429262\n",
      "Iteration 1272, loss = 1.87422181\n",
      "Iteration 1273, loss = 1.87415097\n",
      "Iteration 1274, loss = 1.87408009\n",
      "Iteration 1275, loss = 1.87400933\n",
      "Iteration 1276, loss = 1.87393844\n",
      "Iteration 1277, loss = 1.87386741\n",
      "Iteration 1278, loss = 1.87379672\n",
      "Iteration 1279, loss = 1.87372589\n",
      "Iteration 1280, loss = 1.87365501\n",
      "Iteration 1281, loss = 1.87358429\n",
      "Iteration 1282, loss = 1.87351345\n",
      "Iteration 1283, loss = 1.87344287\n",
      "Iteration 1284, loss = 1.87337195\n",
      "Iteration 1285, loss = 1.87330123\n",
      "Iteration 1286, loss = 1.87323045\n",
      "Iteration 1287, loss = 1.87315964\n",
      "Iteration 1288, loss = 1.87308887\n",
      "Iteration 1289, loss = 1.87301811\n",
      "Iteration 1290, loss = 1.87294726\n",
      "Iteration 1291, loss = 1.87287664\n",
      "Iteration 1292, loss = 1.87280600\n",
      "Iteration 1293, loss = 1.87273537\n",
      "Iteration 1294, loss = 1.87266468\n",
      "Iteration 1295, loss = 1.87259393\n",
      "Iteration 1296, loss = 1.87252334\n",
      "Iteration 1297, loss = 1.87245276\n",
      "Iteration 1298, loss = 1.87238214\n",
      "Iteration 1299, loss = 1.87231131\n",
      "Iteration 1300, loss = 1.87224075\n",
      "Iteration 1301, loss = 1.87217018\n",
      "Iteration 1302, loss = 1.87209971\n",
      "Iteration 1303, loss = 1.87202906\n",
      "Iteration 1304, loss = 1.87195845\n",
      "Iteration 1305, loss = 1.87188790\n",
      "Iteration 1306, loss = 1.87181716\n",
      "Iteration 1307, loss = 1.87174680\n",
      "Iteration 1308, loss = 1.87167634\n",
      "Iteration 1309, loss = 1.87160565\n",
      "Iteration 1310, loss = 1.87153547\n",
      "Iteration 1311, loss = 1.87146514\n",
      "Iteration 1312, loss = 1.87139433\n",
      "Iteration 1313, loss = 1.87132365\n",
      "Iteration 1314, loss = 1.87125344\n",
      "Iteration 1315, loss = 1.87118300\n",
      "Iteration 1316, loss = 1.87111239\n",
      "Iteration 1317, loss = 1.87104186\n",
      "Iteration 1318, loss = 1.87097143\n",
      "Iteration 1319, loss = 1.87090097\n",
      "Iteration 1320, loss = 1.87083048\n",
      "Iteration 1321, loss = 1.87076002\n",
      "Iteration 1322, loss = 1.87068973\n",
      "Iteration 1323, loss = 1.87061939\n",
      "Iteration 1324, loss = 1.87054893\n",
      "Iteration 1325, loss = 1.87047847\n",
      "Iteration 1326, loss = 1.87040831\n",
      "Iteration 1327, loss = 1.87033762\n",
      "Iteration 1328, loss = 1.87026739\n",
      "Iteration 1329, loss = 1.87019701\n",
      "Iteration 1330, loss = 1.87012682\n",
      "Iteration 1331, loss = 1.87005656\n",
      "Iteration 1332, loss = 1.86998626\n",
      "Iteration 1333, loss = 1.86991594\n",
      "Iteration 1334, loss = 1.86984562\n",
      "Iteration 1335, loss = 1.86977527\n",
      "Iteration 1336, loss = 1.86970491\n",
      "Iteration 1337, loss = 1.86963463\n",
      "Iteration 1338, loss = 1.86956445\n",
      "Iteration 1339, loss = 1.86949446\n",
      "Iteration 1340, loss = 1.86942389\n",
      "Iteration 1341, loss = 1.86935386\n",
      "Iteration 1342, loss = 1.86928380\n",
      "Iteration 1343, loss = 1.86921349\n",
      "Iteration 1344, loss = 1.86914295\n",
      "Iteration 1345, loss = 1.86907331\n",
      "Iteration 1346, loss = 1.86900349\n",
      "Iteration 1347, loss = 1.86893280\n",
      "Iteration 1348, loss = 1.86886242\n",
      "Iteration 1349, loss = 1.86879256\n",
      "Iteration 1350, loss = 1.86872242\n",
      "Iteration 1351, loss = 1.86865206\n",
      "Iteration 1352, loss = 1.86858198\n",
      "Iteration 1353, loss = 1.86851187\n",
      "Iteration 1354, loss = 1.86844147\n",
      "Iteration 1355, loss = 1.86837135\n",
      "Iteration 1356, loss = 1.86830144\n",
      "Iteration 1357, loss = 1.86823158\n",
      "Iteration 1358, loss = 1.86816146\n",
      "Iteration 1359, loss = 1.86809132\n",
      "Iteration 1360, loss = 1.86802149\n",
      "Iteration 1361, loss = 1.86795134\n",
      "Iteration 1362, loss = 1.86788100\n",
      "Iteration 1363, loss = 1.86781099\n",
      "Iteration 1364, loss = 1.86774073\n",
      "Iteration 1365, loss = 1.86767096\n",
      "Iteration 1366, loss = 1.86760063\n",
      "Iteration 1367, loss = 1.86753095\n",
      "Iteration 1368, loss = 1.86746116\n",
      "Iteration 1369, loss = 1.86739114\n",
      "Iteration 1370, loss = 1.86732093\n",
      "Iteration 1371, loss = 1.86725098\n",
      "Iteration 1372, loss = 1.86718088\n",
      "Iteration 1373, loss = 1.86711095\n",
      "Iteration 1374, loss = 1.86704112\n",
      "Iteration 1375, loss = 1.86697111\n",
      "Iteration 1376, loss = 1.86690137\n",
      "Iteration 1377, loss = 1.86683122\n",
      "Iteration 1378, loss = 1.86676126\n",
      "Iteration 1379, loss = 1.86669156\n",
      "Iteration 1380, loss = 1.86662144\n",
      "Iteration 1381, loss = 1.86655184\n",
      "Iteration 1382, loss = 1.86648205\n",
      "Iteration 1383, loss = 1.86641211\n",
      "Iteration 1384, loss = 1.86634208\n",
      "Iteration 1385, loss = 1.86627222\n",
      "Iteration 1386, loss = 1.86620241\n",
      "Iteration 1387, loss = 1.86613267\n",
      "Iteration 1388, loss = 1.86606282\n",
      "Iteration 1389, loss = 1.86599290\n",
      "Iteration 1390, loss = 1.86592316\n",
      "Iteration 1391, loss = 1.86585342\n",
      "Iteration 1392, loss = 1.86578377\n",
      "Iteration 1393, loss = 1.86571392\n",
      "Iteration 1394, loss = 1.86564428\n",
      "Iteration 1395, loss = 1.86557456\n",
      "Iteration 1396, loss = 1.86550476\n",
      "Iteration 1397, loss = 1.86543504\n",
      "Iteration 1398, loss = 1.86536524\n",
      "Iteration 1399, loss = 1.86529536\n",
      "Iteration 1400, loss = 1.86522569\n",
      "Iteration 1401, loss = 1.86515602\n",
      "Iteration 1402, loss = 1.86508645\n",
      "Iteration 1403, loss = 1.86501669\n",
      "Iteration 1404, loss = 1.86494708\n",
      "Iteration 1405, loss = 1.86487737\n",
      "Iteration 1406, loss = 1.86480771\n",
      "Iteration 1407, loss = 1.86473792\n",
      "Iteration 1408, loss = 1.86466882\n",
      "Iteration 1409, loss = 1.86459877\n",
      "Iteration 1410, loss = 1.86452959\n",
      "Iteration 1411, loss = 1.86446025\n",
      "Iteration 1412, loss = 1.86439063\n",
      "Iteration 1413, loss = 1.86432075\n",
      "Iteration 1414, loss = 1.86425064\n",
      "Iteration 1415, loss = 1.86418210\n",
      "Iteration 1416, loss = 1.86411302\n",
      "Iteration 1417, loss = 1.86404253\n",
      "Iteration 1418, loss = 1.86397266\n",
      "Iteration 1419, loss = 1.86390359\n",
      "Iteration 1420, loss = 1.86383422\n",
      "Iteration 1421, loss = 1.86376456\n",
      "Iteration 1422, loss = 1.86369471\n",
      "Iteration 1423, loss = 1.86362519\n",
      "Iteration 1424, loss = 1.86355542\n",
      "Iteration 1425, loss = 1.86348603\n",
      "Iteration 1426, loss = 1.86341654\n",
      "Iteration 1427, loss = 1.86334687\n",
      "Iteration 1428, loss = 1.86327772\n",
      "Iteration 1429, loss = 1.86320825\n",
      "Iteration 1430, loss = 1.86313874\n",
      "Iteration 1431, loss = 1.86306922\n",
      "Iteration 1432, loss = 1.86299993\n",
      "Iteration 1433, loss = 1.86293042\n",
      "Iteration 1434, loss = 1.86286093\n",
      "Iteration 1435, loss = 1.86279155\n",
      "Iteration 1436, loss = 1.86272221\n",
      "Iteration 1437, loss = 1.86265285\n",
      "Iteration 1438, loss = 1.86258398\n",
      "Iteration 1439, loss = 1.86251397\n",
      "Iteration 1440, loss = 1.86244466\n",
      "Iteration 1441, loss = 1.86237542\n",
      "Iteration 1442, loss = 1.86230591\n",
      "Iteration 1443, loss = 1.86223695\n",
      "Iteration 1444, loss = 1.86216769\n",
      "Iteration 1445, loss = 1.86209799\n",
      "Iteration 1446, loss = 1.86202880\n",
      "Iteration 1447, loss = 1.86195970\n",
      "Iteration 1448, loss = 1.86189040\n",
      "Iteration 1449, loss = 1.86182089\n",
      "Iteration 1450, loss = 1.86175153\n",
      "Iteration 1451, loss = 1.86168255\n",
      "Iteration 1452, loss = 1.86161324\n",
      "Iteration 1453, loss = 1.86154376\n",
      "Iteration 1454, loss = 1.86147473\n",
      "Iteration 1455, loss = 1.86140550\n",
      "Iteration 1456, loss = 1.86133617\n",
      "Iteration 1457, loss = 1.86126686\n",
      "Iteration 1458, loss = 1.86119770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1459, loss = 1.86112848\n",
      "Iteration 1460, loss = 1.86105941\n",
      "Iteration 1461, loss = 1.86099018\n",
      "Iteration 1462, loss = 1.86092105\n",
      "Iteration 1463, loss = 1.86085184\n",
      "Iteration 1464, loss = 1.86078266\n",
      "Iteration 1465, loss = 1.86071349\n",
      "Iteration 1466, loss = 1.86064431\n",
      "Iteration 1467, loss = 1.86057536\n",
      "Iteration 1468, loss = 1.86050611\n",
      "Iteration 1469, loss = 1.86043707\n",
      "Iteration 1470, loss = 1.86036797\n",
      "Iteration 1471, loss = 1.86029876\n",
      "Iteration 1472, loss = 1.86022995\n",
      "Iteration 1473, loss = 1.86016055\n",
      "Iteration 1474, loss = 1.86009166\n",
      "Iteration 1475, loss = 1.86002262\n",
      "Iteration 1476, loss = 1.85995351\n",
      "Iteration 1477, loss = 1.85988439\n",
      "Iteration 1478, loss = 1.85981537\n",
      "Iteration 1479, loss = 1.85974631\n",
      "Iteration 1480, loss = 1.85967750\n",
      "Iteration 1481, loss = 1.85960848\n",
      "Iteration 1482, loss = 1.85953924\n",
      "Iteration 1483, loss = 1.85947043\n",
      "Iteration 1484, loss = 1.85940167\n",
      "Iteration 1485, loss = 1.85933293\n",
      "Iteration 1486, loss = 1.85926389\n",
      "Iteration 1487, loss = 1.85919484\n",
      "Iteration 1488, loss = 1.85912602\n",
      "Iteration 1489, loss = 1.85905706\n",
      "Iteration 1490, loss = 1.85898775\n",
      "Iteration 1491, loss = 1.85891902\n",
      "Iteration 1492, loss = 1.85885026\n",
      "Iteration 1493, loss = 1.85878137\n",
      "Iteration 1494, loss = 1.85871242\n",
      "Iteration 1495, loss = 1.85864350\n",
      "Iteration 1496, loss = 1.85857476\n",
      "Iteration 1497, loss = 1.85850599\n",
      "Iteration 1498, loss = 1.85843704\n",
      "Iteration 1499, loss = 1.85836809\n",
      "Iteration 1500, loss = 1.85829925\n",
      "Iteration 1501, loss = 1.85823027\n",
      "Iteration 1502, loss = 1.85816143\n",
      "Iteration 1503, loss = 1.85809246\n",
      "Iteration 1504, loss = 1.85802394\n",
      "Iteration 1505, loss = 1.85795532\n",
      "Iteration 1506, loss = 1.85788641\n",
      "Iteration 1507, loss = 1.85781782\n",
      "Iteration 1508, loss = 1.85774913\n",
      "Iteration 1509, loss = 1.85768018\n",
      "Iteration 1510, loss = 1.85761117\n",
      "Iteration 1511, loss = 1.85754250\n",
      "Iteration 1512, loss = 1.85747379\n",
      "Iteration 1513, loss = 1.85740491\n",
      "Iteration 1514, loss = 1.85733625\n",
      "Iteration 1515, loss = 1.85726779\n",
      "Iteration 1516, loss = 1.85719887\n",
      "Iteration 1517, loss = 1.85713003\n",
      "Iteration 1518, loss = 1.85706135\n",
      "Iteration 1519, loss = 1.85699279\n",
      "Iteration 1520, loss = 1.85692398\n",
      "Iteration 1521, loss = 1.85685528\n",
      "Iteration 1522, loss = 1.85678658\n",
      "Iteration 1523, loss = 1.85671813\n",
      "Iteration 1524, loss = 1.85664936\n",
      "Iteration 1525, loss = 1.85658073\n",
      "Iteration 1526, loss = 1.85651228\n",
      "Iteration 1527, loss = 1.85644366\n",
      "Iteration 1528, loss = 1.85637505\n",
      "Iteration 1529, loss = 1.85630631\n",
      "Iteration 1530, loss = 1.85623755\n",
      "Iteration 1531, loss = 1.85616900\n",
      "Iteration 1532, loss = 1.85610069\n",
      "Iteration 1533, loss = 1.85603221\n",
      "Iteration 1534, loss = 1.85596363\n",
      "Iteration 1535, loss = 1.85589490\n",
      "Iteration 1536, loss = 1.85582634\n",
      "Iteration 1537, loss = 1.85575791\n",
      "Iteration 1538, loss = 1.85568945\n",
      "Iteration 1539, loss = 1.85562087\n",
      "Iteration 1540, loss = 1.85555228\n",
      "Iteration 1541, loss = 1.85548347\n",
      "Iteration 1542, loss = 1.85541498\n",
      "Iteration 1543, loss = 1.85534663\n",
      "Iteration 1544, loss = 1.85527842\n",
      "Iteration 1545, loss = 1.85520976\n",
      "Iteration 1546, loss = 1.85514125\n",
      "Iteration 1547, loss = 1.85507290\n",
      "Iteration 1548, loss = 1.85500439\n",
      "Iteration 1549, loss = 1.85493600\n",
      "Iteration 1550, loss = 1.85486734\n",
      "Iteration 1551, loss = 1.85479894\n",
      "Iteration 1552, loss = 1.85473053\n",
      "Iteration 1553, loss = 1.85466241\n",
      "Iteration 1554, loss = 1.85459400\n",
      "Iteration 1555, loss = 1.85452545\n",
      "Iteration 1556, loss = 1.85445703\n",
      "Iteration 1557, loss = 1.85438875\n",
      "Iteration 1558, loss = 1.85432032\n",
      "Iteration 1559, loss = 1.85425198\n",
      "Iteration 1560, loss = 1.85418350\n",
      "Iteration 1561, loss = 1.85411507\n",
      "Iteration 1562, loss = 1.85404680\n",
      "Iteration 1563, loss = 1.85397857\n",
      "Iteration 1564, loss = 1.85391012\n",
      "Iteration 1565, loss = 1.85384187\n",
      "Iteration 1566, loss = 1.85377367\n",
      "Iteration 1567, loss = 1.85370516\n",
      "Iteration 1568, loss = 1.85363694\n",
      "Iteration 1569, loss = 1.85356893\n",
      "Iteration 1570, loss = 1.85350079\n",
      "Iteration 1571, loss = 1.85343237\n",
      "Iteration 1572, loss = 1.85336415\n",
      "Iteration 1573, loss = 1.85329582\n",
      "Iteration 1574, loss = 1.85322745\n",
      "Iteration 1575, loss = 1.85315930\n",
      "Iteration 1576, loss = 1.85309152\n",
      "Iteration 1577, loss = 1.85302332\n",
      "Iteration 1578, loss = 1.85295465\n",
      "Iteration 1579, loss = 1.85288651\n",
      "Iteration 1580, loss = 1.85281820\n",
      "Iteration 1581, loss = 1.85275010\n",
      "Iteration 1582, loss = 1.85268216\n",
      "Iteration 1583, loss = 1.85261399\n",
      "Iteration 1584, loss = 1.85254571\n",
      "Iteration 1585, loss = 1.85247763\n",
      "Iteration 1586, loss = 1.85240955\n",
      "Iteration 1587, loss = 1.85234145\n",
      "Iteration 1588, loss = 1.85227323\n",
      "Iteration 1589, loss = 1.85220518\n",
      "Iteration 1590, loss = 1.85213678\n",
      "Iteration 1591, loss = 1.85206896\n",
      "Iteration 1592, loss = 1.85200093\n",
      "Iteration 1593, loss = 1.85193300\n",
      "Iteration 1594, loss = 1.85186489\n",
      "Iteration 1595, loss = 1.85179658\n",
      "Iteration 1596, loss = 1.85172841\n",
      "Iteration 1597, loss = 1.85166060\n",
      "Iteration 1598, loss = 1.85159282\n",
      "Iteration 1599, loss = 1.85152459\n",
      "Iteration 1600, loss = 1.85145644\n",
      "Iteration 1601, loss = 1.85138849\n",
      "Iteration 1602, loss = 1.85132042\n",
      "Iteration 1603, loss = 1.85125228\n",
      "Iteration 1604, loss = 1.85118461\n",
      "Iteration 1605, loss = 1.85111653\n",
      "Iteration 1606, loss = 1.85104856\n",
      "Iteration 1607, loss = 1.85098079\n",
      "Iteration 1608, loss = 1.85091277\n",
      "Iteration 1609, loss = 1.85084453\n",
      "Iteration 1610, loss = 1.85077714\n",
      "Iteration 1611, loss = 1.85070911\n",
      "Iteration 1612, loss = 1.85064096\n",
      "Iteration 1613, loss = 1.85057322\n",
      "Iteration 1614, loss = 1.85050515\n",
      "Iteration 1615, loss = 1.85043725\n",
      "Iteration 1616, loss = 1.85036985\n",
      "Iteration 1617, loss = 1.85030161\n",
      "Iteration 1618, loss = 1.85023332\n",
      "Iteration 1619, loss = 1.85016567\n",
      "Iteration 1620, loss = 1.85009782\n",
      "Iteration 1621, loss = 1.85002984\n",
      "Iteration 1622, loss = 1.84996205\n",
      "Iteration 1623, loss = 1.84989426\n",
      "Iteration 1624, loss = 1.84982621\n",
      "Iteration 1625, loss = 1.84975841\n",
      "Iteration 1626, loss = 1.84969085\n",
      "Iteration 1627, loss = 1.84962282\n",
      "Iteration 1628, loss = 1.84955495\n",
      "Iteration 1629, loss = 1.84948717\n",
      "Iteration 1630, loss = 1.84941949\n",
      "Iteration 1631, loss = 1.84935171\n",
      "Iteration 1632, loss = 1.84928412\n",
      "Iteration 1633, loss = 1.84921624\n",
      "Iteration 1634, loss = 1.84914855\n",
      "Iteration 1635, loss = 1.84908078\n",
      "Iteration 1636, loss = 1.84901320\n",
      "Iteration 1637, loss = 1.84894566\n",
      "Iteration 1638, loss = 1.84887780\n",
      "Iteration 1639, loss = 1.84880997\n",
      "Iteration 1640, loss = 1.84874278\n",
      "Iteration 1641, loss = 1.84867466\n",
      "Iteration 1642, loss = 1.84860691\n",
      "Iteration 1643, loss = 1.84853942\n",
      "Iteration 1644, loss = 1.84847163\n",
      "Iteration 1645, loss = 1.84840381\n",
      "Iteration 1646, loss = 1.84833617\n",
      "Iteration 1647, loss = 1.84826844\n",
      "Iteration 1648, loss = 1.84820098\n",
      "Iteration 1649, loss = 1.84813330\n",
      "Iteration 1650, loss = 1.84806563\n",
      "Iteration 1651, loss = 1.84799824\n",
      "Iteration 1652, loss = 1.84793032\n",
      "Iteration 1653, loss = 1.84786265\n",
      "Iteration 1654, loss = 1.84779531\n",
      "Iteration 1655, loss = 1.84772783\n",
      "Iteration 1656, loss = 1.84766017\n",
      "Iteration 1657, loss = 1.84759238\n",
      "Iteration 1658, loss = 1.84752475\n",
      "Iteration 1659, loss = 1.84745724\n",
      "Iteration 1660, loss = 1.84738960\n",
      "Iteration 1661, loss = 1.84732221\n",
      "Iteration 1662, loss = 1.84725468\n",
      "Iteration 1663, loss = 1.84718709\n",
      "Iteration 1664, loss = 1.84711969\n",
      "Iteration 1665, loss = 1.84705216\n",
      "Iteration 1666, loss = 1.84698457\n",
      "Iteration 1667, loss = 1.84691716\n",
      "Iteration 1668, loss = 1.84684977\n",
      "Iteration 1669, loss = 1.84678231\n",
      "Iteration 1670, loss = 1.84671493\n",
      "Iteration 1671, loss = 1.84664719\n",
      "Iteration 1672, loss = 1.84657978\n",
      "Iteration 1673, loss = 1.84651224\n",
      "Iteration 1674, loss = 1.84644470\n",
      "Iteration 1675, loss = 1.84637740\n",
      "Iteration 1676, loss = 1.84631007\n",
      "Iteration 1677, loss = 1.84624263\n",
      "Iteration 1678, loss = 1.84617532\n",
      "Iteration 1679, loss = 1.84610782\n",
      "Iteration 1680, loss = 1.84604018\n",
      "Iteration 1681, loss = 1.84597284\n",
      "Iteration 1682, loss = 1.84590553\n",
      "Iteration 1683, loss = 1.84583837\n",
      "Iteration 1684, loss = 1.84577110\n",
      "Iteration 1685, loss = 1.84570349\n",
      "Iteration 1686, loss = 1.84563642\n",
      "Iteration 1687, loss = 1.84556888\n",
      "Iteration 1688, loss = 1.84550150\n",
      "Iteration 1689, loss = 1.84543450\n",
      "Iteration 1690, loss = 1.84536722\n",
      "Iteration 1691, loss = 1.84529964\n",
      "Iteration 1692, loss = 1.84523219\n",
      "Iteration 1693, loss = 1.84516517\n",
      "Iteration 1694, loss = 1.84509773\n",
      "Iteration 1695, loss = 1.84503042\n",
      "Iteration 1696, loss = 1.84496336\n",
      "Iteration 1697, loss = 1.84489595\n",
      "Iteration 1698, loss = 1.84482844\n",
      "Iteration 1699, loss = 1.84476123\n",
      "Iteration 1700, loss = 1.84469382\n",
      "Iteration 1701, loss = 1.84462670\n",
      "Iteration 1702, loss = 1.84455968\n",
      "Iteration 1703, loss = 1.84449235\n",
      "Iteration 1704, loss = 1.84442506\n",
      "Iteration 1705, loss = 1.84435779\n",
      "Iteration 1706, loss = 1.84429069\n",
      "Iteration 1707, loss = 1.84422346\n",
      "Iteration 1708, loss = 1.84415630\n",
      "Iteration 1709, loss = 1.84408903\n",
      "Iteration 1710, loss = 1.84402191\n",
      "Iteration 1711, loss = 1.84395483\n",
      "Iteration 1712, loss = 1.84388761\n",
      "Iteration 1713, loss = 1.84382050\n",
      "Iteration 1714, loss = 1.84375333\n",
      "Iteration 1715, loss = 1.84368603\n",
      "Iteration 1716, loss = 1.84361921\n",
      "Iteration 1717, loss = 1.84355172\n",
      "Iteration 1718, loss = 1.84348501\n",
      "Iteration 1719, loss = 1.84341800\n",
      "Iteration 1720, loss = 1.84335082\n",
      "Iteration 1721, loss = 1.84328383\n",
      "Iteration 1722, loss = 1.84321642\n",
      "Iteration 1723, loss = 1.84314941\n",
      "Iteration 1724, loss = 1.84308217\n",
      "Iteration 1725, loss = 1.84301581\n",
      "Iteration 1726, loss = 1.84294857\n",
      "Iteration 1727, loss = 1.84288120\n",
      "Iteration 1728, loss = 1.84281435\n",
      "Iteration 1729, loss = 1.84274715\n",
      "Iteration 1730, loss = 1.84268027\n",
      "Iteration 1731, loss = 1.84261369\n",
      "Iteration 1732, loss = 1.84254601\n",
      "Iteration 1733, loss = 1.84247901\n",
      "Iteration 1734, loss = 1.84241204\n",
      "Iteration 1735, loss = 1.84234517\n",
      "Iteration 1736, loss = 1.84227809\n",
      "Iteration 1737, loss = 1.84221131\n",
      "Iteration 1738, loss = 1.84214423\n",
      "Iteration 1739, loss = 1.84207745\n",
      "Iteration 1740, loss = 1.84201018\n",
      "Iteration 1741, loss = 1.84194360\n",
      "Iteration 1742, loss = 1.84187677\n",
      "Iteration 1743, loss = 1.84180982\n",
      "Iteration 1744, loss = 1.84174281\n",
      "Iteration 1745, loss = 1.84167596\n",
      "Iteration 1746, loss = 1.84160873\n",
      "Iteration 1747, loss = 1.84154194\n",
      "Iteration 1748, loss = 1.84147508\n",
      "Iteration 1749, loss = 1.84140816\n",
      "Iteration 1750, loss = 1.84134139\n",
      "Iteration 1751, loss = 1.84127459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1752, loss = 1.84120758\n",
      "Iteration 1753, loss = 1.84114073\n",
      "Iteration 1754, loss = 1.84107386\n",
      "Iteration 1755, loss = 1.84100708\n",
      "Iteration 1756, loss = 1.84094015\n",
      "Iteration 1757, loss = 1.84087338\n",
      "Iteration 1758, loss = 1.84080647\n",
      "Iteration 1759, loss = 1.84073976\n",
      "Iteration 1760, loss = 1.84067303\n",
      "Iteration 1761, loss = 1.84060624\n",
      "Iteration 1762, loss = 1.84053930\n",
      "Iteration 1763, loss = 1.84047256\n",
      "Iteration 1764, loss = 1.84040602\n",
      "Iteration 1765, loss = 1.84033936\n",
      "Iteration 1766, loss = 1.84027254\n",
      "Iteration 1767, loss = 1.84020595\n",
      "Iteration 1768, loss = 1.84013892\n",
      "Iteration 1769, loss = 1.84007209\n",
      "Iteration 1770, loss = 1.84000515\n",
      "Iteration 1771, loss = 1.83993820\n",
      "Iteration 1772, loss = 1.83987123\n",
      "Iteration 1773, loss = 1.83980447\n",
      "Iteration 1774, loss = 1.83973770\n",
      "Iteration 1775, loss = 1.83967080\n",
      "Iteration 1776, loss = 1.83960403\n",
      "Iteration 1777, loss = 1.83953692\n",
      "Iteration 1778, loss = 1.83946997\n",
      "Iteration 1779, loss = 1.83940326\n",
      "Iteration 1780, loss = 1.83933625\n",
      "Iteration 1781, loss = 1.83926936\n",
      "Iteration 1782, loss = 1.83920246\n",
      "Iteration 1783, loss = 1.83913579\n",
      "Iteration 1784, loss = 1.83906873\n",
      "Iteration 1785, loss = 1.83900214\n",
      "Iteration 1786, loss = 1.83893547\n",
      "Iteration 1787, loss = 1.83886837\n",
      "Iteration 1788, loss = 1.83880166\n",
      "Iteration 1789, loss = 1.83873491\n",
      "Iteration 1790, loss = 1.83866785\n",
      "Iteration 1791, loss = 1.83860085\n",
      "Iteration 1792, loss = 1.83853390\n",
      "Iteration 1793, loss = 1.83846706\n",
      "Iteration 1794, loss = 1.83840029\n",
      "Iteration 1795, loss = 1.83833378\n",
      "Iteration 1796, loss = 1.83826691\n",
      "Iteration 1797, loss = 1.83819999\n",
      "Iteration 1798, loss = 1.83813343\n",
      "Iteration 1799, loss = 1.83806653\n",
      "Iteration 1800, loss = 1.83799934\n",
      "Iteration 1801, loss = 1.83793257\n",
      "Iteration 1802, loss = 1.83786593\n",
      "Iteration 1803, loss = 1.83779910\n",
      "Iteration 1804, loss = 1.83773241\n",
      "Iteration 1805, loss = 1.83766589\n",
      "Iteration 1806, loss = 1.83759906\n",
      "Iteration 1807, loss = 1.83753234\n",
      "Iteration 1808, loss = 1.83746549\n",
      "Iteration 1809, loss = 1.83739868\n",
      "Iteration 1810, loss = 1.83733228\n",
      "Iteration 1811, loss = 1.83726543\n",
      "Iteration 1812, loss = 1.83719873\n",
      "Iteration 1813, loss = 1.83713202\n",
      "Iteration 1814, loss = 1.83706510\n",
      "Iteration 1815, loss = 1.83699832\n",
      "Iteration 1816, loss = 1.83693133\n",
      "Iteration 1817, loss = 1.83686495\n",
      "Iteration 1818, loss = 1.83679826\n",
      "Iteration 1819, loss = 1.83673166\n",
      "Iteration 1820, loss = 1.83666493\n",
      "Iteration 1821, loss = 1.83659811\n",
      "Iteration 1822, loss = 1.83653159\n",
      "Iteration 1823, loss = 1.83646490\n",
      "Iteration 1824, loss = 1.83639817\n",
      "Iteration 1825, loss = 1.83633139\n",
      "Iteration 1826, loss = 1.83626483\n",
      "Iteration 1827, loss = 1.83619833\n",
      "Iteration 1828, loss = 1.83613161\n",
      "Iteration 1829, loss = 1.83606513\n",
      "Iteration 1830, loss = 1.83599827\n",
      "Iteration 1831, loss = 1.83593172\n",
      "Iteration 1832, loss = 1.83586506\n",
      "Iteration 1833, loss = 1.83579848\n",
      "Iteration 1834, loss = 1.83573209\n",
      "Iteration 1835, loss = 1.83566540\n",
      "Iteration 1836, loss = 1.83559883\n",
      "Iteration 1837, loss = 1.83553224\n",
      "Iteration 1838, loss = 1.83546561\n",
      "Iteration 1839, loss = 1.83539901\n",
      "Iteration 1840, loss = 1.83533278\n",
      "Iteration 1841, loss = 1.83526607\n",
      "Iteration 1842, loss = 1.83519961\n",
      "Iteration 1843, loss = 1.83513318\n",
      "Iteration 1844, loss = 1.83506624\n",
      "Iteration 1845, loss = 1.83499969\n",
      "Iteration 1846, loss = 1.83493338\n",
      "Iteration 1847, loss = 1.83486691\n",
      "Iteration 1848, loss = 1.83480043\n",
      "Iteration 1849, loss = 1.83473406\n",
      "Iteration 1850, loss = 1.83466713\n",
      "Iteration 1851, loss = 1.83460102\n",
      "Iteration 1852, loss = 1.83453455\n",
      "Iteration 1853, loss = 1.83446782\n",
      "Iteration 1854, loss = 1.83440211\n",
      "Iteration 1855, loss = 1.83433593\n",
      "Iteration 1856, loss = 1.83426845\n",
      "Iteration 1857, loss = 1.83420225\n",
      "Iteration 1858, loss = 1.83413598\n",
      "Iteration 1859, loss = 1.83406942\n",
      "Iteration 1860, loss = 1.83400294\n",
      "Iteration 1861, loss = 1.83393630\n",
      "Iteration 1862, loss = 1.83386979\n",
      "Iteration 1863, loss = 1.83380338\n",
      "Iteration 1864, loss = 1.83373703\n",
      "Iteration 1865, loss = 1.83367054\n",
      "Iteration 1866, loss = 1.83360432\n",
      "Iteration 1867, loss = 1.83353780\n",
      "Iteration 1868, loss = 1.83347161\n",
      "Iteration 1869, loss = 1.83340498\n",
      "Iteration 1870, loss = 1.83333861\n",
      "Iteration 1871, loss = 1.83327216\n",
      "Iteration 1872, loss = 1.83320591\n",
      "Iteration 1873, loss = 1.83313946\n",
      "Iteration 1874, loss = 1.83307321\n",
      "Iteration 1875, loss = 1.83300687\n",
      "Iteration 1876, loss = 1.83294065\n",
      "Iteration 1877, loss = 1.83287439\n",
      "Iteration 1878, loss = 1.83280790\n",
      "Iteration 1879, loss = 1.83274147\n",
      "Iteration 1880, loss = 1.83267518\n",
      "Iteration 1881, loss = 1.83260888\n",
      "Iteration 1882, loss = 1.83254284\n",
      "Iteration 1883, loss = 1.83247641\n",
      "Iteration 1884, loss = 1.83241019\n",
      "Iteration 1885, loss = 1.83234380\n",
      "Iteration 1886, loss = 1.83227749\n",
      "Iteration 1887, loss = 1.83221140\n",
      "Iteration 1888, loss = 1.83214513\n",
      "Iteration 1889, loss = 1.83207886\n",
      "Iteration 1890, loss = 1.83201256\n",
      "Iteration 1891, loss = 1.83194625\n",
      "Iteration 1892, loss = 1.83188025\n",
      "Iteration 1893, loss = 1.83181380\n",
      "Iteration 1894, loss = 1.83174753\n",
      "Iteration 1895, loss = 1.83168111\n",
      "Iteration 1896, loss = 1.83161484\n",
      "Iteration 1897, loss = 1.83154859\n",
      "Iteration 1898, loss = 1.83148259\n",
      "Iteration 1899, loss = 1.83141641\n",
      "Iteration 1900, loss = 1.83134990\n",
      "Iteration 1901, loss = 1.83128374\n",
      "Iteration 1902, loss = 1.83121750\n",
      "Iteration 1903, loss = 1.83115143\n",
      "Iteration 1904, loss = 1.83108530\n",
      "Iteration 1905, loss = 1.83101908\n",
      "Iteration 1906, loss = 1.83095288\n",
      "Iteration 1907, loss = 1.83088674\n",
      "Iteration 1908, loss = 1.83082047\n",
      "Iteration 1909, loss = 1.83075447\n",
      "Iteration 1910, loss = 1.83068840\n",
      "Iteration 1911, loss = 1.83062217\n",
      "Iteration 1912, loss = 1.83055602\n",
      "Iteration 1913, loss = 1.83048986\n",
      "Iteration 1914, loss = 1.83042366\n",
      "Iteration 1915, loss = 1.83035774\n",
      "Iteration 1916, loss = 1.83029154\n",
      "Iteration 1917, loss = 1.83022559\n",
      "Iteration 1918, loss = 1.83015927\n",
      "Iteration 1919, loss = 1.83009361\n",
      "Iteration 1920, loss = 1.83002701\n",
      "Iteration 1921, loss = 1.82996120\n",
      "Iteration 1922, loss = 1.82989511\n",
      "Iteration 1923, loss = 1.82982896\n",
      "Iteration 1924, loss = 1.82976273\n",
      "Iteration 1925, loss = 1.82969678\n",
      "Iteration 1926, loss = 1.82963075\n",
      "Iteration 1927, loss = 1.82956486\n",
      "Iteration 1928, loss = 1.82949864\n",
      "Iteration 1929, loss = 1.82943336\n",
      "Iteration 1930, loss = 1.82936687\n",
      "Iteration 1931, loss = 1.82930083\n",
      "Iteration 1932, loss = 1.82923485\n",
      "Iteration 1933, loss = 1.82916858\n",
      "Iteration 1934, loss = 1.82910272\n",
      "Iteration 1935, loss = 1.82903668\n",
      "Iteration 1936, loss = 1.82897065\n",
      "Iteration 1937, loss = 1.82890450\n",
      "Iteration 1938, loss = 1.82883875\n",
      "Iteration 1939, loss = 1.82877268\n",
      "Iteration 1940, loss = 1.82870703\n",
      "Iteration 1941, loss = 1.82864084\n",
      "Iteration 1942, loss = 1.82857524\n",
      "Iteration 1943, loss = 1.82850929\n",
      "Iteration 1944, loss = 1.82844288\n",
      "Iteration 1945, loss = 1.82837715\n",
      "Iteration 1946, loss = 1.82831122\n",
      "Iteration 1947, loss = 1.82824546\n",
      "Iteration 1948, loss = 1.82817931\n",
      "Iteration 1949, loss = 1.82811345\n",
      "Iteration 1950, loss = 1.82804728\n",
      "Iteration 1951, loss = 1.82798178\n",
      "Iteration 1952, loss = 1.82791568\n",
      "Iteration 1953, loss = 1.82785003\n",
      "Iteration 1954, loss = 1.82778452\n",
      "Iteration 1955, loss = 1.82771825\n",
      "Iteration 1956, loss = 1.82765181\n",
      "Iteration 1957, loss = 1.82758701\n",
      "Iteration 1958, loss = 1.82752082\n",
      "Iteration 1959, loss = 1.82745441\n",
      "Iteration 1960, loss = 1.82738893\n",
      "Iteration 1961, loss = 1.82732289\n",
      "Iteration 1962, loss = 1.82725674\n",
      "Iteration 1963, loss = 1.82719180\n",
      "Iteration 1964, loss = 1.82712547\n",
      "Iteration 1965, loss = 1.82705937\n",
      "Iteration 1966, loss = 1.82699385\n",
      "Iteration 1967, loss = 1.82692765\n",
      "Iteration 1968, loss = 1.82686187\n",
      "Iteration 1969, loss = 1.82679605\n",
      "Iteration 1970, loss = 1.82672994\n",
      "Iteration 1971, loss = 1.82666446\n",
      "Iteration 1972, loss = 1.82659878\n",
      "Iteration 1973, loss = 1.82653289\n",
      "Iteration 1974, loss = 1.82646742\n",
      "Iteration 1975, loss = 1.82640158\n",
      "Iteration 1976, loss = 1.82633559\n",
      "Iteration 1977, loss = 1.82626965\n",
      "Iteration 1978, loss = 1.82620388\n",
      "Iteration 1979, loss = 1.82613825\n",
      "Iteration 1980, loss = 1.82607265\n",
      "Iteration 1981, loss = 1.82600673\n",
      "Iteration 1982, loss = 1.82594112\n",
      "Iteration 1983, loss = 1.82587520\n",
      "Iteration 1984, loss = 1.82580942\n",
      "Iteration 1985, loss = 1.82574372\n",
      "Iteration 1986, loss = 1.82567785\n",
      "Iteration 1987, loss = 1.82561242\n",
      "Iteration 1988, loss = 1.82554675\n",
      "Iteration 1989, loss = 1.82548100\n",
      "Iteration 1990, loss = 1.82541554\n",
      "Iteration 1991, loss = 1.82534971\n",
      "Iteration 1992, loss = 1.82528371\n",
      "Iteration 1993, loss = 1.82521787\n",
      "Iteration 1994, loss = 1.82515213\n",
      "Iteration 1995, loss = 1.82508698\n",
      "Iteration 1996, loss = 1.82502128\n",
      "Iteration 1997, loss = 1.82495542\n",
      "Iteration 1998, loss = 1.82488978\n",
      "Iteration 1999, loss = 1.82482394\n",
      "Iteration 2000, loss = 1.82475844\n",
      "Iteration 2001, loss = 1.82469289\n",
      "Iteration 2002, loss = 1.82462727\n",
      "Iteration 2003, loss = 1.82456132\n",
      "Iteration 2004, loss = 1.82449577\n",
      "Iteration 2005, loss = 1.82443050\n",
      "Iteration 2006, loss = 1.82436483\n",
      "Iteration 2007, loss = 1.82429953\n",
      "Iteration 2008, loss = 1.82423317\n",
      "Iteration 2009, loss = 1.82416800\n",
      "Iteration 2010, loss = 1.82410247\n",
      "Iteration 2011, loss = 1.82403666\n",
      "Iteration 2012, loss = 1.82397154\n",
      "Iteration 2013, loss = 1.82390591\n",
      "Iteration 2014, loss = 1.82383989\n",
      "Iteration 2015, loss = 1.82377429\n",
      "Iteration 2016, loss = 1.82370871\n",
      "Iteration 2017, loss = 1.82364301\n",
      "Iteration 2018, loss = 1.82357765\n",
      "Iteration 2019, loss = 1.82351193\n",
      "Iteration 2020, loss = 1.82344656\n",
      "Iteration 2021, loss = 1.82338107\n",
      "Iteration 2022, loss = 1.82331564\n",
      "Iteration 2023, loss = 1.82325000\n",
      "Iteration 2024, loss = 1.82318459\n",
      "Iteration 2025, loss = 1.82311876\n",
      "Iteration 2026, loss = 1.82305320\n",
      "Iteration 2027, loss = 1.82298792\n",
      "Iteration 2028, loss = 1.82292243\n",
      "Iteration 2029, loss = 1.82285723\n",
      "Iteration 2030, loss = 1.82279134\n",
      "Iteration 2031, loss = 1.82272598\n",
      "Iteration 2032, loss = 1.82266026\n",
      "Iteration 2033, loss = 1.82259460\n",
      "Iteration 2034, loss = 1.82252961\n",
      "Iteration 2035, loss = 1.82246391\n",
      "Iteration 2036, loss = 1.82239875\n",
      "Iteration 2037, loss = 1.82233289\n",
      "Iteration 2038, loss = 1.82226755\n",
      "Iteration 2039, loss = 1.82220220\n",
      "Iteration 2040, loss = 1.82213702\n",
      "Iteration 2041, loss = 1.82207164\n",
      "Iteration 2042, loss = 1.82200598\n",
      "Iteration 2043, loss = 1.82194047\n",
      "Iteration 2044, loss = 1.82187504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2045, loss = 1.82180985\n",
      "Iteration 2046, loss = 1.82174440\n",
      "Iteration 2047, loss = 1.82167883\n",
      "Iteration 2048, loss = 1.82161314\n",
      "Iteration 2049, loss = 1.82154756\n",
      "Iteration 2050, loss = 1.82148234\n",
      "Iteration 2051, loss = 1.82141702\n",
      "Iteration 2052, loss = 1.82135148\n",
      "Iteration 2053, loss = 1.82128605\n",
      "Iteration 2054, loss = 1.82122136\n",
      "Iteration 2055, loss = 1.82115545\n",
      "Iteration 2056, loss = 1.82109045\n",
      "Iteration 2057, loss = 1.82102508\n",
      "Iteration 2058, loss = 1.82095959\n",
      "Iteration 2059, loss = 1.82089414\n",
      "Iteration 2060, loss = 1.82082898\n",
      "Iteration 2061, loss = 1.82076366\n",
      "Iteration 2062, loss = 1.82069837\n",
      "Iteration 2063, loss = 1.82063315\n",
      "Iteration 2064, loss = 1.82056749\n",
      "Iteration 2065, loss = 1.82050213\n",
      "Iteration 2066, loss = 1.82043694\n",
      "Iteration 2067, loss = 1.82037161\n",
      "Iteration 2068, loss = 1.82030635\n",
      "Iteration 2069, loss = 1.82024108\n",
      "Iteration 2070, loss = 1.82017568\n",
      "Iteration 2071, loss = 1.82011014\n",
      "Iteration 2072, loss = 1.82004495\n",
      "Iteration 2073, loss = 1.81997981\n",
      "Iteration 2074, loss = 1.81991465\n",
      "Iteration 2075, loss = 1.81984901\n",
      "Iteration 2076, loss = 1.81978402\n",
      "Iteration 2077, loss = 1.81971847\n",
      "Iteration 2078, loss = 1.81965372\n",
      "Iteration 2079, loss = 1.81958839\n",
      "Iteration 2080, loss = 1.81952299\n",
      "Iteration 2081, loss = 1.81945755\n",
      "Iteration 2082, loss = 1.81939247\n",
      "Iteration 2083, loss = 1.81932715\n",
      "Iteration 2084, loss = 1.81926190\n",
      "Iteration 2085, loss = 1.81919720\n",
      "Iteration 2086, loss = 1.81913199\n",
      "Iteration 2087, loss = 1.81906671\n",
      "Iteration 2088, loss = 1.81900154\n",
      "Iteration 2089, loss = 1.81893586\n",
      "Iteration 2090, loss = 1.81887112\n",
      "Iteration 2091, loss = 1.81880601\n",
      "Iteration 2092, loss = 1.81874068\n",
      "Iteration 2093, loss = 1.81867546\n",
      "Iteration 2094, loss = 1.81861002\n",
      "Iteration 2095, loss = 1.81854477\n",
      "Iteration 2096, loss = 1.81847972\n",
      "Iteration 2097, loss = 1.81841455\n",
      "Iteration 2098, loss = 1.81834933\n",
      "Iteration 2099, loss = 1.81828421\n",
      "Iteration 2100, loss = 1.81821917\n",
      "Iteration 2101, loss = 1.81815377\n",
      "Iteration 2102, loss = 1.81808889\n",
      "Iteration 2103, loss = 1.81802388\n",
      "Iteration 2104, loss = 1.81795862\n",
      "Iteration 2105, loss = 1.81789341\n",
      "Iteration 2106, loss = 1.81782839\n",
      "Iteration 2107, loss = 1.81776335\n",
      "Iteration 2108, loss = 1.81769821\n",
      "Iteration 2109, loss = 1.81763308\n",
      "Iteration 2110, loss = 1.81756785\n",
      "Iteration 2111, loss = 1.81750270\n",
      "Iteration 2112, loss = 1.81743768\n",
      "Iteration 2113, loss = 1.81737258\n",
      "Iteration 2114, loss = 1.81730766\n",
      "Iteration 2115, loss = 1.81724273\n",
      "Iteration 2116, loss = 1.81717763\n",
      "Iteration 2117, loss = 1.81711242\n",
      "Iteration 2118, loss = 1.81704714\n",
      "Iteration 2119, loss = 1.81698193\n",
      "Iteration 2120, loss = 1.81691725\n",
      "Iteration 2121, loss = 1.81685223\n",
      "Iteration 2122, loss = 1.81678722\n",
      "Iteration 2123, loss = 1.81672218\n",
      "Iteration 2124, loss = 1.81665699\n",
      "Iteration 2125, loss = 1.81659177\n",
      "Iteration 2126, loss = 1.81652674\n",
      "Iteration 2127, loss = 1.81646198\n",
      "Iteration 2128, loss = 1.81639712\n",
      "Iteration 2129, loss = 1.81633198\n",
      "Iteration 2130, loss = 1.81626682\n",
      "Iteration 2131, loss = 1.81620193\n",
      "Iteration 2132, loss = 1.81613692\n",
      "Iteration 2133, loss = 1.81607228\n",
      "Iteration 2134, loss = 1.81600738\n",
      "Iteration 2135, loss = 1.81594213\n",
      "Iteration 2136, loss = 1.81587726\n",
      "Iteration 2137, loss = 1.81581223\n",
      "Iteration 2138, loss = 1.81574723\n",
      "Iteration 2139, loss = 1.81568224\n",
      "Iteration 2140, loss = 1.81561721\n",
      "Iteration 2141, loss = 1.81555223\n",
      "Iteration 2142, loss = 1.81548732\n",
      "Iteration 2143, loss = 1.81542252\n",
      "Iteration 2144, loss = 1.81535737\n",
      "Iteration 2145, loss = 1.81529235\n",
      "Iteration 2146, loss = 1.81522735\n",
      "Iteration 2147, loss = 1.81516295\n",
      "Iteration 2148, loss = 1.81509800\n",
      "Iteration 2149, loss = 1.81503268\n",
      "Iteration 2150, loss = 1.81496833\n",
      "Iteration 2151, loss = 1.81490342\n",
      "Iteration 2152, loss = 1.81483808\n",
      "Iteration 2153, loss = 1.81477401\n",
      "Iteration 2154, loss = 1.81470816\n",
      "Iteration 2155, loss = 1.81464332\n",
      "Iteration 2156, loss = 1.81457827\n",
      "Iteration 2157, loss = 1.81451359\n",
      "Iteration 2158, loss = 1.81444872\n",
      "Iteration 2159, loss = 1.81438383\n",
      "Iteration 2160, loss = 1.81431866\n",
      "Iteration 2161, loss = 1.81425400\n",
      "Iteration 2162, loss = 1.81418916\n",
      "Iteration 2163, loss = 1.81412427\n",
      "Iteration 2164, loss = 1.81405920\n",
      "Iteration 2165, loss = 1.81399475\n",
      "Iteration 2166, loss = 1.81392985\n",
      "Iteration 2167, loss = 1.81386556\n",
      "Iteration 2168, loss = 1.81380033\n",
      "Iteration 2169, loss = 1.81373584\n",
      "Iteration 2170, loss = 1.81367125\n",
      "Iteration 2171, loss = 1.81360564\n",
      "Iteration 2172, loss = 1.81354117\n",
      "Iteration 2173, loss = 1.81347680\n",
      "Iteration 2174, loss = 1.81341184\n",
      "Iteration 2175, loss = 1.81334662\n",
      "Iteration 2176, loss = 1.81328176\n",
      "Iteration 2177, loss = 1.81321655\n",
      "Iteration 2178, loss = 1.81315194\n",
      "Iteration 2179, loss = 1.81308716\n",
      "Iteration 2180, loss = 1.81302252\n",
      "Iteration 2181, loss = 1.81295755\n",
      "Iteration 2182, loss = 1.81289276\n",
      "Iteration 2183, loss = 1.81282787\n",
      "Iteration 2184, loss = 1.81276324\n",
      "Iteration 2185, loss = 1.81269846\n",
      "Iteration 2186, loss = 1.81263365\n",
      "Iteration 2187, loss = 1.81256893\n",
      "Iteration 2188, loss = 1.81250402\n",
      "Iteration 2189, loss = 1.81243963\n",
      "Iteration 2190, loss = 1.81237468\n",
      "Iteration 2191, loss = 1.81231009\n",
      "Iteration 2192, loss = 1.81224514\n",
      "Iteration 2193, loss = 1.81218031\n",
      "Iteration 2194, loss = 1.81211560\n",
      "Iteration 2195, loss = 1.81205085\n",
      "Iteration 2196, loss = 1.81198633\n",
      "Iteration 2197, loss = 1.81192153\n",
      "Iteration 2198, loss = 1.81185714\n",
      "Iteration 2199, loss = 1.81179227\n",
      "Iteration 2200, loss = 1.81172754\n",
      "Iteration 2201, loss = 1.81166303\n",
      "Iteration 2202, loss = 1.81159800\n",
      "Iteration 2203, loss = 1.81153347\n",
      "Iteration 2204, loss = 1.81146877\n",
      "Iteration 2205, loss = 1.81140437\n",
      "Iteration 2206, loss = 1.81133976\n",
      "Iteration 2207, loss = 1.81127472\n",
      "Iteration 2208, loss = 1.81121038\n",
      "Iteration 2209, loss = 1.81114559\n",
      "Iteration 2210, loss = 1.81108096\n",
      "Iteration 2211, loss = 1.81101597\n",
      "Iteration 2212, loss = 1.81095149\n",
      "Iteration 2213, loss = 1.81088710\n",
      "Iteration 2214, loss = 1.81082230\n",
      "Iteration 2215, loss = 1.81075842\n",
      "Iteration 2216, loss = 1.81069289\n",
      "Iteration 2217, loss = 1.81062893\n",
      "Iteration 2218, loss = 1.81056513\n",
      "Iteration 2219, loss = 1.81050037\n",
      "Iteration 2220, loss = 1.81043474\n",
      "Iteration 2221, loss = 1.81037020\n",
      "Iteration 2222, loss = 1.81030608\n",
      "Iteration 2223, loss = 1.81024119\n",
      "Iteration 2224, loss = 1.81017580\n",
      "Iteration 2225, loss = 1.81011218\n",
      "Iteration 2226, loss = 1.81004757\n",
      "Iteration 2227, loss = 1.80998198\n",
      "Iteration 2228, loss = 1.80991899\n",
      "Iteration 2229, loss = 1.80985420\n",
      "Iteration 2230, loss = 1.80978821\n",
      "Iteration 2231, loss = 1.80972489\n",
      "Iteration 2232, loss = 1.80966097\n",
      "Iteration 2233, loss = 1.80959597\n",
      "Iteration 2234, loss = 1.80953010\n",
      "Iteration 2235, loss = 1.80946633\n",
      "Iteration 2236, loss = 1.80940223\n",
      "Iteration 2237, loss = 1.80933686\n",
      "Iteration 2238, loss = 1.80927246\n",
      "Iteration 2239, loss = 1.80920841\n",
      "Iteration 2240, loss = 1.80914395\n",
      "Iteration 2241, loss = 1.80907865\n",
      "Iteration 2242, loss = 1.80901447\n",
      "Iteration 2243, loss = 1.80895012\n",
      "Iteration 2244, loss = 1.80888512\n",
      "Iteration 2245, loss = 1.80882060\n",
      "Iteration 2246, loss = 1.80875666\n",
      "Iteration 2247, loss = 1.80869166\n",
      "Iteration 2248, loss = 1.80862758\n",
      "Iteration 2249, loss = 1.80856347\n",
      "Iteration 2250, loss = 1.80849792\n",
      "Iteration 2251, loss = 1.80843387\n",
      "Iteration 2252, loss = 1.80836903\n",
      "Iteration 2253, loss = 1.80830443\n",
      "Iteration 2254, loss = 1.80824003\n",
      "Iteration 2255, loss = 1.80817547\n",
      "Iteration 2256, loss = 1.80811087\n",
      "Iteration 2257, loss = 1.80804657\n",
      "Iteration 2258, loss = 1.80798166\n",
      "Iteration 2259, loss = 1.80791763\n",
      "Iteration 2260, loss = 1.80785317\n",
      "Iteration 2261, loss = 1.80778874\n",
      "Iteration 2262, loss = 1.80772404\n",
      "Iteration 2263, loss = 1.80765918\n",
      "Iteration 2264, loss = 1.80759489\n",
      "Iteration 2265, loss = 1.80753062\n",
      "Iteration 2266, loss = 1.80746628\n",
      "Iteration 2267, loss = 1.80740168\n",
      "Iteration 2268, loss = 1.80733725\n",
      "Iteration 2269, loss = 1.80727267\n",
      "Iteration 2270, loss = 1.80720888\n",
      "Iteration 2271, loss = 1.80714386\n",
      "Iteration 2272, loss = 1.80707993\n",
      "Iteration 2273, loss = 1.80701508\n",
      "Iteration 2274, loss = 1.80695049\n",
      "Iteration 2275, loss = 1.80688587\n",
      "Iteration 2276, loss = 1.80682165\n",
      "Iteration 2277, loss = 1.80675736\n",
      "Iteration 2278, loss = 1.80669321\n",
      "Iteration 2279, loss = 1.80662881\n",
      "Iteration 2280, loss = 1.80656413\n",
      "Iteration 2281, loss = 1.80649958\n",
      "Iteration 2282, loss = 1.80643541\n",
      "Iteration 2283, loss = 1.80637088\n",
      "Iteration 2284, loss = 1.80630643\n",
      "Iteration 2285, loss = 1.80624203\n",
      "Iteration 2286, loss = 1.80617771\n",
      "Iteration 2287, loss = 1.80611348\n",
      "Iteration 2288, loss = 1.80604914\n",
      "Iteration 2289, loss = 1.80598479\n",
      "Iteration 2290, loss = 1.80592035\n",
      "Iteration 2291, loss = 1.80585587\n",
      "Iteration 2292, loss = 1.80579173\n",
      "Iteration 2293, loss = 1.80572722\n",
      "Iteration 2294, loss = 1.80566294\n",
      "Iteration 2295, loss = 1.80559846\n",
      "Iteration 2296, loss = 1.80553388\n",
      "Iteration 2297, loss = 1.80546974\n",
      "Iteration 2298, loss = 1.80540609\n",
      "Iteration 2299, loss = 1.80534174\n",
      "Iteration 2300, loss = 1.80527705\n",
      "Iteration 2301, loss = 1.80521312\n",
      "Iteration 2302, loss = 1.80514837\n",
      "Iteration 2303, loss = 1.80508420\n",
      "Iteration 2304, loss = 1.80501975\n",
      "Iteration 2305, loss = 1.80495560\n",
      "Iteration 2306, loss = 1.80489096\n",
      "Iteration 2307, loss = 1.80482737\n",
      "Iteration 2308, loss = 1.80476311\n",
      "Iteration 2309, loss = 1.80469857\n",
      "Iteration 2310, loss = 1.80463411\n",
      "Iteration 2311, loss = 1.80457018\n",
      "Iteration 2312, loss = 1.80450572\n",
      "Iteration 2313, loss = 1.80444119\n",
      "Iteration 2314, loss = 1.80437728\n",
      "Iteration 2315, loss = 1.80431254\n",
      "Iteration 2316, loss = 1.80424830\n",
      "Iteration 2317, loss = 1.80418381\n",
      "Iteration 2318, loss = 1.80411979\n",
      "Iteration 2319, loss = 1.80405536\n",
      "Iteration 2320, loss = 1.80399124\n",
      "Iteration 2321, loss = 1.80392673\n",
      "Iteration 2322, loss = 1.80386258\n",
      "Iteration 2323, loss = 1.80379869\n",
      "Iteration 2324, loss = 1.80373412\n",
      "Iteration 2325, loss = 1.80366969\n",
      "Iteration 2326, loss = 1.80360594\n",
      "Iteration 2327, loss = 1.80354147\n",
      "Iteration 2328, loss = 1.80347720\n",
      "Iteration 2329, loss = 1.80341347\n",
      "Iteration 2330, loss = 1.80334886\n",
      "Iteration 2331, loss = 1.80328487\n",
      "Iteration 2332, loss = 1.80322076\n",
      "Iteration 2333, loss = 1.80315654\n",
      "Iteration 2334, loss = 1.80309286\n",
      "Iteration 2335, loss = 1.80302799\n",
      "Iteration 2336, loss = 1.80296409\n",
      "Iteration 2337, loss = 1.80289954\n",
      "Iteration 2338, loss = 1.80283591\n",
      "Iteration 2339, loss = 1.80277195\n",
      "Iteration 2340, loss = 1.80270714\n",
      "Iteration 2341, loss = 1.80264298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2342, loss = 1.80257904\n",
      "Iteration 2343, loss = 1.80251449\n",
      "Iteration 2344, loss = 1.80245092\n",
      "Iteration 2345, loss = 1.80238623\n",
      "Iteration 2346, loss = 1.80232234\n",
      "Iteration 2347, loss = 1.80225787\n",
      "Iteration 2348, loss = 1.80219332\n",
      "Iteration 2349, loss = 1.80212945\n",
      "Iteration 2350, loss = 1.80206518\n",
      "Iteration 2351, loss = 1.80200116\n",
      "Iteration 2352, loss = 1.80193669\n",
      "Iteration 2353, loss = 1.80187300\n",
      "Iteration 2354, loss = 1.80180830\n",
      "Iteration 2355, loss = 1.80174469\n",
      "Iteration 2356, loss = 1.80168057\n",
      "Iteration 2357, loss = 1.80161616\n",
      "Iteration 2358, loss = 1.80155211\n",
      "Iteration 2359, loss = 1.80148804\n",
      "Iteration 2360, loss = 1.80142381\n",
      "Iteration 2361, loss = 1.80135962\n",
      "Iteration 2362, loss = 1.80129552\n",
      "Iteration 2363, loss = 1.80123151\n",
      "Iteration 2364, loss = 1.80116731\n",
      "Iteration 2365, loss = 1.80110278\n",
      "Iteration 2366, loss = 1.80103962\n",
      "Iteration 2367, loss = 1.80097550\n",
      "Iteration 2368, loss = 1.80091092\n",
      "Iteration 2369, loss = 1.80084765\n",
      "Iteration 2370, loss = 1.80078369\n",
      "Iteration 2371, loss = 1.80071879\n",
      "Iteration 2372, loss = 1.80065467\n",
      "Iteration 2373, loss = 1.80059077\n",
      "Iteration 2374, loss = 1.80052604\n",
      "Iteration 2375, loss = 1.80046267\n",
      "Iteration 2376, loss = 1.80039820\n",
      "Iteration 2377, loss = 1.80033408\n",
      "Iteration 2378, loss = 1.80027005\n",
      "Iteration 2379, loss = 1.80020574\n",
      "Iteration 2380, loss = 1.80014146\n",
      "Iteration 2381, loss = 1.80007750\n",
      "Iteration 2382, loss = 1.80001362\n",
      "Iteration 2383, loss = 1.79994955\n",
      "Iteration 2384, loss = 1.79988557\n",
      "Iteration 2385, loss = 1.79982136\n",
      "Iteration 2386, loss = 1.79975711\n",
      "Iteration 2387, loss = 1.79969305\n",
      "Iteration 2388, loss = 1.79962907\n",
      "Iteration 2389, loss = 1.79956508\n",
      "Iteration 2390, loss = 1.79950093\n",
      "Iteration 2391, loss = 1.79943671\n",
      "Iteration 2392, loss = 1.79937278\n",
      "Iteration 2393, loss = 1.79930880\n",
      "Iteration 2394, loss = 1.79924471\n",
      "Iteration 2395, loss = 1.79918043\n",
      "Iteration 2396, loss = 1.79911657\n",
      "Iteration 2397, loss = 1.79905272\n",
      "Iteration 2398, loss = 1.79898884\n",
      "Iteration 2399, loss = 1.79892476\n",
      "Iteration 2400, loss = 1.79886073\n",
      "Iteration 2401, loss = 1.79879659\n",
      "Iteration 2402, loss = 1.79873263\n",
      "Iteration 2403, loss = 1.79866866\n",
      "Iteration 2404, loss = 1.79860504\n",
      "Iteration 2405, loss = 1.79854058\n",
      "Iteration 2406, loss = 1.79847673\n",
      "Iteration 2407, loss = 1.79841263\n",
      "Iteration 2408, loss = 1.79834858\n",
      "Iteration 2409, loss = 1.79828468\n",
      "Iteration 2410, loss = 1.79822053\n",
      "Iteration 2411, loss = 1.79815657\n",
      "Iteration 2412, loss = 1.79809261\n",
      "Iteration 2413, loss = 1.79802919\n",
      "Iteration 2414, loss = 1.79796521\n",
      "Iteration 2415, loss = 1.79790155\n",
      "Iteration 2416, loss = 1.79783719\n",
      "Iteration 2417, loss = 1.79777259\n",
      "Iteration 2418, loss = 1.79770916\n",
      "Iteration 2419, loss = 1.79764486\n",
      "Iteration 2420, loss = 1.79758118\n",
      "Iteration 2421, loss = 1.79751679\n",
      "Iteration 2422, loss = 1.79745302\n",
      "Iteration 2423, loss = 1.79738903\n",
      "Iteration 2424, loss = 1.79732507\n",
      "Iteration 2425, loss = 1.79726089\n",
      "Iteration 2426, loss = 1.79719682\n",
      "Iteration 2427, loss = 1.79713302\n",
      "Iteration 2428, loss = 1.79706926\n",
      "Iteration 2429, loss = 1.79700521\n",
      "Iteration 2430, loss = 1.79694114\n",
      "Iteration 2431, loss = 1.79687727\n",
      "Iteration 2432, loss = 1.79681350\n",
      "Iteration 2433, loss = 1.79674927\n",
      "Iteration 2434, loss = 1.79668547\n",
      "Iteration 2435, loss = 1.79662159\n",
      "Iteration 2436, loss = 1.79655772\n",
      "Iteration 2437, loss = 1.79649368\n",
      "Iteration 2438, loss = 1.79642969\n",
      "Iteration 2439, loss = 1.79636632\n",
      "Iteration 2440, loss = 1.79630261\n",
      "Iteration 2441, loss = 1.79623882\n",
      "Iteration 2442, loss = 1.79617465\n",
      "Iteration 2443, loss = 1.79611048\n",
      "Iteration 2444, loss = 1.79604664\n",
      "Iteration 2445, loss = 1.79598265\n",
      "Iteration 2446, loss = 1.79591878\n",
      "Iteration 2447, loss = 1.79585523\n",
      "Iteration 2448, loss = 1.79579101\n",
      "Iteration 2449, loss = 1.79572770\n",
      "Iteration 2450, loss = 1.79566398\n",
      "Iteration 2451, loss = 1.79559929\n",
      "Iteration 2452, loss = 1.79553655\n",
      "Iteration 2453, loss = 1.79547285\n",
      "Iteration 2454, loss = 1.79540799\n",
      "Iteration 2455, loss = 1.79534479\n",
      "Iteration 2456, loss = 1.79528077\n",
      "Iteration 2457, loss = 1.79521605\n",
      "Iteration 2458, loss = 1.79515312\n",
      "Iteration 2459, loss = 1.79508884\n",
      "Iteration 2460, loss = 1.79502524\n",
      "Iteration 2461, loss = 1.79496159\n",
      "Iteration 2462, loss = 1.79489719\n",
      "Iteration 2463, loss = 1.79483303\n",
      "Iteration 2464, loss = 1.79476951\n",
      "Iteration 2465, loss = 1.79470589\n",
      "Iteration 2466, loss = 1.79464223\n",
      "Iteration 2467, loss = 1.79457841\n",
      "Iteration 2468, loss = 1.79451429\n",
      "Iteration 2469, loss = 1.79445013\n",
      "Iteration 2470, loss = 1.79438608\n",
      "Iteration 2471, loss = 1.79432255\n",
      "Iteration 2472, loss = 1.79425869\n",
      "Iteration 2473, loss = 1.79419485\n",
      "Iteration 2474, loss = 1.79413104\n",
      "Iteration 2475, loss = 1.79406710\n",
      "Iteration 2476, loss = 1.79400329\n",
      "Iteration 2477, loss = 1.79393966\n",
      "Iteration 2478, loss = 1.79387597\n",
      "Iteration 2479, loss = 1.79381220\n",
      "Iteration 2480, loss = 1.79374812\n",
      "Iteration 2481, loss = 1.79368475\n",
      "Iteration 2482, loss = 1.79362014\n",
      "Iteration 2483, loss = 1.79355722\n",
      "Iteration 2484, loss = 1.79349262\n",
      "Iteration 2485, loss = 1.79342889\n",
      "Iteration 2486, loss = 1.79336497\n",
      "Iteration 2487, loss = 1.79330106\n",
      "Iteration 2488, loss = 1.79323722\n",
      "Iteration 2489, loss = 1.79317443\n",
      "Iteration 2490, loss = 1.79310987\n",
      "Iteration 2491, loss = 1.79304600\n",
      "Iteration 2492, loss = 1.79298227\n",
      "Iteration 2493, loss = 1.79291885\n",
      "Iteration 2494, loss = 1.79285525\n",
      "Iteration 2495, loss = 1.79279102\n",
      "Iteration 2496, loss = 1.79272729\n",
      "Iteration 2497, loss = 1.79266339\n",
      "Iteration 2498, loss = 1.79260009\n",
      "Iteration 2499, loss = 1.79253640\n",
      "Iteration 2500, loss = 1.79247211\n",
      "Iteration 2501, loss = 1.79240930\n",
      "Iteration 2502, loss = 1.79234554\n",
      "Iteration 2503, loss = 1.79228080\n",
      "Iteration 2504, loss = 1.79221826\n",
      "Iteration 2505, loss = 1.79215360\n",
      "Iteration 2506, loss = 1.79209029\n",
      "Iteration 2507, loss = 1.79202685\n",
      "Iteration 2508, loss = 1.79196208\n",
      "Iteration 2509, loss = 1.79189958\n",
      "Iteration 2510, loss = 1.79183702\n",
      "Iteration 2511, loss = 1.79177157\n",
      "Iteration 2512, loss = 1.79170720\n",
      "Iteration 2513, loss = 1.79164432\n",
      "Iteration 2514, loss = 1.79158007\n",
      "Iteration 2515, loss = 1.79151650\n",
      "Iteration 2516, loss = 1.79145328\n",
      "Iteration 2517, loss = 1.79138876\n",
      "Iteration 2518, loss = 1.79132523\n",
      "Iteration 2519, loss = 1.79126152\n",
      "Iteration 2520, loss = 1.79119732\n",
      "Iteration 2521, loss = 1.79113409\n",
      "Iteration 2522, loss = 1.79107051\n",
      "Iteration 2523, loss = 1.79100659\n",
      "Iteration 2524, loss = 1.79094246\n",
      "Iteration 2525, loss = 1.79087862\n",
      "Iteration 2526, loss = 1.79081545\n",
      "Iteration 2527, loss = 1.79075160\n",
      "Iteration 2528, loss = 1.79068791\n",
      "Iteration 2529, loss = 1.79062423\n",
      "Iteration 2530, loss = 1.79056009\n",
      "Iteration 2531, loss = 1.79049647\n",
      "Iteration 2532, loss = 1.79043282\n",
      "Iteration 2533, loss = 1.79036930\n",
      "Iteration 2534, loss = 1.79030556\n",
      "Iteration 2535, loss = 1.79024170\n",
      "Iteration 2536, loss = 1.79017826\n",
      "Iteration 2537, loss = 1.79011395\n",
      "Iteration 2538, loss = 1.79005028\n",
      "Iteration 2539, loss = 1.78998665\n",
      "Iteration 2540, loss = 1.78992288\n",
      "Iteration 2541, loss = 1.78985888\n",
      "Iteration 2542, loss = 1.78979549\n",
      "Iteration 2543, loss = 1.78973223\n",
      "Iteration 2544, loss = 1.78966868\n",
      "Iteration 2545, loss = 1.78960460\n",
      "Iteration 2546, loss = 1.78954119\n",
      "Iteration 2547, loss = 1.78947751\n",
      "Iteration 2548, loss = 1.78941372\n",
      "Iteration 2549, loss = 1.78934968\n",
      "Iteration 2550, loss = 1.78928589\n",
      "Iteration 2551, loss = 1.78922237\n",
      "Iteration 2552, loss = 1.78915874\n",
      "Iteration 2553, loss = 1.78909508\n",
      "Iteration 2554, loss = 1.78903140\n",
      "Iteration 2555, loss = 1.78896750\n",
      "Iteration 2556, loss = 1.78890399\n",
      "Iteration 2557, loss = 1.78884018\n",
      "Iteration 2558, loss = 1.78877657\n",
      "Iteration 2559, loss = 1.78871318\n",
      "Iteration 2560, loss = 1.78864970\n",
      "Iteration 2561, loss = 1.78858587\n",
      "Iteration 2562, loss = 1.78852224\n",
      "Iteration 2563, loss = 1.78845911\n",
      "Iteration 2564, loss = 1.78839614\n",
      "Iteration 2565, loss = 1.78833170\n",
      "Iteration 2566, loss = 1.78826819\n",
      "Iteration 2567, loss = 1.78820494\n",
      "Iteration 2568, loss = 1.78814074\n",
      "Iteration 2569, loss = 1.78807722\n",
      "Iteration 2570, loss = 1.78801371\n",
      "Iteration 2571, loss = 1.78794989\n",
      "Iteration 2572, loss = 1.78788648\n",
      "Iteration 2573, loss = 1.78782271\n",
      "Iteration 2574, loss = 1.78775867\n",
      "Iteration 2575, loss = 1.78769481\n",
      "Iteration 2576, loss = 1.78763106\n",
      "Iteration 2577, loss = 1.78756736\n",
      "Iteration 2578, loss = 1.78750386\n",
      "Iteration 2579, loss = 1.78744028\n",
      "Iteration 2580, loss = 1.78737661\n",
      "Iteration 2581, loss = 1.78731308\n",
      "Iteration 2582, loss = 1.78724948\n",
      "Iteration 2583, loss = 1.78718646\n",
      "Iteration 2584, loss = 1.78712243\n",
      "Iteration 2585, loss = 1.78705881\n",
      "Iteration 2586, loss = 1.78699560\n",
      "Iteration 2587, loss = 1.78693186\n",
      "Iteration 2588, loss = 1.78686797\n",
      "Iteration 2589, loss = 1.78680447\n",
      "Iteration 2590, loss = 1.78674060\n",
      "Iteration 2591, loss = 1.78667722\n",
      "Iteration 2592, loss = 1.78661291\n",
      "Iteration 2593, loss = 1.78654993\n",
      "Iteration 2594, loss = 1.78648657\n",
      "Iteration 2595, loss = 1.78642291\n",
      "Iteration 2596, loss = 1.78635928\n",
      "Iteration 2597, loss = 1.78629564\n",
      "Iteration 2598, loss = 1.78623198\n",
      "Iteration 2599, loss = 1.78616846\n",
      "Iteration 2600, loss = 1.78610479\n",
      "Iteration 2601, loss = 1.78604090\n",
      "Iteration 2602, loss = 1.78597715\n",
      "Iteration 2603, loss = 1.78591347\n",
      "Iteration 2604, loss = 1.78584979\n",
      "Iteration 2605, loss = 1.78578635\n",
      "Iteration 2606, loss = 1.78572269\n",
      "Iteration 2607, loss = 1.78565928\n",
      "Iteration 2608, loss = 1.78559555\n",
      "Iteration 2609, loss = 1.78553237\n",
      "Iteration 2610, loss = 1.78546866\n",
      "Iteration 2611, loss = 1.78540481\n",
      "Iteration 2612, loss = 1.78534249\n",
      "Iteration 2613, loss = 1.78527809\n",
      "Iteration 2614, loss = 1.78521460\n",
      "Iteration 2615, loss = 1.78515155\n",
      "Iteration 2616, loss = 1.78508741\n",
      "Iteration 2617, loss = 1.78502451\n",
      "Iteration 2618, loss = 1.78496126\n",
      "Iteration 2619, loss = 1.78489736\n",
      "Iteration 2620, loss = 1.78483303\n",
      "Iteration 2621, loss = 1.78477031\n",
      "Iteration 2622, loss = 1.78470655\n",
      "Iteration 2623, loss = 1.78464192\n",
      "Iteration 2624, loss = 1.78457870\n",
      "Iteration 2625, loss = 1.78451511\n",
      "Iteration 2626, loss = 1.78445140\n",
      "Iteration 2627, loss = 1.78438750\n",
      "Iteration 2628, loss = 1.78432435\n",
      "Iteration 2629, loss = 1.78426087\n",
      "Iteration 2630, loss = 1.78419751\n",
      "Iteration 2631, loss = 1.78413420\n",
      "Iteration 2632, loss = 1.78407050\n",
      "Iteration 2633, loss = 1.78400666\n",
      "Iteration 2634, loss = 1.78394299\n",
      "Iteration 2635, loss = 1.78387938\n",
      "Iteration 2636, loss = 1.78381569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2637, loss = 1.78375209\n",
      "Iteration 2638, loss = 1.78368878\n",
      "Iteration 2639, loss = 1.78362485\n",
      "Iteration 2640, loss = 1.78356231\n",
      "Iteration 2641, loss = 1.78349792\n",
      "Iteration 2642, loss = 1.78343482\n",
      "Iteration 2643, loss = 1.78337084\n",
      "Iteration 2644, loss = 1.78330832\n",
      "Iteration 2645, loss = 1.78324401\n",
      "Iteration 2646, loss = 1.78318104\n",
      "Iteration 2647, loss = 1.78311774\n",
      "Iteration 2648, loss = 1.78305367\n",
      "Iteration 2649, loss = 1.78299056\n",
      "Iteration 2650, loss = 1.78292704\n",
      "Iteration 2651, loss = 1.78286275\n",
      "Iteration 2652, loss = 1.78280029\n",
      "Iteration 2653, loss = 1.78273693\n",
      "Iteration 2654, loss = 1.78267206\n",
      "Iteration 2655, loss = 1.78260966\n",
      "Iteration 2656, loss = 1.78254641\n",
      "Iteration 2657, loss = 1.78248162\n",
      "Iteration 2658, loss = 1.78241858\n",
      "Iteration 2659, loss = 1.78235522\n",
      "Iteration 2660, loss = 1.78229070\n",
      "Iteration 2661, loss = 1.78222737\n",
      "Iteration 2662, loss = 1.78216386\n",
      "Iteration 2663, loss = 1.78210023\n",
      "Iteration 2664, loss = 1.78203661\n",
      "Iteration 2665, loss = 1.78197301\n",
      "Iteration 2666, loss = 1.78190940\n",
      "Iteration 2667, loss = 1.78184603\n",
      "Iteration 2668, loss = 1.78178260\n",
      "Iteration 2669, loss = 1.78171867\n",
      "Iteration 2670, loss = 1.78165560\n",
      "Iteration 2671, loss = 1.78159183\n",
      "Iteration 2672, loss = 1.78152887\n",
      "Iteration 2673, loss = 1.78146462\n",
      "Iteration 2674, loss = 1.78140203\n",
      "Iteration 2675, loss = 1.78133788\n",
      "Iteration 2676, loss = 1.78127522\n",
      "Iteration 2677, loss = 1.78121235\n",
      "Iteration 2678, loss = 1.78114793\n",
      "Iteration 2679, loss = 1.78108400\n",
      "Iteration 2680, loss = 1.78102105\n",
      "Iteration 2681, loss = 1.78095689\n",
      "Iteration 2682, loss = 1.78089368\n",
      "Iteration 2683, loss = 1.78083036\n",
      "Iteration 2684, loss = 1.78076733\n",
      "Iteration 2685, loss = 1.78070274\n",
      "Iteration 2686, loss = 1.78063915\n",
      "Iteration 2687, loss = 1.78057594\n",
      "Iteration 2688, loss = 1.78051261\n",
      "Iteration 2689, loss = 1.78044910\n",
      "Iteration 2690, loss = 1.78038513\n",
      "Iteration 2691, loss = 1.78032173\n",
      "Iteration 2692, loss = 1.78025802\n",
      "Iteration 2693, loss = 1.78019517\n",
      "Iteration 2694, loss = 1.78013142\n",
      "Iteration 2695, loss = 1.78006737\n",
      "Iteration 2696, loss = 1.78000468\n",
      "Iteration 2697, loss = 1.77994145\n",
      "Iteration 2698, loss = 1.77987739\n",
      "Iteration 2699, loss = 1.77981443\n",
      "Iteration 2700, loss = 1.77975025\n",
      "Iteration 2701, loss = 1.77968641\n",
      "Iteration 2702, loss = 1.77962279\n",
      "Iteration 2703, loss = 1.77955908\n",
      "Iteration 2704, loss = 1.77949563\n",
      "Iteration 2705, loss = 1.77943200\n",
      "Iteration 2706, loss = 1.77936846\n",
      "Iteration 2707, loss = 1.77930516\n",
      "Iteration 2708, loss = 1.77924117\n",
      "Iteration 2709, loss = 1.77917750\n",
      "Iteration 2710, loss = 1.77911432\n",
      "Iteration 2711, loss = 1.77905051\n",
      "Iteration 2712, loss = 1.77898778\n",
      "Iteration 2713, loss = 1.77892329\n",
      "Iteration 2714, loss = 1.77885983\n",
      "Iteration 2715, loss = 1.77879585\n",
      "Iteration 2716, loss = 1.77873233\n",
      "Iteration 2717, loss = 1.77866821\n",
      "Iteration 2718, loss = 1.77860430\n",
      "Iteration 2719, loss = 1.77854051\n",
      "Iteration 2720, loss = 1.77847725\n",
      "Iteration 2721, loss = 1.77841303\n",
      "Iteration 2722, loss = 1.77835004\n",
      "Iteration 2723, loss = 1.77828560\n",
      "Iteration 2724, loss = 1.77822240\n",
      "Iteration 2725, loss = 1.77815853\n",
      "Iteration 2726, loss = 1.77809405\n",
      "Iteration 2727, loss = 1.77803144\n",
      "Iteration 2728, loss = 1.77796729\n",
      "Iteration 2729, loss = 1.77790288\n",
      "Iteration 2730, loss = 1.77783943\n",
      "Iteration 2731, loss = 1.77777612\n",
      "Iteration 2732, loss = 1.77771101\n",
      "Iteration 2733, loss = 1.77764848\n",
      "Iteration 2734, loss = 1.77758495\n",
      "Iteration 2735, loss = 1.77751947\n",
      "Iteration 2736, loss = 1.77745641\n",
      "Iteration 2737, loss = 1.77739277\n",
      "Iteration 2738, loss = 1.77732793\n",
      "Iteration 2739, loss = 1.77726445\n",
      "Iteration 2740, loss = 1.77720051\n",
      "Iteration 2741, loss = 1.77713642\n",
      "Iteration 2742, loss = 1.77707268\n",
      "Iteration 2743, loss = 1.77700859\n",
      "Iteration 2744, loss = 1.77694449\n",
      "Iteration 2745, loss = 1.77688080\n",
      "Iteration 2746, loss = 1.77681680\n",
      "Iteration 2747, loss = 1.77675264\n",
      "Iteration 2748, loss = 1.77668895\n",
      "Iteration 2749, loss = 1.77662517\n",
      "Iteration 2750, loss = 1.77656140\n",
      "Iteration 2751, loss = 1.77649729\n",
      "Iteration 2752, loss = 1.77643340\n",
      "Iteration 2753, loss = 1.77636958\n",
      "Iteration 2754, loss = 1.77630563\n",
      "Iteration 2755, loss = 1.77624214\n",
      "Iteration 2756, loss = 1.77617801\n",
      "Iteration 2757, loss = 1.77611403\n",
      "Iteration 2758, loss = 1.77605012\n",
      "Iteration 2759, loss = 1.77598631\n",
      "Iteration 2760, loss = 1.77592229\n",
      "Iteration 2761, loss = 1.77585835\n",
      "Iteration 2762, loss = 1.77579439\n",
      "Iteration 2763, loss = 1.77573081\n",
      "Iteration 2764, loss = 1.77566652\n",
      "Iteration 2765, loss = 1.77560289\n",
      "Iteration 2766, loss = 1.77553881\n",
      "Iteration 2767, loss = 1.77547502\n",
      "Iteration 2768, loss = 1.77541100\n",
      "Iteration 2769, loss = 1.77534724\n",
      "Iteration 2770, loss = 1.77528340\n",
      "Iteration 2771, loss = 1.77521950\n",
      "Iteration 2772, loss = 1.77515552\n",
      "Iteration 2773, loss = 1.77509166\n",
      "Iteration 2774, loss = 1.77502854\n",
      "Iteration 2775, loss = 1.77496394\n",
      "Iteration 2776, loss = 1.77490035\n",
      "Iteration 2777, loss = 1.77483747\n",
      "Iteration 2778, loss = 1.77477263\n",
      "Iteration 2779, loss = 1.77470823\n",
      "Iteration 2780, loss = 1.77464538\n",
      "Iteration 2781, loss = 1.77458065\n",
      "Iteration 2782, loss = 1.77451674\n",
      "Iteration 2783, loss = 1.77445314\n",
      "Iteration 2784, loss = 1.77438916\n",
      "Iteration 2785, loss = 1.77432446\n",
      "Iteration 2786, loss = 1.77426084\n",
      "Iteration 2787, loss = 1.77419680\n",
      "Iteration 2788, loss = 1.77413309\n",
      "Iteration 2789, loss = 1.77406916\n",
      "Iteration 2790, loss = 1.77400514\n",
      "Iteration 2791, loss = 1.77394127\n",
      "Iteration 2792, loss = 1.77387728\n",
      "Iteration 2793, loss = 1.77381374\n",
      "Iteration 2794, loss = 1.77375005\n",
      "Iteration 2795, loss = 1.77368630\n",
      "Iteration 2796, loss = 1.77362204\n",
      "Iteration 2797, loss = 1.77355783\n",
      "Iteration 2798, loss = 1.77349351\n",
      "Iteration 2799, loss = 1.77342994\n",
      "Iteration 2800, loss = 1.77336644\n",
      "Iteration 2801, loss = 1.77330234\n",
      "Iteration 2802, loss = 1.77323839\n",
      "Iteration 2803, loss = 1.77317427\n",
      "Iteration 2804, loss = 1.77311099\n",
      "Iteration 2805, loss = 1.77304611\n",
      "Iteration 2806, loss = 1.77298372\n",
      "Iteration 2807, loss = 1.77291907\n",
      "Iteration 2808, loss = 1.77285537\n",
      "Iteration 2809, loss = 1.77279202\n",
      "Iteration 2810, loss = 1.77272745\n",
      "Iteration 2811, loss = 1.77266367\n",
      "Iteration 2812, loss = 1.77259961\n",
      "Iteration 2813, loss = 1.77253557\n",
      "Iteration 2814, loss = 1.77247251\n",
      "Iteration 2815, loss = 1.77240839\n",
      "Iteration 2816, loss = 1.77234407\n",
      "Iteration 2817, loss = 1.77228015\n",
      "Iteration 2818, loss = 1.77221558\n",
      "Iteration 2819, loss = 1.77215194\n",
      "Iteration 2820, loss = 1.77208816\n",
      "Iteration 2821, loss = 1.77202429\n",
      "Iteration 2822, loss = 1.77196069\n",
      "Iteration 2823, loss = 1.77189625\n",
      "Iteration 2824, loss = 1.77183279\n",
      "Iteration 2825, loss = 1.77176862\n",
      "Iteration 2826, loss = 1.77170441\n",
      "Iteration 2827, loss = 1.77164166\n",
      "Iteration 2828, loss = 1.77157676\n",
      "Iteration 2829, loss = 1.77151284\n",
      "Iteration 2830, loss = 1.77144936\n",
      "Iteration 2831, loss = 1.77138512\n",
      "Iteration 2832, loss = 1.77132080\n",
      "Iteration 2833, loss = 1.77125731\n",
      "Iteration 2834, loss = 1.77119321\n",
      "Iteration 2835, loss = 1.77112883\n",
      "Iteration 2836, loss = 1.77106517\n",
      "Iteration 2837, loss = 1.77100070\n",
      "Iteration 2838, loss = 1.77093721\n",
      "Iteration 2839, loss = 1.77087294\n",
      "Iteration 2840, loss = 1.77080896\n",
      "Iteration 2841, loss = 1.77074475\n",
      "Iteration 2842, loss = 1.77068057\n",
      "Iteration 2843, loss = 1.77061743\n",
      "Iteration 2844, loss = 1.77055330\n",
      "Iteration 2845, loss = 1.77048982\n",
      "Iteration 2846, loss = 1.77042557\n",
      "Iteration 2847, loss = 1.77036159\n",
      "Iteration 2848, loss = 1.77029753\n",
      "Iteration 2849, loss = 1.77023335\n",
      "Iteration 2850, loss = 1.77016921\n",
      "Iteration 2851, loss = 1.77010514\n",
      "Iteration 2852, loss = 1.77004151\n",
      "Iteration 2853, loss = 1.76997772\n",
      "Iteration 2854, loss = 1.76991374\n",
      "Iteration 2855, loss = 1.76984960\n",
      "Iteration 2856, loss = 1.76978527\n",
      "Iteration 2857, loss = 1.76972166\n",
      "Iteration 2858, loss = 1.76965770\n",
      "Iteration 2859, loss = 1.76959345\n",
      "Iteration 2860, loss = 1.76952984\n",
      "Iteration 2861, loss = 1.76946552\n",
      "Iteration 2862, loss = 1.76940158\n",
      "Iteration 2863, loss = 1.76933807\n",
      "Iteration 2864, loss = 1.76927374\n",
      "Iteration 2865, loss = 1.76921024\n",
      "Iteration 2866, loss = 1.76914594\n",
      "Iteration 2867, loss = 1.76908194\n",
      "Iteration 2868, loss = 1.76901777\n",
      "Iteration 2869, loss = 1.76895405\n",
      "Iteration 2870, loss = 1.76889019\n",
      "Iteration 2871, loss = 1.76882653\n",
      "Iteration 2872, loss = 1.76876227\n",
      "Iteration 2873, loss = 1.76869792\n",
      "Iteration 2874, loss = 1.76863375\n",
      "Iteration 2875, loss = 1.76857005\n",
      "Iteration 2876, loss = 1.76850617\n",
      "Iteration 2877, loss = 1.76844237\n",
      "Iteration 2878, loss = 1.76837799\n",
      "Iteration 2879, loss = 1.76831382\n",
      "Iteration 2880, loss = 1.76824991\n",
      "Iteration 2881, loss = 1.76818615\n",
      "Iteration 2882, loss = 1.76812251\n",
      "Iteration 2883, loss = 1.76805853\n",
      "Iteration 2884, loss = 1.76799422\n",
      "Iteration 2885, loss = 1.76793010\n",
      "Iteration 2886, loss = 1.76786648\n",
      "Iteration 2887, loss = 1.76780230\n",
      "Iteration 2888, loss = 1.76773838\n",
      "Iteration 2889, loss = 1.76767391\n",
      "Iteration 2890, loss = 1.76760960\n",
      "Iteration 2891, loss = 1.76754619\n",
      "Iteration 2892, loss = 1.76748217\n",
      "Iteration 2893, loss = 1.76741826\n",
      "Iteration 2894, loss = 1.76735381\n",
      "Iteration 2895, loss = 1.76729034\n",
      "Iteration 2896, loss = 1.76722586\n",
      "Iteration 2897, loss = 1.76716261\n",
      "Iteration 2898, loss = 1.76709883\n",
      "Iteration 2899, loss = 1.76703434\n",
      "Iteration 2900, loss = 1.76697004\n",
      "Iteration 2901, loss = 1.76690639\n",
      "Iteration 2902, loss = 1.76684170\n",
      "Iteration 2903, loss = 1.76677857\n",
      "Iteration 2904, loss = 1.76671463\n",
      "Iteration 2905, loss = 1.76664976\n",
      "Iteration 2906, loss = 1.76658583\n",
      "Iteration 2907, loss = 1.76652127\n",
      "Iteration 2908, loss = 1.76645908\n",
      "Iteration 2909, loss = 1.76639466\n",
      "Iteration 2910, loss = 1.76632942\n",
      "Iteration 2911, loss = 1.76626653\n",
      "Iteration 2912, loss = 1.76620177\n",
      "Iteration 2913, loss = 1.76613789\n",
      "Iteration 2914, loss = 1.76607396\n",
      "Iteration 2915, loss = 1.76600973\n",
      "Iteration 2916, loss = 1.76594578\n",
      "Iteration 2917, loss = 1.76588149\n",
      "Iteration 2918, loss = 1.76581719\n",
      "Iteration 2919, loss = 1.76575356\n",
      "Iteration 2920, loss = 1.76568938\n",
      "Iteration 2921, loss = 1.76562451\n",
      "Iteration 2922, loss = 1.76556050\n",
      "Iteration 2923, loss = 1.76549649\n",
      "Iteration 2924, loss = 1.76543283\n",
      "Iteration 2925, loss = 1.76536847\n",
      "Iteration 2926, loss = 1.76530492\n",
      "Iteration 2927, loss = 1.76524074\n",
      "Iteration 2928, loss = 1.76517593\n",
      "Iteration 2929, loss = 1.76511196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2930, loss = 1.76504779\n",
      "Iteration 2931, loss = 1.76498373\n",
      "Iteration 2932, loss = 1.76491971\n",
      "Iteration 2933, loss = 1.76485537\n",
      "Iteration 2934, loss = 1.76479163\n",
      "Iteration 2935, loss = 1.76472726\n",
      "Iteration 2936, loss = 1.76466444\n",
      "Iteration 2937, loss = 1.76459964\n",
      "Iteration 2938, loss = 1.76453598\n",
      "Iteration 2939, loss = 1.76447206\n",
      "Iteration 2940, loss = 1.76440741\n",
      "Iteration 2941, loss = 1.76434302\n",
      "Iteration 2942, loss = 1.76427881\n",
      "Iteration 2943, loss = 1.76421448\n",
      "Iteration 2944, loss = 1.76415023\n",
      "Iteration 2945, loss = 1.76408644\n",
      "Iteration 2946, loss = 1.76402230\n",
      "Iteration 2947, loss = 1.76395813\n",
      "Iteration 2948, loss = 1.76389357\n",
      "Iteration 2949, loss = 1.76382925\n",
      "Iteration 2950, loss = 1.76376536\n",
      "Iteration 2951, loss = 1.76370152\n",
      "Iteration 2952, loss = 1.76363717\n",
      "Iteration 2953, loss = 1.76357268\n",
      "Iteration 2954, loss = 1.76350844\n",
      "Iteration 2955, loss = 1.76344419\n",
      "Iteration 2956, loss = 1.76338036\n",
      "Iteration 2957, loss = 1.76331715\n",
      "Iteration 2958, loss = 1.76325400\n",
      "Iteration 2959, loss = 1.76318796\n",
      "Iteration 2960, loss = 1.76312413\n",
      "Iteration 2961, loss = 1.76305958\n",
      "Iteration 2962, loss = 1.76299594\n",
      "Iteration 2963, loss = 1.76293235\n",
      "Iteration 2964, loss = 1.76286819\n",
      "Iteration 2965, loss = 1.76280345\n",
      "Iteration 2966, loss = 1.76273864\n",
      "Iteration 2967, loss = 1.76267480\n",
      "Iteration 2968, loss = 1.76261113\n",
      "Iteration 2969, loss = 1.76254708\n",
      "Iteration 2970, loss = 1.76248265\n",
      "Iteration 2971, loss = 1.76241849\n",
      "Iteration 2972, loss = 1.76235417\n",
      "Iteration 2973, loss = 1.76229042\n",
      "Iteration 2974, loss = 1.76222599\n",
      "Iteration 2975, loss = 1.76216173\n",
      "Iteration 2976, loss = 1.76209760\n",
      "Iteration 2977, loss = 1.76203356\n",
      "Iteration 2978, loss = 1.76196943\n",
      "Iteration 2979, loss = 1.76190528\n",
      "Iteration 2980, loss = 1.76184115\n",
      "Iteration 2981, loss = 1.76177780\n",
      "Iteration 2982, loss = 1.76171328\n",
      "Iteration 2983, loss = 1.76165021\n",
      "Iteration 2984, loss = 1.76158538\n",
      "Iteration 2985, loss = 1.76152186\n",
      "Iteration 2986, loss = 1.76145824\n",
      "Iteration 2987, loss = 1.76139241\n",
      "Iteration 2988, loss = 1.76132936\n",
      "Iteration 2989, loss = 1.76126542\n",
      "Iteration 2990, loss = 1.76120071\n",
      "Iteration 2991, loss = 1.76113660\n",
      "Iteration 2992, loss = 1.76107289\n",
      "Iteration 2993, loss = 1.76100837\n",
      "Iteration 2994, loss = 1.76094448\n",
      "Iteration 2995, loss = 1.76088053\n",
      "Iteration 2996, loss = 1.76081576\n",
      "Iteration 2997, loss = 1.76075168\n",
      "Iteration 2998, loss = 1.76068836\n",
      "Iteration 2999, loss = 1.76062291\n",
      "Iteration 3000, loss = 1.76055970\n",
      "Iteration 3001, loss = 1.76049477\n",
      "Iteration 3002, loss = 1.76043054\n",
      "Iteration 3003, loss = 1.76036687\n",
      "Iteration 3004, loss = 1.76030247\n",
      "Iteration 3005, loss = 1.76023789\n",
      "Iteration 3006, loss = 1.76017428\n",
      "Iteration 3007, loss = 1.76011005\n",
      "Iteration 3008, loss = 1.76004631\n",
      "Iteration 3009, loss = 1.75998171\n",
      "Iteration 3010, loss = 1.75991727\n",
      "Iteration 3011, loss = 1.75985374\n",
      "Iteration 3012, loss = 1.75978983\n",
      "Iteration 3013, loss = 1.75972639\n",
      "Iteration 3014, loss = 1.75966167\n",
      "Iteration 3015, loss = 1.75959704\n",
      "Iteration 3016, loss = 1.75953301\n",
      "Iteration 3017, loss = 1.75946914\n",
      "Iteration 3018, loss = 1.75940481\n",
      "Iteration 3019, loss = 1.75934082\n",
      "Iteration 3020, loss = 1.75927644\n",
      "Iteration 3021, loss = 1.75921247\n",
      "Iteration 3022, loss = 1.75914831\n",
      "Iteration 3023, loss = 1.75908412\n",
      "Iteration 3024, loss = 1.75901998\n",
      "Iteration 3025, loss = 1.75895610\n",
      "Iteration 3026, loss = 1.75889169\n",
      "Iteration 3027, loss = 1.75882809\n",
      "Iteration 3028, loss = 1.75876390\n",
      "Iteration 3029, loss = 1.75869969\n",
      "Iteration 3030, loss = 1.75863710\n",
      "Iteration 3031, loss = 1.75857179\n",
      "Iteration 3032, loss = 1.75850730\n",
      "Iteration 3033, loss = 1.75844505\n",
      "Iteration 3034, loss = 1.75837902\n",
      "Iteration 3035, loss = 1.75831456\n",
      "Iteration 3036, loss = 1.75825102\n",
      "Iteration 3037, loss = 1.75818633\n",
      "Iteration 3038, loss = 1.75812238\n",
      "Iteration 3039, loss = 1.75805798\n",
      "Iteration 3040, loss = 1.75799332\n",
      "Iteration 3041, loss = 1.75792896\n",
      "Iteration 3042, loss = 1.75786516\n",
      "Iteration 3043, loss = 1.75780059\n",
      "Iteration 3044, loss = 1.75773674\n",
      "Iteration 3045, loss = 1.75767170\n",
      "Iteration 3046, loss = 1.75760765\n",
      "Iteration 3047, loss = 1.75754363\n",
      "Iteration 3048, loss = 1.75747986\n",
      "Iteration 3049, loss = 1.75741526\n",
      "Iteration 3050, loss = 1.75735098\n",
      "Iteration 3051, loss = 1.75728668\n",
      "Iteration 3052, loss = 1.75722296\n",
      "Iteration 3053, loss = 1.75715847\n",
      "Iteration 3054, loss = 1.75709485\n",
      "Iteration 3055, loss = 1.75702981\n",
      "Iteration 3056, loss = 1.75696665\n",
      "Iteration 3057, loss = 1.75690188\n",
      "Iteration 3058, loss = 1.75683806\n",
      "Iteration 3059, loss = 1.75677312\n",
      "Iteration 3060, loss = 1.75670901\n",
      "Iteration 3061, loss = 1.75664462\n",
      "Iteration 3062, loss = 1.75658046\n",
      "Iteration 3063, loss = 1.75651609\n",
      "Iteration 3064, loss = 1.75645158\n",
      "Iteration 3065, loss = 1.75638762\n",
      "Iteration 3066, loss = 1.75632337\n",
      "Iteration 3067, loss = 1.75625903\n",
      "Iteration 3068, loss = 1.75619512\n",
      "Iteration 3069, loss = 1.75613076\n",
      "Iteration 3070, loss = 1.75606685\n",
      "Iteration 3071, loss = 1.75600196\n",
      "Iteration 3072, loss = 1.75593785\n",
      "Iteration 3073, loss = 1.75587349\n",
      "Iteration 3074, loss = 1.75580935\n",
      "Iteration 3075, loss = 1.75574479\n",
      "Iteration 3076, loss = 1.75568084\n",
      "Iteration 3077, loss = 1.75561636\n",
      "Iteration 3078, loss = 1.75555235\n",
      "Iteration 3079, loss = 1.75548787\n",
      "Iteration 3080, loss = 1.75542309\n",
      "Iteration 3081, loss = 1.75535881\n",
      "Iteration 3082, loss = 1.75529489\n",
      "Iteration 3083, loss = 1.75523287\n",
      "Iteration 3084, loss = 1.75516849\n",
      "Iteration 3085, loss = 1.75510242\n",
      "Iteration 3086, loss = 1.75504251\n",
      "Iteration 3087, loss = 1.75497649\n",
      "Iteration 3088, loss = 1.75491042\n",
      "Iteration 3089, loss = 1.75484759\n",
      "Iteration 3090, loss = 1.75478241\n",
      "Iteration 3091, loss = 1.75471781\n",
      "Iteration 3092, loss = 1.75465404\n",
      "Iteration 3093, loss = 1.75458957\n",
      "Iteration 3094, loss = 1.75452467\n",
      "Iteration 3095, loss = 1.75446019\n",
      "Iteration 3096, loss = 1.75439580\n",
      "Iteration 3097, loss = 1.75433182\n",
      "Iteration 3098, loss = 1.75426746\n",
      "Iteration 3099, loss = 1.75420276\n",
      "Iteration 3100, loss = 1.75413827\n",
      "Iteration 3101, loss = 1.75407391\n",
      "Iteration 3102, loss = 1.75401023\n",
      "Iteration 3103, loss = 1.75394531\n",
      "Iteration 3104, loss = 1.75388126\n",
      "Iteration 3105, loss = 1.75381680\n",
      "Iteration 3106, loss = 1.75375207\n",
      "Iteration 3107, loss = 1.75368786\n",
      "Iteration 3108, loss = 1.75362385\n",
      "Iteration 3109, loss = 1.75355959\n",
      "Iteration 3110, loss = 1.75349495\n",
      "Iteration 3111, loss = 1.75343055\n",
      "Iteration 3112, loss = 1.75336725\n",
      "Iteration 3113, loss = 1.75330164\n",
      "Iteration 3114, loss = 1.75323832\n",
      "Iteration 3115, loss = 1.75317408\n",
      "Iteration 3116, loss = 1.75310948\n",
      "Iteration 3117, loss = 1.75304436\n",
      "Iteration 3118, loss = 1.75297976\n",
      "Iteration 3119, loss = 1.75291564\n",
      "Iteration 3120, loss = 1.75285095\n",
      "Iteration 3121, loss = 1.75278697\n",
      "Iteration 3122, loss = 1.75272424\n",
      "Iteration 3123, loss = 1.75265850\n",
      "Iteration 3124, loss = 1.75259503\n",
      "Iteration 3125, loss = 1.75252979\n",
      "Iteration 3126, loss = 1.75246638\n",
      "Iteration 3127, loss = 1.75240200\n",
      "Iteration 3128, loss = 1.75233662\n",
      "Iteration 3129, loss = 1.75227319\n",
      "Iteration 3130, loss = 1.75220818\n",
      "Iteration 3131, loss = 1.75214506\n",
      "Iteration 3132, loss = 1.75208084\n",
      "Iteration 3133, loss = 1.75201457\n",
      "Iteration 3134, loss = 1.75195219\n",
      "Iteration 3135, loss = 1.75188673\n",
      "Iteration 3136, loss = 1.75182201\n",
      "Iteration 3137, loss = 1.75175914\n",
      "Iteration 3138, loss = 1.75169291\n",
      "Iteration 3139, loss = 1.75162919\n",
      "Iteration 3140, loss = 1.75156509\n",
      "Iteration 3141, loss = 1.75149938\n",
      "Iteration 3142, loss = 1.75143698\n",
      "Iteration 3143, loss = 1.75137042\n",
      "Iteration 3144, loss = 1.75130859\n",
      "Iteration 3145, loss = 1.75124443\n",
      "Iteration 3146, loss = 1.75117746\n",
      "Iteration 3147, loss = 1.75111479\n",
      "Iteration 3148, loss = 1.75104965\n",
      "Iteration 3149, loss = 1.75098383\n",
      "Iteration 3150, loss = 1.75092084\n",
      "Iteration 3151, loss = 1.75085566\n",
      "Iteration 3152, loss = 1.75079106\n",
      "Iteration 3153, loss = 1.75072645\n",
      "Iteration 3154, loss = 1.75066184\n",
      "Iteration 3155, loss = 1.75059699\n",
      "Iteration 3156, loss = 1.75053246\n",
      "Iteration 3157, loss = 1.75046814\n",
      "Iteration 3158, loss = 1.75040516\n",
      "Iteration 3159, loss = 1.75033979\n",
      "Iteration 3160, loss = 1.75027557\n",
      "Iteration 3161, loss = 1.75021136\n",
      "Iteration 3162, loss = 1.75014557\n",
      "Iteration 3163, loss = 1.75008200\n",
      "Iteration 3164, loss = 1.75001658\n",
      "Iteration 3165, loss = 1.74995304\n",
      "Iteration 3166, loss = 1.74988805\n",
      "Iteration 3167, loss = 1.74982333\n",
      "Iteration 3168, loss = 1.74975894\n",
      "Iteration 3169, loss = 1.74969424\n",
      "Iteration 3170, loss = 1.74962999\n",
      "Iteration 3171, loss = 1.74956531\n",
      "Iteration 3172, loss = 1.74950075\n",
      "Iteration 3173, loss = 1.74943596\n",
      "Iteration 3174, loss = 1.74937155\n",
      "Iteration 3175, loss = 1.74930684\n",
      "Iteration 3176, loss = 1.74924247\n",
      "Iteration 3177, loss = 1.74917776\n",
      "Iteration 3178, loss = 1.74911334\n",
      "Iteration 3179, loss = 1.74904891\n",
      "Iteration 3180, loss = 1.74898440\n",
      "Iteration 3181, loss = 1.74892001\n",
      "Iteration 3182, loss = 1.74885524\n",
      "Iteration 3183, loss = 1.74879115\n",
      "Iteration 3184, loss = 1.74872634\n",
      "Iteration 3185, loss = 1.74866246\n",
      "Iteration 3186, loss = 1.74859784\n",
      "Iteration 3187, loss = 1.74853268\n",
      "Iteration 3188, loss = 1.74846924\n",
      "Iteration 3189, loss = 1.74840365\n",
      "Iteration 3190, loss = 1.74833913\n",
      "Iteration 3191, loss = 1.74827421\n",
      "Iteration 3192, loss = 1.74820974\n",
      "Iteration 3193, loss = 1.74814607\n",
      "Iteration 3194, loss = 1.74808057\n",
      "Iteration 3195, loss = 1.74801670\n",
      "Iteration 3196, loss = 1.74795145\n",
      "Iteration 3197, loss = 1.74788735\n",
      "Iteration 3198, loss = 1.74782252\n",
      "Iteration 3199, loss = 1.74775851\n",
      "Iteration 3200, loss = 1.74769350\n",
      "Iteration 3201, loss = 1.74762939\n",
      "Iteration 3202, loss = 1.74756495\n",
      "Iteration 3203, loss = 1.74750055\n",
      "Iteration 3204, loss = 1.74743568\n",
      "Iteration 3205, loss = 1.74737033\n",
      "Iteration 3206, loss = 1.74730610\n",
      "Iteration 3207, loss = 1.74724170\n",
      "Iteration 3208, loss = 1.74717697\n",
      "Iteration 3209, loss = 1.74711260\n",
      "Iteration 3210, loss = 1.74704766\n",
      "Iteration 3211, loss = 1.74698248\n",
      "Iteration 3212, loss = 1.74691777\n",
      "Iteration 3213, loss = 1.74685304\n",
      "Iteration 3214, loss = 1.74678832\n",
      "Iteration 3215, loss = 1.74672311\n",
      "Iteration 3216, loss = 1.74665878\n",
      "Iteration 3217, loss = 1.74659484\n",
      "Iteration 3218, loss = 1.74653006\n",
      "Iteration 3219, loss = 1.74646502\n",
      "Iteration 3220, loss = 1.74640079\n",
      "Iteration 3221, loss = 1.74633606\n",
      "Iteration 3222, loss = 1.74627040\n",
      "Iteration 3223, loss = 1.74620568\n",
      "Iteration 3224, loss = 1.74614126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3225, loss = 1.74607829\n",
      "Iteration 3226, loss = 1.74601330\n",
      "Iteration 3227, loss = 1.74594758\n",
      "Iteration 3228, loss = 1.74588467\n",
      "Iteration 3229, loss = 1.74581793\n",
      "Iteration 3230, loss = 1.74575339\n",
      "Iteration 3231, loss = 1.74568941\n",
      "Iteration 3232, loss = 1.74562447\n",
      "Iteration 3233, loss = 1.74556007\n",
      "Iteration 3234, loss = 1.74549535\n",
      "Iteration 3235, loss = 1.74543074\n",
      "Iteration 3236, loss = 1.74536548\n",
      "Iteration 3237, loss = 1.74530049\n",
      "Iteration 3238, loss = 1.74523628\n",
      "Iteration 3239, loss = 1.74517109\n",
      "Iteration 3240, loss = 1.74510748\n",
      "Iteration 3241, loss = 1.74504201\n",
      "Iteration 3242, loss = 1.74497733\n",
      "Iteration 3243, loss = 1.74491297\n",
      "Iteration 3244, loss = 1.74484855\n",
      "Iteration 3245, loss = 1.74478288\n",
      "Iteration 3246, loss = 1.74471902\n",
      "Iteration 3247, loss = 1.74465329\n",
      "Iteration 3248, loss = 1.74458863\n",
      "Iteration 3249, loss = 1.74452405\n",
      "Iteration 3250, loss = 1.74445922\n",
      "Iteration 3251, loss = 1.74439424\n",
      "Iteration 3252, loss = 1.74432922\n",
      "Iteration 3253, loss = 1.74426549\n",
      "Iteration 3254, loss = 1.74419998\n",
      "Iteration 3255, loss = 1.74413493\n",
      "Iteration 3256, loss = 1.74407038\n",
      "Iteration 3257, loss = 1.74400549\n",
      "Iteration 3258, loss = 1.74394079\n",
      "Iteration 3259, loss = 1.74387864\n",
      "Iteration 3260, loss = 1.74381260\n",
      "Iteration 3261, loss = 1.74374853\n",
      "Iteration 3262, loss = 1.74368378\n",
      "Iteration 3263, loss = 1.74361738\n",
      "Iteration 3264, loss = 1.74355361\n",
      "Iteration 3265, loss = 1.74348785\n",
      "Iteration 3266, loss = 1.74342324\n",
      "Iteration 3267, loss = 1.74335848\n",
      "Iteration 3268, loss = 1.74329374\n",
      "Iteration 3269, loss = 1.74322862\n",
      "Iteration 3270, loss = 1.74316512\n",
      "Iteration 3271, loss = 1.74309892\n",
      "Iteration 3272, loss = 1.74303584\n",
      "Iteration 3273, loss = 1.74296999\n",
      "Iteration 3274, loss = 1.74290583\n",
      "Iteration 3275, loss = 1.74284079\n",
      "Iteration 3276, loss = 1.74277596\n",
      "Iteration 3277, loss = 1.74271063\n",
      "Iteration 3278, loss = 1.74264524\n",
      "Iteration 3279, loss = 1.74258065\n",
      "Iteration 3280, loss = 1.74251562\n",
      "Iteration 3281, loss = 1.74245117\n",
      "Iteration 3282, loss = 1.74238607\n",
      "Iteration 3283, loss = 1.74232131\n",
      "Iteration 3284, loss = 1.74225666\n",
      "Iteration 3285, loss = 1.74219108\n",
      "Iteration 3286, loss = 1.74212806\n",
      "Iteration 3287, loss = 1.74206232\n",
      "Iteration 3288, loss = 1.74199715\n",
      "Iteration 3289, loss = 1.74193381\n",
      "Iteration 3290, loss = 1.74186795\n",
      "Iteration 3291, loss = 1.74180506\n",
      "Iteration 3292, loss = 1.74173800\n",
      "Iteration 3293, loss = 1.74167377\n",
      "Iteration 3294, loss = 1.74160814\n",
      "Iteration 3295, loss = 1.74154368\n",
      "Iteration 3296, loss = 1.74147878\n",
      "Iteration 3297, loss = 1.74141431\n",
      "Iteration 3298, loss = 1.74134892\n",
      "Iteration 3299, loss = 1.74128411\n",
      "Iteration 3300, loss = 1.74121863\n",
      "Iteration 3301, loss = 1.74115651\n",
      "Iteration 3302, loss = 1.74109019\n",
      "Iteration 3303, loss = 1.74102797\n",
      "Iteration 3304, loss = 1.74096332\n",
      "Iteration 3305, loss = 1.74089459\n",
      "Iteration 3306, loss = 1.74083418\n",
      "Iteration 3307, loss = 1.74076705\n",
      "Iteration 3308, loss = 1.74070224\n",
      "Iteration 3309, loss = 1.74063890\n",
      "Iteration 3310, loss = 1.74057119\n",
      "Iteration 3311, loss = 1.74050729\n",
      "Iteration 3312, loss = 1.74044373\n",
      "Iteration 3313, loss = 1.74037790\n",
      "Iteration 3314, loss = 1.74031207\n",
      "Iteration 3315, loss = 1.74024814\n",
      "Iteration 3316, loss = 1.74018369\n",
      "Iteration 3317, loss = 1.74011660\n",
      "Iteration 3318, loss = 1.74005328\n",
      "Iteration 3319, loss = 1.73998861\n",
      "Iteration 3320, loss = 1.73992130\n",
      "Iteration 3321, loss = 1.73985851\n",
      "Iteration 3322, loss = 1.73979239\n",
      "Iteration 3323, loss = 1.73972787\n",
      "Iteration 3324, loss = 1.73966369\n",
      "Iteration 3325, loss = 1.73959712\n",
      "Iteration 3326, loss = 1.73953276\n",
      "Iteration 3327, loss = 1.73946789\n",
      "Iteration 3328, loss = 1.73940217\n",
      "Iteration 3329, loss = 1.73933710\n",
      "Iteration 3330, loss = 1.73927167\n",
      "Iteration 3331, loss = 1.73920725\n",
      "Iteration 3332, loss = 1.73914276\n",
      "Iteration 3333, loss = 1.73907694\n",
      "Iteration 3334, loss = 1.73901252\n",
      "Iteration 3335, loss = 1.73894693\n",
      "Iteration 3336, loss = 1.73888183\n",
      "Iteration 3337, loss = 1.73881673\n",
      "Iteration 3338, loss = 1.73875147\n",
      "Iteration 3339, loss = 1.73868714\n",
      "Iteration 3340, loss = 1.73862150\n",
      "Iteration 3341, loss = 1.73855929\n",
      "Iteration 3342, loss = 1.73849242\n",
      "Iteration 3343, loss = 1.73842970\n",
      "Iteration 3344, loss = 1.73836336\n",
      "Iteration 3345, loss = 1.73829804\n",
      "Iteration 3346, loss = 1.73823330\n",
      "Iteration 3347, loss = 1.73816719\n",
      "Iteration 3348, loss = 1.73810341\n",
      "Iteration 3349, loss = 1.73803751\n",
      "Iteration 3350, loss = 1.73797182\n",
      "Iteration 3351, loss = 1.73790694\n",
      "Iteration 3352, loss = 1.73784194\n",
      "Iteration 3353, loss = 1.73777526\n",
      "Iteration 3354, loss = 1.73771095\n",
      "Iteration 3355, loss = 1.73764392\n",
      "Iteration 3356, loss = 1.73757877\n",
      "Iteration 3357, loss = 1.73751330\n",
      "Iteration 3358, loss = 1.73744817\n",
      "Iteration 3359, loss = 1.73738384\n",
      "Iteration 3360, loss = 1.73731836\n",
      "Iteration 3361, loss = 1.73725114\n",
      "Iteration 3362, loss = 1.73718620\n",
      "Iteration 3363, loss = 1.73712205\n",
      "Iteration 3364, loss = 1.73705624\n",
      "Iteration 3365, loss = 1.73699057\n",
      "Iteration 3366, loss = 1.73692625\n",
      "Iteration 3367, loss = 1.73685954\n",
      "Iteration 3368, loss = 1.73679463\n",
      "Iteration 3369, loss = 1.73672850\n",
      "Iteration 3370, loss = 1.73666347\n",
      "Iteration 3371, loss = 1.73659790\n",
      "Iteration 3372, loss = 1.73653241\n",
      "Iteration 3373, loss = 1.73646710\n",
      "Iteration 3374, loss = 1.73640169\n",
      "Iteration 3375, loss = 1.73633569\n",
      "Iteration 3376, loss = 1.73627090\n",
      "Iteration 3377, loss = 1.73620610\n",
      "Iteration 3378, loss = 1.73614062\n",
      "Iteration 3379, loss = 1.73607514\n",
      "Iteration 3380, loss = 1.73600997\n",
      "Iteration 3381, loss = 1.73594389\n",
      "Iteration 3382, loss = 1.73587847\n",
      "Iteration 3383, loss = 1.73581367\n",
      "Iteration 3384, loss = 1.73574899\n",
      "Iteration 3385, loss = 1.73568297\n",
      "Iteration 3386, loss = 1.73561758\n",
      "Iteration 3387, loss = 1.73555310\n",
      "Iteration 3388, loss = 1.73548771\n",
      "Iteration 3389, loss = 1.73542125\n",
      "Iteration 3390, loss = 1.73535819\n",
      "Iteration 3391, loss = 1.73529070\n",
      "Iteration 3392, loss = 1.73522688\n",
      "Iteration 3393, loss = 1.73516110\n",
      "Iteration 3394, loss = 1.73509437\n",
      "Iteration 3395, loss = 1.73502904\n",
      "Iteration 3396, loss = 1.73496362\n",
      "Iteration 3397, loss = 1.73489716\n",
      "Iteration 3398, loss = 1.73483292\n",
      "Iteration 3399, loss = 1.73476605\n",
      "Iteration 3400, loss = 1.73470285\n",
      "Iteration 3401, loss = 1.73463588\n",
      "Iteration 3402, loss = 1.73457021\n",
      "Iteration 3403, loss = 1.73450619\n",
      "Iteration 3404, loss = 1.73443943\n",
      "Iteration 3405, loss = 1.73437503\n",
      "Iteration 3406, loss = 1.73430819\n",
      "Iteration 3407, loss = 1.73424373\n",
      "Iteration 3408, loss = 1.73417693\n",
      "Iteration 3409, loss = 1.73411128\n",
      "Iteration 3410, loss = 1.73404640\n",
      "Iteration 3411, loss = 1.73398058\n",
      "Iteration 3412, loss = 1.73391727\n",
      "Iteration 3413, loss = 1.73384945\n",
      "Iteration 3414, loss = 1.73378654\n",
      "Iteration 3415, loss = 1.73371913\n",
      "Iteration 3416, loss = 1.73365410\n",
      "Iteration 3417, loss = 1.73358780\n",
      "Iteration 3418, loss = 1.73352309\n",
      "Iteration 3419, loss = 1.73345711\n",
      "Iteration 3420, loss = 1.73339149\n",
      "Iteration 3421, loss = 1.73332728\n",
      "Iteration 3422, loss = 1.73326111\n",
      "Iteration 3423, loss = 1.73319480\n",
      "Iteration 3424, loss = 1.73313232\n",
      "Iteration 3425, loss = 1.73306323\n",
      "Iteration 3426, loss = 1.73300152\n",
      "Iteration 3427, loss = 1.73293456\n",
      "Iteration 3428, loss = 1.73286904\n",
      "Iteration 3429, loss = 1.73280517\n",
      "Iteration 3430, loss = 1.73273735\n",
      "Iteration 3431, loss = 1.73267392\n",
      "Iteration 3432, loss = 1.73260865\n",
      "Iteration 3433, loss = 1.73254191\n",
      "Iteration 3434, loss = 1.73247624\n",
      "Iteration 3435, loss = 1.73241117\n",
      "Iteration 3436, loss = 1.73234524\n",
      "Iteration 3437, loss = 1.73227824\n",
      "Iteration 3438, loss = 1.73221445\n",
      "Iteration 3439, loss = 1.73214755\n",
      "Iteration 3440, loss = 1.73208198\n",
      "Iteration 3441, loss = 1.73201772\n",
      "Iteration 3442, loss = 1.73195072\n",
      "Iteration 3443, loss = 1.73188787\n",
      "Iteration 3444, loss = 1.73181949\n",
      "Iteration 3445, loss = 1.73175602\n",
      "Iteration 3446, loss = 1.73169031\n",
      "Iteration 3447, loss = 1.73162269\n",
      "Iteration 3448, loss = 1.73155893\n",
      "Iteration 3449, loss = 1.73149309\n",
      "Iteration 3450, loss = 1.73142670\n",
      "Iteration 3451, loss = 1.73136172\n",
      "Iteration 3452, loss = 1.73129684\n",
      "Iteration 3453, loss = 1.73122999\n",
      "Iteration 3454, loss = 1.73116294\n",
      "Iteration 3455, loss = 1.73109852\n",
      "Iteration 3456, loss = 1.73103076\n",
      "Iteration 3457, loss = 1.73096813\n",
      "Iteration 3458, loss = 1.73090134\n",
      "Iteration 3459, loss = 1.73083708\n",
      "Iteration 3460, loss = 1.73077164\n",
      "Iteration 3461, loss = 1.73070285\n",
      "Iteration 3462, loss = 1.73063812\n",
      "Iteration 3463, loss = 1.73057223\n",
      "Iteration 3464, loss = 1.73050771\n",
      "Iteration 3465, loss = 1.73044182\n",
      "Iteration 3466, loss = 1.73037545\n",
      "Iteration 3467, loss = 1.73030885\n",
      "Iteration 3468, loss = 1.73024296\n",
      "Iteration 3469, loss = 1.73017793\n",
      "Iteration 3470, loss = 1.73011220\n",
      "Iteration 3471, loss = 1.73004617\n",
      "Iteration 3472, loss = 1.72998089\n",
      "Iteration 3473, loss = 1.72991533\n",
      "Iteration 3474, loss = 1.72984984\n",
      "Iteration 3475, loss = 1.72978292\n",
      "Iteration 3476, loss = 1.72971729\n",
      "Iteration 3477, loss = 1.72965175\n",
      "Iteration 3478, loss = 1.72958583\n",
      "Iteration 3479, loss = 1.72952037\n",
      "Iteration 3480, loss = 1.72945500\n",
      "Iteration 3481, loss = 1.72938891\n",
      "Iteration 3482, loss = 1.72932280\n",
      "Iteration 3483, loss = 1.72925853\n",
      "Iteration 3484, loss = 1.72919383\n",
      "Iteration 3485, loss = 1.72912894\n",
      "Iteration 3486, loss = 1.72906064\n",
      "Iteration 3487, loss = 1.72899844\n",
      "Iteration 3488, loss = 1.72893099\n",
      "Iteration 3489, loss = 1.72886526\n",
      "Iteration 3490, loss = 1.72880151\n",
      "Iteration 3491, loss = 1.72873228\n",
      "Iteration 3492, loss = 1.72866764\n",
      "Iteration 3493, loss = 1.72860321\n",
      "Iteration 3494, loss = 1.72853503\n",
      "Iteration 3495, loss = 1.72847317\n",
      "Iteration 3496, loss = 1.72840770\n",
      "Iteration 3497, loss = 1.72833935\n",
      "Iteration 3498, loss = 1.72827404\n",
      "Iteration 3499, loss = 1.72820945\n",
      "Iteration 3500, loss = 1.72814206\n",
      "Iteration 3501, loss = 1.72807547\n",
      "Iteration 3502, loss = 1.72801162\n",
      "Iteration 3503, loss = 1.72794554\n",
      "Iteration 3504, loss = 1.72787767\n",
      "Iteration 3505, loss = 1.72781398\n",
      "Iteration 3506, loss = 1.72774722\n",
      "Iteration 3507, loss = 1.72768059\n",
      "Iteration 3508, loss = 1.72761517\n",
      "Iteration 3509, loss = 1.72754773\n",
      "Iteration 3510, loss = 1.72748257\n",
      "Iteration 3511, loss = 1.72741581\n",
      "Iteration 3512, loss = 1.72735145\n",
      "Iteration 3513, loss = 1.72728462\n",
      "Iteration 3514, loss = 1.72721917\n",
      "Iteration 3515, loss = 1.72715250\n",
      "Iteration 3516, loss = 1.72708740\n",
      "Iteration 3517, loss = 1.72702084\n",
      "Iteration 3518, loss = 1.72695763\n",
      "Iteration 3519, loss = 1.72688945\n",
      "Iteration 3520, loss = 1.72682433\n",
      "Iteration 3521, loss = 1.72675765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3522, loss = 1.72669206\n",
      "Iteration 3523, loss = 1.72662664\n",
      "Iteration 3524, loss = 1.72655991\n",
      "Iteration 3525, loss = 1.72649437\n",
      "Iteration 3526, loss = 1.72642828\n",
      "Iteration 3527, loss = 1.72636247\n",
      "Iteration 3528, loss = 1.72629711\n",
      "Iteration 3529, loss = 1.72623362\n",
      "Iteration 3530, loss = 1.72616540\n",
      "Iteration 3531, loss = 1.72610427\n",
      "Iteration 3532, loss = 1.72603593\n",
      "Iteration 3533, loss = 1.72597165\n",
      "Iteration 3534, loss = 1.72590736\n",
      "Iteration 3535, loss = 1.72583960\n",
      "Iteration 3536, loss = 1.72577374\n",
      "Iteration 3537, loss = 1.72570993\n",
      "Iteration 3538, loss = 1.72564261\n",
      "Iteration 3539, loss = 1.72557534\n",
      "Iteration 3540, loss = 1.72550990\n",
      "Iteration 3541, loss = 1.72544405\n",
      "Iteration 3542, loss = 1.72537628\n",
      "Iteration 3543, loss = 1.72531157\n",
      "Iteration 3544, loss = 1.72524459\n",
      "Iteration 3545, loss = 1.72517982\n",
      "Iteration 3546, loss = 1.72511258\n",
      "Iteration 3547, loss = 1.72504685\n",
      "Iteration 3548, loss = 1.72498035\n",
      "Iteration 3549, loss = 1.72491589\n",
      "Iteration 3550, loss = 1.72484893\n",
      "Iteration 3551, loss = 1.72478491\n",
      "Iteration 3552, loss = 1.72471675\n",
      "Iteration 3553, loss = 1.72465299\n",
      "Iteration 3554, loss = 1.72458586\n",
      "Iteration 3555, loss = 1.72452079\n",
      "Iteration 3556, loss = 1.72445312\n",
      "Iteration 3557, loss = 1.72438769\n",
      "Iteration 3558, loss = 1.72432107\n",
      "Iteration 3559, loss = 1.72425524\n",
      "Iteration 3560, loss = 1.72419165\n",
      "Iteration 3561, loss = 1.72412429\n",
      "Iteration 3562, loss = 1.72405843\n",
      "Iteration 3563, loss = 1.72399212\n",
      "Iteration 3564, loss = 1.72393013\n",
      "Iteration 3565, loss = 1.72386409\n",
      "Iteration 3566, loss = 1.72379646\n",
      "Iteration 3567, loss = 1.72373449\n",
      "Iteration 3568, loss = 1.72366785\n",
      "Iteration 3569, loss = 1.72359934\n",
      "Iteration 3570, loss = 1.72353471\n",
      "Iteration 3571, loss = 1.72346859\n",
      "Iteration 3572, loss = 1.72340089\n",
      "Iteration 3573, loss = 1.72333599\n",
      "Iteration 3574, loss = 1.72327059\n",
      "Iteration 3575, loss = 1.72320311\n",
      "Iteration 3576, loss = 1.72313778\n",
      "Iteration 3577, loss = 1.72307102\n",
      "Iteration 3578, loss = 1.72300476\n",
      "Iteration 3579, loss = 1.72293842\n",
      "Iteration 3580, loss = 1.72287404\n",
      "Iteration 3581, loss = 1.72280763\n",
      "Iteration 3582, loss = 1.72274251\n",
      "Iteration 3583, loss = 1.72267497\n",
      "Iteration 3584, loss = 1.72261026\n",
      "Iteration 3585, loss = 1.72254352\n",
      "Iteration 3586, loss = 1.72247761\n",
      "Iteration 3587, loss = 1.72241264\n",
      "Iteration 3588, loss = 1.72234600\n",
      "Iteration 3589, loss = 1.72228007\n",
      "Iteration 3590, loss = 1.72221567\n",
      "Iteration 3591, loss = 1.72214806\n",
      "Iteration 3592, loss = 1.72208359\n",
      "Iteration 3593, loss = 1.72201636\n",
      "Iteration 3594, loss = 1.72195018\n",
      "Iteration 3595, loss = 1.72188484\n",
      "Iteration 3596, loss = 1.72182001\n",
      "Iteration 3597, loss = 1.72175307\n",
      "Iteration 3598, loss = 1.72168774\n",
      "Iteration 3599, loss = 1.72162067\n",
      "Iteration 3600, loss = 1.72155504\n",
      "Iteration 3601, loss = 1.72148978\n",
      "Iteration 3602, loss = 1.72142237\n",
      "Iteration 3603, loss = 1.72135727\n",
      "Iteration 3604, loss = 1.72129196\n",
      "Iteration 3605, loss = 1.72122831\n",
      "Iteration 3606, loss = 1.72115907\n",
      "Iteration 3607, loss = 1.72109426\n",
      "Iteration 3608, loss = 1.72102748\n",
      "Iteration 3609, loss = 1.72096110\n",
      "Iteration 3610, loss = 1.72089563\n",
      "Iteration 3611, loss = 1.72082872\n",
      "Iteration 3612, loss = 1.72076215\n",
      "Iteration 3613, loss = 1.72069806\n",
      "Iteration 3614, loss = 1.72063197\n",
      "Iteration 3615, loss = 1.72056418\n",
      "Iteration 3616, loss = 1.72050006\n",
      "Iteration 3617, loss = 1.72043147\n",
      "Iteration 3618, loss = 1.72036771\n",
      "Iteration 3619, loss = 1.72030047\n",
      "Iteration 3620, loss = 1.72023736\n",
      "Iteration 3621, loss = 1.72016810\n",
      "Iteration 3622, loss = 1.72010158\n",
      "Iteration 3623, loss = 1.72003670\n",
      "Iteration 3624, loss = 1.71997004\n",
      "Iteration 3625, loss = 1.71990472\n",
      "Iteration 3626, loss = 1.71983898\n",
      "Iteration 3627, loss = 1.71977211\n",
      "Iteration 3628, loss = 1.71970880\n",
      "Iteration 3629, loss = 1.71963975\n",
      "Iteration 3630, loss = 1.71957344\n",
      "Iteration 3631, loss = 1.71951256\n",
      "Iteration 3632, loss = 1.71944395\n",
      "Iteration 3633, loss = 1.71937857\n",
      "Iteration 3634, loss = 1.71931268\n",
      "Iteration 3635, loss = 1.71924422\n",
      "Iteration 3636, loss = 1.71918047\n",
      "Iteration 3637, loss = 1.71911195\n",
      "Iteration 3638, loss = 1.71904930\n",
      "Iteration 3639, loss = 1.71898324\n",
      "Iteration 3640, loss = 1.71891416\n",
      "Iteration 3641, loss = 1.71885104\n",
      "Iteration 3642, loss = 1.71878291\n",
      "Iteration 3643, loss = 1.71871680\n",
      "Iteration 3644, loss = 1.71865345\n",
      "Iteration 3645, loss = 1.71858428\n",
      "Iteration 3646, loss = 1.71851801\n",
      "Iteration 3647, loss = 1.71845439\n",
      "Iteration 3648, loss = 1.71838510\n",
      "Iteration 3649, loss = 1.71832082\n",
      "Iteration 3650, loss = 1.71825274\n",
      "Iteration 3651, loss = 1.71818785\n",
      "Iteration 3652, loss = 1.71812139\n",
      "Iteration 3653, loss = 1.71805426\n",
      "Iteration 3654, loss = 1.71799068\n",
      "Iteration 3655, loss = 1.71792160\n",
      "Iteration 3656, loss = 1.71785863\n",
      "Iteration 3657, loss = 1.71778926\n",
      "Iteration 3658, loss = 1.71772633\n",
      "Iteration 3659, loss = 1.71765916\n",
      "Iteration 3660, loss = 1.71759233\n",
      "Iteration 3661, loss = 1.71752690\n",
      "Iteration 3662, loss = 1.71745948\n",
      "Iteration 3663, loss = 1.71739450\n",
      "Iteration 3664, loss = 1.71732740\n",
      "Iteration 3665, loss = 1.71726055\n",
      "Iteration 3666, loss = 1.71719640\n",
      "Iteration 3667, loss = 1.71712867\n",
      "Iteration 3668, loss = 1.71706305\n",
      "Iteration 3669, loss = 1.71699534\n",
      "Iteration 3670, loss = 1.71693005\n",
      "Iteration 3671, loss = 1.71686537\n",
      "Iteration 3672, loss = 1.71679749\n",
      "Iteration 3673, loss = 1.71673391\n",
      "Iteration 3674, loss = 1.71666565\n",
      "Iteration 3675, loss = 1.71660113\n",
      "Iteration 3676, loss = 1.71653500\n",
      "Iteration 3677, loss = 1.71646748\n",
      "Iteration 3678, loss = 1.71640204\n",
      "Iteration 3679, loss = 1.71633615\n",
      "Iteration 3680, loss = 1.71626869\n",
      "Iteration 3681, loss = 1.71620412\n",
      "Iteration 3682, loss = 1.71613634\n",
      "Iteration 3683, loss = 1.71607014\n",
      "Iteration 3684, loss = 1.71600632\n",
      "Iteration 3685, loss = 1.71593628\n",
      "Iteration 3686, loss = 1.71587401\n",
      "Iteration 3687, loss = 1.71580457\n",
      "Iteration 3688, loss = 1.71574500\n",
      "Iteration 3689, loss = 1.71567931\n",
      "Iteration 3690, loss = 1.71560501\n",
      "Iteration 3691, loss = 1.71554689\n",
      "Iteration 3692, loss = 1.71548240\n",
      "Iteration 3693, loss = 1.71541047\n",
      "Iteration 3694, loss = 1.71534391\n",
      "Iteration 3695, loss = 1.71528234\n",
      "Iteration 3696, loss = 1.71521139\n",
      "Iteration 3697, loss = 1.71514436\n",
      "Iteration 3698, loss = 1.71507945\n",
      "Iteration 3699, loss = 1.71501478\n",
      "Iteration 3700, loss = 1.71494667\n",
      "Iteration 3701, loss = 1.71487832\n",
      "Iteration 3702, loss = 1.71481412\n",
      "Iteration 3703, loss = 1.71474607\n",
      "Iteration 3704, loss = 1.71467883\n",
      "Iteration 3705, loss = 1.71461375\n",
      "Iteration 3706, loss = 1.71454526\n",
      "Iteration 3707, loss = 1.71448285\n",
      "Iteration 3708, loss = 1.71441274\n",
      "Iteration 3709, loss = 1.71435098\n",
      "Iteration 3710, loss = 1.71428343\n",
      "Iteration 3711, loss = 1.71421594\n",
      "Iteration 3712, loss = 1.71414856\n",
      "Iteration 3713, loss = 1.71408313\n",
      "Iteration 3714, loss = 1.71401782\n",
      "Iteration 3715, loss = 1.71394987\n",
      "Iteration 3716, loss = 1.71388512\n",
      "Iteration 3717, loss = 1.71381699\n",
      "Iteration 3718, loss = 1.71375199\n",
      "Iteration 3719, loss = 1.71368472\n",
      "Iteration 3720, loss = 1.71361896\n",
      "Iteration 3721, loss = 1.71355242\n",
      "Iteration 3722, loss = 1.71348801\n",
      "Iteration 3723, loss = 1.71342408\n",
      "Iteration 3724, loss = 1.71335410\n",
      "Iteration 3725, loss = 1.71328789\n",
      "Iteration 3726, loss = 1.71322284\n",
      "Iteration 3727, loss = 1.71315441\n",
      "Iteration 3728, loss = 1.71309075\n",
      "Iteration 3729, loss = 1.71302217\n",
      "Iteration 3730, loss = 1.71295470\n",
      "Iteration 3731, loss = 1.71288855\n",
      "Iteration 3732, loss = 1.71282121\n",
      "Iteration 3733, loss = 1.71275657\n",
      "Iteration 3734, loss = 1.71268903\n",
      "Iteration 3735, loss = 1.71262228\n",
      "Iteration 3736, loss = 1.71255735\n",
      "Iteration 3737, loss = 1.71248943\n",
      "Iteration 3738, loss = 1.71242633\n",
      "Iteration 3739, loss = 1.71235660\n",
      "Iteration 3740, loss = 1.71229131\n",
      "Iteration 3741, loss = 1.71222361\n",
      "Iteration 3742, loss = 1.71215990\n",
      "Iteration 3743, loss = 1.71209087\n",
      "Iteration 3744, loss = 1.71202474\n",
      "Iteration 3745, loss = 1.71196245\n",
      "Iteration 3746, loss = 1.71189258\n",
      "Iteration 3747, loss = 1.71183123\n",
      "Iteration 3748, loss = 1.71176576\n",
      "Iteration 3749, loss = 1.71169506\n",
      "Iteration 3750, loss = 1.71163021\n",
      "Iteration 3751, loss = 1.71156252\n",
      "Iteration 3752, loss = 1.71149600\n",
      "Iteration 3753, loss = 1.71143171\n",
      "Iteration 3754, loss = 1.71136294\n",
      "Iteration 3755, loss = 1.71129640\n",
      "Iteration 3756, loss = 1.71123304\n",
      "Iteration 3757, loss = 1.71116303\n",
      "Iteration 3758, loss = 1.71109925\n",
      "Iteration 3759, loss = 1.71103256\n",
      "Iteration 3760, loss = 1.71096356\n",
      "Iteration 3761, loss = 1.71090124\n",
      "Iteration 3762, loss = 1.71083407\n",
      "Iteration 3763, loss = 1.71076428\n",
      "Iteration 3764, loss = 1.71069963\n",
      "Iteration 3765, loss = 1.71063109\n",
      "Iteration 3766, loss = 1.71056844\n",
      "Iteration 3767, loss = 1.71049964\n",
      "Iteration 3768, loss = 1.71043529\n",
      "Iteration 3769, loss = 1.71037000\n",
      "Iteration 3770, loss = 1.71030013\n",
      "Iteration 3771, loss = 1.71023557\n",
      "Iteration 3772, loss = 1.71016738\n",
      "Iteration 3773, loss = 1.71010142\n",
      "Iteration 3774, loss = 1.71003605\n",
      "Iteration 3775, loss = 1.70996873\n",
      "Iteration 3776, loss = 1.70990049\n",
      "Iteration 3777, loss = 1.70983449\n",
      "Iteration 3778, loss = 1.70976790\n",
      "Iteration 3779, loss = 1.70970102\n",
      "Iteration 3780, loss = 1.70963475\n",
      "Iteration 3781, loss = 1.70956817\n",
      "Iteration 3782, loss = 1.70950064\n",
      "Iteration 3783, loss = 1.70943381\n",
      "Iteration 3784, loss = 1.70937050\n",
      "Iteration 3785, loss = 1.70930116\n",
      "Iteration 3786, loss = 1.70923704\n",
      "Iteration 3787, loss = 1.70916880\n",
      "Iteration 3788, loss = 1.70910721\n",
      "Iteration 3789, loss = 1.70903795\n",
      "Iteration 3790, loss = 1.70897352\n",
      "Iteration 3791, loss = 1.70890886\n",
      "Iteration 3792, loss = 1.70883776\n",
      "Iteration 3793, loss = 1.70877306\n",
      "Iteration 3794, loss = 1.70870700\n",
      "Iteration 3795, loss = 1.70863830\n",
      "Iteration 3796, loss = 1.70857481\n",
      "Iteration 3797, loss = 1.70850846\n",
      "Iteration 3798, loss = 1.70843895\n",
      "Iteration 3799, loss = 1.70837444\n",
      "Iteration 3800, loss = 1.70831012\n",
      "Iteration 3801, loss = 1.70823940\n",
      "Iteration 3802, loss = 1.70817477\n",
      "Iteration 3803, loss = 1.70810900\n",
      "Iteration 3804, loss = 1.70803887\n",
      "Iteration 3805, loss = 1.70797491\n",
      "Iteration 3806, loss = 1.70790739\n",
      "Iteration 3807, loss = 1.70783947\n",
      "Iteration 3808, loss = 1.70777465\n",
      "Iteration 3809, loss = 1.70770592\n",
      "Iteration 3810, loss = 1.70764189\n",
      "Iteration 3811, loss = 1.70757395\n",
      "Iteration 3812, loss = 1.70750877\n",
      "Iteration 3813, loss = 1.70744264\n",
      "Iteration 3814, loss = 1.70737405\n",
      "Iteration 3815, loss = 1.70730882\n",
      "Iteration 3816, loss = 1.70723984\n",
      "Iteration 3817, loss = 1.70717493\n",
      "Iteration 3818, loss = 1.70710715\n",
      "Iteration 3819, loss = 1.70704348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3820, loss = 1.70697455\n",
      "Iteration 3821, loss = 1.70691007\n",
      "Iteration 3822, loss = 1.70684344\n",
      "Iteration 3823, loss = 1.70677454\n",
      "Iteration 3824, loss = 1.70671073\n",
      "Iteration 3825, loss = 1.70664189\n",
      "Iteration 3826, loss = 1.70657570\n",
      "Iteration 3827, loss = 1.70650910\n",
      "Iteration 3828, loss = 1.70644165\n",
      "Iteration 3829, loss = 1.70637393\n",
      "Iteration 3830, loss = 1.70630758\n",
      "Iteration 3831, loss = 1.70624115\n",
      "Iteration 3832, loss = 1.70617639\n",
      "Iteration 3833, loss = 1.70610887\n",
      "Iteration 3834, loss = 1.70604096\n",
      "Iteration 3835, loss = 1.70597444\n",
      "Iteration 3836, loss = 1.70591058\n",
      "Iteration 3837, loss = 1.70584163\n",
      "Iteration 3838, loss = 1.70577750\n",
      "Iteration 3839, loss = 1.70571019\n",
      "Iteration 3840, loss = 1.70564280\n",
      "Iteration 3841, loss = 1.70557564\n",
      "Iteration 3842, loss = 1.70550901\n",
      "Iteration 3843, loss = 1.70544195\n",
      "Iteration 3844, loss = 1.70537528\n",
      "Iteration 3845, loss = 1.70530858\n",
      "Iteration 3846, loss = 1.70524134\n",
      "Iteration 3847, loss = 1.70517495\n",
      "Iteration 3848, loss = 1.70510841\n",
      "Iteration 3849, loss = 1.70504127\n",
      "Iteration 3850, loss = 1.70497459\n",
      "Iteration 3851, loss = 1.70491037\n",
      "Iteration 3852, loss = 1.70484269\n",
      "Iteration 3853, loss = 1.70477572\n",
      "Iteration 3854, loss = 1.70471002\n",
      "Iteration 3855, loss = 1.70464105\n",
      "Iteration 3856, loss = 1.70457625\n",
      "Iteration 3857, loss = 1.70450801\n",
      "Iteration 3858, loss = 1.70444505\n",
      "Iteration 3859, loss = 1.70437486\n",
      "Iteration 3860, loss = 1.70431284\n",
      "Iteration 3861, loss = 1.70424536\n",
      "Iteration 3862, loss = 1.70417555\n",
      "Iteration 3863, loss = 1.70411144\n",
      "Iteration 3864, loss = 1.70404246\n",
      "Iteration 3865, loss = 1.70397665\n",
      "Iteration 3866, loss = 1.70390857\n",
      "Iteration 3867, loss = 1.70384261\n",
      "Iteration 3868, loss = 1.70377489\n",
      "Iteration 3869, loss = 1.70370783\n",
      "Iteration 3870, loss = 1.70364146\n",
      "Iteration 3871, loss = 1.70357527\n",
      "Iteration 3872, loss = 1.70350845\n",
      "Iteration 3873, loss = 1.70344098\n",
      "Iteration 3874, loss = 1.70337405\n",
      "Iteration 3875, loss = 1.70331002\n",
      "Iteration 3876, loss = 1.70324066\n",
      "Iteration 3877, loss = 1.70317474\n",
      "Iteration 3878, loss = 1.70310795\n",
      "Iteration 3879, loss = 1.70304100\n",
      "Iteration 3880, loss = 1.70297592\n",
      "Iteration 3881, loss = 1.70290872\n",
      "Iteration 3882, loss = 1.70284358\n",
      "Iteration 3883, loss = 1.70277344\n",
      "Iteration 3884, loss = 1.70271188\n",
      "Iteration 3885, loss = 1.70264394\n",
      "Iteration 3886, loss = 1.70257550\n",
      "Iteration 3887, loss = 1.70251102\n",
      "Iteration 3888, loss = 1.70244234\n",
      "Iteration 3889, loss = 1.70237616\n",
      "Iteration 3890, loss = 1.70230974\n",
      "Iteration 3891, loss = 1.70224128\n",
      "Iteration 3892, loss = 1.70217928\n",
      "Iteration 3893, loss = 1.70210964\n",
      "Iteration 3894, loss = 1.70204188\n",
      "Iteration 3895, loss = 1.70197616\n",
      "Iteration 3896, loss = 1.70190753\n",
      "Iteration 3897, loss = 1.70184228\n",
      "Iteration 3898, loss = 1.70177397\n",
      "Iteration 3899, loss = 1.70170760\n",
      "Iteration 3900, loss = 1.70164080\n",
      "Iteration 3901, loss = 1.70157310\n",
      "Iteration 3902, loss = 1.70150696\n",
      "Iteration 3903, loss = 1.70143920\n",
      "Iteration 3904, loss = 1.70137346\n",
      "Iteration 3905, loss = 1.70130876\n",
      "Iteration 3906, loss = 1.70123830\n",
      "Iteration 3907, loss = 1.70117756\n",
      "Iteration 3908, loss = 1.70110978\n",
      "Iteration 3909, loss = 1.70103939\n",
      "Iteration 3910, loss = 1.70097633\n",
      "Iteration 3911, loss = 1.70090664\n",
      "Iteration 3912, loss = 1.70084039\n",
      "Iteration 3913, loss = 1.70077372\n",
      "Iteration 3914, loss = 1.70070543\n",
      "Iteration 3915, loss = 1.70063916\n",
      "Iteration 3916, loss = 1.70057191\n",
      "Iteration 3917, loss = 1.70050979\n",
      "Iteration 3918, loss = 1.70044229\n",
      "Iteration 3919, loss = 1.70037391\n",
      "Iteration 3920, loss = 1.70030969\n",
      "Iteration 3921, loss = 1.70023743\n",
      "Iteration 3922, loss = 1.70017600\n",
      "Iteration 3923, loss = 1.70010711\n",
      "Iteration 3924, loss = 1.70003890\n",
      "Iteration 3925, loss = 1.69997684\n",
      "Iteration 3926, loss = 1.69990769\n",
      "Iteration 3927, loss = 1.69984092\n",
      "Iteration 3928, loss = 1.69977645\n",
      "Iteration 3929, loss = 1.69970945\n",
      "Iteration 3930, loss = 1.69964051\n",
      "Iteration 3931, loss = 1.69957161\n",
      "Iteration 3932, loss = 1.69950964\n",
      "Iteration 3933, loss = 1.69943905\n",
      "Iteration 3934, loss = 1.69937336\n",
      "Iteration 3935, loss = 1.69930989\n",
      "Iteration 3936, loss = 1.69924179\n",
      "Iteration 3937, loss = 1.69917108\n",
      "Iteration 3938, loss = 1.69910689\n",
      "Iteration 3939, loss = 1.69903995\n",
      "Iteration 3940, loss = 1.69896935\n",
      "Iteration 3941, loss = 1.69890746\n",
      "Iteration 3942, loss = 1.69884097\n",
      "Iteration 3943, loss = 1.69876895\n",
      "Iteration 3944, loss = 1.69871057\n",
      "Iteration 3945, loss = 1.69864425\n",
      "Iteration 3946, loss = 1.69856882\n",
      "Iteration 3947, loss = 1.69850572\n",
      "Iteration 3948, loss = 1.69843898\n",
      "Iteration 3949, loss = 1.69836869\n",
      "Iteration 3950, loss = 1.69830817\n",
      "Iteration 3951, loss = 1.69823974\n",
      "Iteration 3952, loss = 1.69816892\n",
      "Iteration 3953, loss = 1.69810454\n",
      "Iteration 3954, loss = 1.69803759\n",
      "Iteration 3955, loss = 1.69796837\n",
      "Iteration 3956, loss = 1.69790372\n",
      "Iteration 3957, loss = 1.69783503\n",
      "Iteration 3958, loss = 1.69776757\n",
      "Iteration 3959, loss = 1.69770265\n",
      "Iteration 3960, loss = 1.69763400\n",
      "Iteration 3961, loss = 1.69756684\n",
      "Iteration 3962, loss = 1.69750068\n",
      "Iteration 3963, loss = 1.69743309\n",
      "Iteration 3964, loss = 1.69736699\n",
      "Iteration 3965, loss = 1.69729883\n",
      "Iteration 3966, loss = 1.69723125\n",
      "Iteration 3967, loss = 1.69716792\n",
      "Iteration 3968, loss = 1.69709857\n",
      "Iteration 3969, loss = 1.69703419\n",
      "Iteration 3970, loss = 1.69696482\n",
      "Iteration 3971, loss = 1.69689948\n",
      "Iteration 3972, loss = 1.69683163\n",
      "Iteration 3973, loss = 1.69676423\n",
      "Iteration 3974, loss = 1.69669821\n",
      "Iteration 3975, loss = 1.69663030\n",
      "Iteration 3976, loss = 1.69656585\n",
      "Iteration 3977, loss = 1.69649577\n",
      "Iteration 3978, loss = 1.69643354\n",
      "Iteration 3979, loss = 1.69636194\n",
      "Iteration 3980, loss = 1.69629597\n",
      "Iteration 3981, loss = 1.69623012\n",
      "Iteration 3982, loss = 1.69616203\n",
      "Iteration 3983, loss = 1.69609501\n",
      "Iteration 3984, loss = 1.69602821\n",
      "Iteration 3985, loss = 1.69596220\n",
      "Iteration 3986, loss = 1.69589523\n",
      "Iteration 3987, loss = 1.69582729\n",
      "Iteration 3988, loss = 1.69576186\n",
      "Iteration 3989, loss = 1.69569489\n",
      "Iteration 3990, loss = 1.69562711\n",
      "Iteration 3991, loss = 1.69556194\n",
      "Iteration 3992, loss = 1.69549413\n",
      "Iteration 3993, loss = 1.69542764\n",
      "Iteration 3994, loss = 1.69535980\n",
      "Iteration 3995, loss = 1.69529477\n",
      "Iteration 3996, loss = 1.69522538\n",
      "Iteration 3997, loss = 1.69516209\n",
      "Iteration 3998, loss = 1.69509154\n",
      "Iteration 3999, loss = 1.69503319\n",
      "Iteration 4000, loss = 1.69496455\n",
      "Iteration 4001, loss = 1.69489328\n",
      "Iteration 4002, loss = 1.69482803\n",
      "Iteration 4003, loss = 1.69475795\n",
      "Iteration 4004, loss = 1.69469158\n",
      "Iteration 4005, loss = 1.69462460\n",
      "Iteration 4006, loss = 1.69455858\n",
      "Iteration 4007, loss = 1.69449030\n",
      "Iteration 4008, loss = 1.69442410\n",
      "Iteration 4009, loss = 1.69435921\n",
      "Iteration 4010, loss = 1.69429100\n",
      "Iteration 4011, loss = 1.69422564\n",
      "Iteration 4012, loss = 1.69415794\n",
      "Iteration 4013, loss = 1.69409082\n",
      "Iteration 4014, loss = 1.69402702\n",
      "Iteration 4015, loss = 1.69395648\n",
      "Iteration 4016, loss = 1.69389189\n",
      "Iteration 4017, loss = 1.69382287\n",
      "Iteration 4018, loss = 1.69375808\n",
      "Iteration 4019, loss = 1.69369085\n",
      "Iteration 4020, loss = 1.69362148\n",
      "Iteration 4021, loss = 1.69355722\n",
      "Iteration 4022, loss = 1.69348830\n",
      "Iteration 4023, loss = 1.69342266\n",
      "Iteration 4024, loss = 1.69335595\n",
      "Iteration 4025, loss = 1.69328710\n",
      "Iteration 4026, loss = 1.69321999\n",
      "Iteration 4027, loss = 1.69315620\n",
      "Iteration 4028, loss = 1.69308795\n",
      "Iteration 4029, loss = 1.69302278\n",
      "Iteration 4030, loss = 1.69295434\n",
      "Iteration 4031, loss = 1.69288740\n",
      "Iteration 4032, loss = 1.69282145\n",
      "Iteration 4033, loss = 1.69275322\n",
      "Iteration 4034, loss = 1.69268575\n",
      "Iteration 4035, loss = 1.69261831\n",
      "Iteration 4036, loss = 1.69255165\n",
      "Iteration 4037, loss = 1.69248495\n",
      "Iteration 4038, loss = 1.69241824\n",
      "Iteration 4039, loss = 1.69235058\n",
      "Iteration 4040, loss = 1.69228402\n",
      "Iteration 4041, loss = 1.69221727\n",
      "Iteration 4042, loss = 1.69215156\n",
      "Iteration 4043, loss = 1.69208567\n",
      "Iteration 4044, loss = 1.69201850\n",
      "Iteration 4045, loss = 1.69195223\n",
      "Iteration 4046, loss = 1.69188541\n",
      "Iteration 4047, loss = 1.69181587\n",
      "Iteration 4048, loss = 1.69174873\n",
      "Iteration 4049, loss = 1.69168092\n",
      "Iteration 4050, loss = 1.69161534\n",
      "Iteration 4051, loss = 1.69154842\n",
      "Iteration 4052, loss = 1.69148052\n",
      "Iteration 4053, loss = 1.69141313\n",
      "Iteration 4054, loss = 1.69134717\n",
      "Iteration 4055, loss = 1.69128091\n",
      "Iteration 4056, loss = 1.69121392\n",
      "Iteration 4057, loss = 1.69114658\n",
      "Iteration 4058, loss = 1.69107928\n",
      "Iteration 4059, loss = 1.69101215\n",
      "Iteration 4060, loss = 1.69094444\n",
      "Iteration 4061, loss = 1.69087770\n",
      "Iteration 4062, loss = 1.69081194\n",
      "Iteration 4063, loss = 1.69074484\n",
      "Iteration 4064, loss = 1.69067762\n",
      "Iteration 4065, loss = 1.69061269\n",
      "Iteration 4066, loss = 1.69054512\n",
      "Iteration 4067, loss = 1.69047722\n",
      "Iteration 4068, loss = 1.69041006\n",
      "Iteration 4069, loss = 1.69034567\n",
      "Iteration 4070, loss = 1.69027879\n",
      "Iteration 4071, loss = 1.69020926\n",
      "Iteration 4072, loss = 1.69014786\n",
      "Iteration 4073, loss = 1.69007455\n",
      "Iteration 4074, loss = 1.69000959\n",
      "Iteration 4075, loss = 1.68994074\n",
      "Iteration 4076, loss = 1.68987620\n",
      "Iteration 4077, loss = 1.68980768\n",
      "Iteration 4078, loss = 1.68973944\n",
      "Iteration 4079, loss = 1.68967304\n",
      "Iteration 4080, loss = 1.68960633\n",
      "Iteration 4081, loss = 1.68953918\n",
      "Iteration 4082, loss = 1.68947326\n",
      "Iteration 4083, loss = 1.68940875\n",
      "Iteration 4084, loss = 1.68934078\n",
      "Iteration 4085, loss = 1.68927481\n",
      "Iteration 4086, loss = 1.68920565\n",
      "Iteration 4087, loss = 1.68913848\n",
      "Iteration 4088, loss = 1.68907254\n",
      "Iteration 4089, loss = 1.68900562\n",
      "Iteration 4090, loss = 1.68893903\n",
      "Iteration 4091, loss = 1.68887162\n",
      "Iteration 4092, loss = 1.68880746\n",
      "Iteration 4093, loss = 1.68873927\n",
      "Iteration 4094, loss = 1.68867341\n",
      "Iteration 4095, loss = 1.68860637\n",
      "Iteration 4096, loss = 1.68853693\n",
      "Iteration 4097, loss = 1.68847178\n",
      "Iteration 4098, loss = 1.68840543\n",
      "Iteration 4099, loss = 1.68834194\n",
      "Iteration 4100, loss = 1.68827205\n",
      "Iteration 4101, loss = 1.68820769\n",
      "Iteration 4102, loss = 1.68814064\n",
      "Iteration 4103, loss = 1.68806987\n",
      "Iteration 4104, loss = 1.68800876\n",
      "Iteration 4105, loss = 1.68793667\n",
      "Iteration 4106, loss = 1.68787250\n",
      "Iteration 4107, loss = 1.68780541\n",
      "Iteration 4108, loss = 1.68773543\n",
      "Iteration 4109, loss = 1.68767329\n",
      "Iteration 4110, loss = 1.68760254\n",
      "Iteration 4111, loss = 1.68753776\n",
      "Iteration 4112, loss = 1.68747133\n",
      "Iteration 4113, loss = 1.68740100\n",
      "Iteration 4114, loss = 1.68733753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4115, loss = 1.68726780\n",
      "Iteration 4116, loss = 1.68720140\n",
      "Iteration 4117, loss = 1.68713509\n",
      "Iteration 4118, loss = 1.68706700\n",
      "Iteration 4119, loss = 1.68700078\n",
      "Iteration 4120, loss = 1.68693384\n",
      "Iteration 4121, loss = 1.68686605\n",
      "Iteration 4122, loss = 1.68679845\n",
      "Iteration 4123, loss = 1.68673419\n",
      "Iteration 4124, loss = 1.68666630\n",
      "Iteration 4125, loss = 1.68660112\n",
      "Iteration 4126, loss = 1.68653147\n",
      "Iteration 4127, loss = 1.68646618\n",
      "Iteration 4128, loss = 1.68639863\n",
      "Iteration 4129, loss = 1.68633296\n",
      "Iteration 4130, loss = 1.68626542\n",
      "Iteration 4131, loss = 1.68619789\n",
      "Iteration 4132, loss = 1.68613099\n",
      "Iteration 4133, loss = 1.68606368\n",
      "Iteration 4134, loss = 1.68599810\n",
      "Iteration 4135, loss = 1.68593137\n",
      "Iteration 4136, loss = 1.68586564\n",
      "Iteration 4137, loss = 1.68579625\n",
      "Iteration 4138, loss = 1.68573112\n",
      "Iteration 4139, loss = 1.68566377\n",
      "Iteration 4140, loss = 1.68559647\n",
      "Iteration 4141, loss = 1.68553228\n",
      "Iteration 4142, loss = 1.68546293\n",
      "Iteration 4143, loss = 1.68539962\n",
      "Iteration 4144, loss = 1.68532866\n",
      "Iteration 4145, loss = 1.68526401\n",
      "Iteration 4146, loss = 1.68519484\n",
      "Iteration 4147, loss = 1.68512803\n",
      "Iteration 4148, loss = 1.68506169\n",
      "Iteration 4149, loss = 1.68499467\n",
      "Iteration 4150, loss = 1.68492807\n",
      "Iteration 4151, loss = 1.68486289\n",
      "Iteration 4152, loss = 1.68479396\n",
      "Iteration 4153, loss = 1.68473084\n",
      "Iteration 4154, loss = 1.68466296\n",
      "Iteration 4155, loss = 1.68459641\n",
      "Iteration 4156, loss = 1.68452955\n",
      "Iteration 4157, loss = 1.68445948\n",
      "Iteration 4158, loss = 1.68439483\n",
      "Iteration 4159, loss = 1.68433009\n",
      "Iteration 4160, loss = 1.68426420\n",
      "Iteration 4161, loss = 1.68419563\n",
      "Iteration 4162, loss = 1.68412682\n",
      "Iteration 4163, loss = 1.68405929\n",
      "Iteration 4164, loss = 1.68399728\n",
      "Iteration 4165, loss = 1.68392902\n",
      "Iteration 4166, loss = 1.68386289\n",
      "Iteration 4167, loss = 1.68379437\n",
      "Iteration 4168, loss = 1.68372867\n",
      "Iteration 4169, loss = 1.68365823\n",
      "Iteration 4170, loss = 1.68359311\n",
      "Iteration 4171, loss = 1.68352493\n",
      "Iteration 4172, loss = 1.68346182\n",
      "Iteration 4173, loss = 1.68339384\n",
      "Iteration 4174, loss = 1.68332582\n",
      "Iteration 4175, loss = 1.68325889\n",
      "Iteration 4176, loss = 1.68319143\n",
      "Iteration 4177, loss = 1.68312561\n",
      "Iteration 4178, loss = 1.68305719\n",
      "Iteration 4179, loss = 1.68299197\n",
      "Iteration 4180, loss = 1.68292312\n",
      "Iteration 4181, loss = 1.68285711\n",
      "Iteration 4182, loss = 1.68279211\n",
      "Iteration 4183, loss = 1.68272342\n",
      "Iteration 4184, loss = 1.68266287\n",
      "Iteration 4185, loss = 1.68259117\n",
      "Iteration 4186, loss = 1.68252613\n",
      "Iteration 4187, loss = 1.68245964\n",
      "Iteration 4188, loss = 1.68238927\n",
      "Iteration 4189, loss = 1.68232724\n",
      "Iteration 4190, loss = 1.68225578\n",
      "Iteration 4191, loss = 1.68219001\n",
      "Iteration 4192, loss = 1.68212153\n",
      "Iteration 4193, loss = 1.68205713\n",
      "Iteration 4194, loss = 1.68198908\n",
      "Iteration 4195, loss = 1.68192057\n",
      "Iteration 4196, loss = 1.68185882\n",
      "Iteration 4197, loss = 1.68178841\n",
      "Iteration 4198, loss = 1.68172038\n",
      "Iteration 4199, loss = 1.68165922\n",
      "Iteration 4200, loss = 1.68158702\n",
      "Iteration 4201, loss = 1.68152410\n",
      "Iteration 4202, loss = 1.68145342\n",
      "Iteration 4203, loss = 1.68139231\n",
      "Iteration 4204, loss = 1.68132048\n",
      "Iteration 4205, loss = 1.68125499\n",
      "Iteration 4206, loss = 1.68118746\n",
      "Iteration 4207, loss = 1.68112022\n",
      "Iteration 4208, loss = 1.68105311\n",
      "Iteration 4209, loss = 1.68098727\n",
      "Iteration 4210, loss = 1.68091972\n",
      "Iteration 4211, loss = 1.68085284\n",
      "Iteration 4212, loss = 1.68078635\n",
      "Iteration 4213, loss = 1.68072211\n",
      "Iteration 4214, loss = 1.68065277\n",
      "Iteration 4215, loss = 1.68058675\n",
      "Iteration 4216, loss = 1.68052407\n",
      "Iteration 4217, loss = 1.68045674\n",
      "Iteration 4218, loss = 1.68038612\n",
      "Iteration 4219, loss = 1.68032473\n",
      "Iteration 4220, loss = 1.68025425\n",
      "Iteration 4221, loss = 1.68019169\n",
      "Iteration 4222, loss = 1.68012259\n",
      "Iteration 4223, loss = 1.68005469\n",
      "Iteration 4224, loss = 1.67998997\n",
      "Iteration 4225, loss = 1.67992082\n",
      "Iteration 4226, loss = 1.67985834\n",
      "Iteration 4227, loss = 1.67979120\n",
      "Iteration 4228, loss = 1.67971948\n",
      "Iteration 4229, loss = 1.67966211\n",
      "Iteration 4230, loss = 1.67958813\n",
      "Iteration 4231, loss = 1.67952386\n",
      "Iteration 4232, loss = 1.67945980\n",
      "Iteration 4233, loss = 1.67938723\n",
      "Iteration 4234, loss = 1.67932283\n",
      "Iteration 4235, loss = 1.67925377\n",
      "Iteration 4236, loss = 1.67918789\n",
      "Iteration 4237, loss = 1.67912363\n",
      "Iteration 4238, loss = 1.67905230\n",
      "Iteration 4239, loss = 1.67899365\n",
      "Iteration 4240, loss = 1.67892266\n",
      "Iteration 4241, loss = 1.67885814\n",
      "Iteration 4242, loss = 1.67879607\n",
      "Iteration 4243, loss = 1.67872482\n",
      "Iteration 4244, loss = 1.67865385\n",
      "Iteration 4245, loss = 1.67859457\n",
      "Iteration 4246, loss = 1.67852046\n",
      "Iteration 4247, loss = 1.67845536\n",
      "Iteration 4248, loss = 1.67838984\n",
      "Iteration 4249, loss = 1.67832155\n",
      "Iteration 4250, loss = 1.67825352\n",
      "Iteration 4251, loss = 1.67818711\n",
      "Iteration 4252, loss = 1.67811905\n",
      "Iteration 4253, loss = 1.67805380\n",
      "Iteration 4254, loss = 1.67798530\n",
      "Iteration 4255, loss = 1.67791847\n",
      "Iteration 4256, loss = 1.67785315\n",
      "Iteration 4257, loss = 1.67778573\n",
      "Iteration 4258, loss = 1.67771837\n",
      "Iteration 4259, loss = 1.67765139\n",
      "Iteration 4260, loss = 1.67758575\n",
      "Iteration 4261, loss = 1.67752088\n",
      "Iteration 4262, loss = 1.67745247\n",
      "Iteration 4263, loss = 1.67738561\n",
      "Iteration 4264, loss = 1.67731781\n",
      "Iteration 4265, loss = 1.67725272\n",
      "Iteration 4266, loss = 1.67718688\n",
      "Iteration 4267, loss = 1.67711913\n",
      "Iteration 4268, loss = 1.67705481\n",
      "Iteration 4269, loss = 1.67698552\n",
      "Iteration 4270, loss = 1.67692264\n",
      "Iteration 4271, loss = 1.67685276\n",
      "Iteration 4272, loss = 1.67678683\n",
      "Iteration 4273, loss = 1.67672228\n",
      "Iteration 4274, loss = 1.67665382\n",
      "Iteration 4275, loss = 1.67658794\n",
      "Iteration 4276, loss = 1.67651955\n",
      "Iteration 4277, loss = 1.67646009\n",
      "Iteration 4278, loss = 1.67638906\n",
      "Iteration 4279, loss = 1.67632417\n",
      "Iteration 4280, loss = 1.67625937\n",
      "Iteration 4281, loss = 1.67618710\n",
      "Iteration 4282, loss = 1.67612676\n",
      "Iteration 4283, loss = 1.67605816\n",
      "Iteration 4284, loss = 1.67598964\n",
      "Iteration 4285, loss = 1.67592661\n",
      "Iteration 4286, loss = 1.67585836\n",
      "Iteration 4287, loss = 1.67578861\n",
      "Iteration 4288, loss = 1.67572462\n",
      "Iteration 4289, loss = 1.67565449\n",
      "Iteration 4290, loss = 1.67558803\n",
      "Iteration 4291, loss = 1.67552101\n",
      "Iteration 4292, loss = 1.67545405\n",
      "Iteration 4293, loss = 1.67538881\n",
      "Iteration 4294, loss = 1.67532030\n",
      "Iteration 4295, loss = 1.67525366\n",
      "Iteration 4296, loss = 1.67518730\n",
      "Iteration 4297, loss = 1.67512182\n",
      "Iteration 4298, loss = 1.67505650\n",
      "Iteration 4299, loss = 1.67498765\n",
      "Iteration 4300, loss = 1.67492171\n",
      "Iteration 4301, loss = 1.67485608\n",
      "Iteration 4302, loss = 1.67478866\n",
      "Iteration 4303, loss = 1.67472393\n",
      "Iteration 4304, loss = 1.67465448\n",
      "Iteration 4305, loss = 1.67458968\n",
      "Iteration 4306, loss = 1.67452200\n",
      "Iteration 4307, loss = 1.67445547\n",
      "Iteration 4308, loss = 1.67438978\n",
      "Iteration 4309, loss = 1.67432213\n",
      "Iteration 4310, loss = 1.67425599\n",
      "Iteration 4311, loss = 1.67419134\n",
      "Iteration 4312, loss = 1.67412353\n",
      "Iteration 4313, loss = 1.67405536\n",
      "Iteration 4314, loss = 1.67398885\n",
      "Iteration 4315, loss = 1.67392792\n",
      "Iteration 4316, loss = 1.67385653\n",
      "Iteration 4317, loss = 1.67379217\n",
      "Iteration 4318, loss = 1.67372404\n",
      "Iteration 4319, loss = 1.67365723\n",
      "Iteration 4320, loss = 1.67359236\n",
      "Iteration 4321, loss = 1.67352457\n",
      "Iteration 4322, loss = 1.67345822\n",
      "Iteration 4323, loss = 1.67339216\n",
      "Iteration 4324, loss = 1.67332722\n",
      "Iteration 4325, loss = 1.67326256\n",
      "Iteration 4326, loss = 1.67319193\n",
      "Iteration 4327, loss = 1.67312731\n",
      "Iteration 4328, loss = 1.67305802\n",
      "Iteration 4329, loss = 1.67299241\n",
      "Iteration 4330, loss = 1.67292563\n",
      "Iteration 4331, loss = 1.67285930\n",
      "Iteration 4332, loss = 1.67279137\n",
      "Iteration 4333, loss = 1.67272607\n",
      "Iteration 4334, loss = 1.67266029\n",
      "Iteration 4335, loss = 1.67259309\n",
      "Iteration 4336, loss = 1.67252571\n",
      "Iteration 4337, loss = 1.67246061\n",
      "Iteration 4338, loss = 1.67239483\n",
      "Iteration 4339, loss = 1.67232690\n",
      "Iteration 4340, loss = 1.67225992\n",
      "Iteration 4341, loss = 1.67219608\n",
      "Iteration 4342, loss = 1.67212747\n",
      "Iteration 4343, loss = 1.67206137\n",
      "Iteration 4344, loss = 1.67199748\n",
      "Iteration 4345, loss = 1.67192974\n",
      "Iteration 4346, loss = 1.67186355\n",
      "Iteration 4347, loss = 1.67179732\n",
      "Iteration 4348, loss = 1.67173010\n",
      "Iteration 4349, loss = 1.67166556\n",
      "Iteration 4350, loss = 1.67160073\n",
      "Iteration 4351, loss = 1.67153034\n",
      "Iteration 4352, loss = 1.67146980\n",
      "Iteration 4353, loss = 1.67139837\n",
      "Iteration 4354, loss = 1.67133167\n",
      "Iteration 4355, loss = 1.67126696\n",
      "Iteration 4356, loss = 1.67119728\n",
      "Iteration 4357, loss = 1.67113485\n",
      "Iteration 4358, loss = 1.67106434\n",
      "Iteration 4359, loss = 1.67099947\n",
      "Iteration 4360, loss = 1.67093275\n",
      "Iteration 4361, loss = 1.67086634\n",
      "Iteration 4362, loss = 1.67079957\n",
      "Iteration 4363, loss = 1.67073308\n",
      "Iteration 4364, loss = 1.67066791\n",
      "Iteration 4365, loss = 1.67060065\n",
      "Iteration 4366, loss = 1.67053345\n",
      "Iteration 4367, loss = 1.67046605\n",
      "Iteration 4368, loss = 1.67039960\n",
      "Iteration 4369, loss = 1.67033711\n",
      "Iteration 4370, loss = 1.67026785\n",
      "Iteration 4371, loss = 1.67020977\n",
      "Iteration 4372, loss = 1.67014126\n",
      "Iteration 4373, loss = 1.67007112\n",
      "Iteration 4374, loss = 1.67000725\n",
      "Iteration 4375, loss = 1.66993653\n",
      "Iteration 4376, loss = 1.66987938\n",
      "Iteration 4377, loss = 1.66980704\n",
      "Iteration 4378, loss = 1.66974463\n",
      "Iteration 4379, loss = 1.66968641\n",
      "Iteration 4380, loss = 1.66961929\n",
      "Iteration 4381, loss = 1.66954537\n",
      "Iteration 4382, loss = 1.66947290\n",
      "Iteration 4383, loss = 1.66942285\n",
      "Iteration 4384, loss = 1.66934979\n",
      "Iteration 4385, loss = 1.66927771\n",
      "Iteration 4386, loss = 1.66921911\n",
      "Iteration 4387, loss = 1.66915451\n",
      "Iteration 4388, loss = 1.66908255\n",
      "Iteration 4389, loss = 1.66901043\n",
      "Iteration 4390, loss = 1.66895190\n",
      "Iteration 4391, loss = 1.66888155\n",
      "Iteration 4392, loss = 1.66881160\n",
      "Iteration 4393, loss = 1.66874827\n",
      "Iteration 4394, loss = 1.66868201\n",
      "Iteration 4395, loss = 1.66861310\n",
      "Iteration 4396, loss = 1.66854494\n",
      "Iteration 4397, loss = 1.66848087\n",
      "Iteration 4398, loss = 1.66841084\n",
      "Iteration 4399, loss = 1.66834716\n",
      "Iteration 4400, loss = 1.66827889\n",
      "Iteration 4401, loss = 1.66821675\n",
      "Iteration 4402, loss = 1.66814885\n",
      "Iteration 4403, loss = 1.66808077\n",
      "Iteration 4404, loss = 1.66801372\n",
      "Iteration 4405, loss = 1.66794871\n",
      "Iteration 4406, loss = 1.66788172\n",
      "Iteration 4407, loss = 1.66781570\n",
      "Iteration 4408, loss = 1.66774902\n",
      "Iteration 4409, loss = 1.66768309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4410, loss = 1.66761525\n",
      "Iteration 4411, loss = 1.66754993\n",
      "Iteration 4412, loss = 1.66748180\n",
      "Iteration 4413, loss = 1.66741914\n",
      "Iteration 4414, loss = 1.66735007\n",
      "Iteration 4415, loss = 1.66728342\n",
      "Iteration 4416, loss = 1.66722059\n",
      "Iteration 4417, loss = 1.66715079\n",
      "Iteration 4418, loss = 1.66708549\n",
      "Iteration 4419, loss = 1.66701884\n",
      "Iteration 4420, loss = 1.66695553\n",
      "Iteration 4421, loss = 1.66688769\n",
      "Iteration 4422, loss = 1.66682576\n",
      "Iteration 4423, loss = 1.66675284\n",
      "Iteration 4424, loss = 1.66668830\n",
      "Iteration 4425, loss = 1.66662043\n",
      "Iteration 4426, loss = 1.66655717\n",
      "Iteration 4427, loss = 1.66649244\n",
      "Iteration 4428, loss = 1.66642616\n",
      "Iteration 4429, loss = 1.66635772\n",
      "Iteration 4430, loss = 1.66629339\n",
      "Iteration 4431, loss = 1.66622667\n",
      "Iteration 4432, loss = 1.66616209\n",
      "Iteration 4433, loss = 1.66609358\n",
      "Iteration 4434, loss = 1.66602961\n",
      "Iteration 4435, loss = 1.66596086\n",
      "Iteration 4436, loss = 1.66589733\n",
      "Iteration 4437, loss = 1.66582948\n",
      "Iteration 4438, loss = 1.66576309\n",
      "Iteration 4439, loss = 1.66569992\n",
      "Iteration 4440, loss = 1.66563091\n",
      "Iteration 4441, loss = 1.66556793\n",
      "Iteration 4442, loss = 1.66549728\n",
      "Iteration 4443, loss = 1.66543896\n",
      "Iteration 4444, loss = 1.66536701\n",
      "Iteration 4445, loss = 1.66530221\n",
      "Iteration 4446, loss = 1.66523682\n",
      "Iteration 4447, loss = 1.66516683\n",
      "Iteration 4448, loss = 1.66510444\n",
      "Iteration 4449, loss = 1.66503368\n",
      "Iteration 4450, loss = 1.66497058\n",
      "Iteration 4451, loss = 1.66490334\n",
      "Iteration 4452, loss = 1.66483912\n",
      "Iteration 4453, loss = 1.66477110\n",
      "Iteration 4454, loss = 1.66470633\n",
      "Iteration 4455, loss = 1.66464091\n",
      "Iteration 4456, loss = 1.66457121\n",
      "Iteration 4457, loss = 1.66451183\n",
      "Iteration 4458, loss = 1.66444104\n",
      "Iteration 4459, loss = 1.66437738\n",
      "Iteration 4460, loss = 1.66431117\n",
      "Iteration 4461, loss = 1.66424307\n",
      "Iteration 4462, loss = 1.66417836\n",
      "Iteration 4463, loss = 1.66411090\n",
      "Iteration 4464, loss = 1.66404379\n",
      "Iteration 4465, loss = 1.66398209\n",
      "Iteration 4466, loss = 1.66391386\n",
      "Iteration 4467, loss = 1.66384692\n",
      "Iteration 4468, loss = 1.66378171\n",
      "Iteration 4469, loss = 1.66371330\n",
      "Iteration 4470, loss = 1.66365060\n",
      "Iteration 4471, loss = 1.66358043\n",
      "Iteration 4472, loss = 1.66351745\n",
      "Iteration 4473, loss = 1.66344908\n",
      "Iteration 4474, loss = 1.66338730\n",
      "Iteration 4475, loss = 1.66331629\n",
      "Iteration 4476, loss = 1.66325082\n",
      "Iteration 4477, loss = 1.66318487\n",
      "Iteration 4478, loss = 1.66311825\n",
      "Iteration 4479, loss = 1.66305226\n",
      "Iteration 4480, loss = 1.66298856\n",
      "Iteration 4481, loss = 1.66292257\n",
      "Iteration 4482, loss = 1.66285600\n",
      "Iteration 4483, loss = 1.66278811\n",
      "Iteration 4484, loss = 1.66272326\n",
      "Iteration 4485, loss = 1.66265746\n",
      "Iteration 4486, loss = 1.66259124\n",
      "Iteration 4487, loss = 1.66252469\n",
      "Iteration 4488, loss = 1.66245892\n",
      "Iteration 4489, loss = 1.66239343\n",
      "Iteration 4490, loss = 1.66232640\n",
      "Iteration 4491, loss = 1.66226174\n",
      "Iteration 4492, loss = 1.66219745\n",
      "Iteration 4493, loss = 1.66212998\n",
      "Iteration 4494, loss = 1.66206806\n",
      "Iteration 4495, loss = 1.66199963\n",
      "Iteration 4496, loss = 1.66193862\n",
      "Iteration 4497, loss = 1.66186874\n",
      "Iteration 4498, loss = 1.66180289\n",
      "Iteration 4499, loss = 1.66173654\n",
      "Iteration 4500, loss = 1.66166927\n",
      "Iteration 4501, loss = 1.66160363\n",
      "Iteration 4502, loss = 1.66153865\n",
      "Iteration 4503, loss = 1.66147398\n",
      "Iteration 4504, loss = 1.66140734\n",
      "Iteration 4505, loss = 1.66133903\n",
      "Iteration 4506, loss = 1.66127714\n",
      "Iteration 4507, loss = 1.66120672\n",
      "Iteration 4508, loss = 1.66114446\n",
      "Iteration 4509, loss = 1.66107615\n",
      "Iteration 4510, loss = 1.66101233\n",
      "Iteration 4511, loss = 1.66094314\n",
      "Iteration 4512, loss = 1.66087961\n",
      "Iteration 4513, loss = 1.66081183\n",
      "Iteration 4514, loss = 1.66075197\n",
      "Iteration 4515, loss = 1.66068131\n",
      "Iteration 4516, loss = 1.66061812\n",
      "Iteration 4517, loss = 1.66055119\n",
      "Iteration 4518, loss = 1.66048343\n",
      "Iteration 4519, loss = 1.66041953\n",
      "Iteration 4520, loss = 1.66035044\n",
      "Iteration 4521, loss = 1.66028591\n",
      "Iteration 4522, loss = 1.66022145\n",
      "Iteration 4523, loss = 1.66015534\n",
      "Iteration 4524, loss = 1.66009062\n",
      "Iteration 4525, loss = 1.66002138\n",
      "Iteration 4526, loss = 1.65995967\n",
      "Iteration 4527, loss = 1.65989061\n",
      "Iteration 4528, loss = 1.65982832\n",
      "Iteration 4529, loss = 1.65976201\n",
      "Iteration 4530, loss = 1.65969492\n",
      "Iteration 4531, loss = 1.65963077\n",
      "Iteration 4532, loss = 1.65956070\n",
      "Iteration 4533, loss = 1.65949588\n",
      "Iteration 4534, loss = 1.65943225\n",
      "Iteration 4535, loss = 1.65936819\n",
      "Iteration 4536, loss = 1.65929892\n",
      "Iteration 4537, loss = 1.65923408\n",
      "Iteration 4538, loss = 1.65916628\n",
      "Iteration 4539, loss = 1.65910315\n",
      "Iteration 4540, loss = 1.65903556\n",
      "Iteration 4541, loss = 1.65897042\n",
      "Iteration 4542, loss = 1.65890241\n",
      "Iteration 4543, loss = 1.65883753\n",
      "Iteration 4544, loss = 1.65877248\n",
      "Iteration 4545, loss = 1.65870575\n",
      "Iteration 4546, loss = 1.65864145\n",
      "Iteration 4547, loss = 1.65857504\n",
      "Iteration 4548, loss = 1.65850985\n",
      "Iteration 4549, loss = 1.65844357\n",
      "Iteration 4550, loss = 1.65837832\n",
      "Iteration 4551, loss = 1.65831055\n",
      "Iteration 4552, loss = 1.65824520\n",
      "Iteration 4553, loss = 1.65817961\n",
      "Iteration 4554, loss = 1.65811451\n",
      "Iteration 4555, loss = 1.65805186\n",
      "Iteration 4556, loss = 1.65798627\n",
      "Iteration 4557, loss = 1.65792019\n",
      "Iteration 4558, loss = 1.65785542\n",
      "Iteration 4559, loss = 1.65779298\n",
      "Iteration 4560, loss = 1.65772426\n",
      "Iteration 4561, loss = 1.65766307\n",
      "Iteration 4562, loss = 1.65759380\n",
      "Iteration 4563, loss = 1.65752634\n",
      "Iteration 4564, loss = 1.65746158\n",
      "Iteration 4565, loss = 1.65739300\n",
      "Iteration 4566, loss = 1.65732782\n",
      "Iteration 4567, loss = 1.65726143\n",
      "Iteration 4568, loss = 1.65719490\n",
      "Iteration 4569, loss = 1.65713110\n",
      "Iteration 4570, loss = 1.65706544\n",
      "Iteration 4571, loss = 1.65700051\n",
      "Iteration 4572, loss = 1.65693450\n",
      "Iteration 4573, loss = 1.65686987\n",
      "Iteration 4574, loss = 1.65680084\n",
      "Iteration 4575, loss = 1.65674101\n",
      "Iteration 4576, loss = 1.65667336\n",
      "Iteration 4577, loss = 1.65661068\n",
      "Iteration 4578, loss = 1.65654449\n",
      "Iteration 4579, loss = 1.65647654\n",
      "Iteration 4580, loss = 1.65641205\n",
      "Iteration 4581, loss = 1.65634384\n",
      "Iteration 4582, loss = 1.65628312\n",
      "Iteration 4583, loss = 1.65621280\n",
      "Iteration 4584, loss = 1.65615154\n",
      "Iteration 4585, loss = 1.65608543\n",
      "Iteration 4586, loss = 1.65601592\n",
      "Iteration 4587, loss = 1.65595421\n",
      "Iteration 4588, loss = 1.65588773\n",
      "Iteration 4589, loss = 1.65582512\n",
      "Iteration 4590, loss = 1.65575572\n",
      "Iteration 4591, loss = 1.65569087\n",
      "Iteration 4592, loss = 1.65562396\n",
      "Iteration 4593, loss = 1.65556112\n",
      "Iteration 4594, loss = 1.65549643\n",
      "Iteration 4595, loss = 1.65542637\n",
      "Iteration 4596, loss = 1.65537313\n",
      "Iteration 4597, loss = 1.65529934\n",
      "Iteration 4598, loss = 1.65523788\n",
      "Iteration 4599, loss = 1.65517925\n",
      "Iteration 4600, loss = 1.65511112\n",
      "Iteration 4601, loss = 1.65503791\n",
      "Iteration 4602, loss = 1.65497847\n",
      "Iteration 4603, loss = 1.65491173\n",
      "Iteration 4604, loss = 1.65484193\n",
      "Iteration 4605, loss = 1.65478170\n",
      "Iteration 4606, loss = 1.65471661\n",
      "Iteration 4607, loss = 1.65464712\n",
      "Iteration 4608, loss = 1.65457917\n",
      "Iteration 4609, loss = 1.65451631\n",
      "Iteration 4610, loss = 1.65444780\n",
      "Iteration 4611, loss = 1.65438208\n",
      "Iteration 4612, loss = 1.65431706\n",
      "Iteration 4613, loss = 1.65425109\n",
      "Iteration 4614, loss = 1.65418602\n",
      "Iteration 4615, loss = 1.65411922\n",
      "Iteration 4616, loss = 1.65405330\n",
      "Iteration 4617, loss = 1.65398889\n",
      "Iteration 4618, loss = 1.65392330\n",
      "Iteration 4619, loss = 1.65385750\n",
      "Iteration 4620, loss = 1.65379303\n",
      "Iteration 4621, loss = 1.65372837\n",
      "Iteration 4622, loss = 1.65366401\n",
      "Iteration 4623, loss = 1.65359848\n",
      "Iteration 4624, loss = 1.65353410\n",
      "Iteration 4625, loss = 1.65346560\n",
      "Iteration 4626, loss = 1.65340235\n",
      "Iteration 4627, loss = 1.65333570\n",
      "Iteration 4628, loss = 1.65326914\n",
      "Iteration 4629, loss = 1.65320515\n",
      "Iteration 4630, loss = 1.65314055\n",
      "Iteration 4631, loss = 1.65307414\n",
      "Iteration 4632, loss = 1.65300996\n",
      "Iteration 4633, loss = 1.65294322\n",
      "Iteration 4634, loss = 1.65288077\n",
      "Iteration 4635, loss = 1.65281213\n",
      "Iteration 4636, loss = 1.65275221\n",
      "Iteration 4637, loss = 1.65268215\n",
      "Iteration 4638, loss = 1.65262219\n",
      "Iteration 4639, loss = 1.65255646\n",
      "Iteration 4640, loss = 1.65248564\n",
      "Iteration 4641, loss = 1.65242072\n",
      "Iteration 4642, loss = 1.65235778\n",
      "Iteration 4643, loss = 1.65229245\n",
      "Iteration 4644, loss = 1.65222582\n",
      "Iteration 4645, loss = 1.65216024\n",
      "Iteration 4646, loss = 1.65209486\n",
      "Iteration 4647, loss = 1.65203131\n",
      "Iteration 4648, loss = 1.65196630\n",
      "Iteration 4649, loss = 1.65189936\n",
      "Iteration 4650, loss = 1.65183453\n",
      "Iteration 4651, loss = 1.65177048\n",
      "Iteration 4652, loss = 1.65170669\n",
      "Iteration 4653, loss = 1.65164027\n",
      "Iteration 4654, loss = 1.65157477\n",
      "Iteration 4655, loss = 1.65151222\n",
      "Iteration 4656, loss = 1.65144343\n",
      "Iteration 4657, loss = 1.65137802\n",
      "Iteration 4658, loss = 1.65131556\n",
      "Iteration 4659, loss = 1.65124941\n",
      "Iteration 4660, loss = 1.65118588\n",
      "Iteration 4661, loss = 1.65111800\n",
      "Iteration 4662, loss = 1.65105572\n",
      "Iteration 4663, loss = 1.65098863\n",
      "Iteration 4664, loss = 1.65092449\n",
      "Iteration 4665, loss = 1.65085762\n",
      "Iteration 4666, loss = 1.65079586\n",
      "Iteration 4667, loss = 1.65072652\n",
      "Iteration 4668, loss = 1.65066307\n",
      "Iteration 4669, loss = 1.65059766\n",
      "Iteration 4670, loss = 1.65053217\n",
      "Iteration 4671, loss = 1.65046675\n",
      "Iteration 4672, loss = 1.65040094\n",
      "Iteration 4673, loss = 1.65033965\n",
      "Iteration 4674, loss = 1.65027193\n",
      "Iteration 4675, loss = 1.65021011\n",
      "Iteration 4676, loss = 1.65014261\n",
      "Iteration 4677, loss = 1.65007817\n",
      "Iteration 4678, loss = 1.65001198\n",
      "Iteration 4679, loss = 1.64995211\n",
      "Iteration 4680, loss = 1.64988188\n",
      "Iteration 4681, loss = 1.64981823\n",
      "Iteration 4682, loss = 1.64975348\n",
      "Iteration 4683, loss = 1.64968702\n",
      "Iteration 4684, loss = 1.64962179\n",
      "Iteration 4685, loss = 1.64955615\n",
      "Iteration 4686, loss = 1.64949166\n",
      "Iteration 4687, loss = 1.64942747\n",
      "Iteration 4688, loss = 1.64936281\n",
      "Iteration 4689, loss = 1.64929594\n",
      "Iteration 4690, loss = 1.64923202\n",
      "Iteration 4691, loss = 1.64916848\n",
      "Iteration 4692, loss = 1.64910266\n",
      "Iteration 4693, loss = 1.64904098\n",
      "Iteration 4694, loss = 1.64897237\n",
      "Iteration 4695, loss = 1.64890704\n",
      "Iteration 4696, loss = 1.64884182\n",
      "Iteration 4697, loss = 1.64877737\n",
      "Iteration 4698, loss = 1.64871473\n",
      "Iteration 4699, loss = 1.64864804\n",
      "Iteration 4700, loss = 1.64858230\n",
      "Iteration 4701, loss = 1.64851827\n",
      "Iteration 4702, loss = 1.64845317\n",
      "Iteration 4703, loss = 1.64839117\n",
      "Iteration 4704, loss = 1.64832247\n",
      "Iteration 4705, loss = 1.64826221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4706, loss = 1.64819703\n",
      "Iteration 4707, loss = 1.64813258\n",
      "Iteration 4708, loss = 1.64806787\n",
      "Iteration 4709, loss = 1.64800370\n",
      "Iteration 4710, loss = 1.64793550\n",
      "Iteration 4711, loss = 1.64787204\n",
      "Iteration 4712, loss = 1.64780488\n",
      "Iteration 4713, loss = 1.64774324\n",
      "Iteration 4714, loss = 1.64767821\n",
      "Iteration 4715, loss = 1.64761350\n",
      "Iteration 4716, loss = 1.64754780\n",
      "Iteration 4717, loss = 1.64748209\n",
      "Iteration 4718, loss = 1.64742110\n",
      "Iteration 4719, loss = 1.64735709\n",
      "Iteration 4720, loss = 1.64728668\n",
      "Iteration 4721, loss = 1.64723019\n",
      "Iteration 4722, loss = 1.64715745\n",
      "Iteration 4723, loss = 1.64710043\n",
      "Iteration 4724, loss = 1.64703672\n",
      "Iteration 4725, loss = 1.64696540\n",
      "Iteration 4726, loss = 1.64690779\n",
      "Iteration 4727, loss = 1.64683756\n",
      "Iteration 4728, loss = 1.64677689\n",
      "Iteration 4729, loss = 1.64671758\n",
      "Iteration 4730, loss = 1.64664914\n",
      "Iteration 4731, loss = 1.64657769\n",
      "Iteration 4732, loss = 1.64652061\n",
      "Iteration 4733, loss = 1.64644984\n",
      "Iteration 4734, loss = 1.64638517\n",
      "Iteration 4735, loss = 1.64632217\n",
      "Iteration 4736, loss = 1.64625489\n",
      "Iteration 4737, loss = 1.64619038\n",
      "Iteration 4738, loss = 1.64612470\n",
      "Iteration 4739, loss = 1.64605985\n",
      "Iteration 4740, loss = 1.64599462\n",
      "Iteration 4741, loss = 1.64593415\n",
      "Iteration 4742, loss = 1.64586554\n",
      "Iteration 4743, loss = 1.64580557\n",
      "Iteration 4744, loss = 1.64574018\n",
      "Iteration 4745, loss = 1.64567209\n",
      "Iteration 4746, loss = 1.64560625\n",
      "Iteration 4747, loss = 1.64554322\n",
      "Iteration 4748, loss = 1.64547716\n",
      "Iteration 4749, loss = 1.64541449\n",
      "Iteration 4750, loss = 1.64535033\n",
      "Iteration 4751, loss = 1.64528644\n",
      "Iteration 4752, loss = 1.64521909\n",
      "Iteration 4753, loss = 1.64515925\n",
      "Iteration 4754, loss = 1.64509018\n",
      "Iteration 4755, loss = 1.64503149\n",
      "Iteration 4756, loss = 1.64496434\n",
      "Iteration 4757, loss = 1.64490109\n",
      "Iteration 4758, loss = 1.64483547\n",
      "Iteration 4759, loss = 1.64477167\n",
      "Iteration 4760, loss = 1.64470863\n",
      "Iteration 4761, loss = 1.64464046\n",
      "Iteration 4762, loss = 1.64458260\n",
      "Iteration 4763, loss = 1.64451149\n",
      "Iteration 4764, loss = 1.64444902\n",
      "Iteration 4765, loss = 1.64438396\n",
      "Iteration 4766, loss = 1.64431632\n",
      "Iteration 4767, loss = 1.64425978\n",
      "Iteration 4768, loss = 1.64418961\n",
      "Iteration 4769, loss = 1.64412710\n",
      "Iteration 4770, loss = 1.64406050\n",
      "Iteration 4771, loss = 1.64399838\n",
      "Iteration 4772, loss = 1.64393077\n",
      "Iteration 4773, loss = 1.64386658\n",
      "Iteration 4774, loss = 1.64380263\n",
      "Iteration 4775, loss = 1.64373793\n",
      "Iteration 4776, loss = 1.64367497\n",
      "Iteration 4777, loss = 1.64361002\n",
      "Iteration 4778, loss = 1.64354837\n",
      "Iteration 4779, loss = 1.64348114\n",
      "Iteration 4780, loss = 1.64342049\n",
      "Iteration 4781, loss = 1.64335343\n",
      "Iteration 4782, loss = 1.64329253\n",
      "Iteration 4783, loss = 1.64322987\n",
      "Iteration 4784, loss = 1.64316119\n",
      "Iteration 4785, loss = 1.64309896\n",
      "Iteration 4786, loss = 1.64303380\n",
      "Iteration 4787, loss = 1.64296599\n",
      "Iteration 4788, loss = 1.64290146\n",
      "Iteration 4789, loss = 1.64283826\n",
      "Iteration 4790, loss = 1.64277355\n",
      "Iteration 4791, loss = 1.64270899\n",
      "Iteration 4792, loss = 1.64264466\n",
      "Iteration 4793, loss = 1.64258138\n",
      "Iteration 4794, loss = 1.64251740\n",
      "Iteration 4795, loss = 1.64245216\n",
      "Iteration 4796, loss = 1.64238649\n",
      "Iteration 4797, loss = 1.64232297\n",
      "Iteration 4798, loss = 1.64226132\n",
      "Iteration 4799, loss = 1.64219653\n",
      "Iteration 4800, loss = 1.64213248\n",
      "Iteration 4801, loss = 1.64206625\n",
      "Iteration 4802, loss = 1.64200246\n",
      "Iteration 4803, loss = 1.64193777\n",
      "Iteration 4804, loss = 1.64187280\n",
      "Iteration 4805, loss = 1.64180894\n",
      "Iteration 4806, loss = 1.64174443\n",
      "Iteration 4807, loss = 1.64168105\n",
      "Iteration 4808, loss = 1.64161691\n",
      "Iteration 4809, loss = 1.64155426\n",
      "Iteration 4810, loss = 1.64149425\n",
      "Iteration 4811, loss = 1.64143126\n",
      "Iteration 4812, loss = 1.64136002\n",
      "Iteration 4813, loss = 1.64131524\n",
      "Iteration 4814, loss = 1.64124616\n",
      "Iteration 4815, loss = 1.64117276\n",
      "Iteration 4816, loss = 1.64111592\n",
      "Iteration 4817, loss = 1.64104982\n",
      "Iteration 4818, loss = 1.64097864\n",
      "Iteration 4819, loss = 1.64092324\n",
      "Iteration 4820, loss = 1.64085470\n",
      "Iteration 4821, loss = 1.64078792\n",
      "Iteration 4822, loss = 1.64072782\n",
      "Iteration 4823, loss = 1.64066332\n",
      "Iteration 4824, loss = 1.64059498\n",
      "Iteration 4825, loss = 1.64053257\n",
      "Iteration 4826, loss = 1.64046944\n",
      "Iteration 4827, loss = 1.64040273\n",
      "Iteration 4828, loss = 1.64033983\n",
      "Iteration 4829, loss = 1.64027411\n",
      "Iteration 4830, loss = 1.64021125\n",
      "Iteration 4831, loss = 1.64014567\n",
      "Iteration 4832, loss = 1.64008228\n",
      "Iteration 4833, loss = 1.64001992\n",
      "Iteration 4834, loss = 1.63995378\n",
      "Iteration 4835, loss = 1.63989286\n",
      "Iteration 4836, loss = 1.63982732\n",
      "Iteration 4837, loss = 1.63976508\n",
      "Iteration 4838, loss = 1.63969801\n",
      "Iteration 4839, loss = 1.63963719\n",
      "Iteration 4840, loss = 1.63957439\n",
      "Iteration 4841, loss = 1.63950567\n",
      "Iteration 4842, loss = 1.63945125\n",
      "Iteration 4843, loss = 1.63938242\n",
      "Iteration 4844, loss = 1.63931887\n",
      "Iteration 4845, loss = 1.63925895\n",
      "Iteration 4846, loss = 1.63919079\n",
      "Iteration 4847, loss = 1.63912356\n",
      "Iteration 4848, loss = 1.63906053\n",
      "Iteration 4849, loss = 1.63899696\n",
      "Iteration 4850, loss = 1.63893351\n",
      "Iteration 4851, loss = 1.63886651\n",
      "Iteration 4852, loss = 1.63881194\n",
      "Iteration 4853, loss = 1.63873858\n",
      "Iteration 4854, loss = 1.63867697\n",
      "Iteration 4855, loss = 1.63861226\n",
      "Iteration 4856, loss = 1.63854761\n",
      "Iteration 4857, loss = 1.63848465\n",
      "Iteration 4858, loss = 1.63842048\n",
      "Iteration 4859, loss = 1.63835670\n",
      "Iteration 4860, loss = 1.63829276\n",
      "Iteration 4861, loss = 1.63822995\n",
      "Iteration 4862, loss = 1.63816502\n",
      "Iteration 4863, loss = 1.63810107\n",
      "Iteration 4864, loss = 1.63803737\n",
      "Iteration 4865, loss = 1.63798125\n",
      "Iteration 4866, loss = 1.63791265\n",
      "Iteration 4867, loss = 1.63785143\n",
      "Iteration 4868, loss = 1.63778343\n",
      "Iteration 4869, loss = 1.63772546\n",
      "Iteration 4870, loss = 1.63765890\n",
      "Iteration 4871, loss = 1.63759631\n",
      "Iteration 4872, loss = 1.63752882\n",
      "Iteration 4873, loss = 1.63746601\n",
      "Iteration 4874, loss = 1.63740513\n",
      "Iteration 4875, loss = 1.63734200\n",
      "Iteration 4876, loss = 1.63727475\n",
      "Iteration 4877, loss = 1.63722045\n",
      "Iteration 4878, loss = 1.63714812\n",
      "Iteration 4879, loss = 1.63709293\n",
      "Iteration 4880, loss = 1.63703272\n",
      "Iteration 4881, loss = 1.63696424\n",
      "Iteration 4882, loss = 1.63689561\n",
      "Iteration 4883, loss = 1.63683440\n",
      "Iteration 4884, loss = 1.63676937\n",
      "Iteration 4885, loss = 1.63670730\n",
      "Iteration 4886, loss = 1.63663985\n",
      "Iteration 4887, loss = 1.63658055\n",
      "Iteration 4888, loss = 1.63651340\n",
      "Iteration 4889, loss = 1.63645285\n",
      "Iteration 4890, loss = 1.63638944\n",
      "Iteration 4891, loss = 1.63632206\n",
      "Iteration 4892, loss = 1.63626165\n",
      "Iteration 4893, loss = 1.63619415\n",
      "Iteration 4894, loss = 1.63613141\n",
      "Iteration 4895, loss = 1.63606867\n",
      "Iteration 4896, loss = 1.63600406\n",
      "Iteration 4897, loss = 1.63594225\n",
      "Iteration 4898, loss = 1.63587907\n",
      "Iteration 4899, loss = 1.63581490\n",
      "Iteration 4900, loss = 1.63575247\n",
      "Iteration 4901, loss = 1.63568788\n",
      "Iteration 4902, loss = 1.63562326\n",
      "Iteration 4903, loss = 1.63556206\n",
      "Iteration 4904, loss = 1.63549820\n",
      "Iteration 4905, loss = 1.63543591\n",
      "Iteration 4906, loss = 1.63537386\n",
      "Iteration 4907, loss = 1.63530797\n",
      "Iteration 4908, loss = 1.63524485\n",
      "Iteration 4909, loss = 1.63518115\n",
      "Iteration 4910, loss = 1.63511837\n",
      "Iteration 4911, loss = 1.63505518\n",
      "Iteration 4912, loss = 1.63499169\n",
      "Iteration 4913, loss = 1.63492796\n",
      "Iteration 4914, loss = 1.63486412\n",
      "Iteration 4915, loss = 1.63480135\n",
      "Iteration 4916, loss = 1.63473853\n",
      "Iteration 4917, loss = 1.63467487\n",
      "Iteration 4918, loss = 1.63461870\n",
      "Iteration 4919, loss = 1.63454843\n",
      "Iteration 4920, loss = 1.63448839\n",
      "Iteration 4921, loss = 1.63442136\n",
      "Iteration 4922, loss = 1.63436698\n",
      "Iteration 4923, loss = 1.63429599\n",
      "Iteration 4924, loss = 1.63424403\n",
      "Iteration 4925, loss = 1.63418769\n",
      "Iteration 4926, loss = 1.63412269\n",
      "Iteration 4927, loss = 1.63405006\n",
      "Iteration 4928, loss = 1.63398567\n",
      "Iteration 4929, loss = 1.63392849\n",
      "Iteration 4930, loss = 1.63385562\n",
      "Iteration 4931, loss = 1.63379711\n",
      "Iteration 4932, loss = 1.63373518\n",
      "Iteration 4933, loss = 1.63366930\n",
      "Iteration 4934, loss = 1.63360252\n",
      "Iteration 4935, loss = 1.63354432\n",
      "Iteration 4936, loss = 1.63347491\n",
      "Iteration 4937, loss = 1.63341362\n",
      "Iteration 4938, loss = 1.63334851\n",
      "Iteration 4939, loss = 1.63328681\n",
      "Iteration 4940, loss = 1.63322579\n",
      "Iteration 4941, loss = 1.63316167\n",
      "Iteration 4942, loss = 1.63310000\n",
      "Iteration 4943, loss = 1.63303303\n",
      "Iteration 4944, loss = 1.63297738\n",
      "Iteration 4945, loss = 1.63291654\n",
      "Iteration 4946, loss = 1.63284802\n",
      "Iteration 4947, loss = 1.63278409\n",
      "Iteration 4948, loss = 1.63272018\n",
      "Iteration 4949, loss = 1.63265984\n",
      "Iteration 4950, loss = 1.63259951\n",
      "Iteration 4951, loss = 1.63253355\n",
      "Iteration 4952, loss = 1.63246805\n",
      "Iteration 4953, loss = 1.63240956\n",
      "Iteration 4954, loss = 1.63234316\n",
      "Iteration 4955, loss = 1.63228511\n",
      "Iteration 4956, loss = 1.63221934\n",
      "Iteration 4957, loss = 1.63215489\n",
      "Iteration 4958, loss = 1.63209612\n",
      "Iteration 4959, loss = 1.63202560\n",
      "Iteration 4960, loss = 1.63196916\n",
      "Iteration 4961, loss = 1.63190484\n",
      "Iteration 4962, loss = 1.63183719\n",
      "Iteration 4963, loss = 1.63177485\n",
      "Iteration 4964, loss = 1.63171150\n",
      "Iteration 4965, loss = 1.63164744\n",
      "Iteration 4966, loss = 1.63158525\n",
      "Iteration 4967, loss = 1.63152255\n",
      "Iteration 4968, loss = 1.63146059\n",
      "Iteration 4969, loss = 1.63139978\n",
      "Iteration 4970, loss = 1.63133737\n",
      "Iteration 4971, loss = 1.63127396\n",
      "Iteration 4972, loss = 1.63121411\n",
      "Iteration 4973, loss = 1.63114953\n",
      "Iteration 4974, loss = 1.63108569\n",
      "Iteration 4975, loss = 1.63102464\n",
      "Iteration 4976, loss = 1.63096076\n",
      "Iteration 4977, loss = 1.63089871\n",
      "Iteration 4978, loss = 1.63083278\n",
      "Iteration 4979, loss = 1.63077196\n",
      "Iteration 4980, loss = 1.63070859\n",
      "Iteration 4981, loss = 1.63064637\n",
      "Iteration 4982, loss = 1.63058244\n",
      "Iteration 4983, loss = 1.63052028\n",
      "Iteration 4984, loss = 1.63045947\n",
      "Iteration 4985, loss = 1.63039511\n",
      "Iteration 4986, loss = 1.63033298\n",
      "Iteration 4987, loss = 1.63026864\n",
      "Iteration 4988, loss = 1.63020626\n",
      "Iteration 4989, loss = 1.63014318\n",
      "Iteration 4990, loss = 1.63008115\n",
      "Iteration 4991, loss = 1.63001898\n",
      "Iteration 4992, loss = 1.62995606\n",
      "Iteration 4993, loss = 1.62989355\n",
      "Iteration 4994, loss = 1.62983036\n",
      "Iteration 4995, loss = 1.62977010\n",
      "Iteration 4996, loss = 1.62970523\n",
      "Iteration 4997, loss = 1.62964559\n",
      "Iteration 4998, loss = 1.62958182\n",
      "Iteration 4999, loss = 1.62952154\n",
      "Iteration 5000, loss = 1.62945721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5001, loss = 1.62939379\n",
      "Iteration 5002, loss = 1.62933253\n",
      "Iteration 5003, loss = 1.62926939\n",
      "Iteration 5004, loss = 1.62920807\n",
      "Iteration 5005, loss = 1.62914466\n",
      "Iteration 5006, loss = 1.62908153\n",
      "Iteration 5007, loss = 1.62901882\n",
      "Iteration 5008, loss = 1.62895352\n",
      "Iteration 5009, loss = 1.62889273\n",
      "Iteration 5010, loss = 1.62882949\n",
      "Iteration 5011, loss = 1.62876644\n",
      "Iteration 5012, loss = 1.62870451\n",
      "Iteration 5013, loss = 1.62864412\n",
      "Iteration 5014, loss = 1.62857974\n",
      "Iteration 5015, loss = 1.62851983\n",
      "Iteration 5016, loss = 1.62845412\n",
      "Iteration 5017, loss = 1.62840129\n",
      "Iteration 5018, loss = 1.62832940\n",
      "Iteration 5019, loss = 1.62826841\n",
      "Iteration 5020, loss = 1.62820585\n",
      "Iteration 5021, loss = 1.62814602\n",
      "Iteration 5022, loss = 1.62808334\n",
      "Iteration 5023, loss = 1.62801907\n",
      "Iteration 5024, loss = 1.62795665\n",
      "Iteration 5025, loss = 1.62789437\n",
      "Iteration 5026, loss = 1.62783084\n",
      "Iteration 5027, loss = 1.62776847\n",
      "Iteration 5028, loss = 1.62770912\n",
      "Iteration 5029, loss = 1.62764459\n",
      "Iteration 5030, loss = 1.62759198\n",
      "Iteration 5031, loss = 1.62752186\n",
      "Iteration 5032, loss = 1.62746532\n",
      "Iteration 5033, loss = 1.62740497\n",
      "Iteration 5034, loss = 1.62733720\n",
      "Iteration 5035, loss = 1.62727483\n",
      "Iteration 5036, loss = 1.62721509\n",
      "Iteration 5037, loss = 1.62715140\n",
      "Iteration 5038, loss = 1.62709328\n",
      "Iteration 5039, loss = 1.62703095\n",
      "Iteration 5040, loss = 1.62696499\n",
      "Iteration 5041, loss = 1.62690098\n",
      "Iteration 5042, loss = 1.62684371\n",
      "Iteration 5043, loss = 1.62677456\n",
      "Iteration 5044, loss = 1.62671702\n",
      "Iteration 5045, loss = 1.62665218\n",
      "Iteration 5046, loss = 1.62659364\n",
      "Iteration 5047, loss = 1.62652653\n",
      "Iteration 5048, loss = 1.62647200\n",
      "Iteration 5049, loss = 1.62641415\n",
      "Iteration 5050, loss = 1.62634833\n",
      "Iteration 5051, loss = 1.62627740\n",
      "Iteration 5052, loss = 1.62622450\n",
      "Iteration 5053, loss = 1.62615466\n",
      "Iteration 5054, loss = 1.62609940\n",
      "Iteration 5055, loss = 1.62604054\n",
      "Iteration 5056, loss = 1.62597380\n",
      "Iteration 5057, loss = 1.62590581\n",
      "Iteration 5058, loss = 1.62585631\n",
      "Iteration 5059, loss = 1.62578382\n",
      "Iteration 5060, loss = 1.62572824\n",
      "Iteration 5061, loss = 1.62567323\n",
      "Iteration 5062, loss = 1.62560993\n",
      "Iteration 5063, loss = 1.62553941\n",
      "Iteration 5064, loss = 1.62547526\n",
      "Iteration 5065, loss = 1.62541993\n",
      "Iteration 5066, loss = 1.62534906\n",
      "Iteration 5067, loss = 1.62529249\n",
      "Iteration 5068, loss = 1.62523618\n",
      "Iteration 5069, loss = 1.62517189\n",
      "Iteration 5070, loss = 1.62510291\n",
      "Iteration 5071, loss = 1.62504718\n",
      "Iteration 5072, loss = 1.62498117\n",
      "Iteration 5073, loss = 1.62491621\n",
      "Iteration 5074, loss = 1.62485722\n",
      "Iteration 5075, loss = 1.62479245\n",
      "Iteration 5076, loss = 1.62473470\n",
      "Iteration 5077, loss = 1.62467019\n",
      "Iteration 5078, loss = 1.62460756\n",
      "Iteration 5079, loss = 1.62454400\n",
      "Iteration 5080, loss = 1.62448507\n",
      "Iteration 5081, loss = 1.62441953\n",
      "Iteration 5082, loss = 1.62436376\n",
      "Iteration 5083, loss = 1.62430115\n",
      "Iteration 5084, loss = 1.62423593\n",
      "Iteration 5085, loss = 1.62418297\n",
      "Iteration 5086, loss = 1.62411133\n",
      "Iteration 5087, loss = 1.62405563\n",
      "Iteration 5088, loss = 1.62399259\n",
      "Iteration 5089, loss = 1.62392782\n",
      "Iteration 5090, loss = 1.62386862\n",
      "Iteration 5091, loss = 1.62380429\n",
      "Iteration 5092, loss = 1.62374230\n",
      "Iteration 5093, loss = 1.62368354\n",
      "Iteration 5094, loss = 1.62361727\n",
      "Iteration 5095, loss = 1.62355932\n",
      "Iteration 5096, loss = 1.62349548\n",
      "Iteration 5097, loss = 1.62344059\n",
      "Iteration 5098, loss = 1.62337096\n",
      "Iteration 5099, loss = 1.62331600\n",
      "Iteration 5100, loss = 1.62325433\n",
      "Iteration 5101, loss = 1.62318694\n",
      "Iteration 5102, loss = 1.62313683\n",
      "Iteration 5103, loss = 1.62306424\n",
      "Iteration 5104, loss = 1.62300587\n",
      "Iteration 5105, loss = 1.62294466\n",
      "Iteration 5106, loss = 1.62288094\n",
      "Iteration 5107, loss = 1.62282020\n",
      "Iteration 5108, loss = 1.62275740\n",
      "Iteration 5109, loss = 1.62269559\n",
      "Iteration 5110, loss = 1.62263529\n",
      "Iteration 5111, loss = 1.62257239\n",
      "Iteration 5112, loss = 1.62251353\n",
      "Iteration 5113, loss = 1.62245086\n",
      "Iteration 5114, loss = 1.62239215\n",
      "Iteration 5115, loss = 1.62232654\n",
      "Iteration 5116, loss = 1.62226754\n",
      "Iteration 5117, loss = 1.62220358\n",
      "Iteration 5118, loss = 1.62214783\n",
      "Iteration 5119, loss = 1.62208096\n",
      "Iteration 5120, loss = 1.62202012\n",
      "Iteration 5121, loss = 1.62196081\n",
      "Iteration 5122, loss = 1.62189942\n",
      "Iteration 5123, loss = 1.62183558\n",
      "Iteration 5124, loss = 1.62177562\n",
      "Iteration 5125, loss = 1.62171552\n",
      "Iteration 5126, loss = 1.62165514\n",
      "Iteration 5127, loss = 1.62159535\n",
      "Iteration 5128, loss = 1.62153091\n",
      "Iteration 5129, loss = 1.62147053\n",
      "Iteration 5130, loss = 1.62140570\n",
      "Iteration 5131, loss = 1.62135157\n",
      "Iteration 5132, loss = 1.62128475\n",
      "Iteration 5133, loss = 1.62122784\n",
      "Iteration 5134, loss = 1.62116395\n",
      "Iteration 5135, loss = 1.62110402\n",
      "Iteration 5136, loss = 1.62104505\n",
      "Iteration 5137, loss = 1.62097868\n",
      "Iteration 5138, loss = 1.62092272\n",
      "Iteration 5139, loss = 1.62086093\n",
      "Iteration 5140, loss = 1.62079651\n",
      "Iteration 5141, loss = 1.62073873\n",
      "Iteration 5142, loss = 1.62067307\n",
      "Iteration 5143, loss = 1.62061434\n",
      "Iteration 5144, loss = 1.62055274\n",
      "Iteration 5145, loss = 1.62049260\n",
      "Iteration 5146, loss = 1.62043424\n",
      "Iteration 5147, loss = 1.62036792\n",
      "Iteration 5148, loss = 1.62031145\n",
      "Iteration 5149, loss = 1.62024899\n",
      "Iteration 5150, loss = 1.62018446\n",
      "Iteration 5151, loss = 1.62012805\n",
      "Iteration 5152, loss = 1.62006457\n",
      "Iteration 5153, loss = 1.62000162\n",
      "Iteration 5154, loss = 1.61994112\n",
      "Iteration 5155, loss = 1.61987823\n",
      "Iteration 5156, loss = 1.61981719\n",
      "Iteration 5157, loss = 1.61975602\n",
      "Iteration 5158, loss = 1.61969516\n",
      "Iteration 5159, loss = 1.61963333\n",
      "Iteration 5160, loss = 1.61957281\n",
      "Iteration 5161, loss = 1.61951114\n",
      "Iteration 5162, loss = 1.61944954\n",
      "Iteration 5163, loss = 1.61939172\n",
      "Iteration 5164, loss = 1.61932825\n",
      "Iteration 5165, loss = 1.61926773\n",
      "Iteration 5166, loss = 1.61920688\n",
      "Iteration 5167, loss = 1.61914435\n",
      "Iteration 5168, loss = 1.61908453\n",
      "Iteration 5169, loss = 1.61902435\n",
      "Iteration 5170, loss = 1.61896888\n",
      "Iteration 5171, loss = 1.61891041\n",
      "Iteration 5172, loss = 1.61884438\n",
      "Iteration 5173, loss = 1.61878887\n",
      "Iteration 5174, loss = 1.61872001\n",
      "Iteration 5175, loss = 1.61866453\n",
      "Iteration 5176, loss = 1.61860539\n",
      "Iteration 5177, loss = 1.61853896\n",
      "Iteration 5178, loss = 1.61848637\n",
      "Iteration 5179, loss = 1.61841718\n",
      "Iteration 5180, loss = 1.61836062\n",
      "Iteration 5181, loss = 1.61830323\n",
      "Iteration 5182, loss = 1.61823857\n",
      "Iteration 5183, loss = 1.61817295\n",
      "Iteration 5184, loss = 1.61812165\n",
      "Iteration 5185, loss = 1.61805133\n",
      "Iteration 5186, loss = 1.61799296\n",
      "Iteration 5187, loss = 1.61793136\n",
      "Iteration 5188, loss = 1.61786835\n",
      "Iteration 5189, loss = 1.61780944\n",
      "Iteration 5190, loss = 1.61774954\n",
      "Iteration 5191, loss = 1.61768788\n",
      "Iteration 5192, loss = 1.61762564\n",
      "Iteration 5193, loss = 1.61756609\n",
      "Iteration 5194, loss = 1.61750525\n",
      "Iteration 5195, loss = 1.61744599\n",
      "Iteration 5196, loss = 1.61738265\n",
      "Iteration 5197, loss = 1.61732356\n",
      "Iteration 5198, loss = 1.61726218\n",
      "Iteration 5199, loss = 1.61720181\n",
      "Iteration 5200, loss = 1.61714057\n",
      "Iteration 5201, loss = 1.61708111\n",
      "Iteration 5202, loss = 1.61702447\n",
      "Iteration 5203, loss = 1.61696516\n",
      "Iteration 5204, loss = 1.61690132\n",
      "Iteration 5205, loss = 1.61684370\n",
      "Iteration 5206, loss = 1.61677900\n",
      "Iteration 5207, loss = 1.61671856\n",
      "Iteration 5208, loss = 1.61665867\n",
      "Iteration 5209, loss = 1.61659968\n",
      "Iteration 5210, loss = 1.61654027\n",
      "Iteration 5211, loss = 1.61647575\n",
      "Iteration 5212, loss = 1.61641726\n",
      "Iteration 5213, loss = 1.61635548\n",
      "Iteration 5214, loss = 1.61629962\n",
      "Iteration 5215, loss = 1.61623953\n",
      "Iteration 5216, loss = 1.61617465\n",
      "Iteration 5217, loss = 1.61612061\n",
      "Iteration 5218, loss = 1.61606057\n",
      "Iteration 5219, loss = 1.61599390\n",
      "Iteration 5220, loss = 1.61594018\n",
      "Iteration 5221, loss = 1.61587408\n",
      "Iteration 5222, loss = 1.61581297\n",
      "Iteration 5223, loss = 1.61575050\n",
      "Iteration 5224, loss = 1.61569618\n",
      "Iteration 5225, loss = 1.61562970\n",
      "Iteration 5226, loss = 1.61557316\n",
      "Iteration 5227, loss = 1.61551499\n",
      "Iteration 5228, loss = 1.61545075\n",
      "Iteration 5229, loss = 1.61539441\n",
      "Iteration 5230, loss = 1.61533286\n",
      "Iteration 5231, loss = 1.61527127\n",
      "Iteration 5232, loss = 1.61521393\n",
      "Iteration 5233, loss = 1.61514927\n",
      "Iteration 5234, loss = 1.61509552\n",
      "Iteration 5235, loss = 1.61503046\n",
      "Iteration 5236, loss = 1.61497225\n",
      "Iteration 5237, loss = 1.61491599\n",
      "Iteration 5238, loss = 1.61485235\n",
      "Iteration 5239, loss = 1.61478624\n",
      "Iteration 5240, loss = 1.61473257\n",
      "Iteration 5241, loss = 1.61466608\n",
      "Iteration 5242, loss = 1.61460863\n",
      "Iteration 5243, loss = 1.61454810\n",
      "Iteration 5244, loss = 1.61448454\n",
      "Iteration 5245, loss = 1.61443265\n",
      "Iteration 5246, loss = 1.61436541\n",
      "Iteration 5247, loss = 1.61431568\n",
      "Iteration 5248, loss = 1.61426218\n",
      "Iteration 5249, loss = 1.61420097\n",
      "Iteration 5250, loss = 1.61413308\n",
      "Iteration 5251, loss = 1.61406865\n",
      "Iteration 5252, loss = 1.61401182\n",
      "Iteration 5253, loss = 1.61394907\n",
      "Iteration 5254, loss = 1.61389379\n",
      "Iteration 5255, loss = 1.61383122\n",
      "Iteration 5256, loss = 1.61376583\n",
      "Iteration 5257, loss = 1.61371713\n",
      "Iteration 5258, loss = 1.61364747\n",
      "Iteration 5259, loss = 1.61359086\n",
      "Iteration 5260, loss = 1.61353731\n",
      "Iteration 5261, loss = 1.61347697\n",
      "Iteration 5262, loss = 1.61340975\n",
      "Iteration 5263, loss = 1.61335212\n",
      "Iteration 5264, loss = 1.61330455\n",
      "Iteration 5265, loss = 1.61322971\n",
      "Iteration 5266, loss = 1.61317441\n",
      "Iteration 5267, loss = 1.61312320\n",
      "Iteration 5268, loss = 1.61306420\n",
      "Iteration 5269, loss = 1.61299825\n",
      "Iteration 5270, loss = 1.61293085\n",
      "Iteration 5271, loss = 1.61288093\n",
      "Iteration 5272, loss = 1.61282106\n",
      "Iteration 5273, loss = 1.61275186\n",
      "Iteration 5274, loss = 1.61269550\n",
      "Iteration 5275, loss = 1.61263907\n",
      "Iteration 5276, loss = 1.61257748\n",
      "Iteration 5277, loss = 1.61251155\n",
      "Iteration 5278, loss = 1.61245583\n",
      "Iteration 5279, loss = 1.61239388\n",
      "Iteration 5280, loss = 1.61233302\n",
      "Iteration 5281, loss = 1.61228100\n",
      "Iteration 5282, loss = 1.61222146\n",
      "Iteration 5283, loss = 1.61215508\n",
      "Iteration 5284, loss = 1.61209285\n",
      "Iteration 5285, loss = 1.61203821\n",
      "Iteration 5286, loss = 1.61197168\n",
      "Iteration 5287, loss = 1.61191336\n",
      "Iteration 5288, loss = 1.61185404\n",
      "Iteration 5289, loss = 1.61179144\n",
      "Iteration 5290, loss = 1.61173793\n",
      "Iteration 5291, loss = 1.61167287\n",
      "Iteration 5292, loss = 1.61161781\n",
      "Iteration 5293, loss = 1.61156016\n",
      "Iteration 5294, loss = 1.61149534\n",
      "Iteration 5295, loss = 1.61144103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5296, loss = 1.61138114\n",
      "Iteration 5297, loss = 1.61131455\n",
      "Iteration 5298, loss = 1.61126027\n",
      "Iteration 5299, loss = 1.61119962\n",
      "Iteration 5300, loss = 1.61113625\n",
      "Iteration 5301, loss = 1.61107698\n",
      "Iteration 5302, loss = 1.61101622\n",
      "Iteration 5303, loss = 1.61095840\n",
      "Iteration 5304, loss = 1.61089727\n",
      "Iteration 5305, loss = 1.61083935\n",
      "Iteration 5306, loss = 1.61077868\n",
      "Iteration 5307, loss = 1.61071916\n",
      "Iteration 5308, loss = 1.61065893\n",
      "Iteration 5309, loss = 1.61059927\n",
      "Iteration 5310, loss = 1.61054064\n",
      "Iteration 5311, loss = 1.61048140\n",
      "Iteration 5312, loss = 1.61042125\n",
      "Iteration 5313, loss = 1.61036158\n",
      "Iteration 5314, loss = 1.61030403\n",
      "Iteration 5315, loss = 1.61024251\n",
      "Iteration 5316, loss = 1.61018517\n",
      "Iteration 5317, loss = 1.61012778\n",
      "Iteration 5318, loss = 1.61006893\n",
      "Iteration 5319, loss = 1.61000422\n",
      "Iteration 5320, loss = 1.60995666\n",
      "Iteration 5321, loss = 1.60988727\n",
      "Iteration 5322, loss = 1.60982946\n",
      "Iteration 5323, loss = 1.60977083\n",
      "Iteration 5324, loss = 1.60971012\n",
      "Iteration 5325, loss = 1.60965110\n",
      "Iteration 5326, loss = 1.60959204\n",
      "Iteration 5327, loss = 1.60953333\n",
      "Iteration 5328, loss = 1.60947257\n",
      "Iteration 5329, loss = 1.60941376\n",
      "Iteration 5330, loss = 1.60935664\n",
      "Iteration 5331, loss = 1.60929763\n",
      "Iteration 5332, loss = 1.60923637\n",
      "Iteration 5333, loss = 1.60917723\n",
      "Iteration 5334, loss = 1.60911757\n",
      "Iteration 5335, loss = 1.60906059\n",
      "Iteration 5336, loss = 1.60899933\n",
      "Iteration 5337, loss = 1.60893949\n",
      "Iteration 5338, loss = 1.60888223\n",
      "Iteration 5339, loss = 1.60882274\n",
      "Iteration 5340, loss = 1.60876409\n",
      "Iteration 5341, loss = 1.60870628\n",
      "Iteration 5342, loss = 1.60864482\n",
      "Iteration 5343, loss = 1.60858548\n",
      "Iteration 5344, loss = 1.60852522\n",
      "Iteration 5345, loss = 1.60846553\n",
      "Iteration 5346, loss = 1.60840893\n",
      "Iteration 5347, loss = 1.60834871\n",
      "Iteration 5348, loss = 1.60829081\n",
      "Iteration 5349, loss = 1.60823171\n",
      "Iteration 5350, loss = 1.60817240\n",
      "Iteration 5351, loss = 1.60811225\n",
      "Iteration 5352, loss = 1.60805476\n",
      "Iteration 5353, loss = 1.60799532\n",
      "Iteration 5354, loss = 1.60793568\n",
      "Iteration 5355, loss = 1.60788523\n",
      "Iteration 5356, loss = 1.60781925\n",
      "Iteration 5357, loss = 1.60776145\n",
      "Iteration 5358, loss = 1.60770126\n",
      "Iteration 5359, loss = 1.60764201\n",
      "Iteration 5360, loss = 1.60758677\n",
      "Iteration 5361, loss = 1.60752897\n",
      "Iteration 5362, loss = 1.60746538\n",
      "Iteration 5363, loss = 1.60741777\n",
      "Iteration 5364, loss = 1.60735109\n",
      "Iteration 5365, loss = 1.60729820\n",
      "Iteration 5366, loss = 1.60724719\n",
      "Iteration 5367, loss = 1.60718860\n",
      "Iteration 5368, loss = 1.60712335\n",
      "Iteration 5369, loss = 1.60705542\n",
      "Iteration 5370, loss = 1.60701226\n",
      "Iteration 5371, loss = 1.60694565\n",
      "Iteration 5372, loss = 1.60688448\n",
      "Iteration 5373, loss = 1.60683702\n",
      "Iteration 5374, loss = 1.60678266\n",
      "Iteration 5375, loss = 1.60672120\n",
      "Iteration 5376, loss = 1.60665361\n",
      "Iteration 5377, loss = 1.60658646\n",
      "Iteration 5378, loss = 1.60653778\n",
      "Iteration 5379, loss = 1.60647573\n",
      "Iteration 5380, loss = 1.60641062\n",
      "Iteration 5381, loss = 1.60635503\n",
      "Iteration 5382, loss = 1.60629671\n",
      "Iteration 5383, loss = 1.60623387\n",
      "Iteration 5384, loss = 1.60617897\n",
      "Iteration 5385, loss = 1.60611605\n",
      "Iteration 5386, loss = 1.60606180\n",
      "Iteration 5387, loss = 1.60600632\n",
      "Iteration 5388, loss = 1.60594392\n",
      "Iteration 5389, loss = 1.60588184\n",
      "Iteration 5390, loss = 1.60582404\n",
      "Iteration 5391, loss = 1.60576789\n",
      "Iteration 5392, loss = 1.60571199\n",
      "Iteration 5393, loss = 1.60564933\n",
      "Iteration 5394, loss = 1.60559246\n",
      "Iteration 5395, loss = 1.60552895\n",
      "Iteration 5396, loss = 1.60547525\n",
      "Iteration 5397, loss = 1.60541793\n",
      "Iteration 5398, loss = 1.60535504\n",
      "Iteration 5399, loss = 1.60530004\n",
      "Iteration 5400, loss = 1.60523729\n",
      "Iteration 5401, loss = 1.60518246\n",
      "Iteration 5402, loss = 1.60512553\n",
      "Iteration 5403, loss = 1.60506294\n",
      "Iteration 5404, loss = 1.60500778\n",
      "Iteration 5405, loss = 1.60494485\n",
      "Iteration 5406, loss = 1.60488679\n",
      "Iteration 5407, loss = 1.60482743\n",
      "Iteration 5408, loss = 1.60477103\n",
      "Iteration 5409, loss = 1.60471034\n",
      "Iteration 5410, loss = 1.60465200\n",
      "Iteration 5411, loss = 1.60459536\n",
      "Iteration 5412, loss = 1.60453600\n",
      "Iteration 5413, loss = 1.60447655\n",
      "Iteration 5414, loss = 1.60441779\n",
      "Iteration 5415, loss = 1.60436003\n",
      "Iteration 5416, loss = 1.60430455\n",
      "Iteration 5417, loss = 1.60424440\n",
      "Iteration 5418, loss = 1.60419585\n",
      "Iteration 5419, loss = 1.60412880\n",
      "Iteration 5420, loss = 1.60407500\n",
      "Iteration 5421, loss = 1.60401522\n",
      "Iteration 5422, loss = 1.60395552\n",
      "Iteration 5423, loss = 1.60389569\n",
      "Iteration 5424, loss = 1.60383725\n",
      "Iteration 5425, loss = 1.60377947\n",
      "Iteration 5426, loss = 1.60372093\n",
      "Iteration 5427, loss = 1.60366504\n",
      "Iteration 5428, loss = 1.60360464\n",
      "Iteration 5429, loss = 1.60355024\n",
      "Iteration 5430, loss = 1.60348917\n",
      "Iteration 5431, loss = 1.60343531\n",
      "Iteration 5432, loss = 1.60337621\n",
      "Iteration 5433, loss = 1.60331469\n",
      "Iteration 5434, loss = 1.60326456\n",
      "Iteration 5435, loss = 1.60319897\n",
      "Iteration 5436, loss = 1.60314485\n",
      "Iteration 5437, loss = 1.60308411\n",
      "Iteration 5438, loss = 1.60302795\n",
      "Iteration 5439, loss = 1.60296530\n",
      "Iteration 5440, loss = 1.60291019\n",
      "Iteration 5441, loss = 1.60284906\n",
      "Iteration 5442, loss = 1.60280390\n",
      "Iteration 5443, loss = 1.60273693\n",
      "Iteration 5444, loss = 1.60268716\n",
      "Iteration 5445, loss = 1.60263781\n",
      "Iteration 5446, loss = 1.60258103\n",
      "Iteration 5447, loss = 1.60251766\n",
      "Iteration 5448, loss = 1.60244954\n",
      "Iteration 5449, loss = 1.60239582\n",
      "Iteration 5450, loss = 1.60233286\n",
      "Iteration 5451, loss = 1.60227599\n",
      "Iteration 5452, loss = 1.60222322\n",
      "Iteration 5453, loss = 1.60216486\n",
      "Iteration 5454, loss = 1.60210091\n",
      "Iteration 5455, loss = 1.60204430\n",
      "Iteration 5456, loss = 1.60198474\n",
      "Iteration 5457, loss = 1.60192629\n",
      "Iteration 5458, loss = 1.60186904\n",
      "Iteration 5459, loss = 1.60180820\n",
      "Iteration 5460, loss = 1.60174990\n",
      "Iteration 5461, loss = 1.60169430\n",
      "Iteration 5462, loss = 1.60163477\n",
      "Iteration 5463, loss = 1.60158110\n",
      "Iteration 5464, loss = 1.60151923\n",
      "Iteration 5465, loss = 1.60146384\n",
      "Iteration 5466, loss = 1.60140281\n",
      "Iteration 5467, loss = 1.60135143\n",
      "Iteration 5468, loss = 1.60128927\n",
      "Iteration 5469, loss = 1.60123152\n",
      "Iteration 5470, loss = 1.60117747\n",
      "Iteration 5471, loss = 1.60111567\n",
      "Iteration 5472, loss = 1.60106116\n",
      "Iteration 5473, loss = 1.60100121\n",
      "Iteration 5474, loss = 1.60094841\n",
      "Iteration 5475, loss = 1.60088645\n",
      "Iteration 5476, loss = 1.60083111\n",
      "Iteration 5477, loss = 1.60077059\n",
      "Iteration 5478, loss = 1.60071430\n",
      "Iteration 5479, loss = 1.60065481\n",
      "Iteration 5480, loss = 1.60059877\n",
      "Iteration 5481, loss = 1.60054012\n",
      "Iteration 5482, loss = 1.60048273\n",
      "Iteration 5483, loss = 1.60042411\n",
      "Iteration 5484, loss = 1.60036625\n",
      "Iteration 5485, loss = 1.60030955\n",
      "Iteration 5486, loss = 1.60025386\n",
      "Iteration 5487, loss = 1.60019730\n",
      "Iteration 5488, loss = 1.60013990\n",
      "Iteration 5489, loss = 1.60008130\n",
      "Iteration 5490, loss = 1.60002306\n",
      "Iteration 5491, loss = 1.59996589\n",
      "Iteration 5492, loss = 1.59990701\n",
      "Iteration 5493, loss = 1.59984956\n",
      "Iteration 5494, loss = 1.59979191\n",
      "Iteration 5495, loss = 1.59974106\n",
      "Iteration 5496, loss = 1.59968005\n",
      "Iteration 5497, loss = 1.59962369\n",
      "Iteration 5498, loss = 1.59956606\n",
      "Iteration 5499, loss = 1.59950712\n",
      "Iteration 5500, loss = 1.59945002\n",
      "Iteration 5501, loss = 1.59939290\n",
      "Iteration 5502, loss = 1.59933551\n",
      "Iteration 5503, loss = 1.59927784\n",
      "Iteration 5504, loss = 1.59922007\n",
      "Iteration 5505, loss = 1.59916292\n",
      "Iteration 5506, loss = 1.59910623\n",
      "Iteration 5507, loss = 1.59904778\n",
      "Iteration 5508, loss = 1.59899148\n",
      "Iteration 5509, loss = 1.59893444\n",
      "Iteration 5510, loss = 1.59887743\n",
      "Iteration 5511, loss = 1.59882021\n",
      "Iteration 5512, loss = 1.59876439\n",
      "Iteration 5513, loss = 1.59870475\n",
      "Iteration 5514, loss = 1.59864820\n",
      "Iteration 5515, loss = 1.59859002\n",
      "Iteration 5516, loss = 1.59853408\n",
      "Iteration 5517, loss = 1.59847636\n",
      "Iteration 5518, loss = 1.59841897\n",
      "Iteration 5519, loss = 1.59836284\n",
      "Iteration 5520, loss = 1.59830425\n",
      "Iteration 5521, loss = 1.59824823\n",
      "Iteration 5522, loss = 1.59819181\n",
      "Iteration 5523, loss = 1.59813281\n",
      "Iteration 5524, loss = 1.59808403\n",
      "Iteration 5525, loss = 1.59802143\n",
      "Iteration 5526, loss = 1.59796704\n",
      "Iteration 5527, loss = 1.59790643\n",
      "Iteration 5528, loss = 1.59785656\n",
      "Iteration 5529, loss = 1.59779237\n",
      "Iteration 5530, loss = 1.59774068\n",
      "Iteration 5531, loss = 1.59768447\n",
      "Iteration 5532, loss = 1.59762215\n",
      "Iteration 5533, loss = 1.59757236\n",
      "Iteration 5534, loss = 1.59751290\n",
      "Iteration 5535, loss = 1.59745386\n",
      "Iteration 5536, loss = 1.59740173\n",
      "Iteration 5537, loss = 1.59734310\n",
      "Iteration 5538, loss = 1.59728076\n",
      "Iteration 5539, loss = 1.59723329\n",
      "Iteration 5540, loss = 1.59716828\n",
      "Iteration 5541, loss = 1.59711230\n",
      "Iteration 5542, loss = 1.59705915\n",
      "Iteration 5543, loss = 1.59699996\n",
      "Iteration 5544, loss = 1.59693748\n",
      "Iteration 5545, loss = 1.59689322\n",
      "Iteration 5546, loss = 1.59682651\n",
      "Iteration 5547, loss = 1.59677794\n",
      "Iteration 5548, loss = 1.59672913\n",
      "Iteration 5549, loss = 1.59667344\n",
      "Iteration 5550, loss = 1.59661153\n",
      "Iteration 5551, loss = 1.59654429\n",
      "Iteration 5552, loss = 1.59649773\n",
      "Iteration 5553, loss = 1.59643754\n",
      "Iteration 5554, loss = 1.59637231\n",
      "Iteration 5555, loss = 1.59632064\n",
      "Iteration 5556, loss = 1.59626272\n",
      "Iteration 5557, loss = 1.59620152\n",
      "Iteration 5558, loss = 1.59615490\n",
      "Iteration 5559, loss = 1.59609056\n",
      "Iteration 5560, loss = 1.59603403\n",
      "Iteration 5561, loss = 1.59598252\n",
      "Iteration 5562, loss = 1.59592466\n",
      "Iteration 5563, loss = 1.59586118\n",
      "Iteration 5564, loss = 1.59581340\n",
      "Iteration 5565, loss = 1.59575027\n",
      "Iteration 5566, loss = 1.59569888\n",
      "Iteration 5567, loss = 1.59564886\n",
      "Iteration 5568, loss = 1.59559222\n",
      "Iteration 5569, loss = 1.59552965\n",
      "Iteration 5570, loss = 1.59546421\n",
      "Iteration 5571, loss = 1.59541762\n",
      "Iteration 5572, loss = 1.59535109\n",
      "Iteration 5573, loss = 1.59529936\n",
      "Iteration 5574, loss = 1.59524341\n",
      "Iteration 5575, loss = 1.59518220\n",
      "Iteration 5576, loss = 1.59513344\n",
      "Iteration 5577, loss = 1.59507075\n",
      "Iteration 5578, loss = 1.59502012\n",
      "Iteration 5579, loss = 1.59496881\n",
      "Iteration 5580, loss = 1.59491105\n",
      "Iteration 5581, loss = 1.59484840\n",
      "Iteration 5582, loss = 1.59478935\n",
      "Iteration 5583, loss = 1.59474065\n",
      "Iteration 5584, loss = 1.59467582\n",
      "Iteration 5585, loss = 1.59462285\n",
      "Iteration 5586, loss = 1.59456549\n",
      "Iteration 5587, loss = 1.59450704\n",
      "Iteration 5588, loss = 1.59444993\n",
      "Iteration 5589, loss = 1.59439553\n",
      "Iteration 5590, loss = 1.59433886\n",
      "Iteration 5591, loss = 1.59428172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5592, loss = 1.59422458\n",
      "Iteration 5593, loss = 1.59416975\n",
      "Iteration 5594, loss = 1.59411245\n",
      "Iteration 5595, loss = 1.59405989\n",
      "Iteration 5596, loss = 1.59400051\n",
      "Iteration 5597, loss = 1.59394659\n",
      "Iteration 5598, loss = 1.59389013\n",
      "Iteration 5599, loss = 1.59383024\n",
      "Iteration 5600, loss = 1.59378352\n",
      "Iteration 5601, loss = 1.59371865\n",
      "Iteration 5602, loss = 1.59366711\n",
      "Iteration 5603, loss = 1.59361076\n",
      "Iteration 5604, loss = 1.59354924\n",
      "Iteration 5605, loss = 1.59350378\n",
      "Iteration 5606, loss = 1.59343739\n",
      "Iteration 5607, loss = 1.59339198\n",
      "Iteration 5608, loss = 1.59334291\n",
      "Iteration 5609, loss = 1.59328735\n",
      "Iteration 5610, loss = 1.59322588\n",
      "Iteration 5611, loss = 1.59315948\n",
      "Iteration 5612, loss = 1.59312123\n",
      "Iteration 5613, loss = 1.59306688\n",
      "Iteration 5614, loss = 1.59298937\n",
      "Iteration 5615, loss = 1.59294620\n",
      "Iteration 5616, loss = 1.59289856\n",
      "Iteration 5617, loss = 1.59284428\n",
      "Iteration 5618, loss = 1.59278397\n",
      "Iteration 5619, loss = 1.59271843\n",
      "Iteration 5620, loss = 1.59265591\n",
      "Iteration 5621, loss = 1.59261186\n",
      "Iteration 5622, loss = 1.59254358\n",
      "Iteration 5623, loss = 1.59249013\n",
      "Iteration 5624, loss = 1.59243516\n",
      "Iteration 5625, loss = 1.59237659\n",
      "Iteration 5626, loss = 1.59231940\n",
      "Iteration 5627, loss = 1.59226329\n",
      "Iteration 5628, loss = 1.59220888\n",
      "Iteration 5629, loss = 1.59215258\n",
      "Iteration 5630, loss = 1.59209670\n",
      "Iteration 5631, loss = 1.59203993\n",
      "Iteration 5632, loss = 1.59198606\n",
      "Iteration 5633, loss = 1.59193048\n",
      "Iteration 5634, loss = 1.59187119\n",
      "Iteration 5635, loss = 1.59182678\n",
      "Iteration 5636, loss = 1.59175978\n",
      "Iteration 5637, loss = 1.59170573\n",
      "Iteration 5638, loss = 1.59164951\n",
      "Iteration 5639, loss = 1.59159449\n",
      "Iteration 5640, loss = 1.59153760\n",
      "Iteration 5641, loss = 1.59148809\n",
      "Iteration 5642, loss = 1.59143652\n",
      "Iteration 5643, loss = 1.59137895\n",
      "Iteration 5644, loss = 1.59131615\n",
      "Iteration 5645, loss = 1.59127168\n",
      "Iteration 5646, loss = 1.59121207\n",
      "Iteration 5647, loss = 1.59115260\n",
      "Iteration 5648, loss = 1.59110384\n",
      "Iteration 5649, loss = 1.59104880\n",
      "Iteration 5650, loss = 1.59098808\n",
      "Iteration 5651, loss = 1.59092974\n",
      "Iteration 5652, loss = 1.59088129\n",
      "Iteration 5653, loss = 1.59081604\n",
      "Iteration 5654, loss = 1.59076329\n",
      "Iteration 5655, loss = 1.59070943\n",
      "Iteration 5656, loss = 1.59065021\n",
      "Iteration 5657, loss = 1.59059526\n",
      "Iteration 5658, loss = 1.59054251\n",
      "Iteration 5659, loss = 1.59048060\n",
      "Iteration 5660, loss = 1.59043061\n",
      "Iteration 5661, loss = 1.59037585\n",
      "Iteration 5662, loss = 1.59031730\n",
      "Iteration 5663, loss = 1.59026726\n",
      "Iteration 5664, loss = 1.59020978\n",
      "Iteration 5665, loss = 1.59015055\n",
      "Iteration 5666, loss = 1.59009807\n",
      "Iteration 5667, loss = 1.59004017\n",
      "Iteration 5668, loss = 1.58998520\n",
      "Iteration 5669, loss = 1.58992833\n",
      "Iteration 5670, loss = 1.58987507\n",
      "Iteration 5671, loss = 1.58981950\n",
      "Iteration 5672, loss = 1.58976120\n",
      "Iteration 5673, loss = 1.58971209\n",
      "Iteration 5674, loss = 1.58965163\n",
      "Iteration 5675, loss = 1.58959903\n",
      "Iteration 5676, loss = 1.58954196\n",
      "Iteration 5677, loss = 1.58948622\n",
      "Iteration 5678, loss = 1.58943262\n",
      "Iteration 5679, loss = 1.58937359\n",
      "Iteration 5680, loss = 1.58931910\n",
      "Iteration 5681, loss = 1.58926360\n",
      "Iteration 5682, loss = 1.58920810\n",
      "Iteration 5683, loss = 1.58915564\n",
      "Iteration 5684, loss = 1.58910006\n",
      "Iteration 5685, loss = 1.58904354\n",
      "Iteration 5686, loss = 1.58898931\n",
      "Iteration 5687, loss = 1.58893337\n",
      "Iteration 5688, loss = 1.58887842\n",
      "Iteration 5689, loss = 1.58882055\n",
      "Iteration 5690, loss = 1.58876479\n",
      "Iteration 5691, loss = 1.58871015\n",
      "Iteration 5692, loss = 1.58865499\n",
      "Iteration 5693, loss = 1.58859972\n",
      "Iteration 5694, loss = 1.58854415\n",
      "Iteration 5695, loss = 1.58849073\n",
      "Iteration 5696, loss = 1.58843540\n",
      "Iteration 5697, loss = 1.58838040\n",
      "Iteration 5698, loss = 1.58832504\n",
      "Iteration 5699, loss = 1.58826879\n",
      "Iteration 5700, loss = 1.58821832\n",
      "Iteration 5701, loss = 1.58816389\n",
      "Iteration 5702, loss = 1.58811091\n",
      "Iteration 5703, loss = 1.58805229\n",
      "Iteration 5704, loss = 1.58800299\n",
      "Iteration 5705, loss = 1.58793996\n",
      "Iteration 5706, loss = 1.58788948\n",
      "Iteration 5707, loss = 1.58783462\n",
      "Iteration 5708, loss = 1.58777654\n",
      "Iteration 5709, loss = 1.58773182\n",
      "Iteration 5710, loss = 1.58766741\n",
      "Iteration 5711, loss = 1.58761612\n",
      "Iteration 5712, loss = 1.58756174\n",
      "Iteration 5713, loss = 1.58750308\n",
      "Iteration 5714, loss = 1.58745167\n",
      "Iteration 5715, loss = 1.58739429\n",
      "Iteration 5716, loss = 1.58733900\n",
      "Iteration 5717, loss = 1.58728532\n",
      "Iteration 5718, loss = 1.58722811\n",
      "Iteration 5719, loss = 1.58717430\n",
      "Iteration 5720, loss = 1.58711782\n",
      "Iteration 5721, loss = 1.58706320\n",
      "Iteration 5722, loss = 1.58700997\n",
      "Iteration 5723, loss = 1.58695376\n",
      "Iteration 5724, loss = 1.58690042\n",
      "Iteration 5725, loss = 1.58684381\n",
      "Iteration 5726, loss = 1.58679422\n",
      "Iteration 5727, loss = 1.58673653\n",
      "Iteration 5728, loss = 1.58668262\n",
      "Iteration 5729, loss = 1.58662767\n",
      "Iteration 5730, loss = 1.58657152\n",
      "Iteration 5731, loss = 1.58652299\n",
      "Iteration 5732, loss = 1.58646196\n",
      "Iteration 5733, loss = 1.58641228\n",
      "Iteration 5734, loss = 1.58636039\n",
      "Iteration 5735, loss = 1.58630280\n",
      "Iteration 5736, loss = 1.58624200\n",
      "Iteration 5737, loss = 1.58619984\n",
      "Iteration 5738, loss = 1.58613290\n",
      "Iteration 5739, loss = 1.58608380\n",
      "Iteration 5740, loss = 1.58603006\n",
      "Iteration 5741, loss = 1.58597100\n",
      "Iteration 5742, loss = 1.58592171\n",
      "Iteration 5743, loss = 1.58586324\n",
      "Iteration 5744, loss = 1.58580969\n",
      "Iteration 5745, loss = 1.58575809\n",
      "Iteration 5746, loss = 1.58570113\n",
      "Iteration 5747, loss = 1.58564340\n",
      "Iteration 5748, loss = 1.58559690\n",
      "Iteration 5749, loss = 1.58553743\n",
      "Iteration 5750, loss = 1.58548315\n",
      "Iteration 5751, loss = 1.58543160\n",
      "Iteration 5752, loss = 1.58537438\n",
      "Iteration 5753, loss = 1.58531557\n",
      "Iteration 5754, loss = 1.58526250\n",
      "Iteration 5755, loss = 1.58520948\n",
      "Iteration 5756, loss = 1.58515642\n",
      "Iteration 5757, loss = 1.58509814\n",
      "Iteration 5758, loss = 1.58505397\n",
      "Iteration 5759, loss = 1.58498896\n",
      "Iteration 5760, loss = 1.58494024\n",
      "Iteration 5761, loss = 1.58488710\n",
      "Iteration 5762, loss = 1.58482857\n",
      "Iteration 5763, loss = 1.58477780\n",
      "Iteration 5764, loss = 1.58472273\n",
      "Iteration 5765, loss = 1.58466567\n",
      "Iteration 5766, loss = 1.58461405\n",
      "Iteration 5767, loss = 1.58455694\n",
      "Iteration 5768, loss = 1.58450208\n",
      "Iteration 5769, loss = 1.58445469\n",
      "Iteration 5770, loss = 1.58439417\n",
      "Iteration 5771, loss = 1.58434397\n",
      "Iteration 5772, loss = 1.58428814\n",
      "Iteration 5773, loss = 1.58423142\n",
      "Iteration 5774, loss = 1.58418035\n",
      "Iteration 5775, loss = 1.58412206\n",
      "Iteration 5776, loss = 1.58407079\n",
      "Iteration 5777, loss = 1.58401679\n",
      "Iteration 5778, loss = 1.58396046\n",
      "Iteration 5779, loss = 1.58390827\n",
      "Iteration 5780, loss = 1.58384963\n",
      "Iteration 5781, loss = 1.58380042\n",
      "Iteration 5782, loss = 1.58374713\n",
      "Iteration 5783, loss = 1.58369489\n",
      "Iteration 5784, loss = 1.58363993\n",
      "Iteration 5785, loss = 1.58358397\n",
      "Iteration 5786, loss = 1.58352687\n",
      "Iteration 5787, loss = 1.58347516\n",
      "Iteration 5788, loss = 1.58342167\n",
      "Iteration 5789, loss = 1.58336886\n",
      "Iteration 5790, loss = 1.58331611\n",
      "Iteration 5791, loss = 1.58326110\n",
      "Iteration 5792, loss = 1.58320753\n",
      "Iteration 5793, loss = 1.58315127\n",
      "Iteration 5794, loss = 1.58309619\n",
      "Iteration 5795, loss = 1.58304049\n",
      "Iteration 5796, loss = 1.58299143\n",
      "Iteration 5797, loss = 1.58293696\n",
      "Iteration 5798, loss = 1.58288185\n",
      "Iteration 5799, loss = 1.58282973\n",
      "Iteration 5800, loss = 1.58277272\n",
      "Iteration 5801, loss = 1.58271928\n",
      "Iteration 5802, loss = 1.58266421\n",
      "Iteration 5803, loss = 1.58261155\n",
      "Iteration 5804, loss = 1.58255900\n",
      "Iteration 5805, loss = 1.58250193\n",
      "Iteration 5806, loss = 1.58245773\n",
      "Iteration 5807, loss = 1.58239384\n",
      "Iteration 5808, loss = 1.58234181\n",
      "Iteration 5809, loss = 1.58228540\n",
      "Iteration 5810, loss = 1.58223203\n",
      "Iteration 5811, loss = 1.58218428\n",
      "Iteration 5812, loss = 1.58213349\n",
      "Iteration 5813, loss = 1.58207712\n",
      "Iteration 5814, loss = 1.58201809\n",
      "Iteration 5815, loss = 1.58196899\n",
      "Iteration 5816, loss = 1.58191037\n",
      "Iteration 5817, loss = 1.58185837\n",
      "Iteration 5818, loss = 1.58180255\n",
      "Iteration 5819, loss = 1.58174995\n",
      "Iteration 5820, loss = 1.58169576\n",
      "Iteration 5821, loss = 1.58164095\n",
      "Iteration 5822, loss = 1.58159583\n",
      "Iteration 5823, loss = 1.58153516\n",
      "Iteration 5824, loss = 1.58148629\n",
      "Iteration 5825, loss = 1.58143265\n",
      "Iteration 5826, loss = 1.58137391\n",
      "Iteration 5827, loss = 1.58133361\n",
      "Iteration 5828, loss = 1.58126780\n",
      "Iteration 5829, loss = 1.58122069\n",
      "Iteration 5830, loss = 1.58117219\n",
      "Iteration 5831, loss = 1.58111805\n",
      "Iteration 5832, loss = 1.58105903\n",
      "Iteration 5833, loss = 1.58100207\n",
      "Iteration 5834, loss = 1.58095590\n",
      "Iteration 5835, loss = 1.58089358\n",
      "Iteration 5836, loss = 1.58084304\n",
      "Iteration 5837, loss = 1.58078911\n",
      "Iteration 5838, loss = 1.58073187\n",
      "Iteration 5839, loss = 1.58068919\n",
      "Iteration 5840, loss = 1.58062754\n",
      "Iteration 5841, loss = 1.58058075\n",
      "Iteration 5842, loss = 1.58053488\n",
      "Iteration 5843, loss = 1.58048320\n",
      "Iteration 5844, loss = 1.58042621\n",
      "Iteration 5845, loss = 1.58036459\n",
      "Iteration 5846, loss = 1.58030881\n",
      "Iteration 5847, loss = 1.58025632\n",
      "Iteration 5848, loss = 1.58020201\n",
      "Iteration 5849, loss = 1.58015287\n",
      "Iteration 5850, loss = 1.58009841\n",
      "Iteration 5851, loss = 1.58003963\n",
      "Iteration 5852, loss = 1.57999058\n",
      "Iteration 5853, loss = 1.57993252\n",
      "Iteration 5854, loss = 1.57988102\n",
      "Iteration 5855, loss = 1.57982698\n",
      "Iteration 5856, loss = 1.57977219\n",
      "Iteration 5857, loss = 1.57972275\n",
      "Iteration 5858, loss = 1.57966920\n",
      "Iteration 5859, loss = 1.57961998\n",
      "Iteration 5860, loss = 1.57956528\n",
      "Iteration 5861, loss = 1.57950603\n",
      "Iteration 5862, loss = 1.57946369\n",
      "Iteration 5863, loss = 1.57939993\n",
      "Iteration 5864, loss = 1.57935009\n",
      "Iteration 5865, loss = 1.57929823\n",
      "Iteration 5866, loss = 1.57924130\n",
      "Iteration 5867, loss = 1.57919068\n",
      "Iteration 5868, loss = 1.57913475\n",
      "Iteration 5869, loss = 1.57908447\n",
      "Iteration 5870, loss = 1.57903189\n",
      "Iteration 5871, loss = 1.57897478\n",
      "Iteration 5872, loss = 1.57893392\n",
      "Iteration 5873, loss = 1.57887292\n",
      "Iteration 5874, loss = 1.57882001\n",
      "Iteration 5875, loss = 1.57876991\n",
      "Iteration 5876, loss = 1.57871459\n",
      "Iteration 5877, loss = 1.57865591\n",
      "Iteration 5878, loss = 1.57860445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5879, loss = 1.57855583\n",
      "Iteration 5880, loss = 1.57850598\n",
      "Iteration 5881, loss = 1.57845096\n",
      "Iteration 5882, loss = 1.57839490\n",
      "Iteration 5883, loss = 1.57834409\n",
      "Iteration 5884, loss = 1.57828867\n",
      "Iteration 5885, loss = 1.57823559\n",
      "Iteration 5886, loss = 1.57818340\n",
      "Iteration 5887, loss = 1.57813136\n",
      "Iteration 5888, loss = 1.57807833\n",
      "Iteration 5889, loss = 1.57802507\n",
      "Iteration 5890, loss = 1.57797136\n",
      "Iteration 5891, loss = 1.57791615\n",
      "Iteration 5892, loss = 1.57786557\n",
      "Iteration 5893, loss = 1.57781144\n",
      "Iteration 5894, loss = 1.57775845\n",
      "Iteration 5895, loss = 1.57770598\n",
      "Iteration 5896, loss = 1.57765426\n",
      "Iteration 5897, loss = 1.57760219\n",
      "Iteration 5898, loss = 1.57755166\n",
      "Iteration 5899, loss = 1.57749643\n",
      "Iteration 5900, loss = 1.57744574\n",
      "Iteration 5901, loss = 1.57738867\n",
      "Iteration 5902, loss = 1.57733748\n",
      "Iteration 5903, loss = 1.57728474\n",
      "Iteration 5904, loss = 1.57723267\n",
      "Iteration 5905, loss = 1.57717776\n",
      "Iteration 5906, loss = 1.57712460\n",
      "Iteration 5907, loss = 1.57707502\n",
      "Iteration 5908, loss = 1.57702041\n",
      "Iteration 5909, loss = 1.57697185\n",
      "Iteration 5910, loss = 1.57692002\n",
      "Iteration 5911, loss = 1.57686327\n",
      "Iteration 5912, loss = 1.57681918\n",
      "Iteration 5913, loss = 1.57676361\n",
      "Iteration 5914, loss = 1.57670644\n",
      "Iteration 5915, loss = 1.57665544\n",
      "Iteration 5916, loss = 1.57659983\n",
      "Iteration 5917, loss = 1.57654773\n",
      "Iteration 5918, loss = 1.57649498\n",
      "Iteration 5919, loss = 1.57644220\n",
      "Iteration 5920, loss = 1.57639072\n",
      "Iteration 5921, loss = 1.57633627\n",
      "Iteration 5922, loss = 1.57628910\n",
      "Iteration 5923, loss = 1.57623272\n",
      "Iteration 5924, loss = 1.57618319\n",
      "Iteration 5925, loss = 1.57612886\n",
      "Iteration 5926, loss = 1.57607833\n",
      "Iteration 5927, loss = 1.57602249\n",
      "Iteration 5928, loss = 1.57597249\n",
      "Iteration 5929, loss = 1.57591991\n",
      "Iteration 5930, loss = 1.57586570\n",
      "Iteration 5931, loss = 1.57581390\n",
      "Iteration 5932, loss = 1.57575989\n",
      "Iteration 5933, loss = 1.57570645\n",
      "Iteration 5934, loss = 1.57565606\n",
      "Iteration 5935, loss = 1.57560854\n",
      "Iteration 5936, loss = 1.57555954\n",
      "Iteration 5937, loss = 1.57550534\n",
      "Iteration 5938, loss = 1.57544699\n",
      "Iteration 5939, loss = 1.57540461\n",
      "Iteration 5940, loss = 1.57534148\n",
      "Iteration 5941, loss = 1.57529265\n",
      "Iteration 5942, loss = 1.57524032\n",
      "Iteration 5943, loss = 1.57518610\n",
      "Iteration 5944, loss = 1.57513452\n",
      "Iteration 5945, loss = 1.57507987\n",
      "Iteration 5946, loss = 1.57502716\n",
      "Iteration 5947, loss = 1.57497554\n",
      "Iteration 5948, loss = 1.57492662\n",
      "Iteration 5949, loss = 1.57487473\n",
      "Iteration 5950, loss = 1.57482403\n",
      "Iteration 5951, loss = 1.57476839\n",
      "Iteration 5952, loss = 1.57472371\n",
      "Iteration 5953, loss = 1.57466282\n",
      "Iteration 5954, loss = 1.57461582\n",
      "Iteration 5955, loss = 1.57456502\n",
      "Iteration 5956, loss = 1.57450945\n",
      "Iteration 5957, loss = 1.57445819\n",
      "Iteration 5958, loss = 1.57441235\n",
      "Iteration 5959, loss = 1.57435361\n",
      "Iteration 5960, loss = 1.57430725\n",
      "Iteration 5961, loss = 1.57425664\n",
      "Iteration 5962, loss = 1.57420114\n",
      "Iteration 5963, loss = 1.57414330\n",
      "Iteration 5964, loss = 1.57410693\n",
      "Iteration 5965, loss = 1.57403948\n",
      "Iteration 5966, loss = 1.57399544\n",
      "Iteration 5967, loss = 1.57395101\n",
      "Iteration 5968, loss = 1.57390104\n",
      "Iteration 5969, loss = 1.57384605\n",
      "Iteration 5970, loss = 1.57378688\n",
      "Iteration 5971, loss = 1.57372751\n",
      "Iteration 5972, loss = 1.57369053\n",
      "Iteration 5973, loss = 1.57362503\n",
      "Iteration 5974, loss = 1.57357884\n",
      "Iteration 5975, loss = 1.57353334\n",
      "Iteration 5976, loss = 1.57348264\n",
      "Iteration 5977, loss = 1.57342711\n",
      "Iteration 5978, loss = 1.57336743\n",
      "Iteration 5979, loss = 1.57332098\n",
      "Iteration 5980, loss = 1.57326882\n",
      "Iteration 5981, loss = 1.57321008\n",
      "Iteration 5982, loss = 1.57316508\n",
      "Iteration 5983, loss = 1.57311565\n",
      "Iteration 5984, loss = 1.57306129\n",
      "Iteration 5985, loss = 1.57300301\n",
      "Iteration 5986, loss = 1.57295702\n",
      "Iteration 5987, loss = 1.57290056\n",
      "Iteration 5988, loss = 1.57285033\n",
      "Iteration 5989, loss = 1.57280172\n",
      "Iteration 5990, loss = 1.57274831\n",
      "Iteration 5991, loss = 1.57269280\n",
      "Iteration 5992, loss = 1.57264988\n",
      "Iteration 5993, loss = 1.57258858\n",
      "Iteration 5994, loss = 1.57254189\n",
      "Iteration 5995, loss = 1.57249163\n",
      "Iteration 5996, loss = 1.57243671\n",
      "Iteration 5997, loss = 1.57238280\n",
      "Iteration 5998, loss = 1.57233098\n",
      "Iteration 5999, loss = 1.57228132\n",
      "Iteration 6000, loss = 1.57223150\n",
      "Iteration 6001, loss = 1.57217689\n",
      "Iteration 6002, loss = 1.57212713\n",
      "Iteration 6003, loss = 1.57207184\n",
      "Iteration 6004, loss = 1.57202061\n",
      "Iteration 6005, loss = 1.57196958\n",
      "Iteration 6006, loss = 1.57191799\n",
      "Iteration 6007, loss = 1.57186846\n",
      "Iteration 6008, loss = 1.57181827\n",
      "Iteration 6009, loss = 1.57176356\n",
      "Iteration 6010, loss = 1.57171346\n",
      "Iteration 6011, loss = 1.57165912\n",
      "Iteration 6012, loss = 1.57160886\n",
      "Iteration 6013, loss = 1.57155706\n",
      "Iteration 6014, loss = 1.57150561\n",
      "Iteration 6015, loss = 1.57145403\n",
      "Iteration 6016, loss = 1.57140112\n",
      "Iteration 6017, loss = 1.57135076\n",
      "Iteration 6018, loss = 1.57130248\n",
      "Iteration 6019, loss = 1.57125151\n",
      "Iteration 6020, loss = 1.57119622\n",
      "Iteration 6021, loss = 1.57114918\n",
      "Iteration 6022, loss = 1.57109704\n",
      "Iteration 6023, loss = 1.57104784\n",
      "Iteration 6024, loss = 1.57099401\n",
      "Iteration 6025, loss = 1.57094296\n",
      "Iteration 6026, loss = 1.57088968\n",
      "Iteration 6027, loss = 1.57083945\n",
      "Iteration 6028, loss = 1.57078897\n",
      "Iteration 6029, loss = 1.57073664\n",
      "Iteration 6030, loss = 1.57068733\n",
      "Iteration 6031, loss = 1.57063428\n",
      "Iteration 6032, loss = 1.57058363\n",
      "Iteration 6033, loss = 1.57053099\n",
      "Iteration 6034, loss = 1.57048701\n",
      "Iteration 6035, loss = 1.57043346\n",
      "Iteration 6036, loss = 1.57038770\n",
      "Iteration 6037, loss = 1.57033685\n",
      "Iteration 6038, loss = 1.57028155\n",
      "Iteration 6039, loss = 1.57022408\n",
      "Iteration 6040, loss = 1.57019355\n",
      "Iteration 6041, loss = 1.57012559\n",
      "Iteration 6042, loss = 1.57008254\n",
      "Iteration 6043, loss = 1.57004253\n",
      "Iteration 6044, loss = 1.56999685\n",
      "Iteration 6045, loss = 1.56994602\n",
      "Iteration 6046, loss = 1.56989055\n",
      "Iteration 6047, loss = 1.56983106\n",
      "Iteration 6048, loss = 1.56976866\n",
      "Iteration 6049, loss = 1.56972966\n",
      "Iteration 6050, loss = 1.56968197\n",
      "Iteration 6051, loss = 1.56961479\n",
      "Iteration 6052, loss = 1.56956850\n",
      "Iteration 6053, loss = 1.56952088\n",
      "Iteration 6054, loss = 1.56946874\n",
      "Iteration 6055, loss = 1.56941332\n",
      "Iteration 6056, loss = 1.56936095\n",
      "Iteration 6057, loss = 1.56931242\n",
      "Iteration 6058, loss = 1.56925688\n",
      "Iteration 6059, loss = 1.56920483\n",
      "Iteration 6060, loss = 1.56915460\n",
      "Iteration 6061, loss = 1.56910411\n",
      "Iteration 6062, loss = 1.56905278\n",
      "Iteration 6063, loss = 1.56900148\n",
      "Iteration 6064, loss = 1.56895010\n",
      "Iteration 6065, loss = 1.56889970\n",
      "Iteration 6066, loss = 1.56884808\n",
      "Iteration 6067, loss = 1.56879727\n",
      "Iteration 6068, loss = 1.56874713\n",
      "Iteration 6069, loss = 1.56869552\n",
      "Iteration 6070, loss = 1.56864478\n",
      "Iteration 6071, loss = 1.56859459\n",
      "Iteration 6072, loss = 1.56854417\n",
      "Iteration 6073, loss = 1.56849441\n",
      "Iteration 6074, loss = 1.56844391\n",
      "Iteration 6075, loss = 1.56839372\n",
      "Iteration 6076, loss = 1.56834215\n",
      "Iteration 6077, loss = 1.56829128\n",
      "Iteration 6078, loss = 1.56824070\n",
      "Iteration 6079, loss = 1.56819032\n",
      "Iteration 6080, loss = 1.56813979\n",
      "Iteration 6081, loss = 1.56808867\n",
      "Iteration 6082, loss = 1.56803735\n",
      "Iteration 6083, loss = 1.56798663\n",
      "Iteration 6084, loss = 1.56793559\n",
      "Iteration 6085, loss = 1.56788582\n",
      "Iteration 6086, loss = 1.56783581\n",
      "Iteration 6087, loss = 1.56778405\n",
      "Iteration 6088, loss = 1.56773465\n",
      "Iteration 6089, loss = 1.56768146\n",
      "Iteration 6090, loss = 1.56764299\n",
      "Iteration 6091, loss = 1.56758453\n",
      "Iteration 6092, loss = 1.56753352\n",
      "Iteration 6093, loss = 1.56748455\n",
      "Iteration 6094, loss = 1.56743101\n",
      "Iteration 6095, loss = 1.56738385\n",
      "Iteration 6096, loss = 1.56732850\n",
      "Iteration 6097, loss = 1.56728223\n",
      "Iteration 6098, loss = 1.56723371\n",
      "Iteration 6099, loss = 1.56718063\n",
      "Iteration 6100, loss = 1.56713014\n",
      "Iteration 6101, loss = 1.56708235\n",
      "Iteration 6102, loss = 1.56702653\n",
      "Iteration 6103, loss = 1.56697939\n",
      "Iteration 6104, loss = 1.56692948\n",
      "Iteration 6105, loss = 1.56687590\n",
      "Iteration 6106, loss = 1.56682824\n",
      "Iteration 6107, loss = 1.56677492\n",
      "Iteration 6108, loss = 1.56672580\n",
      "Iteration 6109, loss = 1.56667636\n",
      "Iteration 6110, loss = 1.56662671\n",
      "Iteration 6111, loss = 1.56657694\n",
      "Iteration 6112, loss = 1.56652349\n",
      "Iteration 6113, loss = 1.56647147\n",
      "Iteration 6114, loss = 1.56642306\n",
      "Iteration 6115, loss = 1.56637372\n",
      "Iteration 6116, loss = 1.56632307\n",
      "Iteration 6117, loss = 1.56627248\n",
      "Iteration 6118, loss = 1.56622015\n",
      "Iteration 6119, loss = 1.56617007\n",
      "Iteration 6120, loss = 1.56612080\n",
      "Iteration 6121, loss = 1.56607114\n",
      "Iteration 6122, loss = 1.56602019\n",
      "Iteration 6123, loss = 1.56596869\n",
      "Iteration 6124, loss = 1.56592075\n",
      "Iteration 6125, loss = 1.56587138\n",
      "Iteration 6126, loss = 1.56582268\n",
      "Iteration 6127, loss = 1.56577054\n",
      "Iteration 6128, loss = 1.56572404\n",
      "Iteration 6129, loss = 1.56566941\n",
      "Iteration 6130, loss = 1.56562049\n",
      "Iteration 6131, loss = 1.56556909\n",
      "Iteration 6132, loss = 1.56552091\n",
      "Iteration 6133, loss = 1.56546864\n",
      "Iteration 6134, loss = 1.56542014\n",
      "Iteration 6135, loss = 1.56536804\n",
      "Iteration 6136, loss = 1.56532349\n",
      "Iteration 6137, loss = 1.56526936\n",
      "Iteration 6138, loss = 1.56522184\n",
      "Iteration 6139, loss = 1.56516992\n",
      "Iteration 6140, loss = 1.56512004\n",
      "Iteration 6141, loss = 1.56506810\n",
      "Iteration 6142, loss = 1.56501949\n",
      "Iteration 6143, loss = 1.56496881\n",
      "Iteration 6144, loss = 1.56491962\n",
      "Iteration 6145, loss = 1.56486908\n",
      "Iteration 6146, loss = 1.56481652\n",
      "Iteration 6147, loss = 1.56477094\n",
      "Iteration 6148, loss = 1.56472166\n",
      "Iteration 6149, loss = 1.56467164\n",
      "Iteration 6150, loss = 1.56462225\n",
      "Iteration 6151, loss = 1.56457136\n",
      "Iteration 6152, loss = 1.56452092\n",
      "Iteration 6153, loss = 1.56447074\n",
      "Iteration 6154, loss = 1.56441954\n",
      "Iteration 6155, loss = 1.56436885\n",
      "Iteration 6156, loss = 1.56431863\n",
      "Iteration 6157, loss = 1.56427095\n",
      "Iteration 6158, loss = 1.56422010\n",
      "Iteration 6159, loss = 1.56417283\n",
      "Iteration 6160, loss = 1.56412099\n",
      "Iteration 6161, loss = 1.56407236\n",
      "Iteration 6162, loss = 1.56402241\n",
      "Iteration 6163, loss = 1.56397344\n",
      "Iteration 6164, loss = 1.56392710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6165, loss = 1.56387391\n",
      "Iteration 6166, loss = 1.56382573\n",
      "Iteration 6167, loss = 1.56377441\n",
      "Iteration 6168, loss = 1.56372637\n",
      "Iteration 6169, loss = 1.56367715\n",
      "Iteration 6170, loss = 1.56362511\n",
      "Iteration 6171, loss = 1.56357593\n",
      "Iteration 6172, loss = 1.56352543\n",
      "Iteration 6173, loss = 1.56347734\n",
      "Iteration 6174, loss = 1.56343247\n",
      "Iteration 6175, loss = 1.56337795\n",
      "Iteration 6176, loss = 1.56333214\n",
      "Iteration 6177, loss = 1.56328449\n",
      "Iteration 6178, loss = 1.56323247\n",
      "Iteration 6179, loss = 1.56317686\n",
      "Iteration 6180, loss = 1.56313785\n",
      "Iteration 6181, loss = 1.56307930\n",
      "Iteration 6182, loss = 1.56303201\n",
      "Iteration 6183, loss = 1.56298560\n",
      "Iteration 6184, loss = 1.56293476\n",
      "Iteration 6185, loss = 1.56288007\n",
      "Iteration 6186, loss = 1.56283747\n",
      "Iteration 6187, loss = 1.56278242\n",
      "Iteration 6188, loss = 1.56273316\n",
      "Iteration 6189, loss = 1.56268751\n",
      "Iteration 6190, loss = 1.56263785\n",
      "Iteration 6191, loss = 1.56258430\n",
      "Iteration 6192, loss = 1.56253663\n",
      "Iteration 6193, loss = 1.56249148\n",
      "Iteration 6194, loss = 1.56243502\n",
      "Iteration 6195, loss = 1.56239093\n",
      "Iteration 6196, loss = 1.56234271\n",
      "Iteration 6197, loss = 1.56229030\n",
      "Iteration 6198, loss = 1.56223585\n",
      "Iteration 6199, loss = 1.56219049\n",
      "Iteration 6200, loss = 1.56213780\n",
      "Iteration 6201, loss = 1.56209089\n",
      "Iteration 6202, loss = 1.56204032\n",
      "Iteration 6203, loss = 1.56198951\n",
      "Iteration 6204, loss = 1.56194247\n",
      "Iteration 6205, loss = 1.56189477\n",
      "Iteration 6206, loss = 1.56184966\n",
      "Iteration 6207, loss = 1.56180005\n",
      "Iteration 6208, loss = 1.56174643\n",
      "Iteration 6209, loss = 1.56169690\n",
      "Iteration 6210, loss = 1.56164808\n",
      "Iteration 6211, loss = 1.56159674\n",
      "Iteration 6212, loss = 1.56154978\n",
      "Iteration 6213, loss = 1.56149945\n",
      "Iteration 6214, loss = 1.56144844\n",
      "Iteration 6215, loss = 1.56140170\n",
      "Iteration 6216, loss = 1.56134930\n",
      "Iteration 6217, loss = 1.56130091\n",
      "Iteration 6218, loss = 1.56125141\n",
      "Iteration 6219, loss = 1.56120177\n",
      "Iteration 6220, loss = 1.56115216\n",
      "Iteration 6221, loss = 1.56110591\n",
      "Iteration 6222, loss = 1.56105724\n",
      "Iteration 6223, loss = 1.56100756\n",
      "Iteration 6224, loss = 1.56095858\n",
      "Iteration 6225, loss = 1.56090860\n",
      "Iteration 6226, loss = 1.56085894\n",
      "Iteration 6227, loss = 1.56080817\n",
      "Iteration 6228, loss = 1.56076154\n",
      "Iteration 6229, loss = 1.56071216\n",
      "Iteration 6230, loss = 1.56066390\n",
      "Iteration 6231, loss = 1.56061474\n",
      "Iteration 6232, loss = 1.56056551\n",
      "Iteration 6233, loss = 1.56051666\n",
      "Iteration 6234, loss = 1.56046615\n",
      "Iteration 6235, loss = 1.56041640\n",
      "Iteration 6236, loss = 1.56037091\n",
      "Iteration 6237, loss = 1.56031858\n",
      "Iteration 6238, loss = 1.56027047\n",
      "Iteration 6239, loss = 1.56022315\n",
      "Iteration 6240, loss = 1.56017487\n",
      "Iteration 6241, loss = 1.56012508\n",
      "Iteration 6242, loss = 1.56007543\n",
      "Iteration 6243, loss = 1.56002613\n",
      "Iteration 6244, loss = 1.55997698\n",
      "Iteration 6245, loss = 1.55992971\n",
      "Iteration 6246, loss = 1.55987958\n",
      "Iteration 6247, loss = 1.55983058\n",
      "Iteration 6248, loss = 1.55978219\n",
      "Iteration 6249, loss = 1.55973243\n",
      "Iteration 6250, loss = 1.55968353\n",
      "Iteration 6251, loss = 1.55963390\n",
      "Iteration 6252, loss = 1.55958734\n",
      "Iteration 6253, loss = 1.55953698\n",
      "Iteration 6254, loss = 1.55948911\n",
      "Iteration 6255, loss = 1.55943919\n",
      "Iteration 6256, loss = 1.55939142\n",
      "Iteration 6257, loss = 1.55934184\n",
      "Iteration 6258, loss = 1.55929375\n",
      "Iteration 6259, loss = 1.55924341\n",
      "Iteration 6260, loss = 1.55919865\n",
      "Iteration 6261, loss = 1.55915293\n",
      "Iteration 6262, loss = 1.55910992\n",
      "Iteration 6263, loss = 1.55906236\n",
      "Iteration 6264, loss = 1.55901069\n",
      "Iteration 6265, loss = 1.55895549\n",
      "Iteration 6266, loss = 1.55891761\n",
      "Iteration 6267, loss = 1.55886535\n",
      "Iteration 6268, loss = 1.55881196\n",
      "Iteration 6269, loss = 1.55876948\n",
      "Iteration 6270, loss = 1.55872250\n",
      "Iteration 6271, loss = 1.55867140\n",
      "Iteration 6272, loss = 1.55861673\n",
      "Iteration 6273, loss = 1.55856869\n",
      "Iteration 6274, loss = 1.55852362\n",
      "Iteration 6275, loss = 1.55847062\n",
      "Iteration 6276, loss = 1.55842738\n",
      "Iteration 6277, loss = 1.55837979\n",
      "Iteration 6278, loss = 1.55832819\n",
      "Iteration 6279, loss = 1.55827321\n",
      "Iteration 6280, loss = 1.55823936\n",
      "Iteration 6281, loss = 1.55817673\n",
      "Iteration 6282, loss = 1.55813452\n",
      "Iteration 6283, loss = 1.55809215\n",
      "Iteration 6284, loss = 1.55804529\n",
      "Iteration 6285, loss = 1.55799433\n",
      "Iteration 6286, loss = 1.55793976\n",
      "Iteration 6287, loss = 1.55788614\n",
      "Iteration 6288, loss = 1.55784690\n",
      "Iteration 6289, loss = 1.55778939\n",
      "Iteration 6290, loss = 1.55774288\n",
      "Iteration 6291, loss = 1.55769445\n",
      "Iteration 6292, loss = 1.55764346\n",
      "Iteration 6293, loss = 1.55759646\n",
      "Iteration 6294, loss = 1.55754578\n",
      "Iteration 6295, loss = 1.55749947\n",
      "Iteration 6296, loss = 1.55745061\n",
      "Iteration 6297, loss = 1.55740349\n",
      "Iteration 6298, loss = 1.55735315\n",
      "Iteration 6299, loss = 1.55730526\n",
      "Iteration 6300, loss = 1.55725897\n",
      "Iteration 6301, loss = 1.55721177\n",
      "Iteration 6302, loss = 1.55715991\n",
      "Iteration 6303, loss = 1.55711274\n",
      "Iteration 6304, loss = 1.55706504\n",
      "Iteration 6305, loss = 1.55701675\n",
      "Iteration 6306, loss = 1.55696784\n",
      "Iteration 6307, loss = 1.55691842\n",
      "Iteration 6308, loss = 1.55686963\n",
      "Iteration 6309, loss = 1.55682167\n",
      "Iteration 6310, loss = 1.55677571\n",
      "Iteration 6311, loss = 1.55672669\n",
      "Iteration 6312, loss = 1.55668154\n",
      "Iteration 6313, loss = 1.55663073\n",
      "Iteration 6314, loss = 1.55658489\n",
      "Iteration 6315, loss = 1.55653552\n",
      "Iteration 6316, loss = 1.55648778\n",
      "Iteration 6317, loss = 1.55643788\n",
      "Iteration 6318, loss = 1.55639020\n",
      "Iteration 6319, loss = 1.55634165\n",
      "Iteration 6320, loss = 1.55629278\n",
      "Iteration 6321, loss = 1.55624468\n",
      "Iteration 6322, loss = 1.55620039\n",
      "Iteration 6323, loss = 1.55615298\n",
      "Iteration 6324, loss = 1.55610185\n",
      "Iteration 6325, loss = 1.55606510\n",
      "Iteration 6326, loss = 1.55600846\n",
      "Iteration 6327, loss = 1.55596406\n",
      "Iteration 6328, loss = 1.55592041\n",
      "Iteration 6329, loss = 1.55587246\n",
      "Iteration 6330, loss = 1.55582080\n",
      "Iteration 6331, loss = 1.55576932\n",
      "Iteration 6332, loss = 1.55573424\n",
      "Iteration 6333, loss = 1.55567353\n",
      "Iteration 6334, loss = 1.55562638\n",
      "Iteration 6335, loss = 1.55558070\n",
      "Iteration 6336, loss = 1.55553218\n",
      "Iteration 6337, loss = 1.55548018\n",
      "Iteration 6338, loss = 1.55543729\n",
      "Iteration 6339, loss = 1.55538708\n",
      "Iteration 6340, loss = 1.55533677\n",
      "Iteration 6341, loss = 1.55529031\n",
      "Iteration 6342, loss = 1.55524023\n",
      "Iteration 6343, loss = 1.55519706\n",
      "Iteration 6344, loss = 1.55514600\n",
      "Iteration 6345, loss = 1.55509973\n",
      "Iteration 6346, loss = 1.55505428\n",
      "Iteration 6347, loss = 1.55500478\n",
      "Iteration 6348, loss = 1.55495282\n",
      "Iteration 6349, loss = 1.55490654\n",
      "Iteration 6350, loss = 1.55485776\n",
      "Iteration 6351, loss = 1.55481042\n",
      "Iteration 6352, loss = 1.55476157\n",
      "Iteration 6353, loss = 1.55471385\n",
      "Iteration 6354, loss = 1.55466717\n",
      "Iteration 6355, loss = 1.55461939\n",
      "Iteration 6356, loss = 1.55457215\n",
      "Iteration 6357, loss = 1.55452585\n",
      "Iteration 6358, loss = 1.55447678\n",
      "Iteration 6359, loss = 1.55443084\n",
      "Iteration 6360, loss = 1.55438218\n",
      "Iteration 6361, loss = 1.55433731\n",
      "Iteration 6362, loss = 1.55429293\n",
      "Iteration 6363, loss = 1.55424450\n",
      "Iteration 6364, loss = 1.55419255\n",
      "Iteration 6365, loss = 1.55414986\n",
      "Iteration 6366, loss = 1.55410232\n",
      "Iteration 6367, loss = 1.55404962\n",
      "Iteration 6368, loss = 1.55400704\n",
      "Iteration 6369, loss = 1.55396059\n",
      "Iteration 6370, loss = 1.55391049\n",
      "Iteration 6371, loss = 1.55385868\n",
      "Iteration 6372, loss = 1.55381815\n",
      "Iteration 6373, loss = 1.55376676\n",
      "Iteration 6374, loss = 1.55372419\n",
      "Iteration 6375, loss = 1.55367755\n",
      "Iteration 6376, loss = 1.55362714\n",
      "Iteration 6377, loss = 1.55357355\n",
      "Iteration 6378, loss = 1.55354184\n",
      "Iteration 6379, loss = 1.55347926\n",
      "Iteration 6380, loss = 1.55343557\n",
      "Iteration 6381, loss = 1.55339136\n",
      "Iteration 6382, loss = 1.55334319\n",
      "Iteration 6383, loss = 1.55329154\n",
      "Iteration 6384, loss = 1.55324664\n",
      "Iteration 6385, loss = 1.55319830\n",
      "Iteration 6386, loss = 1.55314963\n",
      "Iteration 6387, loss = 1.55310541\n",
      "Iteration 6388, loss = 1.55305730\n",
      "Iteration 6389, loss = 1.55300597\n",
      "Iteration 6390, loss = 1.55295984\n",
      "Iteration 6391, loss = 1.55291096\n",
      "Iteration 6392, loss = 1.55286281\n",
      "Iteration 6393, loss = 1.55281679\n",
      "Iteration 6394, loss = 1.55277079\n",
      "Iteration 6395, loss = 1.55272210\n",
      "Iteration 6396, loss = 1.55267365\n",
      "Iteration 6397, loss = 1.55262778\n",
      "Iteration 6398, loss = 1.55258224\n",
      "Iteration 6399, loss = 1.55253683\n",
      "Iteration 6400, loss = 1.55248776\n",
      "Iteration 6401, loss = 1.55244232\n",
      "Iteration 6402, loss = 1.55239385\n",
      "Iteration 6403, loss = 1.55234705\n",
      "Iteration 6404, loss = 1.55230044\n",
      "Iteration 6405, loss = 1.55225070\n",
      "Iteration 6406, loss = 1.55220444\n",
      "Iteration 6407, loss = 1.55215817\n",
      "Iteration 6408, loss = 1.55211146\n",
      "Iteration 6409, loss = 1.55206765\n",
      "Iteration 6410, loss = 1.55201995\n",
      "Iteration 6411, loss = 1.55196879\n",
      "Iteration 6412, loss = 1.55192522\n",
      "Iteration 6413, loss = 1.55187555\n",
      "Iteration 6414, loss = 1.55182639\n",
      "Iteration 6415, loss = 1.55178151\n",
      "Iteration 6416, loss = 1.55173315\n",
      "Iteration 6417, loss = 1.55168970\n",
      "Iteration 6418, loss = 1.55164125\n",
      "Iteration 6419, loss = 1.55159316\n",
      "Iteration 6420, loss = 1.55154934\n",
      "Iteration 6421, loss = 1.55150178\n",
      "Iteration 6422, loss = 1.55145292\n",
      "Iteration 6423, loss = 1.55141050\n",
      "Iteration 6424, loss = 1.55135999\n",
      "Iteration 6425, loss = 1.55131102\n",
      "Iteration 6426, loss = 1.55126544\n",
      "Iteration 6427, loss = 1.55121639\n",
      "Iteration 6428, loss = 1.55117574\n",
      "Iteration 6429, loss = 1.55112323\n",
      "Iteration 6430, loss = 1.55107923\n",
      "Iteration 6431, loss = 1.55103312\n",
      "Iteration 6432, loss = 1.55098344\n",
      "Iteration 6433, loss = 1.55093502\n",
      "Iteration 6434, loss = 1.55088913\n",
      "Iteration 6435, loss = 1.55084237\n",
      "Iteration 6436, loss = 1.55079730\n",
      "Iteration 6437, loss = 1.55074862\n",
      "Iteration 6438, loss = 1.55070234\n",
      "Iteration 6439, loss = 1.55065512\n",
      "Iteration 6440, loss = 1.55061037\n",
      "Iteration 6441, loss = 1.55056718\n",
      "Iteration 6442, loss = 1.55052015\n",
      "Iteration 6443, loss = 1.55046965\n",
      "Iteration 6444, loss = 1.55042791\n",
      "Iteration 6445, loss = 1.55037697\n",
      "Iteration 6446, loss = 1.55032906\n",
      "Iteration 6447, loss = 1.55028399\n",
      "Iteration 6448, loss = 1.55023608\n",
      "Iteration 6449, loss = 1.55018989\n",
      "Iteration 6450, loss = 1.55014378\n",
      "Iteration 6451, loss = 1.55009402\n",
      "Iteration 6452, loss = 1.55004694\n",
      "Iteration 6453, loss = 1.55000017\n",
      "Iteration 6454, loss = 1.54995446\n",
      "Iteration 6455, loss = 1.54990831\n",
      "Iteration 6456, loss = 1.54986015\n",
      "Iteration 6457, loss = 1.54981463\n",
      "Iteration 6458, loss = 1.54976678\n",
      "Iteration 6459, loss = 1.54972054\n",
      "Iteration 6460, loss = 1.54967524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6461, loss = 1.54962776\n",
      "Iteration 6462, loss = 1.54958205\n",
      "Iteration 6463, loss = 1.54953485\n",
      "Iteration 6464, loss = 1.54948880\n",
      "Iteration 6465, loss = 1.54944080\n",
      "Iteration 6466, loss = 1.54939370\n",
      "Iteration 6467, loss = 1.54934723\n",
      "Iteration 6468, loss = 1.54930084\n",
      "Iteration 6469, loss = 1.54925456\n",
      "Iteration 6470, loss = 1.54920720\n",
      "Iteration 6471, loss = 1.54916204\n",
      "Iteration 6472, loss = 1.54911742\n",
      "Iteration 6473, loss = 1.54907017\n",
      "Iteration 6474, loss = 1.54902372\n",
      "Iteration 6475, loss = 1.54897810\n",
      "Iteration 6476, loss = 1.54893105\n",
      "Iteration 6477, loss = 1.54888511\n",
      "Iteration 6478, loss = 1.54883801\n",
      "Iteration 6479, loss = 1.54879171\n",
      "Iteration 6480, loss = 1.54874802\n",
      "Iteration 6481, loss = 1.54870101\n",
      "Iteration 6482, loss = 1.54865331\n",
      "Iteration 6483, loss = 1.54860735\n",
      "Iteration 6484, loss = 1.54856010\n",
      "Iteration 6485, loss = 1.54851871\n",
      "Iteration 6486, loss = 1.54846861\n",
      "Iteration 6487, loss = 1.54842420\n",
      "Iteration 6488, loss = 1.54837632\n",
      "Iteration 6489, loss = 1.54832921\n",
      "Iteration 6490, loss = 1.54828183\n",
      "Iteration 6491, loss = 1.54823532\n",
      "Iteration 6492, loss = 1.54818943\n",
      "Iteration 6493, loss = 1.54814300\n",
      "Iteration 6494, loss = 1.54809692\n",
      "Iteration 6495, loss = 1.54805511\n",
      "Iteration 6496, loss = 1.54801152\n",
      "Iteration 6497, loss = 1.54796428\n",
      "Iteration 6498, loss = 1.54791379\n",
      "Iteration 6499, loss = 1.54787793\n",
      "Iteration 6500, loss = 1.54782774\n",
      "Iteration 6501, loss = 1.54777854\n",
      "Iteration 6502, loss = 1.54773725\n",
      "Iteration 6503, loss = 1.54769209\n",
      "Iteration 6504, loss = 1.54764342\n",
      "Iteration 6505, loss = 1.54759176\n",
      "Iteration 6506, loss = 1.54755444\n",
      "Iteration 6507, loss = 1.54750063\n",
      "Iteration 6508, loss = 1.54745682\n",
      "Iteration 6509, loss = 1.54741480\n",
      "Iteration 6510, loss = 1.54736895\n",
      "Iteration 6511, loss = 1.54731959\n",
      "Iteration 6512, loss = 1.54727061\n",
      "Iteration 6513, loss = 1.54723106\n",
      "Iteration 6514, loss = 1.54718020\n",
      "Iteration 6515, loss = 1.54713420\n",
      "Iteration 6516, loss = 1.54709195\n",
      "Iteration 6517, loss = 1.54704604\n",
      "Iteration 6518, loss = 1.54699695\n",
      "Iteration 6519, loss = 1.54695114\n",
      "Iteration 6520, loss = 1.54690902\n",
      "Iteration 6521, loss = 1.54685714\n",
      "Iteration 6522, loss = 1.54681141\n",
      "Iteration 6523, loss = 1.54676682\n",
      "Iteration 6524, loss = 1.54671890\n",
      "Iteration 6525, loss = 1.54667479\n",
      "Iteration 6526, loss = 1.54663140\n",
      "Iteration 6527, loss = 1.54658110\n",
      "Iteration 6528, loss = 1.54653714\n",
      "Iteration 6529, loss = 1.54649163\n",
      "Iteration 6530, loss = 1.54644276\n",
      "Iteration 6531, loss = 1.54640219\n",
      "Iteration 6532, loss = 1.54635312\n",
      "Iteration 6533, loss = 1.54630929\n",
      "Iteration 6534, loss = 1.54626775\n",
      "Iteration 6535, loss = 1.54622241\n",
      "Iteration 6536, loss = 1.54617364\n",
      "Iteration 6537, loss = 1.54612193\n",
      "Iteration 6538, loss = 1.54608643\n",
      "Iteration 6539, loss = 1.54603377\n",
      "Iteration 6540, loss = 1.54598529\n",
      "Iteration 6541, loss = 1.54594220\n",
      "Iteration 6542, loss = 1.54589571\n",
      "Iteration 6543, loss = 1.54584689\n",
      "Iteration 6544, loss = 1.54580458\n",
      "Iteration 6545, loss = 1.54575527\n",
      "Iteration 6546, loss = 1.54571337\n",
      "Iteration 6547, loss = 1.54566956\n",
      "Iteration 6548, loss = 1.54562239\n",
      "Iteration 6549, loss = 1.54557232\n",
      "Iteration 6550, loss = 1.54554061\n",
      "Iteration 6551, loss = 1.54548095\n",
      "Iteration 6552, loss = 1.54544058\n",
      "Iteration 6553, loss = 1.54539747\n",
      "Iteration 6554, loss = 1.54535076\n",
      "Iteration 6555, loss = 1.54530093\n",
      "Iteration 6556, loss = 1.54525957\n",
      "Iteration 6557, loss = 1.54520701\n",
      "Iteration 6558, loss = 1.54516356\n",
      "Iteration 6559, loss = 1.54511698\n",
      "Iteration 6560, loss = 1.54507408\n",
      "Iteration 6561, loss = 1.54502773\n",
      "Iteration 6562, loss = 1.54498439\n",
      "Iteration 6563, loss = 1.54493759\n",
      "Iteration 6564, loss = 1.54488831\n",
      "Iteration 6565, loss = 1.54484382\n",
      "Iteration 6566, loss = 1.54479960\n",
      "Iteration 6567, loss = 1.54475466\n",
      "Iteration 6568, loss = 1.54470782\n",
      "Iteration 6569, loss = 1.54466629\n",
      "Iteration 6570, loss = 1.54461963\n",
      "Iteration 6571, loss = 1.54457757\n",
      "Iteration 6572, loss = 1.54453199\n",
      "Iteration 6573, loss = 1.54448322\n",
      "Iteration 6574, loss = 1.54443767\n",
      "Iteration 6575, loss = 1.54439368\n",
      "Iteration 6576, loss = 1.54434981\n",
      "Iteration 6577, loss = 1.54430997\n",
      "Iteration 6578, loss = 1.54426632\n",
      "Iteration 6579, loss = 1.54421920\n",
      "Iteration 6580, loss = 1.54416918\n",
      "Iteration 6581, loss = 1.54411933\n",
      "Iteration 6582, loss = 1.54408096\n",
      "Iteration 6583, loss = 1.54403090\n",
      "Iteration 6584, loss = 1.54399047\n",
      "Iteration 6585, loss = 1.54394652\n",
      "Iteration 6586, loss = 1.54389931\n",
      "Iteration 6587, loss = 1.54384927\n",
      "Iteration 6588, loss = 1.54380947\n",
      "Iteration 6589, loss = 1.54375734\n",
      "Iteration 6590, loss = 1.54371599\n",
      "Iteration 6591, loss = 1.54367397\n",
      "Iteration 6592, loss = 1.54362851\n",
      "Iteration 6593, loss = 1.54358002\n",
      "Iteration 6594, loss = 1.54353393\n",
      "Iteration 6595, loss = 1.54348811\n",
      "Iteration 6596, loss = 1.54344200\n",
      "Iteration 6597, loss = 1.54339787\n",
      "Iteration 6598, loss = 1.54335369\n",
      "Iteration 6599, loss = 1.54330847\n",
      "Iteration 6600, loss = 1.54326362\n",
      "Iteration 6601, loss = 1.54321662\n",
      "Iteration 6602, loss = 1.54316935\n",
      "Iteration 6603, loss = 1.54312598\n",
      "Iteration 6604, loss = 1.54308190\n",
      "Iteration 6605, loss = 1.54303595\n",
      "Iteration 6606, loss = 1.54299069\n",
      "Iteration 6607, loss = 1.54294416\n",
      "Iteration 6608, loss = 1.54289991\n",
      "Iteration 6609, loss = 1.54285719\n",
      "Iteration 6610, loss = 1.54281347\n",
      "Iteration 6611, loss = 1.54276664\n",
      "Iteration 6612, loss = 1.54272401\n",
      "Iteration 6613, loss = 1.54267749\n",
      "Iteration 6614, loss = 1.54263109\n",
      "Iteration 6615, loss = 1.54258695\n",
      "Iteration 6616, loss = 1.54254140\n",
      "Iteration 6617, loss = 1.54250345\n",
      "Iteration 6618, loss = 1.54245162\n",
      "Iteration 6619, loss = 1.54240807\n",
      "Iteration 6620, loss = 1.54236219\n",
      "Iteration 6621, loss = 1.54231506\n",
      "Iteration 6622, loss = 1.54227567\n",
      "Iteration 6623, loss = 1.54222888\n",
      "Iteration 6624, loss = 1.54218800\n",
      "Iteration 6625, loss = 1.54214360\n",
      "Iteration 6626, loss = 1.54209603\n",
      "Iteration 6627, loss = 1.54205025\n",
      "Iteration 6628, loss = 1.54200760\n",
      "Iteration 6629, loss = 1.54195730\n",
      "Iteration 6630, loss = 1.54191252\n",
      "Iteration 6631, loss = 1.54186805\n",
      "Iteration 6632, loss = 1.54182354\n",
      "Iteration 6633, loss = 1.54178012\n",
      "Iteration 6634, loss = 1.54173467\n",
      "Iteration 6635, loss = 1.54169026\n",
      "Iteration 6636, loss = 1.54164363\n",
      "Iteration 6637, loss = 1.54160303\n",
      "Iteration 6638, loss = 1.54155631\n",
      "Iteration 6639, loss = 1.54151413\n",
      "Iteration 6640, loss = 1.54146858\n",
      "Iteration 6641, loss = 1.54142021\n",
      "Iteration 6642, loss = 1.54138638\n",
      "Iteration 6643, loss = 1.54133113\n",
      "Iteration 6644, loss = 1.54128940\n",
      "Iteration 6645, loss = 1.54124436\n",
      "Iteration 6646, loss = 1.54119723\n",
      "Iteration 6647, loss = 1.54115424\n",
      "Iteration 6648, loss = 1.54110825\n",
      "Iteration 6649, loss = 1.54106351\n",
      "Iteration 6650, loss = 1.54101895\n",
      "Iteration 6651, loss = 1.54097411\n",
      "Iteration 6652, loss = 1.54092862\n",
      "Iteration 6653, loss = 1.54088410\n",
      "Iteration 6654, loss = 1.54084386\n",
      "Iteration 6655, loss = 1.54079774\n",
      "Iteration 6656, loss = 1.54075520\n",
      "Iteration 6657, loss = 1.54070942\n",
      "Iteration 6658, loss = 1.54066235\n",
      "Iteration 6659, loss = 1.54062973\n",
      "Iteration 6660, loss = 1.54057474\n",
      "Iteration 6661, loss = 1.54053388\n",
      "Iteration 6662, loss = 1.54048967\n",
      "Iteration 6663, loss = 1.54044249\n",
      "Iteration 6664, loss = 1.54039897\n",
      "Iteration 6665, loss = 1.54035468\n",
      "Iteration 6666, loss = 1.54030753\n",
      "Iteration 6667, loss = 1.54026406\n",
      "Iteration 6668, loss = 1.54021820\n",
      "Iteration 6669, loss = 1.54017518\n",
      "Iteration 6670, loss = 1.54013213\n",
      "Iteration 6671, loss = 1.54008839\n",
      "Iteration 6672, loss = 1.54004170\n",
      "Iteration 6673, loss = 1.54000024\n",
      "Iteration 6674, loss = 1.53995339\n",
      "Iteration 6675, loss = 1.53991072\n",
      "Iteration 6676, loss = 1.53986601\n",
      "Iteration 6677, loss = 1.53981962\n",
      "Iteration 6678, loss = 1.53977868\n",
      "Iteration 6679, loss = 1.53972952\n",
      "Iteration 6680, loss = 1.53968565\n",
      "Iteration 6681, loss = 1.53964144\n",
      "Iteration 6682, loss = 1.53959986\n",
      "Iteration 6683, loss = 1.53955451\n",
      "Iteration 6684, loss = 1.53951093\n",
      "Iteration 6685, loss = 1.53946589\n",
      "Iteration 6686, loss = 1.53942335\n",
      "Iteration 6687, loss = 1.53937917\n",
      "Iteration 6688, loss = 1.53933635\n",
      "Iteration 6689, loss = 1.53929040\n",
      "Iteration 6690, loss = 1.53924510\n",
      "Iteration 6691, loss = 1.53920038\n",
      "Iteration 6692, loss = 1.53915631\n",
      "Iteration 6693, loss = 1.53911115\n",
      "Iteration 6694, loss = 1.53906907\n",
      "Iteration 6695, loss = 1.53902487\n",
      "Iteration 6696, loss = 1.53898235\n",
      "Iteration 6697, loss = 1.53893691\n",
      "Iteration 6698, loss = 1.53889639\n",
      "Iteration 6699, loss = 1.53885102\n",
      "Iteration 6700, loss = 1.53880565\n",
      "Iteration 6701, loss = 1.53876292\n",
      "Iteration 6702, loss = 1.53871721\n",
      "Iteration 6703, loss = 1.53867099\n",
      "Iteration 6704, loss = 1.53864182\n",
      "Iteration 6705, loss = 1.53858365\n",
      "Iteration 6706, loss = 1.53854370\n",
      "Iteration 6707, loss = 1.53850050\n",
      "Iteration 6708, loss = 1.53845429\n",
      "Iteration 6709, loss = 1.53841126\n",
      "Iteration 6710, loss = 1.53836792\n",
      "Iteration 6711, loss = 1.53832084\n",
      "Iteration 6712, loss = 1.53827632\n",
      "Iteration 6713, loss = 1.53823180\n",
      "Iteration 6714, loss = 1.53818646\n",
      "Iteration 6715, loss = 1.53814421\n",
      "Iteration 6716, loss = 1.53809961\n",
      "Iteration 6717, loss = 1.53805613\n",
      "Iteration 6718, loss = 1.53801150\n",
      "Iteration 6719, loss = 1.53797208\n",
      "Iteration 6720, loss = 1.53792414\n",
      "Iteration 6721, loss = 1.53788129\n",
      "Iteration 6722, loss = 1.53783621\n",
      "Iteration 6723, loss = 1.53779522\n",
      "Iteration 6724, loss = 1.53774857\n",
      "Iteration 6725, loss = 1.53770577\n",
      "Iteration 6726, loss = 1.53766139\n",
      "Iteration 6727, loss = 1.53761743\n",
      "Iteration 6728, loss = 1.53757250\n",
      "Iteration 6729, loss = 1.53752822\n",
      "Iteration 6730, loss = 1.53748764\n",
      "Iteration 6731, loss = 1.53744443\n",
      "Iteration 6732, loss = 1.53739836\n",
      "Iteration 6733, loss = 1.53736213\n",
      "Iteration 6734, loss = 1.53731020\n",
      "Iteration 6735, loss = 1.53727044\n",
      "Iteration 6736, loss = 1.53722793\n",
      "Iteration 6737, loss = 1.53718248\n",
      "Iteration 6738, loss = 1.53713488\n",
      "Iteration 6739, loss = 1.53710148\n",
      "Iteration 6740, loss = 1.53705069\n",
      "Iteration 6741, loss = 1.53701145\n",
      "Iteration 6742, loss = 1.53696902\n",
      "Iteration 6743, loss = 1.53692360\n",
      "Iteration 6744, loss = 1.53687568\n",
      "Iteration 6745, loss = 1.53683646\n",
      "Iteration 6746, loss = 1.53679154\n",
      "Iteration 6747, loss = 1.53674443\n",
      "Iteration 6748, loss = 1.53670365\n",
      "Iteration 6749, loss = 1.53665984\n",
      "Iteration 6750, loss = 1.53661436\n",
      "Iteration 6751, loss = 1.53657325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6752, loss = 1.53652620\n",
      "Iteration 6753, loss = 1.53648257\n",
      "Iteration 6754, loss = 1.53643857\n",
      "Iteration 6755, loss = 1.53639565\n",
      "Iteration 6756, loss = 1.53635135\n",
      "Iteration 6757, loss = 1.53630693\n",
      "Iteration 6758, loss = 1.53626253\n",
      "Iteration 6759, loss = 1.53622217\n",
      "Iteration 6760, loss = 1.53617917\n",
      "Iteration 6761, loss = 1.53613450\n",
      "Iteration 6762, loss = 1.53609071\n",
      "Iteration 6763, loss = 1.53604712\n",
      "Iteration 6764, loss = 1.53600435\n",
      "Iteration 6765, loss = 1.53596043\n",
      "Iteration 6766, loss = 1.53591834\n",
      "Iteration 6767, loss = 1.53587477\n",
      "Iteration 6768, loss = 1.53583258\n",
      "Iteration 6769, loss = 1.53578755\n",
      "Iteration 6770, loss = 1.53574643\n",
      "Iteration 6771, loss = 1.53570367\n",
      "Iteration 6772, loss = 1.53565836\n",
      "Iteration 6773, loss = 1.53561776\n",
      "Iteration 6774, loss = 1.53557417\n",
      "Iteration 6775, loss = 1.53552791\n",
      "Iteration 6776, loss = 1.53548826\n",
      "Iteration 6777, loss = 1.53544301\n",
      "Iteration 6778, loss = 1.53539738\n",
      "Iteration 6779, loss = 1.53535581\n",
      "Iteration 6780, loss = 1.53531178\n",
      "Iteration 6781, loss = 1.53527035\n",
      "Iteration 6782, loss = 1.53522533\n",
      "Iteration 6783, loss = 1.53518189\n",
      "Iteration 6784, loss = 1.53513707\n",
      "Iteration 6785, loss = 1.53509419\n",
      "Iteration 6786, loss = 1.53505086\n",
      "Iteration 6787, loss = 1.53500875\n",
      "Iteration 6788, loss = 1.53496527\n",
      "Iteration 6789, loss = 1.53492458\n",
      "Iteration 6790, loss = 1.53487775\n",
      "Iteration 6791, loss = 1.53483590\n",
      "Iteration 6792, loss = 1.53479139\n",
      "Iteration 6793, loss = 1.53474993\n",
      "Iteration 6794, loss = 1.53470447\n",
      "Iteration 6795, loss = 1.53466323\n",
      "Iteration 6796, loss = 1.53462103\n",
      "Iteration 6797, loss = 1.53457609\n",
      "Iteration 6798, loss = 1.53453288\n",
      "Iteration 6799, loss = 1.53448855\n",
      "Iteration 6800, loss = 1.53444594\n",
      "Iteration 6801, loss = 1.53440223\n",
      "Iteration 6802, loss = 1.53436323\n",
      "Iteration 6803, loss = 1.53431747\n",
      "Iteration 6804, loss = 1.53427600\n",
      "Iteration 6805, loss = 1.53423165\n",
      "Iteration 6806, loss = 1.53418675\n",
      "Iteration 6807, loss = 1.53415087\n",
      "Iteration 6808, loss = 1.53410438\n",
      "Iteration 6809, loss = 1.53406572\n",
      "Iteration 6810, loss = 1.53402383\n",
      "Iteration 6811, loss = 1.53397904\n",
      "Iteration 6812, loss = 1.53393179\n",
      "Iteration 6813, loss = 1.53389143\n",
      "Iteration 6814, loss = 1.53384455\n",
      "Iteration 6815, loss = 1.53380089\n",
      "Iteration 6816, loss = 1.53375803\n",
      "Iteration 6817, loss = 1.53371677\n",
      "Iteration 6818, loss = 1.53367346\n",
      "Iteration 6819, loss = 1.53363037\n",
      "Iteration 6820, loss = 1.53358624\n",
      "Iteration 6821, loss = 1.53354469\n",
      "Iteration 6822, loss = 1.53350342\n",
      "Iteration 6823, loss = 1.53345814\n",
      "Iteration 6824, loss = 1.53341783\n",
      "Iteration 6825, loss = 1.53337508\n",
      "Iteration 6826, loss = 1.53332966\n",
      "Iteration 6827, loss = 1.53329257\n",
      "Iteration 6828, loss = 1.53324648\n",
      "Iteration 6829, loss = 1.53320174\n",
      "Iteration 6830, loss = 1.53316097\n",
      "Iteration 6831, loss = 1.53312009\n",
      "Iteration 6832, loss = 1.53307802\n",
      "Iteration 6833, loss = 1.53303361\n",
      "Iteration 6834, loss = 1.53299081\n",
      "Iteration 6835, loss = 1.53294588\n",
      "Iteration 6836, loss = 1.53290251\n",
      "Iteration 6837, loss = 1.53286020\n",
      "Iteration 6838, loss = 1.53281751\n",
      "Iteration 6839, loss = 1.53277681\n",
      "Iteration 6840, loss = 1.53273370\n",
      "Iteration 6841, loss = 1.53269121\n",
      "Iteration 6842, loss = 1.53264821\n",
      "Iteration 6843, loss = 1.53260446\n",
      "Iteration 6844, loss = 1.53256354\n",
      "Iteration 6845, loss = 1.53251870\n",
      "Iteration 6846, loss = 1.53247536\n",
      "Iteration 6847, loss = 1.53243266\n",
      "Iteration 6848, loss = 1.53239080\n",
      "Iteration 6849, loss = 1.53234817\n",
      "Iteration 6850, loss = 1.53230583\n",
      "Iteration 6851, loss = 1.53226204\n",
      "Iteration 6852, loss = 1.53221886\n",
      "Iteration 6853, loss = 1.53217419\n",
      "Iteration 6854, loss = 1.53214417\n",
      "Iteration 6855, loss = 1.53209177\n",
      "Iteration 6856, loss = 1.53205320\n",
      "Iteration 6857, loss = 1.53201165\n",
      "Iteration 6858, loss = 1.53196737\n",
      "Iteration 6859, loss = 1.53192204\n",
      "Iteration 6860, loss = 1.53188408\n",
      "Iteration 6861, loss = 1.53183886\n",
      "Iteration 6862, loss = 1.53179660\n",
      "Iteration 6863, loss = 1.53175820\n",
      "Iteration 6864, loss = 1.53171683\n",
      "Iteration 6865, loss = 1.53167278\n",
      "Iteration 6866, loss = 1.53162641\n",
      "Iteration 6867, loss = 1.53158598\n",
      "Iteration 6868, loss = 1.53154618\n",
      "Iteration 6869, loss = 1.53149786\n",
      "Iteration 6870, loss = 1.53145636\n",
      "Iteration 6871, loss = 1.53141638\n",
      "Iteration 6872, loss = 1.53137358\n",
      "Iteration 6873, loss = 1.53133049\n",
      "Iteration 6874, loss = 1.53129033\n",
      "Iteration 6875, loss = 1.53124518\n",
      "Iteration 6876, loss = 1.53120184\n",
      "Iteration 6877, loss = 1.53115755\n",
      "Iteration 6878, loss = 1.53111411\n",
      "Iteration 6879, loss = 1.53107412\n",
      "Iteration 6880, loss = 1.53103084\n",
      "Iteration 6881, loss = 1.53098992\n",
      "Iteration 6882, loss = 1.53094781\n",
      "Iteration 6883, loss = 1.53090505\n",
      "Iteration 6884, loss = 1.53086147\n",
      "Iteration 6885, loss = 1.53081797\n",
      "Iteration 6886, loss = 1.53077549\n",
      "Iteration 6887, loss = 1.53073343\n",
      "Iteration 6888, loss = 1.53069257\n",
      "Iteration 6889, loss = 1.53064959\n",
      "Iteration 6890, loss = 1.53060718\n",
      "Iteration 6891, loss = 1.53056481\n",
      "Iteration 6892, loss = 1.53052174\n",
      "Iteration 6893, loss = 1.53048096\n",
      "Iteration 6894, loss = 1.53043758\n",
      "Iteration 6895, loss = 1.53039635\n",
      "Iteration 6896, loss = 1.53035258\n",
      "Iteration 6897, loss = 1.53031656\n",
      "Iteration 6898, loss = 1.53026948\n",
      "Iteration 6899, loss = 1.53022951\n",
      "Iteration 6900, loss = 1.53018669\n",
      "Iteration 6901, loss = 1.53014216\n",
      "Iteration 6902, loss = 1.53010451\n",
      "Iteration 6903, loss = 1.53005836\n",
      "Iteration 6904, loss = 1.53001816\n",
      "Iteration 6905, loss = 1.52997529\n",
      "Iteration 6906, loss = 1.52993286\n",
      "Iteration 6907, loss = 1.52989011\n",
      "Iteration 6908, loss = 1.52984744\n",
      "Iteration 6909, loss = 1.52980458\n",
      "Iteration 6910, loss = 1.52976378\n",
      "Iteration 6911, loss = 1.52972205\n",
      "Iteration 6912, loss = 1.52968064\n",
      "Iteration 6913, loss = 1.52963667\n",
      "Iteration 6914, loss = 1.52960231\n",
      "Iteration 6915, loss = 1.52955438\n",
      "Iteration 6916, loss = 1.52951531\n",
      "Iteration 6917, loss = 1.52947333\n",
      "Iteration 6918, loss = 1.52942880\n",
      "Iteration 6919, loss = 1.52938676\n",
      "Iteration 6920, loss = 1.52934217\n",
      "Iteration 6921, loss = 1.52929960\n",
      "Iteration 6922, loss = 1.52925896\n",
      "Iteration 6923, loss = 1.52921706\n",
      "Iteration 6924, loss = 1.52917475\n",
      "Iteration 6925, loss = 1.52913549\n",
      "Iteration 6926, loss = 1.52909329\n",
      "Iteration 6927, loss = 1.52905358\n",
      "Iteration 6928, loss = 1.52901103\n",
      "Iteration 6929, loss = 1.52896650\n",
      "Iteration 6930, loss = 1.52892575\n",
      "Iteration 6931, loss = 1.52888162\n",
      "Iteration 6932, loss = 1.52884154\n",
      "Iteration 6933, loss = 1.52879987\n",
      "Iteration 6934, loss = 1.52875646\n",
      "Iteration 6935, loss = 1.52871483\n",
      "Iteration 6936, loss = 1.52867375\n",
      "Iteration 6937, loss = 1.52863231\n",
      "Iteration 6938, loss = 1.52859084\n",
      "Iteration 6939, loss = 1.52854882\n",
      "Iteration 6940, loss = 1.52850552\n",
      "Iteration 6941, loss = 1.52846409\n",
      "Iteration 6942, loss = 1.52842260\n",
      "Iteration 6943, loss = 1.52838352\n",
      "Iteration 6944, loss = 1.52834298\n",
      "Iteration 6945, loss = 1.52830432\n",
      "Iteration 6946, loss = 1.52826280\n",
      "Iteration 6947, loss = 1.52821866\n",
      "Iteration 6948, loss = 1.52817271\n",
      "Iteration 6949, loss = 1.52814023\n",
      "Iteration 6950, loss = 1.52809091\n",
      "Iteration 6951, loss = 1.52805290\n",
      "Iteration 6952, loss = 1.52801207\n",
      "Iteration 6953, loss = 1.52796868\n",
      "Iteration 6954, loss = 1.52792311\n",
      "Iteration 6955, loss = 1.52789070\n",
      "Iteration 6956, loss = 1.52784121\n",
      "Iteration 6957, loss = 1.52780048\n",
      "Iteration 6958, loss = 1.52776154\n",
      "Iteration 6959, loss = 1.52771993\n",
      "Iteration 6960, loss = 1.52767598\n",
      "Iteration 6961, loss = 1.52763403\n",
      "Iteration 6962, loss = 1.52759292\n",
      "Iteration 6963, loss = 1.52754816\n",
      "Iteration 6964, loss = 1.52750843\n",
      "Iteration 6965, loss = 1.52746709\n",
      "Iteration 6966, loss = 1.52742687\n",
      "Iteration 6967, loss = 1.52738577\n",
      "Iteration 6968, loss = 1.52734158\n",
      "Iteration 6969, loss = 1.52730047\n",
      "Iteration 6970, loss = 1.52725959\n",
      "Iteration 6971, loss = 1.52722038\n",
      "Iteration 6972, loss = 1.52717846\n",
      "Iteration 6973, loss = 1.52713499\n",
      "Iteration 6974, loss = 1.52709252\n",
      "Iteration 6975, loss = 1.52705222\n",
      "Iteration 6976, loss = 1.52700887\n",
      "Iteration 6977, loss = 1.52696784\n",
      "Iteration 6978, loss = 1.52692708\n",
      "Iteration 6979, loss = 1.52688617\n",
      "Iteration 6980, loss = 1.52684433\n",
      "Iteration 6981, loss = 1.52680273\n",
      "Iteration 6982, loss = 1.52676124\n",
      "Iteration 6983, loss = 1.52672060\n",
      "Iteration 6984, loss = 1.52667866\n",
      "Iteration 6985, loss = 1.52663667\n",
      "Iteration 6986, loss = 1.52659554\n",
      "Iteration 6987, loss = 1.52655342\n",
      "Iteration 6988, loss = 1.52651430\n",
      "Iteration 6989, loss = 1.52647097\n",
      "Iteration 6990, loss = 1.52643105\n",
      "Iteration 6991, loss = 1.52638859\n",
      "Iteration 6992, loss = 1.52634688\n",
      "Iteration 6993, loss = 1.52630462\n",
      "Iteration 6994, loss = 1.52626278\n",
      "Iteration 6995, loss = 1.52622625\n",
      "Iteration 6996, loss = 1.52618128\n",
      "Iteration 6997, loss = 1.52614112\n",
      "Iteration 6998, loss = 1.52609876\n",
      "Iteration 6999, loss = 1.52605722\n",
      "Iteration 7000, loss = 1.52601565\n",
      "Iteration 7001, loss = 1.52597350\n",
      "Iteration 7002, loss = 1.52594193\n",
      "Iteration 7003, loss = 1.52589668\n",
      "Iteration 7004, loss = 1.52586105\n",
      "Iteration 7005, loss = 1.52582239\n",
      "Iteration 7006, loss = 1.52578102\n",
      "Iteration 7007, loss = 1.52573726\n",
      "Iteration 7008, loss = 1.52569149\n",
      "Iteration 7009, loss = 1.52564962\n",
      "Iteration 7010, loss = 1.52561395\n",
      "Iteration 7011, loss = 1.52556383\n",
      "Iteration 7012, loss = 1.52552486\n",
      "Iteration 7013, loss = 1.52548347\n",
      "Iteration 7014, loss = 1.52543995\n",
      "Iteration 7015, loss = 1.52540888\n",
      "Iteration 7016, loss = 1.52535868\n",
      "Iteration 7017, loss = 1.52532201\n",
      "Iteration 7018, loss = 1.52528353\n",
      "Iteration 7019, loss = 1.52524243\n",
      "Iteration 7020, loss = 1.52519896\n",
      "Iteration 7021, loss = 1.52515469\n",
      "Iteration 7022, loss = 1.52511906\n",
      "Iteration 7023, loss = 1.52507240\n",
      "Iteration 7024, loss = 1.52503288\n",
      "Iteration 7025, loss = 1.52499420\n",
      "Iteration 7026, loss = 1.52495294\n",
      "Iteration 7027, loss = 1.52490986\n",
      "Iteration 7028, loss = 1.52487007\n",
      "Iteration 7029, loss = 1.52482704\n",
      "Iteration 7030, loss = 1.52478726\n",
      "Iteration 7031, loss = 1.52474712\n",
      "Iteration 7032, loss = 1.52470464\n",
      "Iteration 7033, loss = 1.52466365\n",
      "Iteration 7034, loss = 1.52462738\n",
      "Iteration 7035, loss = 1.52458099\n",
      "Iteration 7036, loss = 1.52454242\n",
      "Iteration 7037, loss = 1.52450211\n",
      "Iteration 7038, loss = 1.52445942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7039, loss = 1.52441579\n",
      "Iteration 7040, loss = 1.52437929\n",
      "Iteration 7041, loss = 1.52433506\n",
      "Iteration 7042, loss = 1.52429604\n",
      "Iteration 7043, loss = 1.52425456\n",
      "Iteration 7044, loss = 1.52421096\n",
      "Iteration 7045, loss = 1.52418078\n",
      "Iteration 7046, loss = 1.52413053\n",
      "Iteration 7047, loss = 1.52409545\n",
      "Iteration 7048, loss = 1.52405810\n",
      "Iteration 7049, loss = 1.52401806\n",
      "Iteration 7050, loss = 1.52397559\n",
      "Iteration 7051, loss = 1.52393098\n",
      "Iteration 7052, loss = 1.52388633\n",
      "Iteration 7053, loss = 1.52385229\n",
      "Iteration 7054, loss = 1.52380587\n",
      "Iteration 7055, loss = 1.52376904\n",
      "Iteration 7056, loss = 1.52372956\n",
      "Iteration 7057, loss = 1.52368771\n",
      "Iteration 7058, loss = 1.52364369\n",
      "Iteration 7059, loss = 1.52360589\n",
      "Iteration 7060, loss = 1.52356881\n",
      "Iteration 7061, loss = 1.52351926\n",
      "Iteration 7062, loss = 1.52348172\n",
      "Iteration 7063, loss = 1.52344226\n",
      "Iteration 7064, loss = 1.52340046\n",
      "Iteration 7065, loss = 1.52336205\n",
      "Iteration 7066, loss = 1.52331810\n",
      "Iteration 7067, loss = 1.52327988\n",
      "Iteration 7068, loss = 1.52323952\n",
      "Iteration 7069, loss = 1.52319685\n",
      "Iteration 7070, loss = 1.52315510\n",
      "Iteration 7071, loss = 1.52311247\n",
      "Iteration 7072, loss = 1.52307297\n",
      "Iteration 7073, loss = 1.52303343\n",
      "Iteration 7074, loss = 1.52299327\n",
      "Iteration 7075, loss = 1.52295303\n",
      "Iteration 7076, loss = 1.52291170\n",
      "Iteration 7077, loss = 1.52287112\n",
      "Iteration 7078, loss = 1.52283015\n",
      "Iteration 7079, loss = 1.52278890\n",
      "Iteration 7080, loss = 1.52275007\n",
      "Iteration 7081, loss = 1.52270968\n",
      "Iteration 7082, loss = 1.52267003\n",
      "Iteration 7083, loss = 1.52263265\n",
      "Iteration 7084, loss = 1.52258835\n",
      "Iteration 7085, loss = 1.52255097\n",
      "Iteration 7086, loss = 1.52251134\n",
      "Iteration 7087, loss = 1.52246941\n",
      "Iteration 7088, loss = 1.52242538\n",
      "Iteration 7089, loss = 1.52239342\n",
      "Iteration 7090, loss = 1.52235001\n",
      "Iteration 7091, loss = 1.52230521\n",
      "Iteration 7092, loss = 1.52226743\n",
      "Iteration 7093, loss = 1.52222719\n",
      "Iteration 7094, loss = 1.52218505\n",
      "Iteration 7095, loss = 1.52214419\n",
      "Iteration 7096, loss = 1.52210453\n",
      "Iteration 7097, loss = 1.52206166\n",
      "Iteration 7098, loss = 1.52202193\n",
      "Iteration 7099, loss = 1.52198101\n",
      "Iteration 7100, loss = 1.52194250\n",
      "Iteration 7101, loss = 1.52190109\n",
      "Iteration 7102, loss = 1.52186195\n",
      "Iteration 7103, loss = 1.52182282\n",
      "Iteration 7104, loss = 1.52178141\n",
      "Iteration 7105, loss = 1.52173798\n",
      "Iteration 7106, loss = 1.52171175\n",
      "Iteration 7107, loss = 1.52165837\n",
      "Iteration 7108, loss = 1.52162192\n",
      "Iteration 7109, loss = 1.52158446\n",
      "Iteration 7110, loss = 1.52154447\n",
      "Iteration 7111, loss = 1.52150221\n",
      "Iteration 7112, loss = 1.52145808\n",
      "Iteration 7113, loss = 1.52142606\n",
      "Iteration 7114, loss = 1.52137617\n",
      "Iteration 7115, loss = 1.52134024\n",
      "Iteration 7116, loss = 1.52130299\n",
      "Iteration 7117, loss = 1.52126327\n",
      "Iteration 7118, loss = 1.52122131\n",
      "Iteration 7119, loss = 1.52117748\n",
      "Iteration 7120, loss = 1.52114159\n",
      "Iteration 7121, loss = 1.52109752\n",
      "Iteration 7122, loss = 1.52106196\n",
      "Iteration 7123, loss = 1.52102393\n",
      "Iteration 7124, loss = 1.52098340\n",
      "Iteration 7125, loss = 1.52094068\n",
      "Iteration 7126, loss = 1.52089635\n",
      "Iteration 7127, loss = 1.52086294\n",
      "Iteration 7128, loss = 1.52081569\n",
      "Iteration 7129, loss = 1.52078078\n",
      "Iteration 7130, loss = 1.52074640\n",
      "Iteration 7131, loss = 1.52070923\n",
      "Iteration 7132, loss = 1.52066952\n",
      "Iteration 7133, loss = 1.52062754\n",
      "Iteration 7134, loss = 1.52058360\n",
      "Iteration 7135, loss = 1.52053814\n",
      "Iteration 7136, loss = 1.52050170\n",
      "Iteration 7137, loss = 1.52045948\n",
      "Iteration 7138, loss = 1.52041726\n",
      "Iteration 7139, loss = 1.52038023\n",
      "Iteration 7140, loss = 1.52034097\n",
      "Iteration 7141, loss = 1.52029960\n",
      "Iteration 7142, loss = 1.52025723\n",
      "Iteration 7143, loss = 1.52021867\n",
      "Iteration 7144, loss = 1.52017739\n",
      "Iteration 7145, loss = 1.52013586\n",
      "Iteration 7146, loss = 1.52009726\n",
      "Iteration 7147, loss = 1.52005752\n",
      "Iteration 7148, loss = 1.52001877\n",
      "Iteration 7149, loss = 1.51997747\n",
      "Iteration 7150, loss = 1.51993897\n",
      "Iteration 7151, loss = 1.51989984\n",
      "Iteration 7152, loss = 1.51985847\n",
      "Iteration 7153, loss = 1.51981836\n",
      "Iteration 7154, loss = 1.51977930\n",
      "Iteration 7155, loss = 1.51973734\n",
      "Iteration 7156, loss = 1.51969998\n",
      "Iteration 7157, loss = 1.51966049\n",
      "Iteration 7158, loss = 1.51962053\n",
      "Iteration 7159, loss = 1.51958035\n",
      "Iteration 7160, loss = 1.51953965\n",
      "Iteration 7161, loss = 1.51949925\n",
      "Iteration 7162, loss = 1.51946035\n",
      "Iteration 7163, loss = 1.51942051\n",
      "Iteration 7164, loss = 1.51937898\n",
      "Iteration 7165, loss = 1.51934106\n",
      "Iteration 7166, loss = 1.51930153\n",
      "Iteration 7167, loss = 1.51926387\n",
      "Iteration 7168, loss = 1.51922400\n",
      "Iteration 7169, loss = 1.51918211\n",
      "Iteration 7170, loss = 1.51913932\n",
      "Iteration 7171, loss = 1.51909999\n",
      "Iteration 7172, loss = 1.51906182\n",
      "Iteration 7173, loss = 1.51902293\n",
      "Iteration 7174, loss = 1.51898261\n",
      "Iteration 7175, loss = 1.51894333\n",
      "Iteration 7176, loss = 1.51890253\n",
      "Iteration 7177, loss = 1.51886227\n",
      "Iteration 7178, loss = 1.51882482\n",
      "Iteration 7179, loss = 1.51878316\n",
      "Iteration 7180, loss = 1.51874398\n",
      "Iteration 7181, loss = 1.51870368\n",
      "Iteration 7182, loss = 1.51866417\n",
      "Iteration 7183, loss = 1.51862530\n",
      "Iteration 7184, loss = 1.51858546\n",
      "Iteration 7185, loss = 1.51854439\n",
      "Iteration 7186, loss = 1.51850523\n",
      "Iteration 7187, loss = 1.51846556\n",
      "Iteration 7188, loss = 1.51842582\n",
      "Iteration 7189, loss = 1.51838829\n",
      "Iteration 7190, loss = 1.51835030\n",
      "Iteration 7191, loss = 1.51831327\n",
      "Iteration 7192, loss = 1.51827389\n",
      "Iteration 7193, loss = 1.51823239\n",
      "Iteration 7194, loss = 1.51818919\n",
      "Iteration 7195, loss = 1.51815840\n",
      "Iteration 7196, loss = 1.51811459\n",
      "Iteration 7197, loss = 1.51808041\n",
      "Iteration 7198, loss = 1.51804355\n",
      "Iteration 7199, loss = 1.51800425\n",
      "Iteration 7200, loss = 1.51796280\n",
      "Iteration 7201, loss = 1.51791954\n",
      "Iteration 7202, loss = 1.51787486\n",
      "Iteration 7203, loss = 1.51785173\n",
      "Iteration 7204, loss = 1.51779549\n",
      "Iteration 7205, loss = 1.51775962\n",
      "Iteration 7206, loss = 1.51772158\n",
      "Iteration 7207, loss = 1.51768145\n",
      "Iteration 7208, loss = 1.51763943\n",
      "Iteration 7209, loss = 1.51760347\n",
      "Iteration 7210, loss = 1.51756233\n",
      "Iteration 7211, loss = 1.51752174\n",
      "Iteration 7212, loss = 1.51748394\n",
      "Iteration 7213, loss = 1.51744402\n",
      "Iteration 7214, loss = 1.51740326\n",
      "Iteration 7215, loss = 1.51736347\n",
      "Iteration 7216, loss = 1.51732295\n",
      "Iteration 7217, loss = 1.51728544\n",
      "Iteration 7218, loss = 1.51724487\n",
      "Iteration 7219, loss = 1.51720834\n",
      "Iteration 7220, loss = 1.51716976\n",
      "Iteration 7221, loss = 1.51712907\n",
      "Iteration 7222, loss = 1.51708659\n",
      "Iteration 7223, loss = 1.51706278\n",
      "Iteration 7224, loss = 1.51700822\n",
      "Iteration 7225, loss = 1.51697153\n",
      "Iteration 7226, loss = 1.51693262\n",
      "Iteration 7227, loss = 1.51689165\n",
      "Iteration 7228, loss = 1.51685487\n",
      "Iteration 7229, loss = 1.51681477\n",
      "Iteration 7230, loss = 1.51677815\n",
      "Iteration 7231, loss = 1.51673920\n",
      "Iteration 7232, loss = 1.51669818\n",
      "Iteration 7233, loss = 1.51665733\n",
      "Iteration 7234, loss = 1.51662348\n",
      "Iteration 7235, loss = 1.51657870\n",
      "Iteration 7236, loss = 1.51654199\n",
      "Iteration 7237, loss = 1.51650310\n",
      "Iteration 7238, loss = 1.51646225\n",
      "Iteration 7239, loss = 1.51642323\n",
      "Iteration 7240, loss = 1.51638244\n",
      "Iteration 7241, loss = 1.51634415\n",
      "Iteration 7242, loss = 1.51630413\n",
      "Iteration 7243, loss = 1.51626727\n",
      "Iteration 7244, loss = 1.51622672\n",
      "Iteration 7245, loss = 1.51618862\n",
      "Iteration 7246, loss = 1.51615107\n",
      "Iteration 7247, loss = 1.51611136\n",
      "Iteration 7248, loss = 1.51607044\n",
      "Iteration 7249, loss = 1.51603452\n",
      "Iteration 7250, loss = 1.51599332\n",
      "Iteration 7251, loss = 1.51595596\n",
      "Iteration 7252, loss = 1.51591644\n",
      "Iteration 7253, loss = 1.51587745\n",
      "Iteration 7254, loss = 1.51584065\n",
      "Iteration 7255, loss = 1.51579785\n",
      "Iteration 7256, loss = 1.51575932\n",
      "Iteration 7257, loss = 1.51572115\n",
      "Iteration 7258, loss = 1.51568328\n",
      "Iteration 7259, loss = 1.51564463\n",
      "Iteration 7260, loss = 1.51560527\n",
      "Iteration 7261, loss = 1.51556536\n",
      "Iteration 7262, loss = 1.51552520\n",
      "Iteration 7263, loss = 1.51548711\n",
      "Iteration 7264, loss = 1.51544913\n",
      "Iteration 7265, loss = 1.51540914\n",
      "Iteration 7266, loss = 1.51537176\n",
      "Iteration 7267, loss = 1.51533281\n",
      "Iteration 7268, loss = 1.51529224\n",
      "Iteration 7269, loss = 1.51525393\n",
      "Iteration 7270, loss = 1.51521441\n",
      "Iteration 7271, loss = 1.51517565\n",
      "Iteration 7272, loss = 1.51513586\n",
      "Iteration 7273, loss = 1.51509784\n",
      "Iteration 7274, loss = 1.51506000\n",
      "Iteration 7275, loss = 1.51502242\n",
      "Iteration 7276, loss = 1.51498270\n",
      "Iteration 7277, loss = 1.51494257\n",
      "Iteration 7278, loss = 1.51490617\n",
      "Iteration 7279, loss = 1.51486611\n",
      "Iteration 7280, loss = 1.51482880\n",
      "Iteration 7281, loss = 1.51478938\n",
      "Iteration 7282, loss = 1.51475131\n",
      "Iteration 7283, loss = 1.51471266\n",
      "Iteration 7284, loss = 1.51467109\n",
      "Iteration 7285, loss = 1.51463288\n",
      "Iteration 7286, loss = 1.51459398\n",
      "Iteration 7287, loss = 1.51455540\n",
      "Iteration 7288, loss = 1.51451815\n",
      "Iteration 7289, loss = 1.51447927\n",
      "Iteration 7290, loss = 1.51443888\n",
      "Iteration 7291, loss = 1.51440134\n",
      "Iteration 7292, loss = 1.51436187\n",
      "Iteration 7293, loss = 1.51432402\n",
      "Iteration 7294, loss = 1.51428471\n",
      "Iteration 7295, loss = 1.51424610\n",
      "Iteration 7296, loss = 1.51420696\n",
      "Iteration 7297, loss = 1.51416685\n",
      "Iteration 7298, loss = 1.51412886\n",
      "Iteration 7299, loss = 1.51409043\n",
      "Iteration 7300, loss = 1.51405242\n",
      "Iteration 7301, loss = 1.51401466\n",
      "Iteration 7302, loss = 1.51397702\n",
      "Iteration 7303, loss = 1.51393908\n",
      "Iteration 7304, loss = 1.51389919\n",
      "Iteration 7305, loss = 1.51386072\n",
      "Iteration 7306, loss = 1.51382015\n",
      "Iteration 7307, loss = 1.51378088\n",
      "Iteration 7308, loss = 1.51374302\n",
      "Iteration 7309, loss = 1.51370762\n",
      "Iteration 7310, loss = 1.51367168\n",
      "Iteration 7311, loss = 1.51363350\n",
      "Iteration 7312, loss = 1.51359332\n",
      "Iteration 7313, loss = 1.51355309\n",
      "Iteration 7314, loss = 1.51351601\n",
      "Iteration 7315, loss = 1.51347640\n",
      "Iteration 7316, loss = 1.51343934\n",
      "Iteration 7317, loss = 1.51340028\n",
      "Iteration 7318, loss = 1.51336096\n",
      "Iteration 7319, loss = 1.51332275\n",
      "Iteration 7320, loss = 1.51328262\n",
      "Iteration 7321, loss = 1.51324385\n",
      "Iteration 7322, loss = 1.51320551\n",
      "Iteration 7323, loss = 1.51316866\n",
      "Iteration 7324, loss = 1.51313004\n",
      "Iteration 7325, loss = 1.51308978\n",
      "Iteration 7326, loss = 1.51305282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7327, loss = 1.51301308\n",
      "Iteration 7328, loss = 1.51297519\n",
      "Iteration 7329, loss = 1.51293626\n",
      "Iteration 7330, loss = 1.51289709\n",
      "Iteration 7331, loss = 1.51285977\n",
      "Iteration 7332, loss = 1.51282302\n",
      "Iteration 7333, loss = 1.51278695\n",
      "Iteration 7334, loss = 1.51274874\n",
      "Iteration 7335, loss = 1.51270861\n",
      "Iteration 7336, loss = 1.51266968\n",
      "Iteration 7337, loss = 1.51263308\n",
      "Iteration 7338, loss = 1.51259304\n",
      "Iteration 7339, loss = 1.51255695\n",
      "Iteration 7340, loss = 1.51251878\n",
      "Iteration 7341, loss = 1.51247935\n",
      "Iteration 7342, loss = 1.51244291\n",
      "Iteration 7343, loss = 1.51240237\n",
      "Iteration 7344, loss = 1.51236412\n",
      "Iteration 7345, loss = 1.51232507\n",
      "Iteration 7346, loss = 1.51228591\n",
      "Iteration 7347, loss = 1.51225200\n",
      "Iteration 7348, loss = 1.51221063\n",
      "Iteration 7349, loss = 1.51217540\n",
      "Iteration 7350, loss = 1.51213807\n",
      "Iteration 7351, loss = 1.51209879\n",
      "Iteration 7352, loss = 1.51205915\n",
      "Iteration 7353, loss = 1.51202310\n",
      "Iteration 7354, loss = 1.51198485\n",
      "Iteration 7355, loss = 1.51194247\n",
      "Iteration 7356, loss = 1.51190517\n",
      "Iteration 7357, loss = 1.51186684\n",
      "Iteration 7358, loss = 1.51182738\n",
      "Iteration 7359, loss = 1.51179238\n",
      "Iteration 7360, loss = 1.51175130\n",
      "Iteration 7361, loss = 1.51171512\n",
      "Iteration 7362, loss = 1.51167788\n",
      "Iteration 7363, loss = 1.51163905\n",
      "Iteration 7364, loss = 1.51160161\n",
      "Iteration 7365, loss = 1.51156248\n",
      "Iteration 7366, loss = 1.51152365\n",
      "Iteration 7367, loss = 1.51148408\n",
      "Iteration 7368, loss = 1.51144904\n",
      "Iteration 7369, loss = 1.51141035\n",
      "Iteration 7370, loss = 1.51137521\n",
      "Iteration 7371, loss = 1.51133799\n",
      "Iteration 7372, loss = 1.51129882\n",
      "Iteration 7373, loss = 1.51125922\n",
      "Iteration 7374, loss = 1.51122261\n",
      "Iteration 7375, loss = 1.51118312\n",
      "Iteration 7376, loss = 1.51114327\n",
      "Iteration 7377, loss = 1.51110603\n",
      "Iteration 7378, loss = 1.51106837\n",
      "Iteration 7379, loss = 1.51103312\n",
      "Iteration 7380, loss = 1.51099248\n",
      "Iteration 7381, loss = 1.51095521\n",
      "Iteration 7382, loss = 1.51091677\n",
      "Iteration 7383, loss = 1.51087703\n",
      "Iteration 7384, loss = 1.51083836\n",
      "Iteration 7385, loss = 1.51080078\n",
      "Iteration 7386, loss = 1.51076313\n",
      "Iteration 7387, loss = 1.51072541\n",
      "Iteration 7388, loss = 1.51068664\n",
      "Iteration 7389, loss = 1.51064951\n",
      "Iteration 7390, loss = 1.51061251\n",
      "Iteration 7391, loss = 1.51057582\n",
      "Iteration 7392, loss = 1.51053719\n",
      "Iteration 7393, loss = 1.51049836\n",
      "Iteration 7394, loss = 1.51046039\n",
      "Iteration 7395, loss = 1.51042272\n",
      "Iteration 7396, loss = 1.51038523\n",
      "Iteration 7397, loss = 1.51034713\n",
      "Iteration 7398, loss = 1.51030871\n",
      "Iteration 7399, loss = 1.51027226\n",
      "Iteration 7400, loss = 1.51023433\n",
      "Iteration 7401, loss = 1.51019770\n",
      "Iteration 7402, loss = 1.51015949\n",
      "Iteration 7403, loss = 1.51012083\n",
      "Iteration 7404, loss = 1.51008282\n",
      "Iteration 7405, loss = 1.51004620\n",
      "Iteration 7406, loss = 1.51000675\n",
      "Iteration 7407, loss = 1.50996932\n",
      "Iteration 7408, loss = 1.50993059\n",
      "Iteration 7409, loss = 1.50989718\n",
      "Iteration 7410, loss = 1.50985662\n",
      "Iteration 7411, loss = 1.50982090\n",
      "Iteration 7412, loss = 1.50978324\n",
      "Iteration 7413, loss = 1.50974380\n",
      "Iteration 7414, loss = 1.50970935\n",
      "Iteration 7415, loss = 1.50966760\n",
      "Iteration 7416, loss = 1.50963039\n",
      "Iteration 7417, loss = 1.50959223\n",
      "Iteration 7418, loss = 1.50955508\n",
      "Iteration 7419, loss = 1.50951830\n",
      "Iteration 7420, loss = 1.50947923\n",
      "Iteration 7421, loss = 1.50944376\n",
      "Iteration 7422, loss = 1.50940638\n",
      "Iteration 7423, loss = 1.50936731\n",
      "Iteration 7424, loss = 1.50933077\n",
      "Iteration 7425, loss = 1.50929502\n",
      "Iteration 7426, loss = 1.50925398\n",
      "Iteration 7427, loss = 1.50921799\n",
      "Iteration 7428, loss = 1.50918011\n",
      "Iteration 7429, loss = 1.50914057\n",
      "Iteration 7430, loss = 1.50910744\n",
      "Iteration 7431, loss = 1.50906533\n",
      "Iteration 7432, loss = 1.50903151\n",
      "Iteration 7433, loss = 1.50899607\n",
      "Iteration 7434, loss = 1.50895861\n",
      "Iteration 7435, loss = 1.50891935\n",
      "Iteration 7436, loss = 1.50887991\n",
      "Iteration 7437, loss = 1.50884287\n",
      "Iteration 7438, loss = 1.50880279\n",
      "Iteration 7439, loss = 1.50876696\n",
      "Iteration 7440, loss = 1.50872963\n",
      "Iteration 7441, loss = 1.50869166\n",
      "Iteration 7442, loss = 1.50865570\n",
      "Iteration 7443, loss = 1.50861668\n",
      "Iteration 7444, loss = 1.50858058\n",
      "Iteration 7445, loss = 1.50854307\n",
      "Iteration 7446, loss = 1.50850397\n",
      "Iteration 7447, loss = 1.50846862\n",
      "Iteration 7448, loss = 1.50843046\n",
      "Iteration 7449, loss = 1.50839265\n",
      "Iteration 7450, loss = 1.50835727\n",
      "Iteration 7451, loss = 1.50832000\n",
      "Iteration 7452, loss = 1.50828095\n",
      "Iteration 7453, loss = 1.50824485\n",
      "Iteration 7454, loss = 1.50820774\n",
      "Iteration 7455, loss = 1.50816687\n",
      "Iteration 7456, loss = 1.50813084\n",
      "Iteration 7457, loss = 1.50809350\n",
      "Iteration 7458, loss = 1.50805629\n",
      "Iteration 7459, loss = 1.50801861\n",
      "Iteration 7460, loss = 1.50798142\n",
      "Iteration 7461, loss = 1.50794378\n",
      "Iteration 7462, loss = 1.50790997\n",
      "Iteration 7463, loss = 1.50787476\n",
      "Iteration 7464, loss = 1.50783752\n",
      "Iteration 7465, loss = 1.50779855\n",
      "Iteration 7466, loss = 1.50775806\n",
      "Iteration 7467, loss = 1.50772397\n",
      "Iteration 7468, loss = 1.50768557\n",
      "Iteration 7469, loss = 1.50764522\n",
      "Iteration 7470, loss = 1.50760935\n",
      "Iteration 7471, loss = 1.50757171\n",
      "Iteration 7472, loss = 1.50753278\n",
      "Iteration 7473, loss = 1.50749657\n",
      "Iteration 7474, loss = 1.50745731\n",
      "Iteration 7475, loss = 1.50742105\n",
      "Iteration 7476, loss = 1.50738465\n",
      "Iteration 7477, loss = 1.50734809\n",
      "Iteration 7478, loss = 1.50731071\n",
      "Iteration 7479, loss = 1.50727269\n",
      "Iteration 7480, loss = 1.50723562\n",
      "Iteration 7481, loss = 1.50719865\n",
      "Iteration 7482, loss = 1.50716160\n",
      "Iteration 7483, loss = 1.50712540\n",
      "Iteration 7484, loss = 1.50708796\n",
      "Iteration 7485, loss = 1.50704972\n",
      "Iteration 7486, loss = 1.50701465\n",
      "Iteration 7487, loss = 1.50697654\n",
      "Iteration 7488, loss = 1.50694090\n",
      "Iteration 7489, loss = 1.50690346\n",
      "Iteration 7490, loss = 1.50686467\n",
      "Iteration 7491, loss = 1.50682679\n",
      "Iteration 7492, loss = 1.50678916\n",
      "Iteration 7493, loss = 1.50675447\n",
      "Iteration 7494, loss = 1.50671918\n",
      "Iteration 7495, loss = 1.50668206\n",
      "Iteration 7496, loss = 1.50664322\n",
      "Iteration 7497, loss = 1.50660873\n",
      "Iteration 7498, loss = 1.50656767\n",
      "Iteration 7499, loss = 1.50653073\n",
      "Iteration 7500, loss = 1.50649349\n",
      "Iteration 7501, loss = 1.50645799\n",
      "Iteration 7502, loss = 1.50641963\n",
      "Iteration 7503, loss = 1.50638272\n",
      "Iteration 7504, loss = 1.50634611\n",
      "Iteration 7505, loss = 1.50630846\n",
      "Iteration 7506, loss = 1.50627093\n",
      "Iteration 7507, loss = 1.50623491\n",
      "Iteration 7508, loss = 1.50619729\n",
      "Iteration 7509, loss = 1.50616136\n",
      "Iteration 7510, loss = 1.50612451\n",
      "Iteration 7511, loss = 1.50608692\n",
      "Iteration 7512, loss = 1.50605070\n",
      "Iteration 7513, loss = 1.50601302\n",
      "Iteration 7514, loss = 1.50597687\n",
      "Iteration 7515, loss = 1.50594118\n",
      "Iteration 7516, loss = 1.50590364\n",
      "Iteration 7517, loss = 1.50586652\n",
      "Iteration 7518, loss = 1.50582882\n",
      "Iteration 7519, loss = 1.50579165\n",
      "Iteration 7520, loss = 1.50575487\n",
      "Iteration 7521, loss = 1.50571823\n",
      "Iteration 7522, loss = 1.50568085\n",
      "Iteration 7523, loss = 1.50564481\n",
      "Iteration 7524, loss = 1.50560794\n",
      "Iteration 7525, loss = 1.50557050\n",
      "Iteration 7526, loss = 1.50553326\n",
      "Iteration 7527, loss = 1.50549891\n",
      "Iteration 7528, loss = 1.50545923\n",
      "Iteration 7529, loss = 1.50542304\n",
      "Iteration 7530, loss = 1.50538581\n",
      "Iteration 7531, loss = 1.50535042\n",
      "Iteration 7532, loss = 1.50531415\n",
      "Iteration 7533, loss = 1.50527808\n",
      "Iteration 7534, loss = 1.50524028\n",
      "Iteration 7535, loss = 1.50520220\n",
      "Iteration 7536, loss = 1.50517239\n",
      "Iteration 7537, loss = 1.50513077\n",
      "Iteration 7538, loss = 1.50509739\n",
      "Iteration 7539, loss = 1.50506201\n",
      "Iteration 7540, loss = 1.50502481\n",
      "Iteration 7541, loss = 1.50498603\n",
      "Iteration 7542, loss = 1.50494590\n",
      "Iteration 7543, loss = 1.50491664\n",
      "Iteration 7544, loss = 1.50487214\n",
      "Iteration 7545, loss = 1.50483853\n",
      "Iteration 7546, loss = 1.50480343\n",
      "Iteration 7547, loss = 1.50476651\n",
      "Iteration 7548, loss = 1.50472805\n",
      "Iteration 7549, loss = 1.50469076\n",
      "Iteration 7550, loss = 1.50465368\n",
      "Iteration 7551, loss = 1.50461550\n",
      "Iteration 7552, loss = 1.50458074\n",
      "Iteration 7553, loss = 1.50454485\n",
      "Iteration 7554, loss = 1.50450818\n",
      "Iteration 7555, loss = 1.50447282\n",
      "Iteration 7556, loss = 1.50443652\n",
      "Iteration 7557, loss = 1.50439865\n",
      "Iteration 7558, loss = 1.50436199\n",
      "Iteration 7559, loss = 1.50432445\n",
      "Iteration 7560, loss = 1.50428813\n",
      "Iteration 7561, loss = 1.50425171\n",
      "Iteration 7562, loss = 1.50421448\n",
      "Iteration 7563, loss = 1.50418133\n",
      "Iteration 7564, loss = 1.50414425\n",
      "Iteration 7565, loss = 1.50411165\n",
      "Iteration 7566, loss = 1.50407699\n",
      "Iteration 7567, loss = 1.50404049\n",
      "Iteration 7568, loss = 1.50400238\n",
      "Iteration 7569, loss = 1.50396386\n",
      "Iteration 7570, loss = 1.50392758\n",
      "Iteration 7571, loss = 1.50388966\n",
      "Iteration 7572, loss = 1.50385149\n",
      "Iteration 7573, loss = 1.50381366\n",
      "Iteration 7574, loss = 1.50377592\n",
      "Iteration 7575, loss = 1.50374497\n",
      "Iteration 7576, loss = 1.50370540\n",
      "Iteration 7577, loss = 1.50367024\n",
      "Iteration 7578, loss = 1.50363589\n",
      "Iteration 7579, loss = 1.50359977\n",
      "Iteration 7580, loss = 1.50356203\n",
      "Iteration 7581, loss = 1.50352499\n",
      "Iteration 7582, loss = 1.50348811\n",
      "Iteration 7583, loss = 1.50345136\n",
      "Iteration 7584, loss = 1.50341302\n",
      "Iteration 7585, loss = 1.50337898\n",
      "Iteration 7586, loss = 1.50334312\n",
      "Iteration 7587, loss = 1.50330562\n",
      "Iteration 7588, loss = 1.50326666\n",
      "Iteration 7589, loss = 1.50323826\n",
      "Iteration 7590, loss = 1.50319466\n",
      "Iteration 7591, loss = 1.50316155\n",
      "Iteration 7592, loss = 1.50312850\n",
      "Iteration 7593, loss = 1.50309346\n",
      "Iteration 7594, loss = 1.50305664\n",
      "Iteration 7595, loss = 1.50301827\n",
      "Iteration 7596, loss = 1.50297862\n",
      "Iteration 7597, loss = 1.50294101\n",
      "Iteration 7598, loss = 1.50291285\n",
      "Iteration 7599, loss = 1.50286976\n",
      "Iteration 7600, loss = 1.50283821\n",
      "Iteration 7601, loss = 1.50280473\n",
      "Iteration 7602, loss = 1.50276939\n",
      "Iteration 7603, loss = 1.50273239\n",
      "Iteration 7604, loss = 1.50269383\n",
      "Iteration 7605, loss = 1.50265404\n",
      "Iteration 7606, loss = 1.50261595\n",
      "Iteration 7607, loss = 1.50258675\n",
      "Iteration 7608, loss = 1.50254266\n",
      "Iteration 7609, loss = 1.50250565\n",
      "Iteration 7610, loss = 1.50247022\n",
      "Iteration 7611, loss = 1.50243327\n",
      "Iteration 7612, loss = 1.50239821\n",
      "Iteration 7613, loss = 1.50236012\n",
      "Iteration 7614, loss = 1.50232449\n",
      "Iteration 7615, loss = 1.50228768\n",
      "Iteration 7616, loss = 1.50225281\n",
      "Iteration 7617, loss = 1.50221781\n",
      "Iteration 7618, loss = 1.50218426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7619, loss = 1.50214888\n",
      "Iteration 7620, loss = 1.50211185\n",
      "Iteration 7621, loss = 1.50207385\n",
      "Iteration 7622, loss = 1.50203997\n",
      "Iteration 7623, loss = 1.50200174\n",
      "Iteration 7624, loss = 1.50196506\n",
      "Iteration 7625, loss = 1.50192915\n",
      "Iteration 7626, loss = 1.50189241\n",
      "Iteration 7627, loss = 1.50185608\n",
      "Iteration 7628, loss = 1.50182098\n",
      "Iteration 7629, loss = 1.50178347\n",
      "Iteration 7630, loss = 1.50174741\n",
      "Iteration 7631, loss = 1.50171104\n",
      "Iteration 7632, loss = 1.50167428\n",
      "Iteration 7633, loss = 1.50163882\n",
      "Iteration 7634, loss = 1.50160368\n",
      "Iteration 7635, loss = 1.50156716\n",
      "Iteration 7636, loss = 1.50153190\n",
      "Iteration 7637, loss = 1.50149568\n",
      "Iteration 7638, loss = 1.50145872\n",
      "Iteration 7639, loss = 1.50142331\n",
      "Iteration 7640, loss = 1.50138659\n",
      "Iteration 7641, loss = 1.50135086\n",
      "Iteration 7642, loss = 1.50131365\n",
      "Iteration 7643, loss = 1.50128790\n",
      "Iteration 7644, loss = 1.50124740\n",
      "Iteration 7645, loss = 1.50121754\n",
      "Iteration 7646, loss = 1.50118549\n",
      "Iteration 7647, loss = 1.50115145\n",
      "Iteration 7648, loss = 1.50111561\n",
      "Iteration 7649, loss = 1.50107819\n",
      "Iteration 7650, loss = 1.50103949\n",
      "Iteration 7651, loss = 1.50099957\n",
      "Iteration 7652, loss = 1.50095870\n",
      "Iteration 7653, loss = 1.50093032\n",
      "Iteration 7654, loss = 1.50089858\n",
      "Iteration 7655, loss = 1.50085155\n",
      "Iteration 7656, loss = 1.50082119\n",
      "Iteration 7657, loss = 1.50078887\n",
      "Iteration 7658, loss = 1.50075471\n",
      "Iteration 7659, loss = 1.50071884\n",
      "Iteration 7660, loss = 1.50068147\n",
      "Iteration 7661, loss = 1.50064278\n",
      "Iteration 7662, loss = 1.50060323\n",
      "Iteration 7663, loss = 1.50056715\n",
      "Iteration 7664, loss = 1.50053249\n",
      "Iteration 7665, loss = 1.50049211\n",
      "Iteration 7666, loss = 1.50045611\n",
      "Iteration 7667, loss = 1.50042041\n",
      "Iteration 7668, loss = 1.50038740\n",
      "Iteration 7669, loss = 1.50035163\n",
      "Iteration 7670, loss = 1.50031475\n",
      "Iteration 7671, loss = 1.50027953\n",
      "Iteration 7672, loss = 1.50024283\n",
      "Iteration 7673, loss = 1.50020899\n",
      "Iteration 7674, loss = 1.50017309\n",
      "Iteration 7675, loss = 1.50013655\n",
      "Iteration 7676, loss = 1.50010343\n",
      "Iteration 7677, loss = 1.50006876\n",
      "Iteration 7678, loss = 1.50003243\n",
      "Iteration 7679, loss = 1.49999471\n",
      "Iteration 7680, loss = 1.49995583\n",
      "Iteration 7681, loss = 1.49993129\n",
      "Iteration 7682, loss = 1.49988423\n",
      "Iteration 7683, loss = 1.49985081\n",
      "Iteration 7684, loss = 1.49981580\n",
      "Iteration 7685, loss = 1.49977938\n",
      "Iteration 7686, loss = 1.49974192\n",
      "Iteration 7687, loss = 1.49970645\n",
      "Iteration 7688, loss = 1.49967251\n",
      "Iteration 7689, loss = 1.49963558\n",
      "Iteration 7690, loss = 1.49960189\n",
      "Iteration 7691, loss = 1.49956659\n",
      "Iteration 7692, loss = 1.49952984\n",
      "Iteration 7693, loss = 1.49949669\n",
      "Iteration 7694, loss = 1.49945804\n",
      "Iteration 7695, loss = 1.49942253\n",
      "Iteration 7696, loss = 1.49938647\n",
      "Iteration 7697, loss = 1.49934941\n",
      "Iteration 7698, loss = 1.49931610\n",
      "Iteration 7699, loss = 1.49927848\n",
      "Iteration 7700, loss = 1.49924493\n",
      "Iteration 7701, loss = 1.49920977\n",
      "Iteration 7702, loss = 1.49917312\n",
      "Iteration 7703, loss = 1.49913849\n",
      "Iteration 7704, loss = 1.49910465\n",
      "Iteration 7705, loss = 1.49906845\n",
      "Iteration 7706, loss = 1.49903610\n",
      "Iteration 7707, loss = 1.49900194\n",
      "Iteration 7708, loss = 1.49896615\n",
      "Iteration 7709, loss = 1.49892889\n",
      "Iteration 7710, loss = 1.49889039\n",
      "Iteration 7711, loss = 1.49885973\n",
      "Iteration 7712, loss = 1.49882086\n",
      "Iteration 7713, loss = 1.49878447\n",
      "Iteration 7714, loss = 1.49875117\n",
      "Iteration 7715, loss = 1.49871627\n",
      "Iteration 7716, loss = 1.49867989\n",
      "Iteration 7717, loss = 1.49864444\n",
      "Iteration 7718, loss = 1.49860911\n",
      "Iteration 7719, loss = 1.49857257\n",
      "Iteration 7720, loss = 1.49853714\n",
      "Iteration 7721, loss = 1.49850065\n",
      "Iteration 7722, loss = 1.49846512\n",
      "Iteration 7723, loss = 1.49842940\n",
      "Iteration 7724, loss = 1.49839466\n",
      "Iteration 7725, loss = 1.49836016\n",
      "Iteration 7726, loss = 1.49832434\n",
      "Iteration 7727, loss = 1.49828851\n",
      "Iteration 7728, loss = 1.49825529\n",
      "Iteration 7729, loss = 1.49821799\n",
      "Iteration 7730, loss = 1.49818379\n",
      "Iteration 7731, loss = 1.49814880\n",
      "Iteration 7732, loss = 1.49811428\n",
      "Iteration 7733, loss = 1.49807891\n",
      "Iteration 7734, loss = 1.49804269\n",
      "Iteration 7735, loss = 1.49800704\n",
      "Iteration 7736, loss = 1.49797245\n",
      "Iteration 7737, loss = 1.49793639\n",
      "Iteration 7738, loss = 1.49790211\n",
      "Iteration 7739, loss = 1.49786665\n",
      "Iteration 7740, loss = 1.49783084\n",
      "Iteration 7741, loss = 1.49779556\n",
      "Iteration 7742, loss = 1.49776078\n",
      "Iteration 7743, loss = 1.49772458\n",
      "Iteration 7744, loss = 1.49769105\n",
      "Iteration 7745, loss = 1.49765672\n",
      "Iteration 7746, loss = 1.49761899\n",
      "Iteration 7747, loss = 1.49758490\n",
      "Iteration 7748, loss = 1.49754933\n",
      "Iteration 7749, loss = 1.49751383\n",
      "Iteration 7750, loss = 1.49747855\n",
      "Iteration 7751, loss = 1.49744256\n",
      "Iteration 7752, loss = 1.49740752\n",
      "Iteration 7753, loss = 1.49737157\n",
      "Iteration 7754, loss = 1.49734151\n",
      "Iteration 7755, loss = 1.49730351\n",
      "Iteration 7756, loss = 1.49727100\n",
      "Iteration 7757, loss = 1.49723678\n",
      "Iteration 7758, loss = 1.49720101\n",
      "Iteration 7759, loss = 1.49716388\n",
      "Iteration 7760, loss = 1.49713001\n",
      "Iteration 7761, loss = 1.49709360\n",
      "Iteration 7762, loss = 1.49705860\n",
      "Iteration 7763, loss = 1.49702517\n",
      "Iteration 7764, loss = 1.49699018\n",
      "Iteration 7765, loss = 1.49695386\n",
      "Iteration 7766, loss = 1.49691833\n",
      "Iteration 7767, loss = 1.49688213\n",
      "Iteration 7768, loss = 1.49684762\n",
      "Iteration 7769, loss = 1.49681192\n",
      "Iteration 7770, loss = 1.49677823\n",
      "Iteration 7771, loss = 1.49674095\n",
      "Iteration 7772, loss = 1.49670531\n",
      "Iteration 7773, loss = 1.49667311\n",
      "Iteration 7774, loss = 1.49663644\n",
      "Iteration 7775, loss = 1.49660210\n",
      "Iteration 7776, loss = 1.49656635\n",
      "Iteration 7777, loss = 1.49653490\n",
      "Iteration 7778, loss = 1.49649714\n",
      "Iteration 7779, loss = 1.49646352\n",
      "Iteration 7780, loss = 1.49642837\n",
      "Iteration 7781, loss = 1.49639184\n",
      "Iteration 7782, loss = 1.49635928\n",
      "Iteration 7783, loss = 1.49632420\n",
      "Iteration 7784, loss = 1.49629244\n",
      "Iteration 7785, loss = 1.49625895\n",
      "Iteration 7786, loss = 1.49622388\n",
      "Iteration 7787, loss = 1.49618742\n",
      "Iteration 7788, loss = 1.49614982\n",
      "Iteration 7789, loss = 1.49611718\n",
      "Iteration 7790, loss = 1.49608167\n",
      "Iteration 7791, loss = 1.49604430\n",
      "Iteration 7792, loss = 1.49601086\n",
      "Iteration 7793, loss = 1.49597596\n",
      "Iteration 7794, loss = 1.49593978\n",
      "Iteration 7795, loss = 1.49590518\n",
      "Iteration 7796, loss = 1.49586895\n",
      "Iteration 7797, loss = 1.49583465\n",
      "Iteration 7798, loss = 1.49579954\n",
      "Iteration 7799, loss = 1.49576568\n",
      "Iteration 7800, loss = 1.49573016\n",
      "Iteration 7801, loss = 1.49569565\n",
      "Iteration 7802, loss = 1.49566012\n",
      "Iteration 7803, loss = 1.49562496\n",
      "Iteration 7804, loss = 1.49558953\n",
      "Iteration 7805, loss = 1.49555481\n",
      "Iteration 7806, loss = 1.49552310\n",
      "Iteration 7807, loss = 1.49548875\n",
      "Iteration 7808, loss = 1.49545694\n",
      "Iteration 7809, loss = 1.49542344\n",
      "Iteration 7810, loss = 1.49538841\n",
      "Iteration 7811, loss = 1.49535200\n",
      "Iteration 7812, loss = 1.49531437\n",
      "Iteration 7813, loss = 1.49528181\n",
      "Iteration 7814, loss = 1.49524492\n",
      "Iteration 7815, loss = 1.49520946\n",
      "Iteration 7816, loss = 1.49517562\n",
      "Iteration 7817, loss = 1.49514037\n",
      "Iteration 7818, loss = 1.49510391\n",
      "Iteration 7819, loss = 1.49507750\n",
      "Iteration 7820, loss = 1.49503538\n",
      "Iteration 7821, loss = 1.49500279\n",
      "Iteration 7822, loss = 1.49496869\n",
      "Iteration 7823, loss = 1.49493322\n",
      "Iteration 7824, loss = 1.49489755\n",
      "Iteration 7825, loss = 1.49486461\n",
      "Iteration 7826, loss = 1.49482746\n",
      "Iteration 7827, loss = 1.49479321\n",
      "Iteration 7828, loss = 1.49475767\n",
      "Iteration 7829, loss = 1.49472762\n",
      "Iteration 7830, loss = 1.49469084\n",
      "Iteration 7831, loss = 1.49465904\n",
      "Iteration 7832, loss = 1.49462561\n",
      "Iteration 7833, loss = 1.49459070\n",
      "Iteration 7834, loss = 1.49455453\n",
      "Iteration 7835, loss = 1.49451852\n",
      "Iteration 7836, loss = 1.49448709\n",
      "Iteration 7837, loss = 1.49444853\n",
      "Iteration 7838, loss = 1.49441488\n",
      "Iteration 7839, loss = 1.49438146\n",
      "Iteration 7840, loss = 1.49434661\n",
      "Iteration 7841, loss = 1.49431046\n",
      "Iteration 7842, loss = 1.49427447\n",
      "Iteration 7843, loss = 1.49423947\n",
      "Iteration 7844, loss = 1.49420620\n",
      "Iteration 7845, loss = 1.49417156\n",
      "Iteration 7846, loss = 1.49413838\n",
      "Iteration 7847, loss = 1.49410397\n",
      "Iteration 7848, loss = 1.49406834\n",
      "Iteration 7849, loss = 1.49403275\n",
      "Iteration 7850, loss = 1.49399972\n",
      "Iteration 7851, loss = 1.49396709\n",
      "Iteration 7852, loss = 1.49393552\n",
      "Iteration 7853, loss = 1.49390234\n",
      "Iteration 7854, loss = 1.49386768\n",
      "Iteration 7855, loss = 1.49383174\n",
      "Iteration 7856, loss = 1.49379470\n",
      "Iteration 7857, loss = 1.49376214\n",
      "Iteration 7858, loss = 1.49372808\n",
      "Iteration 7859, loss = 1.49369018\n",
      "Iteration 7860, loss = 1.49365758\n",
      "Iteration 7861, loss = 1.49362395\n",
      "Iteration 7862, loss = 1.49358896\n",
      "Iteration 7863, loss = 1.49355276\n",
      "Iteration 7864, loss = 1.49351874\n",
      "Iteration 7865, loss = 1.49348586\n",
      "Iteration 7866, loss = 1.49344949\n",
      "Iteration 7867, loss = 1.49341600\n",
      "Iteration 7868, loss = 1.49338120\n",
      "Iteration 7869, loss = 1.49334656\n",
      "Iteration 7870, loss = 1.49331500\n",
      "Iteration 7871, loss = 1.49327898\n",
      "Iteration 7872, loss = 1.49324688\n",
      "Iteration 7873, loss = 1.49321321\n",
      "Iteration 7874, loss = 1.49317814\n",
      "Iteration 7875, loss = 1.49314186\n",
      "Iteration 7876, loss = 1.49310744\n",
      "Iteration 7877, loss = 1.49307495\n",
      "Iteration 7878, loss = 1.49304025\n",
      "Iteration 7879, loss = 1.49300960\n",
      "Iteration 7880, loss = 1.49297722\n",
      "Iteration 7881, loss = 1.49294332\n",
      "Iteration 7882, loss = 1.49290808\n",
      "Iteration 7883, loss = 1.49287165\n",
      "Iteration 7884, loss = 1.49283417\n",
      "Iteration 7885, loss = 1.49280451\n",
      "Iteration 7886, loss = 1.49277035\n",
      "Iteration 7887, loss = 1.49273211\n",
      "Iteration 7888, loss = 1.49270150\n",
      "Iteration 7889, loss = 1.49266928\n",
      "Iteration 7890, loss = 1.49263554\n",
      "Iteration 7891, loss = 1.49260050\n",
      "Iteration 7892, loss = 1.49256432\n",
      "Iteration 7893, loss = 1.49252786\n",
      "Iteration 7894, loss = 1.49249531\n",
      "Iteration 7895, loss = 1.49245828\n",
      "Iteration 7896, loss = 1.49242472\n",
      "Iteration 7897, loss = 1.49239128\n",
      "Iteration 7898, loss = 1.49235677\n",
      "Iteration 7899, loss = 1.49232262\n",
      "Iteration 7900, loss = 1.49228928\n",
      "Iteration 7901, loss = 1.49225491\n",
      "Iteration 7902, loss = 1.49221963\n",
      "Iteration 7903, loss = 1.49218363\n",
      "Iteration 7904, loss = 1.49215152\n",
      "Iteration 7905, loss = 1.49211825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7906, loss = 1.49208369\n",
      "Iteration 7907, loss = 1.49204932\n",
      "Iteration 7908, loss = 1.49201454\n",
      "Iteration 7909, loss = 1.49198215\n",
      "Iteration 7910, loss = 1.49194870\n",
      "Iteration 7911, loss = 1.49191395\n",
      "Iteration 7912, loss = 1.49187992\n",
      "Iteration 7913, loss = 1.49184579\n",
      "Iteration 7914, loss = 1.49180972\n",
      "Iteration 7915, loss = 1.49177511\n",
      "Iteration 7916, loss = 1.49174176\n",
      "Iteration 7917, loss = 1.49170871\n",
      "Iteration 7918, loss = 1.49167434\n",
      "Iteration 7919, loss = 1.49164112\n",
      "Iteration 7920, loss = 1.49160610\n",
      "Iteration 7921, loss = 1.49157213\n",
      "Iteration 7922, loss = 1.49153796\n",
      "Iteration 7923, loss = 1.49150383\n",
      "Iteration 7924, loss = 1.49146939\n",
      "Iteration 7925, loss = 1.49143791\n",
      "Iteration 7926, loss = 1.49140334\n",
      "Iteration 7927, loss = 1.49137132\n",
      "Iteration 7928, loss = 1.49133785\n",
      "Iteration 7929, loss = 1.49130310\n",
      "Iteration 7930, loss = 1.49126721\n",
      "Iteration 7931, loss = 1.49123615\n",
      "Iteration 7932, loss = 1.49119980\n",
      "Iteration 7933, loss = 1.49116521\n",
      "Iteration 7934, loss = 1.49113201\n",
      "Iteration 7935, loss = 1.49109857\n",
      "Iteration 7936, loss = 1.49106471\n",
      "Iteration 7937, loss = 1.49102968\n",
      "Iteration 7938, loss = 1.49099516\n",
      "Iteration 7939, loss = 1.49096090\n",
      "Iteration 7940, loss = 1.49092711\n",
      "Iteration 7941, loss = 1.49089421\n",
      "Iteration 7942, loss = 1.49086007\n",
      "Iteration 7943, loss = 1.49082735\n",
      "Iteration 7944, loss = 1.49079375\n",
      "Iteration 7945, loss = 1.49076126\n",
      "Iteration 7946, loss = 1.49072746\n",
      "Iteration 7947, loss = 1.49069240\n",
      "Iteration 7948, loss = 1.49065692\n",
      "Iteration 7949, loss = 1.49062694\n",
      "Iteration 7950, loss = 1.49059087\n",
      "Iteration 7951, loss = 1.49055917\n",
      "Iteration 7952, loss = 1.49052609\n",
      "Iteration 7953, loss = 1.49049172\n",
      "Iteration 7954, loss = 1.49045623\n",
      "Iteration 7955, loss = 1.49042360\n",
      "Iteration 7956, loss = 1.49039014\n",
      "Iteration 7957, loss = 1.49035532\n",
      "Iteration 7958, loss = 1.49032404\n",
      "Iteration 7959, loss = 1.49029133\n",
      "Iteration 7960, loss = 1.49025725\n",
      "Iteration 7961, loss = 1.49022350\n",
      "Iteration 7962, loss = 1.49019068\n",
      "Iteration 7963, loss = 1.49015681\n",
      "Iteration 7964, loss = 1.49012199\n",
      "Iteration 7965, loss = 1.49008639\n",
      "Iteration 7966, loss = 1.49005282\n",
      "Iteration 7967, loss = 1.49001874\n",
      "Iteration 7968, loss = 1.48998545\n",
      "Iteration 7969, loss = 1.48995280\n",
      "Iteration 7970, loss = 1.48991786\n",
      "Iteration 7971, loss = 1.48988291\n",
      "Iteration 7972, loss = 1.48985020\n",
      "Iteration 7973, loss = 1.48981655\n",
      "Iteration 7974, loss = 1.48978230\n",
      "Iteration 7975, loss = 1.48974839\n",
      "Iteration 7976, loss = 1.48971434\n",
      "Iteration 7977, loss = 1.48968046\n",
      "Iteration 7978, loss = 1.48964642\n",
      "Iteration 7979, loss = 1.48961380\n",
      "Iteration 7980, loss = 1.48957909\n",
      "Iteration 7981, loss = 1.48954508\n",
      "Iteration 7982, loss = 1.48951671\n",
      "Iteration 7983, loss = 1.48948263\n",
      "Iteration 7984, loss = 1.48945356\n",
      "Iteration 7985, loss = 1.48942280\n",
      "Iteration 7986, loss = 1.48939053\n",
      "Iteration 7987, loss = 1.48935689\n",
      "Iteration 7988, loss = 1.48932201\n",
      "Iteration 7989, loss = 1.48928604\n",
      "Iteration 7990, loss = 1.48924914\n",
      "Iteration 7991, loss = 1.48921144\n",
      "Iteration 7992, loss = 1.48919445\n",
      "Iteration 7993, loss = 1.48914597\n",
      "Iteration 7994, loss = 1.48911463\n",
      "Iteration 7995, loss = 1.48908521\n",
      "Iteration 7996, loss = 1.48905415\n",
      "Iteration 7997, loss = 1.48902161\n",
      "Iteration 7998, loss = 1.48898774\n",
      "Iteration 7999, loss = 1.48895267\n",
      "Iteration 8000, loss = 1.48891661\n",
      "Iteration 8001, loss = 1.48887978\n",
      "Iteration 8002, loss = 1.48884392\n",
      "Iteration 8003, loss = 1.48881669\n",
      "Iteration 8004, loss = 1.48877678\n",
      "Iteration 8005, loss = 1.48874502\n",
      "Iteration 8006, loss = 1.48871205\n",
      "Iteration 8007, loss = 1.48867800\n",
      "Iteration 8008, loss = 1.48864514\n",
      "Iteration 8009, loss = 1.48861190\n",
      "Iteration 8010, loss = 1.48857848\n",
      "Iteration 8011, loss = 1.48854266\n",
      "Iteration 8012, loss = 1.48850944\n",
      "Iteration 8013, loss = 1.48847629\n",
      "Iteration 8014, loss = 1.48844514\n",
      "Iteration 8015, loss = 1.48840984\n",
      "Iteration 8016, loss = 1.48837742\n",
      "Iteration 8017, loss = 1.48834382\n",
      "Iteration 8018, loss = 1.48830975\n",
      "Iteration 8019, loss = 1.48827792\n",
      "Iteration 8020, loss = 1.48824563\n",
      "Iteration 8021, loss = 1.48821477\n",
      "Iteration 8022, loss = 1.48818252\n",
      "Iteration 8023, loss = 1.48814897\n",
      "Iteration 8024, loss = 1.48811428\n",
      "Iteration 8025, loss = 1.48807860\n",
      "Iteration 8026, loss = 1.48804394\n",
      "Iteration 8027, loss = 1.48802062\n",
      "Iteration 8028, loss = 1.48797801\n",
      "Iteration 8029, loss = 1.48794826\n",
      "Iteration 8030, loss = 1.48791710\n",
      "Iteration 8031, loss = 1.48788457\n",
      "Iteration 8032, loss = 1.48785081\n",
      "Iteration 8033, loss = 1.48781592\n",
      "Iteration 8034, loss = 1.48778011\n",
      "Iteration 8035, loss = 1.48774399\n",
      "Iteration 8036, loss = 1.48771948\n",
      "Iteration 8037, loss = 1.48767774\n",
      "Iteration 8038, loss = 1.48764640\n",
      "Iteration 8039, loss = 1.48761378\n",
      "Iteration 8040, loss = 1.48758003\n",
      "Iteration 8041, loss = 1.48754672\n",
      "Iteration 8042, loss = 1.48751234\n",
      "Iteration 8043, loss = 1.48747920\n",
      "Iteration 8044, loss = 1.48744594\n",
      "Iteration 8045, loss = 1.48741243\n",
      "Iteration 8046, loss = 1.48737887\n",
      "Iteration 8047, loss = 1.48734533\n",
      "Iteration 8048, loss = 1.48731271\n",
      "Iteration 8049, loss = 1.48727998\n",
      "Iteration 8050, loss = 1.48724708\n",
      "Iteration 8051, loss = 1.48721313\n",
      "Iteration 8052, loss = 1.48718452\n",
      "Iteration 8053, loss = 1.48714693\n",
      "Iteration 8054, loss = 1.48711492\n",
      "Iteration 8055, loss = 1.48708212\n",
      "Iteration 8056, loss = 1.48704835\n",
      "Iteration 8057, loss = 1.48701641\n",
      "Iteration 8058, loss = 1.48698407\n",
      "Iteration 8059, loss = 1.48695232\n",
      "Iteration 8060, loss = 1.48691930\n",
      "Iteration 8061, loss = 1.48688510\n",
      "Iteration 8062, loss = 1.48685524\n",
      "Iteration 8063, loss = 1.48682125\n",
      "Iteration 8064, loss = 1.48678672\n",
      "Iteration 8065, loss = 1.48675549\n",
      "Iteration 8066, loss = 1.48672291\n",
      "Iteration 8067, loss = 1.48668918\n",
      "Iteration 8068, loss = 1.48665452\n",
      "Iteration 8069, loss = 1.48662171\n",
      "Iteration 8070, loss = 1.48659065\n",
      "Iteration 8071, loss = 1.48655558\n",
      "Iteration 8072, loss = 1.48652458\n",
      "Iteration 8073, loss = 1.48649234\n",
      "Iteration 8074, loss = 1.48645894\n",
      "Iteration 8075, loss = 1.48642453\n",
      "Iteration 8076, loss = 1.48639111\n",
      "Iteration 8077, loss = 1.48635893\n",
      "Iteration 8078, loss = 1.48632707\n",
      "Iteration 8079, loss = 1.48629031\n",
      "Iteration 8080, loss = 1.48625936\n",
      "Iteration 8081, loss = 1.48622755\n",
      "Iteration 8082, loss = 1.48619454\n",
      "Iteration 8083, loss = 1.48616108\n",
      "Iteration 8084, loss = 1.48612833\n",
      "Iteration 8085, loss = 1.48609583\n",
      "Iteration 8086, loss = 1.48606304\n",
      "Iteration 8087, loss = 1.48603140\n",
      "Iteration 8088, loss = 1.48599851\n",
      "Iteration 8089, loss = 1.48596451\n",
      "Iteration 8090, loss = 1.48592958\n",
      "Iteration 8091, loss = 1.48590077\n",
      "Iteration 8092, loss = 1.48586567\n",
      "Iteration 8093, loss = 1.48583219\n",
      "Iteration 8094, loss = 1.48580156\n",
      "Iteration 8095, loss = 1.48576961\n",
      "Iteration 8096, loss = 1.48573646\n",
      "Iteration 8097, loss = 1.48570226\n",
      "Iteration 8098, loss = 1.48567060\n",
      "Iteration 8099, loss = 1.48563851\n",
      "Iteration 8100, loss = 1.48560692\n",
      "Iteration 8101, loss = 1.48557156\n",
      "Iteration 8102, loss = 1.48553696\n",
      "Iteration 8103, loss = 1.48550311\n",
      "Iteration 8104, loss = 1.48547055\n",
      "Iteration 8105, loss = 1.48543884\n",
      "Iteration 8106, loss = 1.48540632\n",
      "Iteration 8107, loss = 1.48537272\n",
      "Iteration 8108, loss = 1.48533917\n",
      "Iteration 8109, loss = 1.48530780\n",
      "Iteration 8110, loss = 1.48527595\n",
      "Iteration 8111, loss = 1.48524194\n",
      "Iteration 8112, loss = 1.48520957\n",
      "Iteration 8113, loss = 1.48517610\n",
      "Iteration 8114, loss = 1.48514167\n",
      "Iteration 8115, loss = 1.48511876\n",
      "Iteration 8116, loss = 1.48507966\n",
      "Iteration 8117, loss = 1.48505149\n",
      "Iteration 8118, loss = 1.48502174\n",
      "Iteration 8119, loss = 1.48499054\n",
      "Iteration 8120, loss = 1.48495804\n",
      "Iteration 8121, loss = 1.48492438\n",
      "Iteration 8122, loss = 1.48488978\n",
      "Iteration 8123, loss = 1.48485435\n",
      "Iteration 8124, loss = 1.48481817\n",
      "Iteration 8125, loss = 1.48479401\n",
      "Iteration 8126, loss = 1.48475795\n",
      "Iteration 8127, loss = 1.48471997\n",
      "Iteration 8128, loss = 1.48468966\n",
      "Iteration 8129, loss = 1.48465809\n",
      "Iteration 8130, loss = 1.48462538\n",
      "Iteration 8131, loss = 1.48459166\n",
      "Iteration 8132, loss = 1.48455897\n",
      "Iteration 8133, loss = 1.48452553\n",
      "Iteration 8134, loss = 1.48449313\n",
      "Iteration 8135, loss = 1.48446084\n",
      "Iteration 8136, loss = 1.48442749\n",
      "Iteration 8137, loss = 1.48439324\n",
      "Iteration 8138, loss = 1.48436062\n",
      "Iteration 8139, loss = 1.48432791\n",
      "Iteration 8140, loss = 1.48429572\n",
      "Iteration 8141, loss = 1.48426438\n",
      "Iteration 8142, loss = 1.48423190\n",
      "Iteration 8143, loss = 1.48420043\n",
      "Iteration 8144, loss = 1.48416822\n",
      "Iteration 8145, loss = 1.48413494\n",
      "Iteration 8146, loss = 1.48410124\n",
      "Iteration 8147, loss = 1.48406837\n",
      "Iteration 8148, loss = 1.48403619\n",
      "Iteration 8149, loss = 1.48400436\n",
      "Iteration 8150, loss = 1.48397077\n",
      "Iteration 8151, loss = 1.48393874\n",
      "Iteration 8152, loss = 1.48390647\n",
      "Iteration 8153, loss = 1.48387369\n",
      "Iteration 8154, loss = 1.48384119\n",
      "Iteration 8155, loss = 1.48380951\n",
      "Iteration 8156, loss = 1.48377738\n",
      "Iteration 8157, loss = 1.48374423\n",
      "Iteration 8158, loss = 1.48371189\n",
      "Iteration 8159, loss = 1.48367828\n",
      "Iteration 8160, loss = 1.48364639\n",
      "Iteration 8161, loss = 1.48361392\n",
      "Iteration 8162, loss = 1.48358137\n",
      "Iteration 8163, loss = 1.48354876\n",
      "Iteration 8164, loss = 1.48351542\n",
      "Iteration 8165, loss = 1.48348707\n",
      "Iteration 8166, loss = 1.48345125\n",
      "Iteration 8167, loss = 1.48342007\n",
      "Iteration 8168, loss = 1.48338769\n",
      "Iteration 8169, loss = 1.48335424\n",
      "Iteration 8170, loss = 1.48332282\n",
      "Iteration 8171, loss = 1.48328920\n",
      "Iteration 8172, loss = 1.48325845\n",
      "Iteration 8173, loss = 1.48322671\n",
      "Iteration 8174, loss = 1.48319383\n",
      "Iteration 8175, loss = 1.48316146\n",
      "Iteration 8176, loss = 1.48312907\n",
      "Iteration 8177, loss = 1.48309621\n",
      "Iteration 8178, loss = 1.48306400\n",
      "Iteration 8179, loss = 1.48303310\n",
      "Iteration 8180, loss = 1.48300107\n",
      "Iteration 8181, loss = 1.48296799\n",
      "Iteration 8182, loss = 1.48293402\n",
      "Iteration 8183, loss = 1.48291274\n",
      "Iteration 8184, loss = 1.48287067\n",
      "Iteration 8185, loss = 1.48284081\n",
      "Iteration 8186, loss = 1.48280966\n",
      "Iteration 8187, loss = 1.48277736\n",
      "Iteration 8188, loss = 1.48274403\n",
      "Iteration 8189, loss = 1.48270978\n",
      "Iteration 8190, loss = 1.48268059\n",
      "Iteration 8191, loss = 1.48264844\n",
      "Iteration 8192, loss = 1.48261264\n",
      "Iteration 8193, loss = 1.48258196\n",
      "Iteration 8194, loss = 1.48255015\n",
      "Iteration 8195, loss = 1.48251731\n",
      "Iteration 8196, loss = 1.48248724\n",
      "Iteration 8197, loss = 1.48245271\n",
      "Iteration 8198, loss = 1.48242152\n",
      "Iteration 8199, loss = 1.48239020\n",
      "Iteration 8200, loss = 1.48235784\n",
      "Iteration 8201, loss = 1.48232453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8202, loss = 1.48229334\n",
      "Iteration 8203, loss = 1.48226673\n",
      "Iteration 8204, loss = 1.48222718\n",
      "Iteration 8205, loss = 1.48219643\n",
      "Iteration 8206, loss = 1.48216452\n",
      "Iteration 8207, loss = 1.48213181\n",
      "Iteration 8208, loss = 1.48209940\n",
      "Iteration 8209, loss = 1.48206730\n",
      "Iteration 8210, loss = 1.48203466\n",
      "Iteration 8211, loss = 1.48200259\n",
      "Iteration 8212, loss = 1.48197021\n",
      "Iteration 8213, loss = 1.48193756\n",
      "Iteration 8214, loss = 1.48190599\n",
      "Iteration 8215, loss = 1.48187368\n",
      "Iteration 8216, loss = 1.48184318\n",
      "Iteration 8217, loss = 1.48180964\n",
      "Iteration 8218, loss = 1.48177871\n",
      "Iteration 8219, loss = 1.48174697\n",
      "Iteration 8220, loss = 1.48171425\n",
      "Iteration 8221, loss = 1.48168070\n",
      "Iteration 8222, loss = 1.48165582\n",
      "Iteration 8223, loss = 1.48161903\n",
      "Iteration 8224, loss = 1.48159039\n",
      "Iteration 8225, loss = 1.48156045\n",
      "Iteration 8226, loss = 1.48152927\n",
      "Iteration 8227, loss = 1.48149695\n",
      "Iteration 8228, loss = 1.48146358\n",
      "Iteration 8229, loss = 1.48142935\n",
      "Iteration 8230, loss = 1.48139639\n",
      "Iteration 8231, loss = 1.48136647\n",
      "Iteration 8232, loss = 1.48132975\n",
      "Iteration 8233, loss = 1.48129884\n",
      "Iteration 8234, loss = 1.48126719\n",
      "Iteration 8235, loss = 1.48123455\n",
      "Iteration 8236, loss = 1.48120679\n",
      "Iteration 8237, loss = 1.48117171\n",
      "Iteration 8238, loss = 1.48114069\n",
      "Iteration 8239, loss = 1.48110956\n",
      "Iteration 8240, loss = 1.48107745\n",
      "Iteration 8241, loss = 1.48104441\n",
      "Iteration 8242, loss = 1.48101093\n",
      "Iteration 8243, loss = 1.48098198\n",
      "Iteration 8244, loss = 1.48094830\n",
      "Iteration 8245, loss = 1.48091826\n",
      "Iteration 8246, loss = 1.48088705\n",
      "Iteration 8247, loss = 1.48085479\n",
      "Iteration 8248, loss = 1.48082157\n",
      "Iteration 8249, loss = 1.48078954\n",
      "Iteration 8250, loss = 1.48075989\n",
      "Iteration 8251, loss = 1.48072607\n",
      "Iteration 8252, loss = 1.48069329\n",
      "Iteration 8253, loss = 1.48066265\n",
      "Iteration 8254, loss = 1.48063096\n",
      "Iteration 8255, loss = 1.48059835\n",
      "Iteration 8256, loss = 1.48056698\n",
      "Iteration 8257, loss = 1.48053510\n",
      "Iteration 8258, loss = 1.48050163\n",
      "Iteration 8259, loss = 1.48046865\n",
      "Iteration 8260, loss = 1.48043958\n",
      "Iteration 8261, loss = 1.48040773\n",
      "Iteration 8262, loss = 1.48037802\n",
      "Iteration 8263, loss = 1.48034716\n",
      "Iteration 8264, loss = 1.48031525\n",
      "Iteration 8265, loss = 1.48028235\n",
      "Iteration 8266, loss = 1.48024858\n",
      "Iteration 8267, loss = 1.48021588\n",
      "Iteration 8268, loss = 1.48019120\n",
      "Iteration 8269, loss = 1.48015272\n",
      "Iteration 8270, loss = 1.48012367\n",
      "Iteration 8271, loss = 1.48009339\n",
      "Iteration 8272, loss = 1.48006195\n",
      "Iteration 8273, loss = 1.48002946\n",
      "Iteration 8274, loss = 1.47999606\n",
      "Iteration 8275, loss = 1.47996321\n",
      "Iteration 8276, loss = 1.47993068\n",
      "Iteration 8277, loss = 1.47989911\n",
      "Iteration 8278, loss = 1.47986764\n",
      "Iteration 8279, loss = 1.47983560\n",
      "Iteration 8280, loss = 1.47980462\n",
      "Iteration 8281, loss = 1.47977339\n",
      "Iteration 8282, loss = 1.47974225\n",
      "Iteration 8283, loss = 1.47971016\n",
      "Iteration 8284, loss = 1.47967769\n",
      "Iteration 8285, loss = 1.47964609\n",
      "Iteration 8286, loss = 1.47961433\n",
      "Iteration 8287, loss = 1.47958285\n",
      "Iteration 8288, loss = 1.47955054\n",
      "Iteration 8289, loss = 1.47951872\n",
      "Iteration 8290, loss = 1.47948835\n",
      "Iteration 8291, loss = 1.47945581\n",
      "Iteration 8292, loss = 1.47942524\n",
      "Iteration 8293, loss = 1.47939391\n",
      "Iteration 8294, loss = 1.47936162\n",
      "Iteration 8295, loss = 1.47932851\n",
      "Iteration 8296, loss = 1.47930751\n",
      "Iteration 8297, loss = 1.47926916\n",
      "Iteration 8298, loss = 1.47924236\n",
      "Iteration 8299, loss = 1.47921416\n",
      "Iteration 8300, loss = 1.47918460\n",
      "Iteration 8301, loss = 1.47915382\n",
      "Iteration 8302, loss = 1.47912194\n",
      "Iteration 8303, loss = 1.47908907\n",
      "Iteration 8304, loss = 1.47905536\n",
      "Iteration 8305, loss = 1.47902097\n",
      "Iteration 8306, loss = 1.47898603\n",
      "Iteration 8307, loss = 1.47895319\n",
      "Iteration 8308, loss = 1.47893140\n",
      "Iteration 8309, loss = 1.47888949\n",
      "Iteration 8310, loss = 1.47886176\n",
      "Iteration 8311, loss = 1.47883276\n",
      "Iteration 8312, loss = 1.47880261\n",
      "Iteration 8313, loss = 1.47877139\n",
      "Iteration 8314, loss = 1.47873919\n",
      "Iteration 8315, loss = 1.47870610\n",
      "Iteration 8316, loss = 1.47867227\n",
      "Iteration 8317, loss = 1.47863782\n",
      "Iteration 8318, loss = 1.47861141\n",
      "Iteration 8319, loss = 1.47857561\n",
      "Iteration 8320, loss = 1.47854423\n",
      "Iteration 8321, loss = 1.47851467\n",
      "Iteration 8322, loss = 1.47848403\n",
      "Iteration 8323, loss = 1.47845240\n",
      "Iteration 8324, loss = 1.47841988\n",
      "Iteration 8325, loss = 1.47838656\n",
      "Iteration 8326, loss = 1.47835442\n",
      "Iteration 8327, loss = 1.47832172\n",
      "Iteration 8328, loss = 1.47828993\n",
      "Iteration 8329, loss = 1.47826171\n",
      "Iteration 8330, loss = 1.47822829\n",
      "Iteration 8331, loss = 1.47819921\n",
      "Iteration 8332, loss = 1.47816919\n",
      "Iteration 8333, loss = 1.47813811\n",
      "Iteration 8334, loss = 1.47810610\n",
      "Iteration 8335, loss = 1.47807325\n",
      "Iteration 8336, loss = 1.47803972\n",
      "Iteration 8337, loss = 1.47801890\n",
      "Iteration 8338, loss = 1.47797861\n",
      "Iteration 8339, loss = 1.47795049\n",
      "Iteration 8340, loss = 1.47792116\n",
      "Iteration 8341, loss = 1.47789070\n",
      "Iteration 8342, loss = 1.47785917\n",
      "Iteration 8343, loss = 1.47782672\n",
      "Iteration 8344, loss = 1.47779347\n",
      "Iteration 8345, loss = 1.47776037\n",
      "Iteration 8346, loss = 1.47773107\n",
      "Iteration 8347, loss = 1.47769656\n",
      "Iteration 8348, loss = 1.47766533\n",
      "Iteration 8349, loss = 1.47763410\n",
      "Iteration 8350, loss = 1.47760485\n",
      "Iteration 8351, loss = 1.47757293\n",
      "Iteration 8352, loss = 1.47754280\n",
      "Iteration 8353, loss = 1.47751168\n",
      "Iteration 8354, loss = 1.47747968\n",
      "Iteration 8355, loss = 1.47744861\n",
      "Iteration 8356, loss = 1.47741789\n",
      "Iteration 8357, loss = 1.47738408\n",
      "Iteration 8358, loss = 1.47735264\n",
      "Iteration 8359, loss = 1.47732187\n",
      "Iteration 8360, loss = 1.47729343\n",
      "Iteration 8361, loss = 1.47726095\n",
      "Iteration 8362, loss = 1.47723205\n",
      "Iteration 8363, loss = 1.47720199\n",
      "Iteration 8364, loss = 1.47717088\n",
      "Iteration 8365, loss = 1.47713886\n",
      "Iteration 8366, loss = 1.47710602\n",
      "Iteration 8367, loss = 1.47707863\n",
      "Iteration 8368, loss = 1.47704604\n",
      "Iteration 8369, loss = 1.47701145\n",
      "Iteration 8370, loss = 1.47698098\n",
      "Iteration 8371, loss = 1.47694958\n",
      "Iteration 8372, loss = 1.47691778\n",
      "Iteration 8373, loss = 1.47688917\n",
      "Iteration 8374, loss = 1.47685594\n",
      "Iteration 8375, loss = 1.47682518\n",
      "Iteration 8376, loss = 1.47679355\n",
      "Iteration 8377, loss = 1.47676331\n",
      "Iteration 8378, loss = 1.47673171\n",
      "Iteration 8379, loss = 1.47670130\n",
      "Iteration 8380, loss = 1.47666994\n",
      "Iteration 8381, loss = 1.47663784\n",
      "Iteration 8382, loss = 1.47660754\n",
      "Iteration 8383, loss = 1.47657603\n",
      "Iteration 8384, loss = 1.47654512\n",
      "Iteration 8385, loss = 1.47651391\n",
      "Iteration 8386, loss = 1.47648184\n",
      "Iteration 8387, loss = 1.47645514\n",
      "Iteration 8388, loss = 1.47642037\n",
      "Iteration 8389, loss = 1.47639144\n",
      "Iteration 8390, loss = 1.47636227\n",
      "Iteration 8391, loss = 1.47633202\n",
      "Iteration 8392, loss = 1.47630083\n",
      "Iteration 8393, loss = 1.47626877\n",
      "Iteration 8394, loss = 1.47623599\n",
      "Iteration 8395, loss = 1.47620663\n",
      "Iteration 8396, loss = 1.47618227\n",
      "Iteration 8397, loss = 1.47614373\n",
      "Iteration 8398, loss = 1.47611652\n",
      "Iteration 8399, loss = 1.47608809\n",
      "Iteration 8400, loss = 1.47605849\n",
      "Iteration 8401, loss = 1.47602781\n",
      "Iteration 8402, loss = 1.47599616\n",
      "Iteration 8403, loss = 1.47596363\n",
      "Iteration 8404, loss = 1.47593042\n",
      "Iteration 8405, loss = 1.47589662\n",
      "Iteration 8406, loss = 1.47586270\n",
      "Iteration 8407, loss = 1.47584539\n",
      "Iteration 8408, loss = 1.47580202\n",
      "Iteration 8409, loss = 1.47577402\n",
      "Iteration 8410, loss = 1.47574498\n",
      "Iteration 8411, loss = 1.47571490\n",
      "Iteration 8412, loss = 1.47568385\n",
      "Iteration 8413, loss = 1.47565196\n",
      "Iteration 8414, loss = 1.47561939\n",
      "Iteration 8415, loss = 1.47558667\n",
      "Iteration 8416, loss = 1.47555780\n",
      "Iteration 8417, loss = 1.47552467\n",
      "Iteration 8418, loss = 1.47549511\n",
      "Iteration 8419, loss = 1.47546565\n",
      "Iteration 8420, loss = 1.47543524\n",
      "Iteration 8421, loss = 1.47540395\n",
      "Iteration 8422, loss = 1.47537187\n",
      "Iteration 8423, loss = 1.47534206\n",
      "Iteration 8424, loss = 1.47531368\n",
      "Iteration 8425, loss = 1.47527897\n",
      "Iteration 8426, loss = 1.47525007\n",
      "Iteration 8427, loss = 1.47522021\n",
      "Iteration 8428, loss = 1.47518941\n",
      "Iteration 8429, loss = 1.47515776\n",
      "Iteration 8430, loss = 1.47512547\n",
      "Iteration 8431, loss = 1.47509672\n",
      "Iteration 8432, loss = 1.47506382\n",
      "Iteration 8433, loss = 1.47503307\n",
      "Iteration 8434, loss = 1.47500151\n",
      "Iteration 8435, loss = 1.47497035\n",
      "Iteration 8436, loss = 1.47494057\n",
      "Iteration 8437, loss = 1.47490906\n",
      "Iteration 8438, loss = 1.47487949\n",
      "Iteration 8439, loss = 1.47484900\n",
      "Iteration 8440, loss = 1.47481765\n",
      "Iteration 8441, loss = 1.47478809\n",
      "Iteration 8442, loss = 1.47476102\n",
      "Iteration 8443, loss = 1.47472549\n",
      "Iteration 8444, loss = 1.47469597\n",
      "Iteration 8445, loss = 1.47466549\n",
      "Iteration 8446, loss = 1.47463413\n",
      "Iteration 8447, loss = 1.47460205\n",
      "Iteration 8448, loss = 1.47457337\n",
      "Iteration 8449, loss = 1.47454208\n",
      "Iteration 8450, loss = 1.47451014\n",
      "Iteration 8451, loss = 1.47448045\n",
      "Iteration 8452, loss = 1.47444985\n",
      "Iteration 8453, loss = 1.47441844\n",
      "Iteration 8454, loss = 1.47438828\n",
      "Iteration 8455, loss = 1.47435652\n",
      "Iteration 8456, loss = 1.47432613\n",
      "Iteration 8457, loss = 1.47429591\n",
      "Iteration 8458, loss = 1.47426516\n",
      "Iteration 8459, loss = 1.47423454\n",
      "Iteration 8460, loss = 1.47420384\n",
      "Iteration 8461, loss = 1.47417269\n",
      "Iteration 8462, loss = 1.47414530\n",
      "Iteration 8463, loss = 1.47411212\n",
      "Iteration 8464, loss = 1.47408245\n",
      "Iteration 8465, loss = 1.47405187\n",
      "Iteration 8466, loss = 1.47402196\n",
      "Iteration 8467, loss = 1.47399090\n",
      "Iteration 8468, loss = 1.47396041\n",
      "Iteration 8469, loss = 1.47392906\n",
      "Iteration 8470, loss = 1.47389937\n",
      "Iteration 8471, loss = 1.47386981\n",
      "Iteration 8472, loss = 1.47383836\n",
      "Iteration 8473, loss = 1.47381008\n",
      "Iteration 8474, loss = 1.47378070\n",
      "Iteration 8475, loss = 1.47375037\n",
      "Iteration 8476, loss = 1.47371914\n",
      "Iteration 8477, loss = 1.47368716\n",
      "Iteration 8478, loss = 1.47365505\n",
      "Iteration 8479, loss = 1.47362854\n",
      "Iteration 8480, loss = 1.47359368\n",
      "Iteration 8481, loss = 1.47356388\n",
      "Iteration 8482, loss = 1.47353320\n",
      "Iteration 8483, loss = 1.47350175\n",
      "Iteration 8484, loss = 1.47347470\n",
      "Iteration 8485, loss = 1.47344108\n",
      "Iteration 8486, loss = 1.47341237\n",
      "Iteration 8487, loss = 1.47338290\n",
      "Iteration 8488, loss = 1.47335249\n",
      "Iteration 8489, loss = 1.47332127\n",
      "Iteration 8490, loss = 1.47328936\n",
      "Iteration 8491, loss = 1.47326226\n",
      "Iteration 8492, loss = 1.47322815\n",
      "Iteration 8493, loss = 1.47319806\n",
      "Iteration 8494, loss = 1.47316800\n",
      "Iteration 8495, loss = 1.47313718\n",
      "Iteration 8496, loss = 1.47310850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8497, loss = 1.47307892\n",
      "Iteration 8498, loss = 1.47304587\n",
      "Iteration 8499, loss = 1.47301738\n",
      "Iteration 8500, loss = 1.47298802\n",
      "Iteration 8501, loss = 1.47295772\n",
      "Iteration 8502, loss = 1.47292660\n",
      "Iteration 8503, loss = 1.47289707\n",
      "Iteration 8504, loss = 1.47286901\n",
      "Iteration 8505, loss = 1.47283517\n",
      "Iteration 8506, loss = 1.47280593\n",
      "Iteration 8507, loss = 1.47277578\n",
      "Iteration 8508, loss = 1.47274477\n",
      "Iteration 8509, loss = 1.47271306\n",
      "Iteration 8510, loss = 1.47268263\n",
      "Iteration 8511, loss = 1.47265544\n",
      "Iteration 8512, loss = 1.47262173\n",
      "Iteration 8513, loss = 1.47259320\n",
      "Iteration 8514, loss = 1.47256369\n",
      "Iteration 8515, loss = 1.47253327\n",
      "Iteration 8516, loss = 1.47250204\n",
      "Iteration 8517, loss = 1.47247014\n",
      "Iteration 8518, loss = 1.47244444\n",
      "Iteration 8519, loss = 1.47240958\n",
      "Iteration 8520, loss = 1.47238010\n",
      "Iteration 8521, loss = 1.47235071\n",
      "Iteration 8522, loss = 1.47232044\n",
      "Iteration 8523, loss = 1.47228940\n",
      "Iteration 8524, loss = 1.47226263\n",
      "Iteration 8525, loss = 1.47222974\n",
      "Iteration 8526, loss = 1.47219955\n",
      "Iteration 8527, loss = 1.47217038\n",
      "Iteration 8528, loss = 1.47214030\n",
      "Iteration 8529, loss = 1.47210938\n",
      "Iteration 8530, loss = 1.47207788\n",
      "Iteration 8531, loss = 1.47204973\n",
      "Iteration 8532, loss = 1.47201808\n",
      "Iteration 8533, loss = 1.47198926\n",
      "Iteration 8534, loss = 1.47195984\n",
      "Iteration 8535, loss = 1.47192953\n",
      "Iteration 8536, loss = 1.47189840\n",
      "Iteration 8537, loss = 1.47186661\n",
      "Iteration 8538, loss = 1.47184139\n",
      "Iteration 8539, loss = 1.47180660\n",
      "Iteration 8540, loss = 1.47177796\n",
      "Iteration 8541, loss = 1.47174963\n",
      "Iteration 8542, loss = 1.47172034\n",
      "Iteration 8543, loss = 1.47169018\n",
      "Iteration 8544, loss = 1.47165923\n",
      "Iteration 8545, loss = 1.47162759\n",
      "Iteration 8546, loss = 1.47159532\n",
      "Iteration 8547, loss = 1.47157283\n",
      "Iteration 8548, loss = 1.47153576\n",
      "Iteration 8549, loss = 1.47150553\n",
      "Iteration 8550, loss = 1.47147621\n",
      "Iteration 8551, loss = 1.47144606\n",
      "Iteration 8552, loss = 1.47141517\n",
      "Iteration 8553, loss = 1.47138485\n",
      "Iteration 8554, loss = 1.47135523\n",
      "Iteration 8555, loss = 1.47132491\n",
      "Iteration 8556, loss = 1.47129413\n",
      "Iteration 8557, loss = 1.47126488\n",
      "Iteration 8558, loss = 1.47123479\n",
      "Iteration 8559, loss = 1.47120397\n",
      "Iteration 8560, loss = 1.47117712\n",
      "Iteration 8561, loss = 1.47114526\n",
      "Iteration 8562, loss = 1.47111707\n",
      "Iteration 8563, loss = 1.47108791\n",
      "Iteration 8564, loss = 1.47105784\n",
      "Iteration 8565, loss = 1.47102699\n",
      "Iteration 8566, loss = 1.47099543\n",
      "Iteration 8567, loss = 1.47096415\n",
      "Iteration 8568, loss = 1.47093325\n",
      "Iteration 8569, loss = 1.47090510\n",
      "Iteration 8570, loss = 1.47087510\n",
      "Iteration 8571, loss = 1.47084668\n",
      "Iteration 8572, loss = 1.47081732\n",
      "Iteration 8573, loss = 1.47078711\n",
      "Iteration 8574, loss = 1.47075617\n",
      "Iteration 8575, loss = 1.47072459\n",
      "Iteration 8576, loss = 1.47070212\n",
      "Iteration 8577, loss = 1.47066457\n",
      "Iteration 8578, loss = 1.47063589\n",
      "Iteration 8579, loss = 1.47060637\n",
      "Iteration 8580, loss = 1.47057604\n",
      "Iteration 8581, loss = 1.47054745\n",
      "Iteration 8582, loss = 1.47051810\n",
      "Iteration 8583, loss = 1.47048725\n",
      "Iteration 8584, loss = 1.47045592\n",
      "Iteration 8585, loss = 1.47042606\n",
      "Iteration 8586, loss = 1.47039553\n",
      "Iteration 8587, loss = 1.47036656\n",
      "Iteration 8588, loss = 1.47033556\n",
      "Iteration 8589, loss = 1.47030586\n",
      "Iteration 8590, loss = 1.47027598\n",
      "Iteration 8591, loss = 1.47024557\n",
      "Iteration 8592, loss = 1.47021696\n",
      "Iteration 8593, loss = 1.47018919\n",
      "Iteration 8594, loss = 1.47016197\n",
      "Iteration 8595, loss = 1.47013372\n",
      "Iteration 8596, loss = 1.47010454\n",
      "Iteration 8597, loss = 1.47007455\n",
      "Iteration 8598, loss = 1.47004379\n",
      "Iteration 8599, loss = 1.47001233\n",
      "Iteration 8600, loss = 1.46998029\n",
      "Iteration 8601, loss = 1.46995441\n",
      "Iteration 8602, loss = 1.46992364\n",
      "Iteration 8603, loss = 1.46988908\n",
      "Iteration 8604, loss = 1.46985992\n",
      "Iteration 8605, loss = 1.46983002\n",
      "Iteration 8606, loss = 1.46980668\n",
      "Iteration 8607, loss = 1.46977297\n",
      "Iteration 8608, loss = 1.46974556\n",
      "Iteration 8609, loss = 1.46971717\n",
      "Iteration 8610, loss = 1.46968787\n",
      "Iteration 8611, loss = 1.46965773\n",
      "Iteration 8612, loss = 1.46962684\n",
      "Iteration 8613, loss = 1.46959529\n",
      "Iteration 8614, loss = 1.46956317\n",
      "Iteration 8615, loss = 1.46953306\n",
      "Iteration 8616, loss = 1.46951152\n",
      "Iteration 8617, loss = 1.46947367\n",
      "Iteration 8618, loss = 1.46944739\n",
      "Iteration 8619, loss = 1.46942004\n",
      "Iteration 8620, loss = 1.46939169\n",
      "Iteration 8621, loss = 1.46936242\n",
      "Iteration 8622, loss = 1.46933233\n",
      "Iteration 8623, loss = 1.46930148\n",
      "Iteration 8624, loss = 1.46926998\n",
      "Iteration 8625, loss = 1.46923846\n",
      "Iteration 8626, loss = 1.46920962\n",
      "Iteration 8627, loss = 1.46917934\n",
      "Iteration 8628, loss = 1.46914785\n",
      "Iteration 8629, loss = 1.46911828\n",
      "Iteration 8630, loss = 1.46908875\n",
      "Iteration 8631, loss = 1.46905854\n",
      "Iteration 8632, loss = 1.46903118\n",
      "Iteration 8633, loss = 1.46900059\n",
      "Iteration 8634, loss = 1.46897092\n",
      "Iteration 8635, loss = 1.46894243\n",
      "Iteration 8636, loss = 1.46891313\n",
      "Iteration 8637, loss = 1.46888309\n",
      "Iteration 8638, loss = 1.46885240\n",
      "Iteration 8639, loss = 1.46882111\n",
      "Iteration 8640, loss = 1.46879567\n",
      "Iteration 8641, loss = 1.46876151\n",
      "Iteration 8642, loss = 1.46873457\n",
      "Iteration 8643, loss = 1.46870759\n",
      "Iteration 8644, loss = 1.46867963\n",
      "Iteration 8645, loss = 1.46865076\n",
      "Iteration 8646, loss = 1.46862106\n",
      "Iteration 8647, loss = 1.46859069\n",
      "Iteration 8648, loss = 1.46855966\n",
      "Iteration 8649, loss = 1.46852806\n",
      "Iteration 8650, loss = 1.46849719\n",
      "Iteration 8651, loss = 1.46846794\n",
      "Iteration 8652, loss = 1.46843782\n",
      "Iteration 8653, loss = 1.46840880\n",
      "Iteration 8654, loss = 1.46837909\n",
      "Iteration 8655, loss = 1.46834966\n",
      "Iteration 8656, loss = 1.46831994\n",
      "Iteration 8657, loss = 1.46829068\n",
      "Iteration 8658, loss = 1.46826170\n",
      "Iteration 8659, loss = 1.46823196\n",
      "Iteration 8660, loss = 1.46820155\n",
      "Iteration 8661, loss = 1.46817314\n",
      "Iteration 8662, loss = 1.46814289\n",
      "Iteration 8663, loss = 1.46811311\n",
      "Iteration 8664, loss = 1.46808383\n",
      "Iteration 8665, loss = 1.46805384\n",
      "Iteration 8666, loss = 1.46802559\n",
      "Iteration 8667, loss = 1.46799466\n",
      "Iteration 8668, loss = 1.46796532\n",
      "Iteration 8669, loss = 1.46793570\n",
      "Iteration 8670, loss = 1.46790716\n",
      "Iteration 8671, loss = 1.46787824\n",
      "Iteration 8672, loss = 1.46784855\n",
      "Iteration 8673, loss = 1.46781819\n",
      "Iteration 8674, loss = 1.46779211\n",
      "Iteration 8675, loss = 1.46776125\n",
      "Iteration 8676, loss = 1.46773426\n",
      "Iteration 8677, loss = 1.46770629\n",
      "Iteration 8678, loss = 1.46767741\n",
      "Iteration 8679, loss = 1.46764770\n",
      "Iteration 8680, loss = 1.46761725\n",
      "Iteration 8681, loss = 1.46758615\n",
      "Iteration 8682, loss = 1.46755452\n",
      "Iteration 8683, loss = 1.46753426\n",
      "Iteration 8684, loss = 1.46749536\n",
      "Iteration 8685, loss = 1.46746782\n",
      "Iteration 8686, loss = 1.46743973\n",
      "Iteration 8687, loss = 1.46741080\n",
      "Iteration 8688, loss = 1.46738110\n",
      "Iteration 8689, loss = 1.46735072\n",
      "Iteration 8690, loss = 1.46732112\n",
      "Iteration 8691, loss = 1.46729260\n",
      "Iteration 8692, loss = 1.46726377\n",
      "Iteration 8693, loss = 1.46723229\n",
      "Iteration 8694, loss = 1.46720382\n",
      "Iteration 8695, loss = 1.46717458\n",
      "Iteration 8696, loss = 1.46714464\n",
      "Iteration 8697, loss = 1.46711414\n",
      "Iteration 8698, loss = 1.46709100\n",
      "Iteration 8699, loss = 1.46705653\n",
      "Iteration 8700, loss = 1.46702911\n",
      "Iteration 8701, loss = 1.46700081\n",
      "Iteration 8702, loss = 1.46697171\n",
      "Iteration 8703, loss = 1.46694189\n",
      "Iteration 8704, loss = 1.46691144\n",
      "Iteration 8705, loss = 1.46688046\n",
      "Iteration 8706, loss = 1.46686021\n",
      "Iteration 8707, loss = 1.46682306\n",
      "Iteration 8708, loss = 1.46679616\n",
      "Iteration 8709, loss = 1.46676838\n",
      "Iteration 8710, loss = 1.46673975\n",
      "Iteration 8711, loss = 1.46671036\n",
      "Iteration 8712, loss = 1.46668028\n",
      "Iteration 8713, loss = 1.46664962\n",
      "Iteration 8714, loss = 1.46661844\n",
      "Iteration 8715, loss = 1.46659026\n",
      "Iteration 8716, loss = 1.46656268\n",
      "Iteration 8717, loss = 1.46653208\n",
      "Iteration 8718, loss = 1.46650591\n",
      "Iteration 8719, loss = 1.46647876\n",
      "Iteration 8720, loss = 1.46645066\n",
      "Iteration 8721, loss = 1.46642170\n",
      "Iteration 8722, loss = 1.46639197\n",
      "Iteration 8723, loss = 1.46636156\n",
      "Iteration 8724, loss = 1.46633063\n",
      "Iteration 8725, loss = 1.46629931\n",
      "Iteration 8726, loss = 1.46626791\n",
      "Iteration 8727, loss = 1.46624786\n",
      "Iteration 8728, loss = 1.46621070\n",
      "Iteration 8729, loss = 1.46618384\n",
      "Iteration 8730, loss = 1.46615620\n",
      "Iteration 8731, loss = 1.46612773\n",
      "Iteration 8732, loss = 1.46609851\n",
      "Iteration 8733, loss = 1.46606862\n",
      "Iteration 8734, loss = 1.46603813\n",
      "Iteration 8735, loss = 1.46600970\n",
      "Iteration 8736, loss = 1.46598166\n",
      "Iteration 8737, loss = 1.46594964\n",
      "Iteration 8738, loss = 1.46592158\n",
      "Iteration 8739, loss = 1.46589402\n",
      "Iteration 8740, loss = 1.46586566\n",
      "Iteration 8741, loss = 1.46583652\n",
      "Iteration 8742, loss = 1.46580670\n",
      "Iteration 8743, loss = 1.46577631\n",
      "Iteration 8744, loss = 1.46574738\n",
      "Iteration 8745, loss = 1.46572113\n",
      "Iteration 8746, loss = 1.46568901\n",
      "Iteration 8747, loss = 1.46565945\n",
      "Iteration 8748, loss = 1.46563099\n",
      "Iteration 8749, loss = 1.46560184\n",
      "Iteration 8750, loss = 1.46557208\n",
      "Iteration 8751, loss = 1.46554350\n",
      "Iteration 8752, loss = 1.46551474\n",
      "Iteration 8753, loss = 1.46548689\n",
      "Iteration 8754, loss = 1.46545828\n",
      "Iteration 8755, loss = 1.46542897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8756, loss = 1.46539903\n",
      "Iteration 8757, loss = 1.46537029\n",
      "Iteration 8758, loss = 1.46534192\n",
      "Iteration 8759, loss = 1.46531060\n",
      "Iteration 8760, loss = 1.46528182\n",
      "Iteration 8761, loss = 1.46525570\n",
      "Iteration 8762, loss = 1.46522482\n",
      "Iteration 8763, loss = 1.46519648\n",
      "Iteration 8764, loss = 1.46516739\n",
      "Iteration 8765, loss = 1.46513769\n",
      "Iteration 8766, loss = 1.46510884\n",
      "Iteration 8767, loss = 1.46508023\n",
      "Iteration 8768, loss = 1.46505101\n",
      "Iteration 8769, loss = 1.46502365\n",
      "Iteration 8770, loss = 1.46499443\n",
      "Iteration 8771, loss = 1.46496681\n",
      "Iteration 8772, loss = 1.46493837\n",
      "Iteration 8773, loss = 1.46490918\n",
      "Iteration 8774, loss = 1.46487934\n",
      "Iteration 8775, loss = 1.46485031\n",
      "Iteration 8776, loss = 1.46482173\n",
      "Iteration 8777, loss = 1.46479240\n",
      "Iteration 8778, loss = 1.46476313\n",
      "Iteration 8779, loss = 1.46473520\n",
      "Iteration 8780, loss = 1.46470654\n",
      "Iteration 8781, loss = 1.46467722\n",
      "Iteration 8782, loss = 1.46464733\n",
      "Iteration 8783, loss = 1.46461989\n",
      "Iteration 8784, loss = 1.46459090\n",
      "Iteration 8785, loss = 1.46456399\n",
      "Iteration 8786, loss = 1.46453626\n",
      "Iteration 8787, loss = 1.46450775\n",
      "Iteration 8788, loss = 1.46447855\n",
      "Iteration 8789, loss = 1.46444876\n",
      "Iteration 8790, loss = 1.46441844\n",
      "Iteration 8791, loss = 1.46439152\n",
      "Iteration 8792, loss = 1.46436018\n",
      "Iteration 8793, loss = 1.46433121\n",
      "Iteration 8794, loss = 1.46430296\n",
      "Iteration 8795, loss = 1.46427400\n",
      "Iteration 8796, loss = 1.46424514\n",
      "Iteration 8797, loss = 1.46421662\n",
      "Iteration 8798, loss = 1.46418746\n",
      "Iteration 8799, loss = 1.46415864\n",
      "Iteration 8800, loss = 1.46412921\n",
      "Iteration 8801, loss = 1.46410476\n",
      "Iteration 8802, loss = 1.46407314\n",
      "Iteration 8803, loss = 1.46404618\n",
      "Iteration 8804, loss = 1.46401836\n",
      "Iteration 8805, loss = 1.46398976\n",
      "Iteration 8806, loss = 1.46396045\n",
      "Iteration 8807, loss = 1.46393054\n",
      "Iteration 8808, loss = 1.46390028\n",
      "Iteration 8809, loss = 1.46387277\n",
      "Iteration 8810, loss = 1.46384274\n",
      "Iteration 8811, loss = 1.46381491\n",
      "Iteration 8812, loss = 1.46378674\n",
      "Iteration 8813, loss = 1.46375791\n",
      "Iteration 8814, loss = 1.46373195\n",
      "Iteration 8815, loss = 1.46370082\n",
      "Iteration 8816, loss = 1.46367244\n",
      "Iteration 8817, loss = 1.46364339\n",
      "Iteration 8818, loss = 1.46361378\n",
      "Iteration 8819, loss = 1.46358875\n",
      "Iteration 8820, loss = 1.46355910\n",
      "Iteration 8821, loss = 1.46353365\n",
      "Iteration 8822, loss = 1.46350724\n",
      "Iteration 8823, loss = 1.46347992\n",
      "Iteration 8824, loss = 1.46345178\n",
      "Iteration 8825, loss = 1.46342291\n",
      "Iteration 8826, loss = 1.46339337\n",
      "Iteration 8827, loss = 1.46336325\n",
      "Iteration 8828, loss = 1.46333268\n",
      "Iteration 8829, loss = 1.46330171\n",
      "Iteration 8830, loss = 1.46327040\n",
      "Iteration 8831, loss = 1.46325952\n",
      "Iteration 8832, loss = 1.46321498\n",
      "Iteration 8833, loss = 1.46319021\n",
      "Iteration 8834, loss = 1.46316453\n",
      "Iteration 8835, loss = 1.46313796\n",
      "Iteration 8836, loss = 1.46311053\n",
      "Iteration 8837, loss = 1.46308232\n",
      "Iteration 8838, loss = 1.46305341\n",
      "Iteration 8839, loss = 1.46302388\n",
      "Iteration 8840, loss = 1.46299380\n",
      "Iteration 8841, loss = 1.46296326\n",
      "Iteration 8842, loss = 1.46293235\n",
      "Iteration 8843, loss = 1.46290493\n",
      "Iteration 8844, loss = 1.46287918\n",
      "Iteration 8845, loss = 1.46284879\n",
      "Iteration 8846, loss = 1.46281625\n",
      "Iteration 8847, loss = 1.46278912\n",
      "Iteration 8848, loss = 1.46276122\n",
      "Iteration 8849, loss = 1.46273265\n",
      "Iteration 8850, loss = 1.46270346\n",
      "Iteration 8851, loss = 1.46267377\n",
      "Iteration 8852, loss = 1.46264636\n",
      "Iteration 8853, loss = 1.46261958\n",
      "Iteration 8854, loss = 1.46258751\n",
      "Iteration 8855, loss = 1.46256019\n",
      "Iteration 8856, loss = 1.46253220\n",
      "Iteration 8857, loss = 1.46250355\n",
      "Iteration 8858, loss = 1.46247435\n",
      "Iteration 8859, loss = 1.46244464\n",
      "Iteration 8860, loss = 1.46242319\n",
      "Iteration 8861, loss = 1.46239047\n",
      "Iteration 8862, loss = 1.46236551\n",
      "Iteration 8863, loss = 1.46233965\n",
      "Iteration 8864, loss = 1.46231292\n",
      "Iteration 8865, loss = 1.46228538\n",
      "Iteration 8866, loss = 1.46225711\n",
      "Iteration 8867, loss = 1.46222824\n",
      "Iteration 8868, loss = 1.46219879\n",
      "Iteration 8869, loss = 1.46216882\n",
      "Iteration 8870, loss = 1.46213840\n",
      "Iteration 8871, loss = 1.46210765\n",
      "Iteration 8872, loss = 1.46207657\n",
      "Iteration 8873, loss = 1.46206116\n",
      "Iteration 8874, loss = 1.46202034\n",
      "Iteration 8875, loss = 1.46199457\n",
      "Iteration 8876, loss = 1.46196798\n",
      "Iteration 8877, loss = 1.46194064\n",
      "Iteration 8878, loss = 1.46191261\n",
      "Iteration 8879, loss = 1.46188397\n",
      "Iteration 8880, loss = 1.46185476\n",
      "Iteration 8881, loss = 1.46182503\n",
      "Iteration 8882, loss = 1.46179484\n",
      "Iteration 8883, loss = 1.46176652\n",
      "Iteration 8884, loss = 1.46174169\n",
      "Iteration 8885, loss = 1.46171029\n",
      "Iteration 8886, loss = 1.46168129\n",
      "Iteration 8887, loss = 1.46165469\n",
      "Iteration 8888, loss = 1.46162731\n",
      "Iteration 8889, loss = 1.46159924\n",
      "Iteration 8890, loss = 1.46157055\n",
      "Iteration 8891, loss = 1.46154297\n",
      "Iteration 8892, loss = 1.46151448\n",
      "Iteration 8893, loss = 1.46148439\n",
      "Iteration 8894, loss = 1.46145563\n",
      "Iteration 8895, loss = 1.46142752\n",
      "Iteration 8896, loss = 1.46139889\n",
      "Iteration 8897, loss = 1.46137084\n",
      "Iteration 8898, loss = 1.46134220\n",
      "Iteration 8899, loss = 1.46131304\n",
      "Iteration 8900, loss = 1.46129143\n",
      "Iteration 8901, loss = 1.46125751\n",
      "Iteration 8902, loss = 1.46123087\n",
      "Iteration 8903, loss = 1.46120352\n",
      "Iteration 8904, loss = 1.46117548\n",
      "Iteration 8905, loss = 1.46114686\n",
      "Iteration 8906, loss = 1.46111767\n",
      "Iteration 8907, loss = 1.46108797\n",
      "Iteration 8908, loss = 1.46106159\n",
      "Iteration 8909, loss = 1.46103315\n",
      "Iteration 8910, loss = 1.46100574\n",
      "Iteration 8911, loss = 1.46098071\n",
      "Iteration 8912, loss = 1.46095476\n",
      "Iteration 8913, loss = 1.46092797\n",
      "Iteration 8914, loss = 1.46090041\n",
      "Iteration 8915, loss = 1.46087213\n",
      "Iteration 8916, loss = 1.46084322\n",
      "Iteration 8917, loss = 1.46081379\n",
      "Iteration 8918, loss = 1.46078392\n",
      "Iteration 8919, loss = 1.46075372\n",
      "Iteration 8920, loss = 1.46072321\n",
      "Iteration 8921, loss = 1.46070510\n",
      "Iteration 8922, loss = 1.46066589\n",
      "Iteration 8923, loss = 1.46063861\n",
      "Iteration 8924, loss = 1.46061073\n",
      "Iteration 8925, loss = 1.46058230\n",
      "Iteration 8926, loss = 1.46055340\n",
      "Iteration 8927, loss = 1.46052849\n",
      "Iteration 8928, loss = 1.46049880\n",
      "Iteration 8929, loss = 1.46047277\n",
      "Iteration 8930, loss = 1.46044597\n",
      "Iteration 8931, loss = 1.46041846\n",
      "Iteration 8932, loss = 1.46039029\n",
      "Iteration 8933, loss = 1.46036153\n",
      "Iteration 8934, loss = 1.46033224\n",
      "Iteration 8935, loss = 1.46030307\n",
      "Iteration 8936, loss = 1.46027478\n",
      "Iteration 8937, loss = 1.46024610\n",
      "Iteration 8938, loss = 1.46021775\n",
      "Iteration 8939, loss = 1.46018961\n",
      "Iteration 8940, loss = 1.46016202\n",
      "Iteration 8941, loss = 1.46013453\n",
      "Iteration 8942, loss = 1.46010668\n",
      "Iteration 8943, loss = 1.46007825\n",
      "Iteration 8944, loss = 1.46005142\n",
      "Iteration 8945, loss = 1.46002347\n",
      "Iteration 8946, loss = 1.45999474\n",
      "Iteration 8947, loss = 1.45996769\n",
      "Iteration 8948, loss = 1.45994001\n",
      "Iteration 8949, loss = 1.45991174\n",
      "Iteration 8950, loss = 1.45988293\n",
      "Iteration 8951, loss = 1.45985771\n",
      "Iteration 8952, loss = 1.45982733\n",
      "Iteration 8953, loss = 1.45980031\n",
      "Iteration 8954, loss = 1.45977262\n",
      "Iteration 8955, loss = 1.45974434\n",
      "Iteration 8956, loss = 1.45971556\n",
      "Iteration 8957, loss = 1.45969050\n",
      "Iteration 8958, loss = 1.45965972\n",
      "Iteration 8959, loss = 1.45963248\n",
      "Iteration 8960, loss = 1.45960462\n",
      "Iteration 8961, loss = 1.45957618\n",
      "Iteration 8962, loss = 1.45954886\n",
      "Iteration 8963, loss = 1.45952239\n",
      "Iteration 8964, loss = 1.45949228\n",
      "Iteration 8965, loss = 1.45946493\n",
      "Iteration 8966, loss = 1.45943693\n",
      "Iteration 8967, loss = 1.45940879\n",
      "Iteration 8968, loss = 1.45938065\n",
      "Iteration 8969, loss = 1.45935278\n",
      "Iteration 8970, loss = 1.45932473\n",
      "Iteration 8971, loss = 1.45929663\n",
      "Iteration 8972, loss = 1.45927147\n",
      "Iteration 8973, loss = 1.45924223\n",
      "Iteration 8974, loss = 1.45921572\n",
      "Iteration 8975, loss = 1.45918851\n",
      "Iteration 8976, loss = 1.45916066\n",
      "Iteration 8977, loss = 1.45913227\n",
      "Iteration 8978, loss = 1.45910338\n",
      "Iteration 8979, loss = 1.45907611\n",
      "Iteration 8980, loss = 1.45904666\n",
      "Iteration 8981, loss = 1.45901869\n",
      "Iteration 8982, loss = 1.45899108\n",
      "Iteration 8983, loss = 1.45896311\n",
      "Iteration 8984, loss = 1.45893526\n",
      "Iteration 8985, loss = 1.45890746\n",
      "Iteration 8986, loss = 1.45887934\n",
      "Iteration 8987, loss = 1.45885158\n",
      "Iteration 8988, loss = 1.45882531\n",
      "Iteration 8989, loss = 1.45879858\n",
      "Iteration 8990, loss = 1.45877116\n",
      "Iteration 8991, loss = 1.45874315\n",
      "Iteration 8992, loss = 1.45871460\n",
      "Iteration 8993, loss = 1.45868564\n",
      "Iteration 8994, loss = 1.45865769\n",
      "Iteration 8995, loss = 1.45863003\n",
      "Iteration 8996, loss = 1.45860318\n",
      "Iteration 8997, loss = 1.45857585\n",
      "Iteration 8998, loss = 1.45854889\n",
      "Iteration 8999, loss = 1.45852130\n",
      "Iteration 9000, loss = 1.45849314\n",
      "Iteration 9001, loss = 1.45846491\n",
      "Iteration 9002, loss = 1.45843640\n",
      "Iteration 9003, loss = 1.45840916\n",
      "Iteration 9004, loss = 1.45838224\n",
      "Iteration 9005, loss = 1.45835595\n",
      "Iteration 9006, loss = 1.45832898\n",
      "Iteration 9007, loss = 1.45830137\n",
      "Iteration 9008, loss = 1.45827317\n",
      "Iteration 9009, loss = 1.45824446\n",
      "Iteration 9010, loss = 1.45821975\n",
      "Iteration 9011, loss = 1.45818886\n",
      "Iteration 9012, loss = 1.45816182\n",
      "Iteration 9013, loss = 1.45813418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9014, loss = 1.45810738\n",
      "Iteration 9015, loss = 1.45807920\n",
      "Iteration 9016, loss = 1.45805117\n",
      "Iteration 9017, loss = 1.45802343\n",
      "Iteration 9018, loss = 1.45799538\n",
      "Iteration 9019, loss = 1.45796770\n",
      "Iteration 9020, loss = 1.45794046\n",
      "Iteration 9021, loss = 1.45791279\n",
      "Iteration 9022, loss = 1.45788459\n",
      "Iteration 9023, loss = 1.45786170\n",
      "Iteration 9024, loss = 1.45782953\n",
      "Iteration 9025, loss = 1.45780308\n",
      "Iteration 9026, loss = 1.45777634\n",
      "Iteration 9027, loss = 1.45774894\n",
      "Iteration 9028, loss = 1.45772099\n",
      "Iteration 9029, loss = 1.45769258\n",
      "Iteration 9030, loss = 1.45767110\n",
      "Iteration 9031, loss = 1.45763788\n",
      "Iteration 9032, loss = 1.45761136\n",
      "Iteration 9033, loss = 1.45758423\n",
      "Iteration 9034, loss = 1.45755653\n",
      "Iteration 9035, loss = 1.45752833\n",
      "Iteration 9036, loss = 1.45749969\n",
      "Iteration 9037, loss = 1.45747124\n",
      "Iteration 9038, loss = 1.45744516\n",
      "Iteration 9039, loss = 1.45741696\n",
      "Iteration 9040, loss = 1.45739084\n",
      "Iteration 9041, loss = 1.45736404\n",
      "Iteration 9042, loss = 1.45733662\n",
      "Iteration 9043, loss = 1.45730864\n",
      "Iteration 9044, loss = 1.45728017\n",
      "Iteration 9045, loss = 1.45725216\n",
      "Iteration 9046, loss = 1.45722879\n",
      "Iteration 9047, loss = 1.45719855\n",
      "Iteration 9048, loss = 1.45717334\n",
      "Iteration 9049, loss = 1.45714736\n",
      "Iteration 9050, loss = 1.45712065\n",
      "Iteration 9051, loss = 1.45709330\n",
      "Iteration 9052, loss = 1.45706536\n",
      "Iteration 9053, loss = 1.45703690\n",
      "Iteration 9054, loss = 1.45700800\n",
      "Iteration 9055, loss = 1.45698026\n",
      "Iteration 9056, loss = 1.45695629\n",
      "Iteration 9057, loss = 1.45692554\n",
      "Iteration 9058, loss = 1.45690018\n",
      "Iteration 9059, loss = 1.45687412\n",
      "Iteration 9060, loss = 1.45684734\n",
      "Iteration 9061, loss = 1.45681991\n",
      "Iteration 9062, loss = 1.45679194\n",
      "Iteration 9063, loss = 1.45676354\n",
      "Iteration 9064, loss = 1.45673472\n",
      "Iteration 9065, loss = 1.45670809\n",
      "Iteration 9066, loss = 1.45668376\n",
      "Iteration 9067, loss = 1.45665268\n",
      "Iteration 9068, loss = 1.45662487\n",
      "Iteration 9069, loss = 1.45659838\n",
      "Iteration 9070, loss = 1.45657128\n",
      "Iteration 9071, loss = 1.45654368\n",
      "Iteration 9072, loss = 1.45651559\n",
      "Iteration 9073, loss = 1.45648799\n",
      "Iteration 9074, loss = 1.45646080\n",
      "Iteration 9075, loss = 1.45643220\n",
      "Iteration 9076, loss = 1.45640541\n",
      "Iteration 9077, loss = 1.45637820\n",
      "Iteration 9078, loss = 1.45635048\n",
      "Iteration 9079, loss = 1.45632232\n",
      "Iteration 9080, loss = 1.45629521\n",
      "Iteration 9081, loss = 1.45626675\n",
      "Iteration 9082, loss = 1.45623922\n",
      "Iteration 9083, loss = 1.45621178\n",
      "Iteration 9084, loss = 1.45618457\n",
      "Iteration 9085, loss = 1.45615736\n",
      "Iteration 9086, loss = 1.45612963\n",
      "Iteration 9087, loss = 1.45610537\n",
      "Iteration 9088, loss = 1.45607718\n",
      "Iteration 9089, loss = 1.45605215\n",
      "Iteration 9090, loss = 1.45602637\n",
      "Iteration 9091, loss = 1.45599989\n",
      "Iteration 9092, loss = 1.45597278\n",
      "Iteration 9093, loss = 1.45594511\n",
      "Iteration 9094, loss = 1.45591697\n",
      "Iteration 9095, loss = 1.45588840\n",
      "Iteration 9096, loss = 1.45585944\n",
      "Iteration 9097, loss = 1.45583348\n",
      "Iteration 9098, loss = 1.45580382\n",
      "Iteration 9099, loss = 1.45577696\n",
      "Iteration 9100, loss = 1.45574958\n",
      "Iteration 9101, loss = 1.45572514\n",
      "Iteration 9102, loss = 1.45569570\n",
      "Iteration 9103, loss = 1.45566905\n",
      "Iteration 9104, loss = 1.45564185\n",
      "Iteration 9105, loss = 1.45561414\n",
      "Iteration 9106, loss = 1.45558633\n",
      "Iteration 9107, loss = 1.45555864\n",
      "Iteration 9108, loss = 1.45553211\n",
      "Iteration 9109, loss = 1.45550502\n",
      "Iteration 9110, loss = 1.45547851\n",
      "Iteration 9111, loss = 1.45545164\n",
      "Iteration 9112, loss = 1.45542478\n",
      "Iteration 9113, loss = 1.45539757\n",
      "Iteration 9114, loss = 1.45536985\n",
      "Iteration 9115, loss = 1.45534386\n",
      "Iteration 9116, loss = 1.45531638\n",
      "Iteration 9117, loss = 1.45529040\n",
      "Iteration 9118, loss = 1.45526377\n",
      "Iteration 9119, loss = 1.45523657\n",
      "Iteration 9120, loss = 1.45520892\n",
      "Iteration 9121, loss = 1.45518083\n",
      "Iteration 9122, loss = 1.45515275\n",
      "Iteration 9123, loss = 1.45512621\n",
      "Iteration 9124, loss = 1.45509782\n",
      "Iteration 9125, loss = 1.45507120\n",
      "Iteration 9126, loss = 1.45504472\n",
      "Iteration 9127, loss = 1.45501838\n",
      "Iteration 9128, loss = 1.45499148\n",
      "Iteration 9129, loss = 1.45496408\n",
      "Iteration 9130, loss = 1.45493620\n",
      "Iteration 9131, loss = 1.45490836\n",
      "Iteration 9132, loss = 1.45488148\n",
      "Iteration 9133, loss = 1.45485449\n",
      "Iteration 9134, loss = 1.45482700\n",
      "Iteration 9135, loss = 1.45480190\n",
      "Iteration 9136, loss = 1.45477414\n",
      "Iteration 9137, loss = 1.45474852\n",
      "Iteration 9138, loss = 1.45472225\n",
      "Iteration 9139, loss = 1.45469538\n",
      "Iteration 9140, loss = 1.45466799\n",
      "Iteration 9141, loss = 1.45464013\n",
      "Iteration 9142, loss = 1.45461392\n",
      "Iteration 9143, loss = 1.45458748\n",
      "Iteration 9144, loss = 1.45455816\n",
      "Iteration 9145, loss = 1.45453160\n",
      "Iteration 9146, loss = 1.45450454\n",
      "Iteration 9147, loss = 1.45447699\n",
      "Iteration 9148, loss = 1.45445258\n",
      "Iteration 9149, loss = 1.45442280\n",
      "Iteration 9150, loss = 1.45439600\n",
      "Iteration 9151, loss = 1.45436993\n",
      "Iteration 9152, loss = 1.45434252\n",
      "Iteration 9153, loss = 1.45431620\n",
      "Iteration 9154, loss = 1.45428967\n",
      "Iteration 9155, loss = 1.45426259\n",
      "Iteration 9156, loss = 1.45423505\n",
      "Iteration 9157, loss = 1.45420815\n",
      "Iteration 9158, loss = 1.45418051\n",
      "Iteration 9159, loss = 1.45415356\n",
      "Iteration 9160, loss = 1.45412658\n",
      "Iteration 9161, loss = 1.45410084\n",
      "Iteration 9162, loss = 1.45407319\n",
      "Iteration 9163, loss = 1.45404668\n",
      "Iteration 9164, loss = 1.45401962\n",
      "Iteration 9165, loss = 1.45399206\n",
      "Iteration 9166, loss = 1.45396714\n",
      "Iteration 9167, loss = 1.45393807\n",
      "Iteration 9168, loss = 1.45391147\n",
      "Iteration 9169, loss = 1.45388435\n",
      "Iteration 9170, loss = 1.45385776\n",
      "Iteration 9171, loss = 1.45383202\n",
      "Iteration 9172, loss = 1.45380659\n",
      "Iteration 9173, loss = 1.45378051\n",
      "Iteration 9174, loss = 1.45375382\n",
      "Iteration 9175, loss = 1.45372660\n",
      "Iteration 9176, loss = 1.45369888\n",
      "Iteration 9177, loss = 1.45367078\n",
      "Iteration 9178, loss = 1.45364244\n",
      "Iteration 9179, loss = 1.45362026\n",
      "Iteration 9180, loss = 1.45359094\n",
      "Iteration 9181, loss = 1.45356651\n",
      "Iteration 9182, loss = 1.45354135\n",
      "Iteration 9183, loss = 1.45351550\n",
      "Iteration 9184, loss = 1.45348903\n",
      "Iteration 9185, loss = 1.45346199\n",
      "Iteration 9186, loss = 1.45343445\n",
      "Iteration 9187, loss = 1.45340649\n",
      "Iteration 9188, loss = 1.45337818\n",
      "Iteration 9189, loss = 1.45335431\n",
      "Iteration 9190, loss = 1.45332561\n",
      "Iteration 9191, loss = 1.45329766\n",
      "Iteration 9192, loss = 1.45327191\n",
      "Iteration 9193, loss = 1.45324560\n",
      "Iteration 9194, loss = 1.45321877\n",
      "Iteration 9195, loss = 1.45319145\n",
      "Iteration 9196, loss = 1.45316371\n",
      "Iteration 9197, loss = 1.45313558\n",
      "Iteration 9198, loss = 1.45311344\n",
      "Iteration 9199, loss = 1.45308287\n",
      "Iteration 9200, loss = 1.45305701\n",
      "Iteration 9201, loss = 1.45303205\n",
      "Iteration 9202, loss = 1.45300651\n",
      "Iteration 9203, loss = 1.45298041\n",
      "Iteration 9204, loss = 1.45295374\n",
      "Iteration 9205, loss = 1.45292656\n",
      "Iteration 9206, loss = 1.45289895\n",
      "Iteration 9207, loss = 1.45287095\n",
      "Iteration 9208, loss = 1.45284263\n",
      "Iteration 9209, loss = 1.45281522\n",
      "Iteration 9210, loss = 1.45279647\n",
      "Iteration 9211, loss = 1.45276350\n",
      "Iteration 9212, loss = 1.45274010\n",
      "Iteration 9213, loss = 1.45271596\n",
      "Iteration 9214, loss = 1.45269108\n",
      "Iteration 9215, loss = 1.45266549\n",
      "Iteration 9216, loss = 1.45263926\n",
      "Iteration 9217, loss = 1.45261248\n",
      "Iteration 9218, loss = 1.45258521\n",
      "Iteration 9219, loss = 1.45255750\n",
      "Iteration 9220, loss = 1.45252943\n",
      "Iteration 9221, loss = 1.45250105\n",
      "Iteration 9222, loss = 1.45247239\n",
      "Iteration 9223, loss = 1.45244349\n",
      "Iteration 9224, loss = 1.45241788\n",
      "Iteration 9225, loss = 1.45240017\n",
      "Iteration 9226, loss = 1.45236250\n",
      "Iteration 9227, loss = 1.45233808\n",
      "Iteration 9228, loss = 1.45231307\n",
      "Iteration 9229, loss = 1.45228744\n",
      "Iteration 9230, loss = 1.45226124\n",
      "Iteration 9231, loss = 1.45223452\n",
      "Iteration 9232, loss = 1.45220734\n",
      "Iteration 9233, loss = 1.45217977\n",
      "Iteration 9234, loss = 1.45215510\n",
      "Iteration 9235, loss = 1.45212952\n",
      "Iteration 9236, loss = 1.45209991\n",
      "Iteration 9237, loss = 1.45207470\n",
      "Iteration 9238, loss = 1.45204888\n",
      "Iteration 9239, loss = 1.45202251\n",
      "Iteration 9240, loss = 1.45199562\n",
      "Iteration 9241, loss = 1.45196829\n",
      "Iteration 9242, loss = 1.45194056\n",
      "Iteration 9243, loss = 1.45191252\n",
      "Iteration 9244, loss = 1.45188857\n",
      "Iteration 9245, loss = 1.45186371\n",
      "Iteration 9246, loss = 1.45183452\n",
      "Iteration 9247, loss = 1.45180600\n",
      "Iteration 9248, loss = 1.45178031\n",
      "Iteration 9249, loss = 1.45175408\n",
      "Iteration 9250, loss = 1.45172736\n",
      "Iteration 9251, loss = 1.45170023\n",
      "Iteration 9252, loss = 1.45167272\n",
      "Iteration 9253, loss = 1.45164961\n",
      "Iteration 9254, loss = 1.45161937\n",
      "Iteration 9255, loss = 1.45159362\n",
      "Iteration 9256, loss = 1.45156787\n",
      "Iteration 9257, loss = 1.45154189\n",
      "Iteration 9258, loss = 1.45151604\n",
      "Iteration 9259, loss = 1.45148988\n",
      "Iteration 9260, loss = 1.45146323\n",
      "Iteration 9261, loss = 1.45143613\n",
      "Iteration 9262, loss = 1.45141349\n",
      "Iteration 9263, loss = 1.45138384\n",
      "Iteration 9264, loss = 1.45135845\n",
      "Iteration 9265, loss = 1.45133249\n",
      "Iteration 9266, loss = 1.45130600\n",
      "Iteration 9267, loss = 1.45127904\n",
      "Iteration 9268, loss = 1.45125165\n",
      "Iteration 9269, loss = 1.45122395\n",
      "Iteration 9270, loss = 1.45120017\n",
      "Iteration 9271, loss = 1.45117277\n",
      "Iteration 9272, loss = 1.45114457\n",
      "Iteration 9273, loss = 1.45111867\n",
      "Iteration 9274, loss = 1.45109226\n",
      "Iteration 9275, loss = 1.45106540\n",
      "Iteration 9276, loss = 1.45103816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9277, loss = 1.45101737\n",
      "Iteration 9278, loss = 1.45098587\n",
      "Iteration 9279, loss = 1.45096058\n",
      "Iteration 9280, loss = 1.45093472\n",
      "Iteration 9281, loss = 1.45090836\n",
      "Iteration 9282, loss = 1.45088153\n",
      "Iteration 9283, loss = 1.45085434\n",
      "Iteration 9284, loss = 1.45083011\n",
      "Iteration 9285, loss = 1.45080109\n",
      "Iteration 9286, loss = 1.45077562\n",
      "Iteration 9287, loss = 1.45074980\n",
      "Iteration 9288, loss = 1.45072394\n",
      "Iteration 9289, loss = 1.45069774\n",
      "Iteration 9290, loss = 1.45067147\n",
      "Iteration 9291, loss = 1.45064487\n",
      "Iteration 9292, loss = 1.45061865\n",
      "Iteration 9293, loss = 1.45059286\n",
      "Iteration 9294, loss = 1.45056612\n",
      "Iteration 9295, loss = 1.45053964\n",
      "Iteration 9296, loss = 1.45051443\n",
      "Iteration 9297, loss = 1.45048728\n",
      "Iteration 9298, loss = 1.45046129\n",
      "Iteration 9299, loss = 1.45043540\n",
      "Iteration 9300, loss = 1.45040902\n",
      "Iteration 9301, loss = 1.45038246\n",
      "Iteration 9302, loss = 1.45035594\n",
      "Iteration 9303, loss = 1.45033011\n",
      "Iteration 9304, loss = 1.45030354\n",
      "Iteration 9305, loss = 1.45027774\n",
      "Iteration 9306, loss = 1.45025175\n",
      "Iteration 9307, loss = 1.45022528\n",
      "Iteration 9308, loss = 1.45019841\n",
      "Iteration 9309, loss = 1.45017125\n",
      "Iteration 9310, loss = 1.45015358\n",
      "Iteration 9311, loss = 1.45012204\n",
      "Iteration 9312, loss = 1.45009962\n",
      "Iteration 9313, loss = 1.45007639\n",
      "Iteration 9314, loss = 1.45005240\n",
      "Iteration 9315, loss = 1.45002772\n",
      "Iteration 9316, loss = 1.45000241\n",
      "Iteration 9317, loss = 1.44997652\n",
      "Iteration 9318, loss = 1.44995011\n",
      "Iteration 9319, loss = 1.44992324\n",
      "Iteration 9320, loss = 1.44989596\n",
      "Iteration 9321, loss = 1.44986832\n",
      "Iteration 9322, loss = 1.44984036\n",
      "Iteration 9323, loss = 1.44981212\n",
      "Iteration 9324, loss = 1.44978363\n",
      "Iteration 9325, loss = 1.44975774\n",
      "Iteration 9326, loss = 1.44973073\n",
      "Iteration 9327, loss = 1.44970266\n",
      "Iteration 9328, loss = 1.44967651\n",
      "Iteration 9329, loss = 1.44964996\n",
      "Iteration 9330, loss = 1.44962305\n",
      "Iteration 9331, loss = 1.44960215\n",
      "Iteration 9332, loss = 1.44957135\n",
      "Iteration 9333, loss = 1.44954634\n",
      "Iteration 9334, loss = 1.44952080\n",
      "Iteration 9335, loss = 1.44949479\n",
      "Iteration 9336, loss = 1.44946833\n",
      "Iteration 9337, loss = 1.44944147\n",
      "Iteration 9338, loss = 1.44941445\n",
      "Iteration 9339, loss = 1.44938853\n",
      "Iteration 9340, loss = 1.44936294\n",
      "Iteration 9341, loss = 1.44933753\n",
      "Iteration 9342, loss = 1.44931163\n",
      "Iteration 9343, loss = 1.44928526\n",
      "Iteration 9344, loss = 1.44925848\n",
      "Iteration 9345, loss = 1.44923188\n",
      "Iteration 9346, loss = 1.44920586\n",
      "Iteration 9347, loss = 1.44917987\n",
      "Iteration 9348, loss = 1.44915345\n",
      "Iteration 9349, loss = 1.44912828\n",
      "Iteration 9350, loss = 1.44910173\n",
      "Iteration 9351, loss = 1.44907629\n",
      "Iteration 9352, loss = 1.44905088\n",
      "Iteration 9353, loss = 1.44902494\n",
      "Iteration 9354, loss = 1.44899903\n",
      "Iteration 9355, loss = 1.44897265\n",
      "Iteration 9356, loss = 1.44894657\n",
      "Iteration 9357, loss = 1.44892075\n",
      "Iteration 9358, loss = 1.44889509\n",
      "Iteration 9359, loss = 1.44886895\n",
      "Iteration 9360, loss = 1.44884238\n",
      "Iteration 9361, loss = 1.44881606\n",
      "Iteration 9362, loss = 1.44878958\n",
      "Iteration 9363, loss = 1.44876383\n",
      "Iteration 9364, loss = 1.44873788\n",
      "Iteration 9365, loss = 1.44871155\n",
      "Iteration 9366, loss = 1.44868590\n",
      "Iteration 9367, loss = 1.44866001\n",
      "Iteration 9368, loss = 1.44863449\n",
      "Iteration 9369, loss = 1.44860866\n",
      "Iteration 9370, loss = 1.44858240\n",
      "Iteration 9371, loss = 1.44855974\n",
      "Iteration 9372, loss = 1.44853252\n",
      "Iteration 9373, loss = 1.44850869\n",
      "Iteration 9374, loss = 1.44848428\n",
      "Iteration 9375, loss = 1.44845927\n",
      "Iteration 9376, loss = 1.44843370\n",
      "Iteration 9377, loss = 1.44840763\n",
      "Iteration 9378, loss = 1.44838112\n",
      "Iteration 9379, loss = 1.44835422\n",
      "Iteration 9380, loss = 1.44832699\n",
      "Iteration 9381, loss = 1.44830031\n",
      "Iteration 9382, loss = 1.44827509\n",
      "Iteration 9383, loss = 1.44824767\n",
      "Iteration 9384, loss = 1.44822210\n",
      "Iteration 9385, loss = 1.44819609\n",
      "Iteration 9386, loss = 1.44816975\n",
      "Iteration 9387, loss = 1.44814307\n",
      "Iteration 9388, loss = 1.44812019\n",
      "Iteration 9389, loss = 1.44809211\n",
      "Iteration 9390, loss = 1.44806638\n",
      "Iteration 9391, loss = 1.44804137\n",
      "Iteration 9392, loss = 1.44801589\n",
      "Iteration 9393, loss = 1.44798993\n",
      "Iteration 9394, loss = 1.44796355\n",
      "Iteration 9395, loss = 1.44794136\n",
      "Iteration 9396, loss = 1.44791228\n",
      "Iteration 9397, loss = 1.44788808\n",
      "Iteration 9398, loss = 1.44786386\n",
      "Iteration 9399, loss = 1.44783902\n",
      "Iteration 9400, loss = 1.44781365\n",
      "Iteration 9401, loss = 1.44778782\n",
      "Iteration 9402, loss = 1.44776161\n",
      "Iteration 9403, loss = 1.44773501\n",
      "Iteration 9404, loss = 1.44770807\n",
      "Iteration 9405, loss = 1.44768083\n",
      "Iteration 9406, loss = 1.44765394\n",
      "Iteration 9407, loss = 1.44763075\n",
      "Iteration 9408, loss = 1.44760271\n",
      "Iteration 9409, loss = 1.44757809\n",
      "Iteration 9410, loss = 1.44755303\n",
      "Iteration 9411, loss = 1.44752752\n",
      "Iteration 9412, loss = 1.44750159\n",
      "Iteration 9413, loss = 1.44747528\n",
      "Iteration 9414, loss = 1.44744863\n",
      "Iteration 9415, loss = 1.44742167\n",
      "Iteration 9416, loss = 1.44740126\n",
      "Iteration 9417, loss = 1.44737005\n",
      "Iteration 9418, loss = 1.44734515\n",
      "Iteration 9419, loss = 1.44731980\n",
      "Iteration 9420, loss = 1.44729405\n",
      "Iteration 9421, loss = 1.44726793\n",
      "Iteration 9422, loss = 1.44724143\n",
      "Iteration 9423, loss = 1.44721994\n",
      "Iteration 9424, loss = 1.44718960\n",
      "Iteration 9425, loss = 1.44716412\n",
      "Iteration 9426, loss = 1.44713823\n",
      "Iteration 9427, loss = 1.44711333\n",
      "Iteration 9428, loss = 1.44708693\n",
      "Iteration 9429, loss = 1.44706218\n",
      "Iteration 9430, loss = 1.44703750\n",
      "Iteration 9431, loss = 1.44701231\n",
      "Iteration 9432, loss = 1.44698665\n",
      "Iteration 9433, loss = 1.44696060\n",
      "Iteration 9434, loss = 1.44693419\n",
      "Iteration 9435, loss = 1.44691066\n",
      "Iteration 9436, loss = 1.44688253\n",
      "Iteration 9437, loss = 1.44685714\n",
      "Iteration 9438, loss = 1.44683138\n",
      "Iteration 9439, loss = 1.44680631\n",
      "Iteration 9440, loss = 1.44678082\n",
      "Iteration 9441, loss = 1.44675490\n",
      "Iteration 9442, loss = 1.44672912\n",
      "Iteration 9443, loss = 1.44670307\n",
      "Iteration 9444, loss = 1.44667799\n",
      "Iteration 9445, loss = 1.44665253\n",
      "Iteration 9446, loss = 1.44662664\n",
      "Iteration 9447, loss = 1.44660233\n",
      "Iteration 9448, loss = 1.44657626\n",
      "Iteration 9449, loss = 1.44655159\n",
      "Iteration 9450, loss = 1.44652640\n",
      "Iteration 9451, loss = 1.44650074\n",
      "Iteration 9452, loss = 1.44647467\n",
      "Iteration 9453, loss = 1.44644825\n",
      "Iteration 9454, loss = 1.44642368\n",
      "Iteration 9455, loss = 1.44639657\n",
      "Iteration 9456, loss = 1.44637118\n",
      "Iteration 9457, loss = 1.44634540\n",
      "Iteration 9458, loss = 1.44632199\n",
      "Iteration 9459, loss = 1.44629489\n",
      "Iteration 9460, loss = 1.44627001\n",
      "Iteration 9461, loss = 1.44624472\n",
      "Iteration 9462, loss = 1.44621899\n",
      "Iteration 9463, loss = 1.44619341\n",
      "Iteration 9464, loss = 1.44616732\n",
      "Iteration 9465, loss = 1.44614136\n",
      "Iteration 9466, loss = 1.44611661\n",
      "Iteration 9467, loss = 1.44609036\n",
      "Iteration 9468, loss = 1.44606528\n",
      "Iteration 9469, loss = 1.44603978\n",
      "Iteration 9470, loss = 1.44601498\n",
      "Iteration 9471, loss = 1.44598971\n",
      "Iteration 9472, loss = 1.44596402\n",
      "Iteration 9473, loss = 1.44593794\n",
      "Iteration 9474, loss = 1.44591476\n",
      "Iteration 9475, loss = 1.44588707\n",
      "Iteration 9476, loss = 1.44586210\n",
      "Iteration 9477, loss = 1.44583667\n",
      "Iteration 9478, loss = 1.44581083\n",
      "Iteration 9479, loss = 1.44578625\n",
      "Iteration 9480, loss = 1.44575971\n",
      "Iteration 9481, loss = 1.44573408\n",
      "Iteration 9482, loss = 1.44570868\n",
      "Iteration 9483, loss = 1.44568423\n",
      "Iteration 9484, loss = 1.44565794\n",
      "Iteration 9485, loss = 1.44563253\n",
      "Iteration 9486, loss = 1.44560740\n",
      "Iteration 9487, loss = 1.44558177\n",
      "Iteration 9488, loss = 1.44555638\n",
      "Iteration 9489, loss = 1.44553185\n",
      "Iteration 9490, loss = 1.44550554\n",
      "Iteration 9491, loss = 1.44548064\n",
      "Iteration 9492, loss = 1.44545546\n",
      "Iteration 9493, loss = 1.44542985\n",
      "Iteration 9494, loss = 1.44540546\n",
      "Iteration 9495, loss = 1.44537961\n",
      "Iteration 9496, loss = 1.44535398\n",
      "Iteration 9497, loss = 1.44532892\n",
      "Iteration 9498, loss = 1.44530414\n",
      "Iteration 9499, loss = 1.44527852\n",
      "Iteration 9500, loss = 1.44525316\n",
      "Iteration 9501, loss = 1.44522752\n",
      "Iteration 9502, loss = 1.44520227\n",
      "Iteration 9503, loss = 1.44517674\n",
      "Iteration 9504, loss = 1.44515152\n",
      "Iteration 9505, loss = 1.44512576\n",
      "Iteration 9506, loss = 1.44510032\n",
      "Iteration 9507, loss = 1.44507555\n",
      "Iteration 9508, loss = 1.44505038\n",
      "Iteration 9509, loss = 1.44502480\n",
      "Iteration 9510, loss = 1.44499958\n",
      "Iteration 9511, loss = 1.44497374\n",
      "Iteration 9512, loss = 1.44494969\n",
      "Iteration 9513, loss = 1.44492525\n",
      "Iteration 9514, loss = 1.44490032\n",
      "Iteration 9515, loss = 1.44487496\n",
      "Iteration 9516, loss = 1.44484919\n",
      "Iteration 9517, loss = 1.44482308\n",
      "Iteration 9518, loss = 1.44479940\n",
      "Iteration 9519, loss = 1.44477225\n",
      "Iteration 9520, loss = 1.44474735\n",
      "Iteration 9521, loss = 1.44472201\n",
      "Iteration 9522, loss = 1.44469631\n",
      "Iteration 9523, loss = 1.44467125\n",
      "Iteration 9524, loss = 1.44464503\n",
      "Iteration 9525, loss = 1.44461989\n",
      "Iteration 9526, loss = 1.44459571\n",
      "Iteration 9527, loss = 1.44457149\n",
      "Iteration 9528, loss = 1.44454677\n",
      "Iteration 9529, loss = 1.44452159\n",
      "Iteration 9530, loss = 1.44449601\n",
      "Iteration 9531, loss = 1.44447005\n",
      "Iteration 9532, loss = 1.44444377\n",
      "Iteration 9533, loss = 1.44442211\n",
      "Iteration 9534, loss = 1.44439332\n",
      "Iteration 9535, loss = 1.44436892\n",
      "Iteration 9536, loss = 1.44434404\n",
      "Iteration 9537, loss = 1.44431872\n",
      "Iteration 9538, loss = 1.44429301\n",
      "Iteration 9539, loss = 1.44426884\n",
      "Iteration 9540, loss = 1.44424366\n",
      "Iteration 9541, loss = 1.44421771\n",
      "Iteration 9542, loss = 1.44419338\n",
      "Iteration 9543, loss = 1.44416858\n",
      "Iteration 9544, loss = 1.44414337\n",
      "Iteration 9545, loss = 1.44411778\n",
      "Iteration 9546, loss = 1.44409585\n",
      "Iteration 9547, loss = 1.44406768\n",
      "Iteration 9548, loss = 1.44404305\n",
      "Iteration 9549, loss = 1.44401802\n",
      "Iteration 9550, loss = 1.44399263\n",
      "Iteration 9551, loss = 1.44396688\n",
      "Iteration 9552, loss = 1.44394360\n",
      "Iteration 9553, loss = 1.44391868\n",
      "Iteration 9554, loss = 1.44389196\n",
      "Iteration 9555, loss = 1.44386790\n",
      "Iteration 9556, loss = 1.44384336\n",
      "Iteration 9557, loss = 1.44381836\n",
      "Iteration 9558, loss = 1.44379296\n",
      "Iteration 9559, loss = 1.44376721\n",
      "Iteration 9560, loss = 1.44374115\n",
      "Iteration 9561, loss = 1.44371655\n",
      "Iteration 9562, loss = 1.44369194\n",
      "Iteration 9563, loss = 1.44366606\n",
      "Iteration 9564, loss = 1.44364236\n",
      "Iteration 9565, loss = 1.44361815\n",
      "Iteration 9566, loss = 1.44359347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9567, loss = 1.44356836\n",
      "Iteration 9568, loss = 1.44354285\n",
      "Iteration 9569, loss = 1.44351715\n",
      "Iteration 9570, loss = 1.44349197\n",
      "Iteration 9571, loss = 1.44346732\n",
      "Iteration 9572, loss = 1.44344197\n",
      "Iteration 9573, loss = 1.44341700\n",
      "Iteration 9574, loss = 1.44339164\n",
      "Iteration 9575, loss = 1.44336637\n",
      "Iteration 9576, loss = 1.44334083\n",
      "Iteration 9577, loss = 1.44331649\n",
      "Iteration 9578, loss = 1.44329268\n",
      "Iteration 9579, loss = 1.44326941\n",
      "Iteration 9580, loss = 1.44324556\n",
      "Iteration 9581, loss = 1.44322119\n",
      "Iteration 9582, loss = 1.44319635\n",
      "Iteration 9583, loss = 1.44317108\n",
      "Iteration 9584, loss = 1.44314546\n",
      "Iteration 9585, loss = 1.44311951\n",
      "Iteration 9586, loss = 1.44309330\n",
      "Iteration 9587, loss = 1.44306992\n",
      "Iteration 9588, loss = 1.44304580\n",
      "Iteration 9589, loss = 1.44301962\n",
      "Iteration 9590, loss = 1.44299188\n",
      "Iteration 9591, loss = 1.44296712\n",
      "Iteration 9592, loss = 1.44294208\n",
      "Iteration 9593, loss = 1.44291670\n",
      "Iteration 9594, loss = 1.44289102\n",
      "Iteration 9595, loss = 1.44287179\n",
      "Iteration 9596, loss = 1.44284158\n",
      "Iteration 9597, loss = 1.44281761\n",
      "Iteration 9598, loss = 1.44279319\n",
      "Iteration 9599, loss = 1.44276839\n",
      "Iteration 9600, loss = 1.44274321\n",
      "Iteration 9601, loss = 1.44271770\n",
      "Iteration 9602, loss = 1.44269188\n",
      "Iteration 9603, loss = 1.44267190\n",
      "Iteration 9604, loss = 1.44264248\n",
      "Iteration 9605, loss = 1.44261868\n",
      "Iteration 9606, loss = 1.44259439\n",
      "Iteration 9607, loss = 1.44256968\n",
      "Iteration 9608, loss = 1.44254458\n",
      "Iteration 9609, loss = 1.44251913\n",
      "Iteration 9610, loss = 1.44249335\n",
      "Iteration 9611, loss = 1.44246939\n",
      "Iteration 9612, loss = 1.44244314\n",
      "Iteration 9613, loss = 1.44241855\n",
      "Iteration 9614, loss = 1.44239357\n",
      "Iteration 9615, loss = 1.44236824\n",
      "Iteration 9616, loss = 1.44234599\n",
      "Iteration 9617, loss = 1.44231884\n",
      "Iteration 9618, loss = 1.44229464\n",
      "Iteration 9619, loss = 1.44226999\n",
      "Iteration 9620, loss = 1.44224495\n",
      "Iteration 9621, loss = 1.44221956\n",
      "Iteration 9622, loss = 1.44219534\n",
      "Iteration 9623, loss = 1.44217057\n",
      "Iteration 9624, loss = 1.44214613\n",
      "Iteration 9625, loss = 1.44212286\n",
      "Iteration 9626, loss = 1.44209906\n",
      "Iteration 9627, loss = 1.44207478\n",
      "Iteration 9628, loss = 1.44205006\n",
      "Iteration 9629, loss = 1.44202493\n",
      "Iteration 9630, loss = 1.44199944\n",
      "Iteration 9631, loss = 1.44197365\n",
      "Iteration 9632, loss = 1.44194772\n",
      "Iteration 9633, loss = 1.44192255\n",
      "Iteration 9634, loss = 1.44189861\n",
      "Iteration 9635, loss = 1.44187366\n",
      "Iteration 9636, loss = 1.44184928\n",
      "Iteration 9637, loss = 1.44182452\n",
      "Iteration 9638, loss = 1.44179943\n",
      "Iteration 9639, loss = 1.44177570\n",
      "Iteration 9640, loss = 1.44174965\n",
      "Iteration 9641, loss = 1.44172565\n",
      "Iteration 9642, loss = 1.44170145\n",
      "Iteration 9643, loss = 1.44167683\n",
      "Iteration 9644, loss = 1.44165193\n",
      "Iteration 9645, loss = 1.44162751\n",
      "Iteration 9646, loss = 1.44160278\n",
      "Iteration 9647, loss = 1.44157770\n",
      "Iteration 9648, loss = 1.44155404\n",
      "Iteration 9649, loss = 1.44152879\n",
      "Iteration 9650, loss = 1.44150460\n",
      "Iteration 9651, loss = 1.44148113\n",
      "Iteration 9652, loss = 1.44145716\n",
      "Iteration 9653, loss = 1.44143274\n",
      "Iteration 9654, loss = 1.44140793\n",
      "Iteration 9655, loss = 1.44138279\n",
      "Iteration 9656, loss = 1.44135734\n",
      "Iteration 9657, loss = 1.44133161\n",
      "Iteration 9658, loss = 1.44130708\n",
      "Iteration 9659, loss = 1.44128286\n",
      "Iteration 9660, loss = 1.44125639\n",
      "Iteration 9661, loss = 1.44123185\n",
      "Iteration 9662, loss = 1.44120748\n",
      "Iteration 9663, loss = 1.44118274\n",
      "Iteration 9664, loss = 1.44115770\n",
      "Iteration 9665, loss = 1.44113359\n",
      "Iteration 9666, loss = 1.44110806\n",
      "Iteration 9667, loss = 1.44108347\n",
      "Iteration 9668, loss = 1.44105891\n",
      "Iteration 9669, loss = 1.44103635\n",
      "Iteration 9670, loss = 1.44101206\n",
      "Iteration 9671, loss = 1.44098956\n",
      "Iteration 9672, loss = 1.44096647\n",
      "Iteration 9673, loss = 1.44094289\n",
      "Iteration 9674, loss = 1.44091881\n",
      "Iteration 9675, loss = 1.44089429\n",
      "Iteration 9676, loss = 1.44086940\n",
      "Iteration 9677, loss = 1.44084418\n",
      "Iteration 9678, loss = 1.44081867\n",
      "Iteration 9679, loss = 1.44079289\n",
      "Iteration 9680, loss = 1.44076690\n",
      "Iteration 9681, loss = 1.44074072\n",
      "Iteration 9682, loss = 1.44072043\n",
      "Iteration 9683, loss = 1.44069429\n",
      "Iteration 9684, loss = 1.44066720\n",
      "Iteration 9685, loss = 1.44064399\n",
      "Iteration 9686, loss = 1.44062032\n",
      "Iteration 9687, loss = 1.44059627\n",
      "Iteration 9688, loss = 1.44057180\n",
      "Iteration 9689, loss = 1.44054696\n",
      "Iteration 9690, loss = 1.44052180\n",
      "Iteration 9691, loss = 1.44049636\n",
      "Iteration 9692, loss = 1.44047075\n",
      "Iteration 9693, loss = 1.44044600\n",
      "Iteration 9694, loss = 1.44042164\n",
      "Iteration 9695, loss = 1.44039716\n",
      "Iteration 9696, loss = 1.44037236\n",
      "Iteration 9697, loss = 1.44034725\n",
      "Iteration 9698, loss = 1.44032425\n",
      "Iteration 9699, loss = 1.44029837\n",
      "Iteration 9700, loss = 1.44027446\n",
      "Iteration 9701, loss = 1.44025015\n",
      "Iteration 9702, loss = 1.44022548\n",
      "Iteration 9703, loss = 1.44020048\n",
      "Iteration 9704, loss = 1.44017761\n",
      "Iteration 9705, loss = 1.44015146\n",
      "Iteration 9706, loss = 1.44012734\n",
      "Iteration 9707, loss = 1.44010284\n",
      "Iteration 9708, loss = 1.44007799\n",
      "Iteration 9709, loss = 1.44005377\n",
      "Iteration 9710, loss = 1.44002930\n",
      "Iteration 9711, loss = 1.44000535\n",
      "Iteration 9712, loss = 1.43998098\n",
      "Iteration 9713, loss = 1.43995623\n",
      "Iteration 9714, loss = 1.43993164\n",
      "Iteration 9715, loss = 1.43990701\n",
      "Iteration 9716, loss = 1.43988304\n",
      "Iteration 9717, loss = 1.43985896\n",
      "Iteration 9718, loss = 1.43983450\n",
      "Iteration 9719, loss = 1.43980972\n",
      "Iteration 9720, loss = 1.43978506\n",
      "Iteration 9721, loss = 1.43976041\n",
      "Iteration 9722, loss = 1.43973583\n",
      "Iteration 9723, loss = 1.43971204\n",
      "Iteration 9724, loss = 1.43968686\n",
      "Iteration 9725, loss = 1.43966317\n",
      "Iteration 9726, loss = 1.43963920\n",
      "Iteration 9727, loss = 1.43961486\n",
      "Iteration 9728, loss = 1.43959019\n",
      "Iteration 9729, loss = 1.43956921\n",
      "Iteration 9730, loss = 1.43954291\n",
      "Iteration 9731, loss = 1.43952009\n",
      "Iteration 9732, loss = 1.43949677\n",
      "Iteration 9733, loss = 1.43947300\n",
      "Iteration 9734, loss = 1.43944882\n",
      "Iteration 9735, loss = 1.43942425\n",
      "Iteration 9736, loss = 1.43939934\n",
      "Iteration 9737, loss = 1.43937414\n",
      "Iteration 9738, loss = 1.43934869\n",
      "Iteration 9739, loss = 1.43932321\n",
      "Iteration 9740, loss = 1.43929894\n",
      "Iteration 9741, loss = 1.43927454\n",
      "Iteration 9742, loss = 1.43924981\n",
      "Iteration 9743, loss = 1.43922675\n",
      "Iteration 9744, loss = 1.43920256\n",
      "Iteration 9745, loss = 1.43917981\n",
      "Iteration 9746, loss = 1.43915656\n",
      "Iteration 9747, loss = 1.43913285\n",
      "Iteration 9748, loss = 1.43910871\n",
      "Iteration 9749, loss = 1.43908421\n",
      "Iteration 9750, loss = 1.43905937\n",
      "Iteration 9751, loss = 1.43903423\n",
      "Iteration 9752, loss = 1.43900885\n",
      "Iteration 9753, loss = 1.43898723\n",
      "Iteration 9754, loss = 1.43896061\n",
      "Iteration 9755, loss = 1.43893592\n",
      "Iteration 9756, loss = 1.43891212\n",
      "Iteration 9757, loss = 1.43888795\n",
      "Iteration 9758, loss = 1.43886346\n",
      "Iteration 9759, loss = 1.43883868\n",
      "Iteration 9760, loss = 1.43881371\n",
      "Iteration 9761, loss = 1.43879348\n",
      "Iteration 9762, loss = 1.43876557\n",
      "Iteration 9763, loss = 1.43874223\n",
      "Iteration 9764, loss = 1.43871849\n",
      "Iteration 9765, loss = 1.43869437\n",
      "Iteration 9766, loss = 1.43866989\n",
      "Iteration 9767, loss = 1.43864631\n",
      "Iteration 9768, loss = 1.43862124\n",
      "Iteration 9769, loss = 1.43859785\n",
      "Iteration 9770, loss = 1.43857416\n",
      "Iteration 9771, loss = 1.43855008\n",
      "Iteration 9772, loss = 1.43852571\n",
      "Iteration 9773, loss = 1.43850102\n",
      "Iteration 9774, loss = 1.43847606\n",
      "Iteration 9775, loss = 1.43845086\n",
      "Iteration 9776, loss = 1.43842983\n",
      "Iteration 9777, loss = 1.43840279\n",
      "Iteration 9778, loss = 1.43837967\n",
      "Iteration 9779, loss = 1.43835612\n",
      "Iteration 9780, loss = 1.43833218\n",
      "Iteration 9781, loss = 1.43830787\n",
      "Iteration 9782, loss = 1.43828323\n",
      "Iteration 9783, loss = 1.43825831\n",
      "Iteration 9784, loss = 1.43823314\n",
      "Iteration 9785, loss = 1.43821531\n",
      "Iteration 9786, loss = 1.43818786\n",
      "Iteration 9787, loss = 1.43816730\n",
      "Iteration 9788, loss = 1.43814605\n",
      "Iteration 9789, loss = 1.43812416\n",
      "Iteration 9790, loss = 1.43810169\n",
      "Iteration 9791, loss = 1.43807870\n",
      "Iteration 9792, loss = 1.43805524\n",
      "Iteration 9793, loss = 1.43803135\n",
      "Iteration 9794, loss = 1.43800708\n",
      "Iteration 9795, loss = 1.43798246\n",
      "Iteration 9796, loss = 1.43795754\n",
      "Iteration 9797, loss = 1.43793241\n",
      "Iteration 9798, loss = 1.43790705\n",
      "Iteration 9799, loss = 1.43788148\n",
      "Iteration 9800, loss = 1.43785571\n",
      "Iteration 9801, loss = 1.43782980\n",
      "Iteration 9802, loss = 1.43780381\n",
      "Iteration 9803, loss = 1.43777778\n",
      "Iteration 9804, loss = 1.43776031\n",
      "Iteration 9805, loss = 1.43773605\n",
      "Iteration 9806, loss = 1.43770605\n",
      "Iteration 9807, loss = 1.43768436\n",
      "Iteration 9808, loss = 1.43766218\n",
      "Iteration 9809, loss = 1.43763952\n",
      "Iteration 9810, loss = 1.43761639\n",
      "Iteration 9811, loss = 1.43759283\n",
      "Iteration 9812, loss = 1.43756890\n",
      "Iteration 9813, loss = 1.43754461\n",
      "Iteration 9814, loss = 1.43752002\n",
      "Iteration 9815, loss = 1.43749514\n",
      "Iteration 9816, loss = 1.43747000\n",
      "Iteration 9817, loss = 1.43744467\n",
      "Iteration 9818, loss = 1.43741918\n",
      "Iteration 9819, loss = 1.43739602\n",
      "Iteration 9820, loss = 1.43737621\n",
      "Iteration 9821, loss = 1.43734639\n",
      "Iteration 9822, loss = 1.43732372\n",
      "Iteration 9823, loss = 1.43730061\n",
      "Iteration 9824, loss = 1.43727711\n",
      "Iteration 9825, loss = 1.43725323\n",
      "Iteration 9826, loss = 1.43722904\n",
      "Iteration 9827, loss = 1.43720456\n",
      "Iteration 9828, loss = 1.43717984\n",
      "Iteration 9829, loss = 1.43715489\n",
      "Iteration 9830, loss = 1.43712975\n",
      "Iteration 9831, loss = 1.43710998\n",
      "Iteration 9832, loss = 1.43708179\n",
      "Iteration 9833, loss = 1.43705951\n",
      "Iteration 9834, loss = 1.43703699\n",
      "Iteration 9835, loss = 1.43701402\n",
      "Iteration 9836, loss = 1.43699065\n",
      "Iteration 9837, loss = 1.43696688\n",
      "Iteration 9838, loss = 1.43694277\n",
      "Iteration 9839, loss = 1.43691833\n",
      "Iteration 9840, loss = 1.43689362\n",
      "Iteration 9841, loss = 1.43686866\n",
      "Iteration 9842, loss = 1.43684353\n",
      "Iteration 9843, loss = 1.43682269\n",
      "Iteration 9844, loss = 1.43680017\n",
      "Iteration 9845, loss = 1.43677187\n",
      "Iteration 9846, loss = 1.43674873\n",
      "Iteration 9847, loss = 1.43672608\n",
      "Iteration 9848, loss = 1.43670302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9849, loss = 1.43667956\n",
      "Iteration 9850, loss = 1.43665574\n",
      "Iteration 9851, loss = 1.43663159\n",
      "Iteration 9852, loss = 1.43660714\n",
      "Iteration 9853, loss = 1.43658244\n",
      "Iteration 9854, loss = 1.43655885\n",
      "Iteration 9855, loss = 1.43653434\n",
      "Iteration 9856, loss = 1.43651080\n",
      "Iteration 9857, loss = 1.43648692\n",
      "Iteration 9858, loss = 1.43646275\n",
      "Iteration 9859, loss = 1.43643829\n",
      "Iteration 9860, loss = 1.43641851\n",
      "Iteration 9861, loss = 1.43639061\n",
      "Iteration 9862, loss = 1.43636724\n",
      "Iteration 9863, loss = 1.43634351\n",
      "Iteration 9864, loss = 1.43631946\n",
      "Iteration 9865, loss = 1.43629511\n",
      "Iteration 9866, loss = 1.43627099\n",
      "Iteration 9867, loss = 1.43624680\n",
      "Iteration 9868, loss = 1.43622307\n",
      "Iteration 9869, loss = 1.43619934\n",
      "Iteration 9870, loss = 1.43617557\n",
      "Iteration 9871, loss = 1.43615253\n",
      "Iteration 9872, loss = 1.43612891\n",
      "Iteration 9873, loss = 1.43610590\n",
      "Iteration 9874, loss = 1.43608250\n",
      "Iteration 9875, loss = 1.43605874\n",
      "Iteration 9876, loss = 1.43603467\n",
      "Iteration 9877, loss = 1.43601032\n",
      "Iteration 9878, loss = 1.43598575\n",
      "Iteration 9879, loss = 1.43596523\n",
      "Iteration 9880, loss = 1.43593777\n",
      "Iteration 9881, loss = 1.43591424\n",
      "Iteration 9882, loss = 1.43589042\n",
      "Iteration 9883, loss = 1.43586634\n",
      "Iteration 9884, loss = 1.43584810\n",
      "Iteration 9885, loss = 1.43582188\n",
      "Iteration 9886, loss = 1.43580126\n",
      "Iteration 9887, loss = 1.43578003\n",
      "Iteration 9888, loss = 1.43575823\n",
      "Iteration 9889, loss = 1.43573590\n",
      "Iteration 9890, loss = 1.43571310\n",
      "Iteration 9891, loss = 1.43568986\n",
      "Iteration 9892, loss = 1.43566623\n",
      "Iteration 9893, loss = 1.43564226\n",
      "Iteration 9894, loss = 1.43561797\n",
      "Iteration 9895, loss = 1.43559341\n",
      "Iteration 9896, loss = 1.43556863\n",
      "Iteration 9897, loss = 1.43554367\n",
      "Iteration 9898, loss = 1.43551853\n",
      "Iteration 9899, loss = 1.43549326\n",
      "Iteration 9900, loss = 1.43546787\n",
      "Iteration 9901, loss = 1.43544264\n",
      "Iteration 9902, loss = 1.43541985\n",
      "Iteration 9903, loss = 1.43539465\n",
      "Iteration 9904, loss = 1.43537129\n",
      "Iteration 9905, loss = 1.43534766\n",
      "Iteration 9906, loss = 1.43532524\n",
      "Iteration 9907, loss = 1.43530071\n",
      "Iteration 9908, loss = 1.43527812\n",
      "Iteration 9909, loss = 1.43525524\n",
      "Iteration 9910, loss = 1.43523202\n",
      "Iteration 9911, loss = 1.43520847\n",
      "Iteration 9912, loss = 1.43518467\n",
      "Iteration 9913, loss = 1.43516059\n",
      "Iteration 9914, loss = 1.43513627\n",
      "Iteration 9915, loss = 1.43511171\n",
      "Iteration 9916, loss = 1.43509267\n",
      "Iteration 9917, loss = 1.43506544\n",
      "Iteration 9918, loss = 1.43504136\n",
      "Iteration 9919, loss = 1.43501846\n",
      "Iteration 9920, loss = 1.43499519\n",
      "Iteration 9921, loss = 1.43497160\n",
      "Iteration 9922, loss = 1.43494773\n",
      "Iteration 9923, loss = 1.43492467\n",
      "Iteration 9924, loss = 1.43490055\n",
      "Iteration 9925, loss = 1.43487728\n",
      "Iteration 9926, loss = 1.43485407\n",
      "Iteration 9927, loss = 1.43483055\n",
      "Iteration 9928, loss = 1.43480672\n",
      "Iteration 9929, loss = 1.43478261\n",
      "Iteration 9930, loss = 1.43475909\n",
      "Iteration 9931, loss = 1.43473501\n",
      "Iteration 9932, loss = 1.43471087\n",
      "Iteration 9933, loss = 1.43468707\n",
      "Iteration 9934, loss = 1.43466632\n",
      "Iteration 9935, loss = 1.43464173\n",
      "Iteration 9936, loss = 1.43461996\n",
      "Iteration 9937, loss = 1.43459773\n",
      "Iteration 9938, loss = 1.43457505\n",
      "Iteration 9939, loss = 1.43455196\n",
      "Iteration 9940, loss = 1.43452850\n",
      "Iteration 9941, loss = 1.43450471\n",
      "Iteration 9942, loss = 1.43448063\n",
      "Iteration 9943, loss = 1.43445629\n",
      "Iteration 9944, loss = 1.43443175\n",
      "Iteration 9945, loss = 1.43440705\n",
      "Iteration 9946, loss = 1.43438462\n",
      "Iteration 9947, loss = 1.43436189\n",
      "Iteration 9948, loss = 1.43433757\n",
      "Iteration 9949, loss = 1.43431655\n",
      "Iteration 9950, loss = 1.43429500\n",
      "Iteration 9951, loss = 1.43427296\n",
      "Iteration 9952, loss = 1.43425046\n",
      "Iteration 9953, loss = 1.43422755\n",
      "Iteration 9954, loss = 1.43420428\n",
      "Iteration 9955, loss = 1.43418068\n",
      "Iteration 9956, loss = 1.43415679\n",
      "Iteration 9957, loss = 1.43413269\n",
      "Iteration 9958, loss = 1.43410837\n",
      "Iteration 9959, loss = 1.43408386\n",
      "Iteration 9960, loss = 1.43405961\n",
      "Iteration 9961, loss = 1.43403664\n",
      "Iteration 9962, loss = 1.43401224\n",
      "Iteration 9963, loss = 1.43398885\n",
      "Iteration 9964, loss = 1.43396518\n",
      "Iteration 9965, loss = 1.43394128\n",
      "Iteration 9966, loss = 1.43391719\n",
      "Iteration 9967, loss = 1.43389289\n",
      "Iteration 9968, loss = 1.43387313\n",
      "Iteration 9969, loss = 1.43384560\n",
      "Iteration 9970, loss = 1.43382241\n",
      "Iteration 9971, loss = 1.43379893\n",
      "Iteration 9972, loss = 1.43377603\n",
      "Iteration 9973, loss = 1.43375210\n",
      "Iteration 9974, loss = 1.43372933\n",
      "Iteration 9975, loss = 1.43370589\n",
      "Iteration 9976, loss = 1.43368273\n",
      "Iteration 9977, loss = 1.43365927\n",
      "Iteration 9978, loss = 1.43363553\n",
      "Iteration 9979, loss = 1.43361154\n",
      "Iteration 9980, loss = 1.43359111\n",
      "Iteration 9981, loss = 1.43356545\n",
      "Iteration 9982, loss = 1.43354316\n",
      "Iteration 9983, loss = 1.43352046\n",
      "Iteration 9984, loss = 1.43349740\n",
      "Iteration 9985, loss = 1.43347401\n",
      "Iteration 9986, loss = 1.43345033\n",
      "Iteration 9987, loss = 1.43342638\n",
      "Iteration 9988, loss = 1.43340219\n",
      "Iteration 9989, loss = 1.43337781\n",
      "Iteration 9990, loss = 1.43336084\n",
      "Iteration 9991, loss = 1.43333338\n",
      "Iteration 9992, loss = 1.43331292\n",
      "Iteration 9993, loss = 1.43329190\n",
      "Iteration 9994, loss = 1.43327035\n",
      "Iteration 9995, loss = 1.43324832\n",
      "Iteration 9996, loss = 1.43322585\n",
      "Iteration 9997, loss = 1.43320298\n",
      "Iteration 9998, loss = 1.43317980\n",
      "Iteration 9999, loss = 1.43315630\n",
      "Iteration 10000, loss = 1.43313253\n",
      "Iteration 10001, loss = 1.43310850\n",
      "Iteration 10002, loss = 1.43308427\n",
      "Iteration 10003, loss = 1.43305985\n",
      "Iteration 10004, loss = 1.43303527\n",
      "Iteration 10005, loss = 1.43301054\n",
      "Iteration 10006, loss = 1.43298569\n",
      "Iteration 10007, loss = 1.43296490\n",
      "Iteration 10008, loss = 1.43294338\n",
      "Iteration 10009, loss = 1.43291463\n",
      "Iteration 10010, loss = 1.43289234\n",
      "Iteration 10011, loss = 1.43286970\n",
      "Iteration 10012, loss = 1.43284674\n",
      "Iteration 10013, loss = 1.43282351\n",
      "Iteration 10014, loss = 1.43280004\n",
      "Iteration 10015, loss = 1.43277633\n",
      "Iteration 10016, loss = 1.43275241\n",
      "Iteration 10017, loss = 1.43272858\n",
      "Iteration 10018, loss = 1.43270505\n",
      "Iteration 10019, loss = 1.43268295\n",
      "Iteration 10020, loss = 1.43265967\n",
      "Iteration 10021, loss = 1.43263745\n",
      "Iteration 10022, loss = 1.43261489\n",
      "Iteration 10023, loss = 1.43259199\n",
      "Iteration 10024, loss = 1.43256877\n",
      "Iteration 10025, loss = 1.43254529\n",
      "Iteration 10026, loss = 1.43252158\n",
      "Iteration 10027, loss = 1.43249888\n",
      "Iteration 10028, loss = 1.43247449\n",
      "Iteration 10029, loss = 1.43245173\n",
      "Iteration 10030, loss = 1.43242875\n",
      "Iteration 10031, loss = 1.43240549\n",
      "Iteration 10032, loss = 1.43238200\n",
      "Iteration 10033, loss = 1.43235830\n",
      "Iteration 10034, loss = 1.43233539\n",
      "Iteration 10035, loss = 1.43231264\n",
      "Iteration 10036, loss = 1.43229048\n",
      "Iteration 10037, loss = 1.43226926\n",
      "Iteration 10038, loss = 1.43224758\n",
      "Iteration 10039, loss = 1.43222548\n",
      "Iteration 10040, loss = 1.43220301\n",
      "Iteration 10041, loss = 1.43218019\n",
      "Iteration 10042, loss = 1.43215704\n",
      "Iteration 10043, loss = 1.43213361\n",
      "Iteration 10044, loss = 1.43210992\n",
      "Iteration 10045, loss = 1.43208602\n",
      "Iteration 10046, loss = 1.43206192\n",
      "Iteration 10047, loss = 1.43203765\n",
      "Iteration 10048, loss = 1.43201495\n",
      "Iteration 10049, loss = 1.43199384\n",
      "Iteration 10050, loss = 1.43196836\n",
      "Iteration 10051, loss = 1.43194672\n",
      "Iteration 10052, loss = 1.43192466\n",
      "Iteration 10053, loss = 1.43190220\n",
      "Iteration 10054, loss = 1.43187939\n",
      "Iteration 10055, loss = 1.43185626\n",
      "Iteration 10056, loss = 1.43183284\n",
      "Iteration 10057, loss = 1.43180917\n",
      "Iteration 10058, loss = 1.43178528\n",
      "Iteration 10059, loss = 1.43176121\n",
      "Iteration 10060, loss = 1.43173700\n",
      "Iteration 10061, loss = 1.43171289\n",
      "Iteration 10062, loss = 1.43168987\n",
      "Iteration 10063, loss = 1.43166786\n",
      "Iteration 10064, loss = 1.43164429\n",
      "Iteration 10065, loss = 1.43162149\n",
      "Iteration 10066, loss = 1.43159839\n",
      "Iteration 10067, loss = 1.43157545\n",
      "Iteration 10068, loss = 1.43155238\n",
      "Iteration 10069, loss = 1.43152943\n",
      "Iteration 10070, loss = 1.43150671\n",
      "Iteration 10071, loss = 1.43148378\n",
      "Iteration 10072, loss = 1.43146101\n",
      "Iteration 10073, loss = 1.43143797\n",
      "Iteration 10074, loss = 1.43141467\n",
      "Iteration 10075, loss = 1.43139139\n",
      "Iteration 10076, loss = 1.43136844\n",
      "Iteration 10077, loss = 1.43134544\n",
      "Iteration 10078, loss = 1.43132220\n",
      "Iteration 10079, loss = 1.43130166\n",
      "Iteration 10080, loss = 1.43127759\n",
      "Iteration 10081, loss = 1.43125608\n",
      "Iteration 10082, loss = 1.43123415\n",
      "Iteration 10083, loss = 1.43121183\n",
      "Iteration 10084, loss = 1.43118915\n",
      "Iteration 10085, loss = 1.43116615\n",
      "Iteration 10086, loss = 1.43114286\n",
      "Iteration 10087, loss = 1.43111931\n",
      "Iteration 10088, loss = 1.43109555\n",
      "Iteration 10089, loss = 1.43107163\n",
      "Iteration 10090, loss = 1.43105446\n",
      "Iteration 10091, loss = 1.43102583\n",
      "Iteration 10092, loss = 1.43100359\n",
      "Iteration 10093, loss = 1.43098156\n",
      "Iteration 10094, loss = 1.43095917\n",
      "Iteration 10095, loss = 1.43093645\n",
      "Iteration 10096, loss = 1.43091343\n",
      "Iteration 10097, loss = 1.43089014\n",
      "Iteration 10098, loss = 1.43086877\n",
      "Iteration 10099, loss = 1.43084465\n",
      "Iteration 10100, loss = 1.43082140\n",
      "Iteration 10101, loss = 1.43079876\n",
      "Iteration 10102, loss = 1.43077635\n",
      "Iteration 10103, loss = 1.43075353\n",
      "Iteration 10104, loss = 1.43073091\n",
      "Iteration 10105, loss = 1.43070801\n",
      "Iteration 10106, loss = 1.43068487\n",
      "Iteration 10107, loss = 1.43066183\n",
      "Iteration 10108, loss = 1.43063892\n",
      "Iteration 10109, loss = 1.43061601\n",
      "Iteration 10110, loss = 1.43059286\n",
      "Iteration 10111, loss = 1.43057047\n",
      "Iteration 10112, loss = 1.43054743\n",
      "Iteration 10113, loss = 1.43052509\n",
      "Iteration 10114, loss = 1.43050244\n",
      "Iteration 10115, loss = 1.43047952\n",
      "Iteration 10116, loss = 1.43045636\n",
      "Iteration 10117, loss = 1.43043297\n",
      "Iteration 10118, loss = 1.43041152\n",
      "Iteration 10119, loss = 1.43038777\n",
      "Iteration 10120, loss = 1.43036460\n",
      "Iteration 10121, loss = 1.43034224\n",
      "Iteration 10122, loss = 1.43031966\n",
      "Iteration 10123, loss = 1.43029682\n",
      "Iteration 10124, loss = 1.43027373\n",
      "Iteration 10125, loss = 1.43025081\n",
      "Iteration 10126, loss = 1.43022785\n",
      "Iteration 10127, loss = 1.43020500\n",
      "Iteration 10128, loss = 1.43018283\n",
      "Iteration 10129, loss = 1.43016040\n",
      "Iteration 10130, loss = 1.43013854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10131, loss = 1.43011633\n",
      "Iteration 10132, loss = 1.43009379\n",
      "Iteration 10133, loss = 1.43007095\n",
      "Iteration 10134, loss = 1.43004785\n",
      "Iteration 10135, loss = 1.43002455\n",
      "Iteration 10136, loss = 1.43000233\n",
      "Iteration 10137, loss = 1.42997975\n",
      "Iteration 10138, loss = 1.42995588\n",
      "Iteration 10139, loss = 1.42993304\n",
      "Iteration 10140, loss = 1.42991046\n",
      "Iteration 10141, loss = 1.42988762\n",
      "Iteration 10142, loss = 1.42986473\n",
      "Iteration 10143, loss = 1.42984224\n",
      "Iteration 10144, loss = 1.42981965\n",
      "Iteration 10145, loss = 1.42979677\n",
      "Iteration 10146, loss = 1.42977572\n",
      "Iteration 10147, loss = 1.42975198\n",
      "Iteration 10148, loss = 1.42972940\n",
      "Iteration 10149, loss = 1.42970724\n",
      "Iteration 10150, loss = 1.42968474\n",
      "Iteration 10151, loss = 1.42966194\n",
      "Iteration 10152, loss = 1.42963888\n",
      "Iteration 10153, loss = 1.42961838\n",
      "Iteration 10154, loss = 1.42959391\n",
      "Iteration 10155, loss = 1.42957186\n",
      "Iteration 10156, loss = 1.42954950\n",
      "Iteration 10157, loss = 1.42952686\n",
      "Iteration 10158, loss = 1.42950397\n",
      "Iteration 10159, loss = 1.42948086\n",
      "Iteration 10160, loss = 1.42945756\n",
      "Iteration 10161, loss = 1.42943565\n",
      "Iteration 10162, loss = 1.42941208\n",
      "Iteration 10163, loss = 1.42939017\n",
      "Iteration 10164, loss = 1.42936868\n",
      "Iteration 10165, loss = 1.42934685\n",
      "Iteration 10166, loss = 1.42932468\n",
      "Iteration 10167, loss = 1.42930219\n",
      "Iteration 10168, loss = 1.42927941\n",
      "Iteration 10169, loss = 1.42925638\n",
      "Iteration 10170, loss = 1.42923312\n",
      "Iteration 10171, loss = 1.42920968\n",
      "Iteration 10172, loss = 1.42918962\n",
      "Iteration 10173, loss = 1.42916721\n",
      "Iteration 10174, loss = 1.42914218\n",
      "Iteration 10175, loss = 1.42911960\n",
      "Iteration 10176, loss = 1.42909776\n",
      "Iteration 10177, loss = 1.42907560\n",
      "Iteration 10178, loss = 1.42905314\n",
      "Iteration 10179, loss = 1.42903042\n",
      "Iteration 10180, loss = 1.42900746\n",
      "Iteration 10181, loss = 1.42898449\n",
      "Iteration 10182, loss = 1.42896214\n",
      "Iteration 10183, loss = 1.42893941\n",
      "Iteration 10184, loss = 1.42891698\n",
      "Iteration 10185, loss = 1.42889429\n",
      "Iteration 10186, loss = 1.42887138\n",
      "Iteration 10187, loss = 1.42884840\n",
      "Iteration 10188, loss = 1.42882592\n",
      "Iteration 10189, loss = 1.42880368\n",
      "Iteration 10190, loss = 1.42878147\n",
      "Iteration 10191, loss = 1.42875903\n",
      "Iteration 10192, loss = 1.42873636\n",
      "Iteration 10193, loss = 1.42871424\n",
      "Iteration 10194, loss = 1.42869117\n",
      "Iteration 10195, loss = 1.42866862\n",
      "Iteration 10196, loss = 1.42864690\n",
      "Iteration 10197, loss = 1.42862362\n",
      "Iteration 10198, loss = 1.42860114\n",
      "Iteration 10199, loss = 1.42857863\n",
      "Iteration 10200, loss = 1.42855649\n",
      "Iteration 10201, loss = 1.42853426\n",
      "Iteration 10202, loss = 1.42851174\n",
      "Iteration 10203, loss = 1.42848897\n",
      "Iteration 10204, loss = 1.42846649\n",
      "Iteration 10205, loss = 1.42844379\n",
      "Iteration 10206, loss = 1.42842179\n",
      "Iteration 10207, loss = 1.42839968\n",
      "Iteration 10208, loss = 1.42837768\n",
      "Iteration 10209, loss = 1.42835538\n",
      "Iteration 10210, loss = 1.42833281\n",
      "Iteration 10211, loss = 1.42831000\n",
      "Iteration 10212, loss = 1.42828698\n",
      "Iteration 10213, loss = 1.42826444\n",
      "Iteration 10214, loss = 1.42824164\n",
      "Iteration 10215, loss = 1.42821980\n",
      "Iteration 10216, loss = 1.42819785\n",
      "Iteration 10217, loss = 1.42817559\n",
      "Iteration 10218, loss = 1.42815306\n",
      "Iteration 10219, loss = 1.42813029\n",
      "Iteration 10220, loss = 1.42810731\n",
      "Iteration 10221, loss = 1.42809026\n",
      "Iteration 10222, loss = 1.42806357\n",
      "Iteration 10223, loss = 1.42804261\n",
      "Iteration 10224, loss = 1.42802127\n",
      "Iteration 10225, loss = 1.42799958\n",
      "Iteration 10226, loss = 1.42797754\n",
      "Iteration 10227, loss = 1.42795521\n",
      "Iteration 10228, loss = 1.42793261\n",
      "Iteration 10229, loss = 1.42790977\n",
      "Iteration 10230, loss = 1.42788672\n",
      "Iteration 10231, loss = 1.42786348\n",
      "Iteration 10232, loss = 1.42784095\n",
      "Iteration 10233, loss = 1.42781882\n",
      "Iteration 10234, loss = 1.42779641\n",
      "Iteration 10235, loss = 1.42777379\n",
      "Iteration 10236, loss = 1.42775202\n",
      "Iteration 10237, loss = 1.42772901\n",
      "Iteration 10238, loss = 1.42770676\n",
      "Iteration 10239, loss = 1.42768427\n",
      "Iteration 10240, loss = 1.42766330\n",
      "Iteration 10241, loss = 1.42763944\n",
      "Iteration 10242, loss = 1.42761708\n",
      "Iteration 10243, loss = 1.42759448\n",
      "Iteration 10244, loss = 1.42757344\n",
      "Iteration 10245, loss = 1.42755018\n",
      "Iteration 10246, loss = 1.42752838\n",
      "Iteration 10247, loss = 1.42750630\n",
      "Iteration 10248, loss = 1.42748393\n",
      "Iteration 10249, loss = 1.42746133\n",
      "Iteration 10250, loss = 1.42743851\n",
      "Iteration 10251, loss = 1.42741835\n",
      "Iteration 10252, loss = 1.42739427\n",
      "Iteration 10253, loss = 1.42737269\n",
      "Iteration 10254, loss = 1.42735080\n",
      "Iteration 10255, loss = 1.42732862\n",
      "Iteration 10256, loss = 1.42730617\n",
      "Iteration 10257, loss = 1.42728351\n",
      "Iteration 10258, loss = 1.42726198\n",
      "Iteration 10259, loss = 1.42723867\n",
      "Iteration 10260, loss = 1.42721687\n",
      "Iteration 10261, loss = 1.42719500\n",
      "Iteration 10262, loss = 1.42717285\n",
      "Iteration 10263, loss = 1.42715046\n",
      "Iteration 10264, loss = 1.42712788\n",
      "Iteration 10265, loss = 1.42710673\n",
      "Iteration 10266, loss = 1.42708300\n",
      "Iteration 10267, loss = 1.42706065\n",
      "Iteration 10268, loss = 1.42703920\n",
      "Iteration 10269, loss = 1.42701596\n",
      "Iteration 10270, loss = 1.42699418\n",
      "Iteration 10271, loss = 1.42697214\n",
      "Iteration 10272, loss = 1.42694984\n",
      "Iteration 10273, loss = 1.42692730\n",
      "Iteration 10274, loss = 1.42690510\n",
      "Iteration 10275, loss = 1.42688354\n",
      "Iteration 10276, loss = 1.42686223\n",
      "Iteration 10277, loss = 1.42684056\n",
      "Iteration 10278, loss = 1.42681858\n",
      "Iteration 10279, loss = 1.42679630\n",
      "Iteration 10280, loss = 1.42677377\n",
      "Iteration 10281, loss = 1.42675271\n",
      "Iteration 10282, loss = 1.42672893\n",
      "Iteration 10283, loss = 1.42670740\n",
      "Iteration 10284, loss = 1.42668557\n",
      "Iteration 10285, loss = 1.42666348\n",
      "Iteration 10286, loss = 1.42664119\n",
      "Iteration 10287, loss = 1.42661871\n",
      "Iteration 10288, loss = 1.42659682\n",
      "Iteration 10289, loss = 1.42657405\n",
      "Iteration 10290, loss = 1.42655181\n",
      "Iteration 10291, loss = 1.42652943\n",
      "Iteration 10292, loss = 1.42650756\n",
      "Iteration 10293, loss = 1.42648550\n",
      "Iteration 10294, loss = 1.42646324\n",
      "Iteration 10295, loss = 1.42644228\n",
      "Iteration 10296, loss = 1.42641986\n",
      "Iteration 10297, loss = 1.42639860\n",
      "Iteration 10298, loss = 1.42637701\n",
      "Iteration 10299, loss = 1.42635512\n",
      "Iteration 10300, loss = 1.42633297\n",
      "Iteration 10301, loss = 1.42631060\n",
      "Iteration 10302, loss = 1.42628802\n",
      "Iteration 10303, loss = 1.42626525\n",
      "Iteration 10304, loss = 1.42624233\n",
      "Iteration 10305, loss = 1.42622542\n",
      "Iteration 10306, loss = 1.42619953\n",
      "Iteration 10307, loss = 1.42617939\n",
      "Iteration 10308, loss = 1.42615887\n",
      "Iteration 10309, loss = 1.42613794\n",
      "Iteration 10310, loss = 1.42611662\n",
      "Iteration 10311, loss = 1.42609497\n",
      "Iteration 10312, loss = 1.42607301\n",
      "Iteration 10313, loss = 1.42605077\n",
      "Iteration 10314, loss = 1.42602828\n",
      "Iteration 10315, loss = 1.42600558\n",
      "Iteration 10316, loss = 1.42598269\n",
      "Iteration 10317, loss = 1.42595967\n",
      "Iteration 10318, loss = 1.42593654\n",
      "Iteration 10319, loss = 1.42591330\n",
      "Iteration 10320, loss = 1.42589100\n",
      "Iteration 10321, loss = 1.42586846\n",
      "Iteration 10322, loss = 1.42584668\n",
      "Iteration 10323, loss = 1.42582466\n",
      "Iteration 10324, loss = 1.42580241\n",
      "Iteration 10325, loss = 1.42577997\n",
      "Iteration 10326, loss = 1.42575889\n",
      "Iteration 10327, loss = 1.42573624\n",
      "Iteration 10328, loss = 1.42571479\n",
      "Iteration 10329, loss = 1.42569308\n",
      "Iteration 10330, loss = 1.42567111\n",
      "Iteration 10331, loss = 1.42564891\n",
      "Iteration 10332, loss = 1.42562651\n",
      "Iteration 10333, loss = 1.42560502\n",
      "Iteration 10334, loss = 1.42558210\n",
      "Iteration 10335, loss = 1.42556003\n",
      "Iteration 10336, loss = 1.42553861\n",
      "Iteration 10337, loss = 1.42551790\n",
      "Iteration 10338, loss = 1.42549767\n",
      "Iteration 10339, loss = 1.42547706\n",
      "Iteration 10340, loss = 1.42545607\n",
      "Iteration 10341, loss = 1.42543473\n",
      "Iteration 10342, loss = 1.42541311\n",
      "Iteration 10343, loss = 1.42539121\n",
      "Iteration 10344, loss = 1.42536907\n",
      "Iteration 10345, loss = 1.42534670\n",
      "Iteration 10346, loss = 1.42532414\n",
      "Iteration 10347, loss = 1.42530143\n",
      "Iteration 10348, loss = 1.42527857\n",
      "Iteration 10349, loss = 1.42525557\n",
      "Iteration 10350, loss = 1.42524147\n",
      "Iteration 10351, loss = 1.42521225\n",
      "Iteration 10352, loss = 1.42519167\n",
      "Iteration 10353, loss = 1.42517081\n",
      "Iteration 10354, loss = 1.42514965\n",
      "Iteration 10355, loss = 1.42512818\n",
      "Iteration 10356, loss = 1.42510643\n",
      "Iteration 10357, loss = 1.42508443\n",
      "Iteration 10358, loss = 1.42506218\n",
      "Iteration 10359, loss = 1.42503972\n",
      "Iteration 10360, loss = 1.42501707\n",
      "Iteration 10361, loss = 1.42499427\n",
      "Iteration 10362, loss = 1.42497135\n",
      "Iteration 10363, loss = 1.42495191\n",
      "Iteration 10364, loss = 1.42492686\n",
      "Iteration 10365, loss = 1.42490513\n",
      "Iteration 10366, loss = 1.42488318\n",
      "Iteration 10367, loss = 1.42486103\n",
      "Iteration 10368, loss = 1.42483869\n",
      "Iteration 10369, loss = 1.42481687\n",
      "Iteration 10370, loss = 1.42479446\n",
      "Iteration 10371, loss = 1.42477271\n",
      "Iteration 10372, loss = 1.42475117\n",
      "Iteration 10373, loss = 1.42472959\n",
      "Iteration 10374, loss = 1.42470778\n",
      "Iteration 10375, loss = 1.42468574\n",
      "Iteration 10376, loss = 1.42466444\n",
      "Iteration 10377, loss = 1.42464192\n",
      "Iteration 10378, loss = 1.42462032\n",
      "Iteration 10379, loss = 1.42459866\n",
      "Iteration 10380, loss = 1.42457698\n",
      "Iteration 10381, loss = 1.42455508\n",
      "Iteration 10382, loss = 1.42453298\n",
      "Iteration 10383, loss = 1.42451372\n",
      "Iteration 10384, loss = 1.42449009\n",
      "Iteration 10385, loss = 1.42446922\n",
      "Iteration 10386, loss = 1.42444802\n",
      "Iteration 10387, loss = 1.42442653\n",
      "Iteration 10388, loss = 1.42440477\n",
      "Iteration 10389, loss = 1.42438276\n",
      "Iteration 10390, loss = 1.42436054\n",
      "Iteration 10391, loss = 1.42433905\n",
      "Iteration 10392, loss = 1.42431641\n",
      "Iteration 10393, loss = 1.42429485\n",
      "Iteration 10394, loss = 1.42427324\n",
      "Iteration 10395, loss = 1.42425175\n",
      "Iteration 10396, loss = 1.42423000\n",
      "Iteration 10397, loss = 1.42420801\n",
      "Iteration 10398, loss = 1.42418581\n",
      "Iteration 10399, loss = 1.42416492\n",
      "Iteration 10400, loss = 1.42414192\n",
      "Iteration 10401, loss = 1.42412083\n",
      "Iteration 10402, loss = 1.42409959\n",
      "Iteration 10403, loss = 1.42407808\n",
      "Iteration 10404, loss = 1.42405631\n",
      "Iteration 10405, loss = 1.42403445\n",
      "Iteration 10406, loss = 1.42401276\n",
      "Iteration 10407, loss = 1.42399103\n",
      "Iteration 10408, loss = 1.42396990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10409, loss = 1.42394856\n",
      "Iteration 10410, loss = 1.42392695\n",
      "Iteration 10411, loss = 1.42390510\n",
      "Iteration 10412, loss = 1.42388439\n",
      "Iteration 10413, loss = 1.42386163\n",
      "Iteration 10414, loss = 1.42384000\n",
      "Iteration 10415, loss = 1.42381817\n",
      "Iteration 10416, loss = 1.42379615\n",
      "Iteration 10417, loss = 1.42377499\n",
      "Iteration 10418, loss = 1.42375258\n",
      "Iteration 10419, loss = 1.42373092\n",
      "Iteration 10420, loss = 1.42370935\n",
      "Iteration 10421, loss = 1.42368757\n",
      "Iteration 10422, loss = 1.42366808\n",
      "Iteration 10423, loss = 1.42364601\n",
      "Iteration 10424, loss = 1.42362610\n",
      "Iteration 10425, loss = 1.42360578\n",
      "Iteration 10426, loss = 1.42358509\n",
      "Iteration 10427, loss = 1.42356407\n",
      "Iteration 10428, loss = 1.42354273\n",
      "Iteration 10429, loss = 1.42352112\n",
      "Iteration 10430, loss = 1.42349926\n",
      "Iteration 10431, loss = 1.42347718\n",
      "Iteration 10432, loss = 1.42345492\n",
      "Iteration 10433, loss = 1.42343254\n",
      "Iteration 10434, loss = 1.42341003\n",
      "Iteration 10435, loss = 1.42338740\n",
      "Iteration 10436, loss = 1.42336470\n",
      "Iteration 10437, loss = 1.42334573\n",
      "Iteration 10438, loss = 1.42332604\n",
      "Iteration 10439, loss = 1.42329963\n",
      "Iteration 10440, loss = 1.42327900\n",
      "Iteration 10441, loss = 1.42325811\n",
      "Iteration 10442, loss = 1.42323695\n",
      "Iteration 10443, loss = 1.42321554\n",
      "Iteration 10444, loss = 1.42319390\n",
      "Iteration 10445, loss = 1.42317206\n",
      "Iteration 10446, loss = 1.42315003\n",
      "Iteration 10447, loss = 1.42312785\n",
      "Iteration 10448, loss = 1.42310821\n",
      "Iteration 10449, loss = 1.42308511\n",
      "Iteration 10450, loss = 1.42306284\n",
      "Iteration 10451, loss = 1.42304155\n",
      "Iteration 10452, loss = 1.42302003\n",
      "Iteration 10453, loss = 1.42299832\n",
      "Iteration 10454, loss = 1.42297643\n",
      "Iteration 10455, loss = 1.42295639\n",
      "Iteration 10456, loss = 1.42293476\n",
      "Iteration 10457, loss = 1.42291164\n",
      "Iteration 10458, loss = 1.42289065\n",
      "Iteration 10459, loss = 1.42286955\n",
      "Iteration 10460, loss = 1.42284821\n",
      "Iteration 10461, loss = 1.42282664\n",
      "Iteration 10462, loss = 1.42280484\n",
      "Iteration 10463, loss = 1.42278470\n",
      "Iteration 10464, loss = 1.42276149\n",
      "Iteration 10465, loss = 1.42274003\n",
      "Iteration 10466, loss = 1.42271897\n",
      "Iteration 10467, loss = 1.42269777\n",
      "Iteration 10468, loss = 1.42267631\n",
      "Iteration 10469, loss = 1.42265462\n",
      "Iteration 10470, loss = 1.42263289\n",
      "Iteration 10471, loss = 1.42261161\n",
      "Iteration 10472, loss = 1.42259027\n",
      "Iteration 10473, loss = 1.42256873\n",
      "Iteration 10474, loss = 1.42254785\n",
      "Iteration 10475, loss = 1.42252688\n",
      "Iteration 10476, loss = 1.42250646\n",
      "Iteration 10477, loss = 1.42248571\n",
      "Iteration 10478, loss = 1.42246469\n",
      "Iteration 10479, loss = 1.42244339\n",
      "Iteration 10480, loss = 1.42242185\n",
      "Iteration 10481, loss = 1.42240010\n",
      "Iteration 10482, loss = 1.42237814\n",
      "Iteration 10483, loss = 1.42235603\n",
      "Iteration 10484, loss = 1.42233380\n",
      "Iteration 10485, loss = 1.42231146\n",
      "Iteration 10486, loss = 1.42229596\n",
      "Iteration 10487, loss = 1.42227002\n",
      "Iteration 10488, loss = 1.42225066\n",
      "Iteration 10489, loss = 1.42223092\n",
      "Iteration 10490, loss = 1.42221078\n",
      "Iteration 10491, loss = 1.42219029\n",
      "Iteration 10492, loss = 1.42216946\n",
      "Iteration 10493, loss = 1.42214834\n",
      "Iteration 10494, loss = 1.42212695\n",
      "Iteration 10495, loss = 1.42210533\n",
      "Iteration 10496, loss = 1.42208350\n",
      "Iteration 10497, loss = 1.42206150\n",
      "Iteration 10498, loss = 1.42203936\n",
      "Iteration 10499, loss = 1.42201708\n",
      "Iteration 10500, loss = 1.42199471\n",
      "Iteration 10501, loss = 1.42197227\n",
      "Iteration 10502, loss = 1.42194977\n",
      "Iteration 10503, loss = 1.42193153\n",
      "Iteration 10504, loss = 1.42190680\n",
      "Iteration 10505, loss = 1.42188612\n",
      "Iteration 10506, loss = 1.42186518\n",
      "Iteration 10507, loss = 1.42184403\n",
      "Iteration 10508, loss = 1.42182267\n",
      "Iteration 10509, loss = 1.42180113\n",
      "Iteration 10510, loss = 1.42177943\n",
      "Iteration 10511, loss = 1.42175759\n",
      "Iteration 10512, loss = 1.42173561\n",
      "Iteration 10513, loss = 1.42171503\n",
      "Iteration 10514, loss = 1.42169436\n",
      "Iteration 10515, loss = 1.42167281\n",
      "Iteration 10516, loss = 1.42165323\n",
      "Iteration 10517, loss = 1.42163336\n",
      "Iteration 10518, loss = 1.42161314\n",
      "Iteration 10519, loss = 1.42159260\n",
      "Iteration 10520, loss = 1.42157175\n",
      "Iteration 10521, loss = 1.42155062\n",
      "Iteration 10522, loss = 1.42152924\n",
      "Iteration 10523, loss = 1.42150763\n",
      "Iteration 10524, loss = 1.42148582\n",
      "Iteration 10525, loss = 1.42146383\n",
      "Iteration 10526, loss = 1.42144170\n",
      "Iteration 10527, loss = 1.42141943\n",
      "Iteration 10528, loss = 1.42139709\n",
      "Iteration 10529, loss = 1.42137471\n",
      "Iteration 10530, loss = 1.42135501\n",
      "Iteration 10531, loss = 1.42133536\n",
      "Iteration 10532, loss = 1.42131118\n",
      "Iteration 10533, loss = 1.42129144\n",
      "Iteration 10534, loss = 1.42127140\n",
      "Iteration 10535, loss = 1.42125104\n",
      "Iteration 10536, loss = 1.42123038\n",
      "Iteration 10537, loss = 1.42120945\n",
      "Iteration 10538, loss = 1.42118826\n",
      "Iteration 10539, loss = 1.42116686\n",
      "Iteration 10540, loss = 1.42114526\n",
      "Iteration 10541, loss = 1.42112349\n",
      "Iteration 10542, loss = 1.42110157\n",
      "Iteration 10543, loss = 1.42107957\n",
      "Iteration 10544, loss = 1.42106002\n",
      "Iteration 10545, loss = 1.42103887\n",
      "Iteration 10546, loss = 1.42101565\n",
      "Iteration 10547, loss = 1.42099395\n",
      "Iteration 10548, loss = 1.42097299\n",
      "Iteration 10549, loss = 1.42095181\n",
      "Iteration 10550, loss = 1.42093044\n",
      "Iteration 10551, loss = 1.42090890\n",
      "Iteration 10552, loss = 1.42088860\n",
      "Iteration 10553, loss = 1.42086614\n",
      "Iteration 10554, loss = 1.42084519\n",
      "Iteration 10555, loss = 1.42082426\n",
      "Iteration 10556, loss = 1.42080341\n",
      "Iteration 10557, loss = 1.42078233\n",
      "Iteration 10558, loss = 1.42076104\n",
      "Iteration 10559, loss = 1.42073955\n",
      "Iteration 10560, loss = 1.42071789\n",
      "Iteration 10561, loss = 1.42069609\n",
      "Iteration 10562, loss = 1.42068078\n",
      "Iteration 10563, loss = 1.42065550\n",
      "Iteration 10564, loss = 1.42063645\n",
      "Iteration 10565, loss = 1.42061700\n",
      "Iteration 10566, loss = 1.42059716\n",
      "Iteration 10567, loss = 1.42057699\n",
      "Iteration 10568, loss = 1.42055651\n",
      "Iteration 10569, loss = 1.42053574\n",
      "Iteration 10570, loss = 1.42051470\n",
      "Iteration 10571, loss = 1.42049343\n",
      "Iteration 10572, loss = 1.42047196\n",
      "Iteration 10573, loss = 1.42045031\n",
      "Iteration 10574, loss = 1.42042853\n",
      "Iteration 10575, loss = 1.42040664\n",
      "Iteration 10576, loss = 1.42038464\n",
      "Iteration 10577, loss = 1.42036255\n",
      "Iteration 10578, loss = 1.42034041\n",
      "Iteration 10579, loss = 1.42031822\n",
      "Iteration 10580, loss = 1.42029844\n",
      "Iteration 10581, loss = 1.42027511\n",
      "Iteration 10582, loss = 1.42025403\n",
      "Iteration 10583, loss = 1.42023278\n",
      "Iteration 10584, loss = 1.42021149\n",
      "Iteration 10585, loss = 1.42019039\n",
      "Iteration 10586, loss = 1.42016929\n",
      "Iteration 10587, loss = 1.42014877\n",
      "Iteration 10588, loss = 1.42012810\n",
      "Iteration 10589, loss = 1.42010723\n",
      "Iteration 10590, loss = 1.42008615\n",
      "Iteration 10591, loss = 1.42006488\n",
      "Iteration 10592, loss = 1.42004344\n",
      "Iteration 10593, loss = 1.42002379\n",
      "Iteration 10594, loss = 1.42000109\n",
      "Iteration 10595, loss = 1.41998011\n",
      "Iteration 10596, loss = 1.41995908\n",
      "Iteration 10597, loss = 1.41993840\n",
      "Iteration 10598, loss = 1.41991763\n",
      "Iteration 10599, loss = 1.41989662\n",
      "Iteration 10600, loss = 1.41987540\n",
      "Iteration 10601, loss = 1.41985400\n",
      "Iteration 10602, loss = 1.41983471\n",
      "Iteration 10603, loss = 1.41981257\n",
      "Iteration 10604, loss = 1.41979240\n",
      "Iteration 10605, loss = 1.41977195\n",
      "Iteration 10606, loss = 1.41975123\n",
      "Iteration 10607, loss = 1.41973027\n",
      "Iteration 10608, loss = 1.41970908\n",
      "Iteration 10609, loss = 1.41968876\n",
      "Iteration 10610, loss = 1.41966701\n",
      "Iteration 10611, loss = 1.41964672\n",
      "Iteration 10612, loss = 1.41962573\n",
      "Iteration 10613, loss = 1.41960512\n",
      "Iteration 10614, loss = 1.41958428\n",
      "Iteration 10615, loss = 1.41956322\n",
      "Iteration 10616, loss = 1.41954198\n",
      "Iteration 10617, loss = 1.41952060\n",
      "Iteration 10618, loss = 1.41949910\n",
      "Iteration 10619, loss = 1.41947958\n",
      "Iteration 10620, loss = 1.41945722\n",
      "Iteration 10621, loss = 1.41943671\n",
      "Iteration 10622, loss = 1.41941599\n",
      "Iteration 10623, loss = 1.41939506\n",
      "Iteration 10624, loss = 1.41937394\n",
      "Iteration 10625, loss = 1.41935265\n",
      "Iteration 10626, loss = 1.41933136\n",
      "Iteration 10627, loss = 1.41931025\n",
      "Iteration 10628, loss = 1.41928934\n",
      "Iteration 10629, loss = 1.41926864\n",
      "Iteration 10630, loss = 1.41924796\n",
      "Iteration 10631, loss = 1.41922707\n",
      "Iteration 10632, loss = 1.41920597\n",
      "Iteration 10633, loss = 1.41918472\n",
      "Iteration 10634, loss = 1.41916635\n",
      "Iteration 10635, loss = 1.41914354\n",
      "Iteration 10636, loss = 1.41912352\n",
      "Iteration 10637, loss = 1.41910321\n",
      "Iteration 10638, loss = 1.41908264\n",
      "Iteration 10639, loss = 1.41906182\n",
      "Iteration 10640, loss = 1.41904078\n",
      "Iteration 10641, loss = 1.41901955\n",
      "Iteration 10642, loss = 1.41899817\n",
      "Iteration 10643, loss = 1.41898127\n",
      "Iteration 10644, loss = 1.41895655\n",
      "Iteration 10645, loss = 1.41893619\n",
      "Iteration 10646, loss = 1.41891561\n",
      "Iteration 10647, loss = 1.41889482\n",
      "Iteration 10648, loss = 1.41887382\n",
      "Iteration 10649, loss = 1.41885264\n",
      "Iteration 10650, loss = 1.41883132\n",
      "Iteration 10651, loss = 1.41881215\n",
      "Iteration 10652, loss = 1.41879099\n",
      "Iteration 10653, loss = 1.41876901\n",
      "Iteration 10654, loss = 1.41874880\n",
      "Iteration 10655, loss = 1.41872837\n",
      "Iteration 10656, loss = 1.41870771\n",
      "Iteration 10657, loss = 1.41868684\n",
      "Iteration 10658, loss = 1.41866579\n",
      "Iteration 10659, loss = 1.41864458\n",
      "Iteration 10660, loss = 1.41862501\n",
      "Iteration 10661, loss = 1.41860300\n",
      "Iteration 10662, loss = 1.41858251\n",
      "Iteration 10663, loss = 1.41856223\n",
      "Iteration 10664, loss = 1.41854171\n",
      "Iteration 10665, loss = 1.41852097\n",
      "Iteration 10666, loss = 1.41850003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10667, loss = 1.41847891\n",
      "Iteration 10668, loss = 1.41846019\n",
      "Iteration 10669, loss = 1.41843716\n",
      "Iteration 10670, loss = 1.41841697\n",
      "Iteration 10671, loss = 1.41839637\n",
      "Iteration 10672, loss = 1.41837604\n",
      "Iteration 10673, loss = 1.41835547\n",
      "Iteration 10674, loss = 1.41833469\n",
      "Iteration 10675, loss = 1.41831372\n",
      "Iteration 10676, loss = 1.41829259\n",
      "Iteration 10677, loss = 1.41827132\n",
      "Iteration 10678, loss = 1.41825218\n",
      "Iteration 10679, loss = 1.41822931\n",
      "Iteration 10680, loss = 1.41820910\n",
      "Iteration 10681, loss = 1.41818875\n",
      "Iteration 10682, loss = 1.41816820\n",
      "Iteration 10683, loss = 1.41814747\n",
      "Iteration 10684, loss = 1.41812656\n",
      "Iteration 10685, loss = 1.41810860\n",
      "Iteration 10686, loss = 1.41808598\n",
      "Iteration 10687, loss = 1.41806620\n",
      "Iteration 10688, loss = 1.41804616\n",
      "Iteration 10689, loss = 1.41802586\n",
      "Iteration 10690, loss = 1.41800533\n",
      "Iteration 10691, loss = 1.41798459\n",
      "Iteration 10692, loss = 1.41796365\n",
      "Iteration 10693, loss = 1.41794254\n",
      "Iteration 10694, loss = 1.41792128\n",
      "Iteration 10695, loss = 1.41789991\n",
      "Iteration 10696, loss = 1.41787889\n",
      "Iteration 10697, loss = 1.41785789\n",
      "Iteration 10698, loss = 1.41783754\n",
      "Iteration 10699, loss = 1.41781709\n",
      "Iteration 10700, loss = 1.41779644\n",
      "Iteration 10701, loss = 1.41777561\n",
      "Iteration 10702, loss = 1.41775462\n",
      "Iteration 10703, loss = 1.41773652\n",
      "Iteration 10704, loss = 1.41771446\n",
      "Iteration 10705, loss = 1.41769513\n",
      "Iteration 10706, loss = 1.41767549\n",
      "Iteration 10707, loss = 1.41765555\n",
      "Iteration 10708, loss = 1.41763536\n",
      "Iteration 10709, loss = 1.41761492\n",
      "Iteration 10710, loss = 1.41759426\n",
      "Iteration 10711, loss = 1.41757341\n",
      "Iteration 10712, loss = 1.41755238\n",
      "Iteration 10713, loss = 1.41753119\n",
      "Iteration 10714, loss = 1.41750986\n",
      "Iteration 10715, loss = 1.41748840\n",
      "Iteration 10716, loss = 1.41746685\n",
      "Iteration 10717, loss = 1.41745267\n",
      "Iteration 10718, loss = 1.41742589\n",
      "Iteration 10719, loss = 1.41740631\n",
      "Iteration 10720, loss = 1.41738645\n",
      "Iteration 10721, loss = 1.41736632\n",
      "Iteration 10722, loss = 1.41734593\n",
      "Iteration 10723, loss = 1.41732532\n",
      "Iteration 10724, loss = 1.41730451\n",
      "Iteration 10725, loss = 1.41728353\n",
      "Iteration 10726, loss = 1.41726242\n",
      "Iteration 10727, loss = 1.41724121\n",
      "Iteration 10728, loss = 1.41721989\n",
      "Iteration 10729, loss = 1.41720200\n",
      "Iteration 10730, loss = 1.41717945\n",
      "Iteration 10731, loss = 1.41716011\n",
      "Iteration 10732, loss = 1.41714052\n",
      "Iteration 10733, loss = 1.41712068\n",
      "Iteration 10734, loss = 1.41710059\n",
      "Iteration 10735, loss = 1.41708025\n",
      "Iteration 10736, loss = 1.41705970\n",
      "Iteration 10737, loss = 1.41703896\n",
      "Iteration 10738, loss = 1.41701807\n",
      "Iteration 10739, loss = 1.41699703\n",
      "Iteration 10740, loss = 1.41697587\n",
      "Iteration 10741, loss = 1.41695462\n",
      "Iteration 10742, loss = 1.41693330\n",
      "Iteration 10743, loss = 1.41691743\n",
      "Iteration 10744, loss = 1.41689380\n",
      "Iteration 10745, loss = 1.41687214\n",
      "Iteration 10746, loss = 1.41685249\n",
      "Iteration 10747, loss = 1.41683263\n",
      "Iteration 10748, loss = 1.41681256\n",
      "Iteration 10749, loss = 1.41679225\n",
      "Iteration 10750, loss = 1.41677173\n",
      "Iteration 10751, loss = 1.41675104\n",
      "Iteration 10752, loss = 1.41673016\n",
      "Iteration 10753, loss = 1.41670913\n",
      "Iteration 10754, loss = 1.41668796\n",
      "Iteration 10755, loss = 1.41666669\n",
      "Iteration 10756, loss = 1.41665221\n",
      "Iteration 10757, loss = 1.41662650\n",
      "Iteration 10758, loss = 1.41660739\n",
      "Iteration 10759, loss = 1.41658794\n",
      "Iteration 10760, loss = 1.41656821\n",
      "Iteration 10761, loss = 1.41654821\n",
      "Iteration 10762, loss = 1.41652797\n",
      "Iteration 10763, loss = 1.41650750\n",
      "Iteration 10764, loss = 1.41648685\n",
      "Iteration 10765, loss = 1.41646601\n",
      "Iteration 10766, loss = 1.41644503\n",
      "Iteration 10767, loss = 1.41642395\n",
      "Iteration 10768, loss = 1.41640279\n",
      "Iteration 10769, loss = 1.41638156\n",
      "Iteration 10770, loss = 1.41636746\n",
      "Iteration 10771, loss = 1.41634130\n",
      "Iteration 10772, loss = 1.41632108\n",
      "Iteration 10773, loss = 1.41630152\n",
      "Iteration 10774, loss = 1.41628172\n",
      "Iteration 10775, loss = 1.41626171\n",
      "Iteration 10776, loss = 1.41624149\n",
      "Iteration 10777, loss = 1.41622108\n",
      "Iteration 10778, loss = 1.41620049\n",
      "Iteration 10779, loss = 1.41617977\n",
      "Iteration 10780, loss = 1.41615893\n",
      "Iteration 10781, loss = 1.41613798\n",
      "Iteration 10782, loss = 1.41611694\n",
      "Iteration 10783, loss = 1.41609582\n",
      "Iteration 10784, loss = 1.41607598\n",
      "Iteration 10785, loss = 1.41605481\n",
      "Iteration 10786, loss = 1.41603481\n",
      "Iteration 10787, loss = 1.41601462\n",
      "Iteration 10788, loss = 1.41599425\n",
      "Iteration 10789, loss = 1.41597374\n",
      "Iteration 10790, loss = 1.41595309\n",
      "Iteration 10791, loss = 1.41593263\n",
      "Iteration 10792, loss = 1.41591197\n",
      "Iteration 10793, loss = 1.41589147\n",
      "Iteration 10794, loss = 1.41587130\n",
      "Iteration 10795, loss = 1.41585064\n",
      "Iteration 10796, loss = 1.41583031\n",
      "Iteration 10797, loss = 1.41581218\n",
      "Iteration 10798, loss = 1.41579085\n",
      "Iteration 10799, loss = 1.41577158\n",
      "Iteration 10800, loss = 1.41575203\n",
      "Iteration 10801, loss = 1.41573221\n",
      "Iteration 10802, loss = 1.41571214\n",
      "Iteration 10803, loss = 1.41569186\n",
      "Iteration 10804, loss = 1.41567137\n",
      "Iteration 10805, loss = 1.41565072\n",
      "Iteration 10806, loss = 1.41562991\n",
      "Iteration 10807, loss = 1.41560932\n",
      "Iteration 10808, loss = 1.41558888\n",
      "Iteration 10809, loss = 1.41556857\n",
      "Iteration 10810, loss = 1.41554809\n",
      "Iteration 10811, loss = 1.41552747\n",
      "Iteration 10812, loss = 1.41550877\n",
      "Iteration 10813, loss = 1.41548756\n",
      "Iteration 10814, loss = 1.41546817\n",
      "Iteration 10815, loss = 1.41544851\n",
      "Iteration 10816, loss = 1.41542859\n",
      "Iteration 10817, loss = 1.41540844\n",
      "Iteration 10818, loss = 1.41538809\n",
      "Iteration 10819, loss = 1.41536756\n",
      "Iteration 10820, loss = 1.41534687\n",
      "Iteration 10821, loss = 1.41532605\n",
      "Iteration 10822, loss = 1.41530577\n",
      "Iteration 10823, loss = 1.41528490\n",
      "Iteration 10824, loss = 1.41526452\n",
      "Iteration 10825, loss = 1.41524502\n",
      "Iteration 10826, loss = 1.41522390\n",
      "Iteration 10827, loss = 1.41520364\n",
      "Iteration 10828, loss = 1.41518413\n",
      "Iteration 10829, loss = 1.41516344\n",
      "Iteration 10830, loss = 1.41514346\n",
      "Iteration 10831, loss = 1.41512330\n",
      "Iteration 10832, loss = 1.41510298\n",
      "Iteration 10833, loss = 1.41508251\n",
      "Iteration 10834, loss = 1.41506192\n",
      "Iteration 10835, loss = 1.41504487\n",
      "Iteration 10836, loss = 1.41502203\n",
      "Iteration 10837, loss = 1.41500261\n",
      "Iteration 10838, loss = 1.41498294\n",
      "Iteration 10839, loss = 1.41496304\n",
      "Iteration 10840, loss = 1.41494295\n",
      "Iteration 10841, loss = 1.41492269\n",
      "Iteration 10842, loss = 1.41490228\n",
      "Iteration 10843, loss = 1.41488171\n",
      "Iteration 10844, loss = 1.41486251\n",
      "Iteration 10845, loss = 1.41484218\n",
      "Iteration 10846, loss = 1.41482155\n",
      "Iteration 10847, loss = 1.41480207\n",
      "Iteration 10848, loss = 1.41478236\n",
      "Iteration 10849, loss = 1.41476244\n",
      "Iteration 10850, loss = 1.41474234\n",
      "Iteration 10851, loss = 1.41472206\n",
      "Iteration 10852, loss = 1.41470161\n",
      "Iteration 10853, loss = 1.41468101\n",
      "Iteration 10854, loss = 1.41466027\n",
      "Iteration 10855, loss = 1.41464095\n",
      "Iteration 10856, loss = 1.41461968\n",
      "Iteration 10857, loss = 1.41459986\n",
      "Iteration 10858, loss = 1.41458018\n",
      "Iteration 10859, loss = 1.41456027\n",
      "Iteration 10860, loss = 1.41454015\n",
      "Iteration 10861, loss = 1.41451984\n",
      "Iteration 10862, loss = 1.41450005\n",
      "Iteration 10863, loss = 1.41447957\n",
      "Iteration 10864, loss = 1.41445956\n",
      "Iteration 10865, loss = 1.41443936\n",
      "Iteration 10866, loss = 1.41442055\n",
      "Iteration 10867, loss = 1.41439936\n",
      "Iteration 10868, loss = 1.41437952\n",
      "Iteration 10869, loss = 1.41435950\n",
      "Iteration 10870, loss = 1.41433933\n",
      "Iteration 10871, loss = 1.41431904\n",
      "Iteration 10872, loss = 1.41429861\n",
      "Iteration 10873, loss = 1.41427807\n",
      "Iteration 10874, loss = 1.41425896\n",
      "Iteration 10875, loss = 1.41423746\n",
      "Iteration 10876, loss = 1.41421747\n",
      "Iteration 10877, loss = 1.41419761\n",
      "Iteration 10878, loss = 1.41417771\n",
      "Iteration 10879, loss = 1.41415765\n",
      "Iteration 10880, loss = 1.41413743\n",
      "Iteration 10881, loss = 1.41411707\n",
      "Iteration 10882, loss = 1.41410033\n",
      "Iteration 10883, loss = 1.41407820\n",
      "Iteration 10884, loss = 1.41405952\n",
      "Iteration 10885, loss = 1.41404054\n",
      "Iteration 10886, loss = 1.41402127\n",
      "Iteration 10887, loss = 1.41400176\n",
      "Iteration 10888, loss = 1.41398201\n",
      "Iteration 10889, loss = 1.41396205\n",
      "Iteration 10890, loss = 1.41394191\n",
      "Iteration 10891, loss = 1.41392162\n",
      "Iteration 10892, loss = 1.41390118\n",
      "Iteration 10893, loss = 1.41388063\n",
      "Iteration 10894, loss = 1.41385996\n",
      "Iteration 10895, loss = 1.41383922\n",
      "Iteration 10896, loss = 1.41381839\n",
      "Iteration 10897, loss = 1.41380303\n",
      "Iteration 10898, loss = 1.41377939\n",
      "Iteration 10899, loss = 1.41376098\n",
      "Iteration 10900, loss = 1.41374230\n",
      "Iteration 10901, loss = 1.41372332\n",
      "Iteration 10902, loss = 1.41370405\n",
      "Iteration 10903, loss = 1.41368453\n",
      "Iteration 10904, loss = 1.41366478\n",
      "Iteration 10905, loss = 1.41364481\n",
      "Iteration 10906, loss = 1.41362467\n",
      "Iteration 10907, loss = 1.41360436\n",
      "Iteration 10908, loss = 1.41358391\n",
      "Iteration 10909, loss = 1.41356333\n",
      "Iteration 10910, loss = 1.41354264\n",
      "Iteration 10911, loss = 1.41352186\n",
      "Iteration 10912, loss = 1.41350329\n",
      "Iteration 10913, loss = 1.41348222\n",
      "Iteration 10914, loss = 1.41346217\n",
      "Iteration 10915, loss = 1.41344322\n",
      "Iteration 10916, loss = 1.41342404\n",
      "Iteration 10917, loss = 1.41340459\n",
      "Iteration 10918, loss = 1.41338496\n",
      "Iteration 10919, loss = 1.41336513\n",
      "Iteration 10920, loss = 1.41334512\n",
      "Iteration 10921, loss = 1.41332497\n",
      "Iteration 10922, loss = 1.41330467\n",
      "Iteration 10923, loss = 1.41328424\n",
      "Iteration 10924, loss = 1.41326368\n",
      "Iteration 10925, loss = 1.41324307\n",
      "Iteration 10926, loss = 1.41322241\n",
      "Iteration 10927, loss = 1.41320173\n",
      "Iteration 10928, loss = 1.41318562\n",
      "Iteration 10929, loss = 1.41316153\n",
      "Iteration 10930, loss = 1.41314237\n",
      "Iteration 10931, loss = 1.41312305\n",
      "Iteration 10932, loss = 1.41310354\n",
      "Iteration 10933, loss = 1.41308385\n",
      "Iteration 10934, loss = 1.41306402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10935, loss = 1.41304404\n",
      "Iteration 10936, loss = 1.41302390\n",
      "Iteration 10937, loss = 1.41300364\n",
      "Iteration 10938, loss = 1.41298329\n",
      "Iteration 10939, loss = 1.41296414\n",
      "Iteration 10940, loss = 1.41294377\n",
      "Iteration 10941, loss = 1.41292450\n",
      "Iteration 10942, loss = 1.41290502\n",
      "Iteration 10943, loss = 1.41288534\n",
      "Iteration 10944, loss = 1.41286549\n",
      "Iteration 10945, loss = 1.41284547\n",
      "Iteration 10946, loss = 1.41282531\n",
      "Iteration 10947, loss = 1.41280503\n",
      "Iteration 10948, loss = 1.41278466\n",
      "Iteration 10949, loss = 1.41276724\n",
      "Iteration 10950, loss = 1.41274611\n",
      "Iteration 10951, loss = 1.41272525\n",
      "Iteration 10952, loss = 1.41270590\n",
      "Iteration 10953, loss = 1.41268636\n",
      "Iteration 10954, loss = 1.41266663\n",
      "Iteration 10955, loss = 1.41264674\n",
      "Iteration 10956, loss = 1.41262672\n",
      "Iteration 10957, loss = 1.41260658\n",
      "Iteration 10958, loss = 1.41259090\n",
      "Iteration 10959, loss = 1.41256815\n",
      "Iteration 10960, loss = 1.41254974\n",
      "Iteration 10961, loss = 1.41253102\n",
      "Iteration 10962, loss = 1.41251204\n",
      "Iteration 10963, loss = 1.41249281\n",
      "Iteration 10964, loss = 1.41247334\n",
      "Iteration 10965, loss = 1.41245366\n",
      "Iteration 10966, loss = 1.41243380\n",
      "Iteration 10967, loss = 1.41241378\n",
      "Iteration 10968, loss = 1.41239363\n",
      "Iteration 10969, loss = 1.41237335\n",
      "Iteration 10970, loss = 1.41235296\n",
      "Iteration 10971, loss = 1.41233247\n",
      "Iteration 10972, loss = 1.41231191\n",
      "Iteration 10973, loss = 1.41229132\n",
      "Iteration 10974, loss = 1.41227146\n",
      "Iteration 10975, loss = 1.41225109\n",
      "Iteration 10976, loss = 1.41223135\n",
      "Iteration 10977, loss = 1.41221188\n",
      "Iteration 10978, loss = 1.41219217\n",
      "Iteration 10979, loss = 1.41217270\n",
      "Iteration 10980, loss = 1.41215307\n",
      "Iteration 10981, loss = 1.41213330\n",
      "Iteration 10982, loss = 1.41211339\n",
      "Iteration 10983, loss = 1.41209337\n",
      "Iteration 10984, loss = 1.41207490\n",
      "Iteration 10985, loss = 1.41205379\n",
      "Iteration 10986, loss = 1.41203418\n",
      "Iteration 10987, loss = 1.41201468\n",
      "Iteration 10988, loss = 1.41199538\n",
      "Iteration 10989, loss = 1.41197614\n",
      "Iteration 10990, loss = 1.41195670\n",
      "Iteration 10991, loss = 1.41193707\n",
      "Iteration 10992, loss = 1.41191728\n",
      "Iteration 10993, loss = 1.41189735\n",
      "Iteration 10994, loss = 1.41187732\n",
      "Iteration 10995, loss = 1.41185824\n",
      "Iteration 10996, loss = 1.41183769\n",
      "Iteration 10997, loss = 1.41181806\n",
      "Iteration 10998, loss = 1.41179973\n",
      "Iteration 10999, loss = 1.41177947\n",
      "Iteration 11000, loss = 1.41176050\n",
      "Iteration 11001, loss = 1.41174132\n",
      "Iteration 11002, loss = 1.41172192\n",
      "Iteration 11003, loss = 1.41170234\n",
      "Iteration 11004, loss = 1.41168258\n",
      "Iteration 11005, loss = 1.41166266\n",
      "Iteration 11006, loss = 1.41164259\n",
      "Iteration 11007, loss = 1.41162241\n",
      "Iteration 11008, loss = 1.41160215\n",
      "Iteration 11009, loss = 1.41158930\n",
      "Iteration 11010, loss = 1.41156389\n",
      "Iteration 11011, loss = 1.41154569\n",
      "Iteration 11012, loss = 1.41152720\n",
      "Iteration 11013, loss = 1.41150843\n",
      "Iteration 11014, loss = 1.41148940\n",
      "Iteration 11015, loss = 1.41147015\n",
      "Iteration 11016, loss = 1.41145071\n",
      "Iteration 11017, loss = 1.41143109\n",
      "Iteration 11018, loss = 1.41141130\n",
      "Iteration 11019, loss = 1.41139137\n",
      "Iteration 11020, loss = 1.41137132\n",
      "Iteration 11021, loss = 1.41135116\n",
      "Iteration 11022, loss = 1.41133091\n",
      "Iteration 11023, loss = 1.41131059\n",
      "Iteration 11024, loss = 1.41129023\n",
      "Iteration 11025, loss = 1.41127369\n",
      "Iteration 11026, loss = 1.41125161\n",
      "Iteration 11027, loss = 1.41123154\n",
      "Iteration 11028, loss = 1.41121243\n",
      "Iteration 11029, loss = 1.41119316\n",
      "Iteration 11030, loss = 1.41117369\n",
      "Iteration 11031, loss = 1.41115406\n",
      "Iteration 11032, loss = 1.41113429\n",
      "Iteration 11033, loss = 1.41111438\n",
      "Iteration 11034, loss = 1.41109524\n",
      "Iteration 11035, loss = 1.41107499\n",
      "Iteration 11036, loss = 1.41105546\n",
      "Iteration 11037, loss = 1.41103577\n",
      "Iteration 11038, loss = 1.41101721\n",
      "Iteration 11039, loss = 1.41099681\n",
      "Iteration 11040, loss = 1.41097748\n",
      "Iteration 11041, loss = 1.41095797\n",
      "Iteration 11042, loss = 1.41093829\n",
      "Iteration 11043, loss = 1.41091851\n",
      "Iteration 11044, loss = 1.41090096\n",
      "Iteration 11045, loss = 1.41087935\n",
      "Iteration 11046, loss = 1.41085992\n",
      "Iteration 11047, loss = 1.41084034\n",
      "Iteration 11048, loss = 1.41082232\n",
      "Iteration 11049, loss = 1.41080153\n",
      "Iteration 11050, loss = 1.41078226\n",
      "Iteration 11051, loss = 1.41076283\n",
      "Iteration 11052, loss = 1.41074325\n",
      "Iteration 11053, loss = 1.41072503\n",
      "Iteration 11054, loss = 1.41070443\n",
      "Iteration 11055, loss = 1.41068515\n",
      "Iteration 11056, loss = 1.41066571\n",
      "Iteration 11057, loss = 1.41064613\n",
      "Iteration 11058, loss = 1.41062643\n",
      "Iteration 11059, loss = 1.41060904\n",
      "Iteration 11060, loss = 1.41058862\n",
      "Iteration 11061, loss = 1.41057042\n",
      "Iteration 11062, loss = 1.41055195\n",
      "Iteration 11063, loss = 1.41053321\n",
      "Iteration 11064, loss = 1.41051422\n",
      "Iteration 11065, loss = 1.41049502\n",
      "Iteration 11066, loss = 1.41047562\n",
      "Iteration 11067, loss = 1.41045604\n",
      "Iteration 11068, loss = 1.41043630\n",
      "Iteration 11069, loss = 1.41041642\n",
      "Iteration 11070, loss = 1.41039641\n",
      "Iteration 11071, loss = 1.41037630\n",
      "Iteration 11072, loss = 1.41035616\n",
      "Iteration 11073, loss = 1.41033599\n",
      "Iteration 11074, loss = 1.41031800\n",
      "Iteration 11075, loss = 1.41029650\n",
      "Iteration 11076, loss = 1.41027765\n",
      "Iteration 11077, loss = 1.41025874\n",
      "Iteration 11078, loss = 1.41023967\n",
      "Iteration 11079, loss = 1.41022043\n",
      "Iteration 11080, loss = 1.41020102\n",
      "Iteration 11081, loss = 1.41018163\n",
      "Iteration 11082, loss = 1.41016254\n",
      "Iteration 11083, loss = 1.41014342\n",
      "Iteration 11084, loss = 1.41012412\n",
      "Iteration 11085, loss = 1.41010466\n",
      "Iteration 11086, loss = 1.41008506\n",
      "Iteration 11087, loss = 1.41006535\n",
      "Iteration 11088, loss = 1.41004810\n",
      "Iteration 11089, loss = 1.41002701\n",
      "Iteration 11090, loss = 1.41000832\n",
      "Iteration 11091, loss = 1.40998942\n",
      "Iteration 11092, loss = 1.40997033\n",
      "Iteration 11093, loss = 1.40995107\n",
      "Iteration 11094, loss = 1.40993163\n",
      "Iteration 11095, loss = 1.40991204\n",
      "Iteration 11096, loss = 1.40989233\n",
      "Iteration 11097, loss = 1.40987254\n",
      "Iteration 11098, loss = 1.40985267\n",
      "Iteration 11099, loss = 1.40983339\n",
      "Iteration 11100, loss = 1.40981339\n",
      "Iteration 11101, loss = 1.40979471\n",
      "Iteration 11102, loss = 1.40977544\n",
      "Iteration 11103, loss = 1.40975676\n",
      "Iteration 11104, loss = 1.40973792\n",
      "Iteration 11105, loss = 1.40971889\n",
      "Iteration 11106, loss = 1.40969968\n",
      "Iteration 11107, loss = 1.40968031\n",
      "Iteration 11108, loss = 1.40966080\n",
      "Iteration 11109, loss = 1.40964116\n",
      "Iteration 11110, loss = 1.40962141\n",
      "Iteration 11111, loss = 1.40960307\n",
      "Iteration 11112, loss = 1.40958246\n",
      "Iteration 11113, loss = 1.40956319\n",
      "Iteration 11114, loss = 1.40954378\n",
      "Iteration 11115, loss = 1.40952425\n",
      "Iteration 11116, loss = 1.40950540\n",
      "Iteration 11117, loss = 1.40948564\n",
      "Iteration 11118, loss = 1.40946650\n",
      "Iteration 11119, loss = 1.40944722\n",
      "Iteration 11120, loss = 1.40942781\n",
      "Iteration 11121, loss = 1.40941136\n",
      "Iteration 11122, loss = 1.40938996\n",
      "Iteration 11123, loss = 1.40937142\n",
      "Iteration 11124, loss = 1.40935267\n",
      "Iteration 11125, loss = 1.40933372\n",
      "Iteration 11126, loss = 1.40931459\n",
      "Iteration 11127, loss = 1.40929530\n",
      "Iteration 11128, loss = 1.40927586\n",
      "Iteration 11129, loss = 1.40925629\n",
      "Iteration 11130, loss = 1.40923659\n",
      "Iteration 11131, loss = 1.40921680\n",
      "Iteration 11132, loss = 1.40919745\n",
      "Iteration 11133, loss = 1.40917809\n",
      "Iteration 11134, loss = 1.40915895\n",
      "Iteration 11135, loss = 1.40913999\n",
      "Iteration 11136, loss = 1.40912087\n",
      "Iteration 11137, loss = 1.40910158\n",
      "Iteration 11138, loss = 1.40908216\n",
      "Iteration 11139, loss = 1.40906265\n",
      "Iteration 11140, loss = 1.40904414\n",
      "Iteration 11141, loss = 1.40902411\n",
      "Iteration 11142, loss = 1.40900519\n",
      "Iteration 11143, loss = 1.40898633\n",
      "Iteration 11144, loss = 1.40896729\n",
      "Iteration 11145, loss = 1.40894807\n",
      "Iteration 11146, loss = 1.40892909\n",
      "Iteration 11147, loss = 1.40890992\n",
      "Iteration 11148, loss = 1.40889094\n",
      "Iteration 11149, loss = 1.40887180\n",
      "Iteration 11150, loss = 1.40885251\n",
      "Iteration 11151, loss = 1.40883419\n",
      "Iteration 11152, loss = 1.40881436\n",
      "Iteration 11153, loss = 1.40879541\n",
      "Iteration 11154, loss = 1.40877630\n",
      "Iteration 11155, loss = 1.40875704\n",
      "Iteration 11156, loss = 1.40873766\n",
      "Iteration 11157, loss = 1.40871818\n",
      "Iteration 11158, loss = 1.40870267\n",
      "Iteration 11159, loss = 1.40868048\n",
      "Iteration 11160, loss = 1.40866216\n",
      "Iteration 11161, loss = 1.40864362\n",
      "Iteration 11162, loss = 1.40862488\n",
      "Iteration 11163, loss = 1.40860594\n",
      "Iteration 11164, loss = 1.40858683\n",
      "Iteration 11165, loss = 1.40856757\n",
      "Iteration 11166, loss = 1.40854818\n",
      "Iteration 11167, loss = 1.40852872\n",
      "Iteration 11168, loss = 1.40851098\n",
      "Iteration 11169, loss = 1.40849125\n",
      "Iteration 11170, loss = 1.40847136\n",
      "Iteration 11171, loss = 1.40845256\n",
      "Iteration 11172, loss = 1.40843358\n",
      "Iteration 11173, loss = 1.40841445\n",
      "Iteration 11174, loss = 1.40839516\n",
      "Iteration 11175, loss = 1.40837574\n",
      "Iteration 11176, loss = 1.40835622\n",
      "Iteration 11177, loss = 1.40833663\n",
      "Iteration 11178, loss = 1.40831839\n",
      "Iteration 11179, loss = 1.40829805\n",
      "Iteration 11180, loss = 1.40827966\n",
      "Iteration 11181, loss = 1.40826118\n",
      "Iteration 11182, loss = 1.40824252\n",
      "Iteration 11183, loss = 1.40822369\n",
      "Iteration 11184, loss = 1.40820470\n",
      "Iteration 11185, loss = 1.40818555\n",
      "Iteration 11186, loss = 1.40816626\n",
      "Iteration 11187, loss = 1.40814685\n",
      "Iteration 11188, loss = 1.40812734\n",
      "Iteration 11189, loss = 1.40810776\n",
      "Iteration 11190, loss = 1.40808954\n",
      "Iteration 11191, loss = 1.40806963\n",
      "Iteration 11192, loss = 1.40805100\n",
      "Iteration 11193, loss = 1.40803219\n",
      "Iteration 11194, loss = 1.40801323\n",
      "Iteration 11195, loss = 1.40799412\n",
      "Iteration 11196, loss = 1.40797488\n",
      "Iteration 11197, loss = 1.40795555\n",
      "Iteration 11198, loss = 1.40793770\n",
      "Iteration 11199, loss = 1.40791735\n",
      "Iteration 11200, loss = 1.40789844\n",
      "Iteration 11201, loss = 1.40787939\n",
      "Iteration 11202, loss = 1.40786137\n",
      "Iteration 11203, loss = 1.40784232\n",
      "Iteration 11204, loss = 1.40782421\n",
      "Iteration 11205, loss = 1.40780586\n",
      "Iteration 11206, loss = 1.40778729\n",
      "Iteration 11207, loss = 1.40776851\n",
      "Iteration 11208, loss = 1.40774954\n",
      "Iteration 11209, loss = 1.40773043\n",
      "Iteration 11210, loss = 1.40771119\n",
      "Iteration 11211, loss = 1.40769182\n",
      "Iteration 11212, loss = 1.40767237\n",
      "Iteration 11213, loss = 1.40765284\n",
      "Iteration 11214, loss = 1.40763330\n",
      "Iteration 11215, loss = 1.40761433\n",
      "Iteration 11216, loss = 1.40759543\n",
      "Iteration 11217, loss = 1.40757659\n",
      "Iteration 11218, loss = 1.40755763\n",
      "Iteration 11219, loss = 1.40753856\n",
      "Iteration 11220, loss = 1.40752052\n",
      "Iteration 11221, loss = 1.40750079\n",
      "Iteration 11222, loss = 1.40748204\n",
      "Iteration 11223, loss = 1.40746313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11224, loss = 1.40744409\n",
      "Iteration 11225, loss = 1.40742536\n",
      "Iteration 11226, loss = 1.40740639\n",
      "Iteration 11227, loss = 1.40738769\n",
      "Iteration 11228, loss = 1.40736883\n",
      "Iteration 11229, loss = 1.40734982\n",
      "Iteration 11230, loss = 1.40733068\n",
      "Iteration 11231, loss = 1.40731221\n",
      "Iteration 11232, loss = 1.40729266\n",
      "Iteration 11233, loss = 1.40727374\n",
      "Iteration 11234, loss = 1.40725610\n",
      "Iteration 11235, loss = 1.40723631\n",
      "Iteration 11236, loss = 1.40721777\n",
      "Iteration 11237, loss = 1.40719904\n",
      "Iteration 11238, loss = 1.40718015\n",
      "Iteration 11239, loss = 1.40716111\n",
      "Iteration 11240, loss = 1.40714195\n",
      "Iteration 11241, loss = 1.40712468\n",
      "Iteration 11242, loss = 1.40710415\n",
      "Iteration 11243, loss = 1.40708546\n",
      "Iteration 11244, loss = 1.40706662\n",
      "Iteration 11245, loss = 1.40704767\n",
      "Iteration 11246, loss = 1.40702860\n",
      "Iteration 11247, loss = 1.40700954\n",
      "Iteration 11248, loss = 1.40699071\n",
      "Iteration 11249, loss = 1.40697185\n",
      "Iteration 11250, loss = 1.40695296\n",
      "Iteration 11251, loss = 1.40693537\n",
      "Iteration 11252, loss = 1.40691762\n",
      "Iteration 11253, loss = 1.40689963\n",
      "Iteration 11254, loss = 1.40688140\n",
      "Iteration 11255, loss = 1.40686296\n",
      "Iteration 11256, loss = 1.40684432\n",
      "Iteration 11257, loss = 1.40682551\n",
      "Iteration 11258, loss = 1.40680654\n",
      "Iteration 11259, loss = 1.40678744\n",
      "Iteration 11260, loss = 1.40676821\n",
      "Iteration 11261, loss = 1.40674890\n",
      "Iteration 11262, loss = 1.40672953\n",
      "Iteration 11263, loss = 1.40671012\n",
      "Iteration 11264, loss = 1.40669068\n",
      "Iteration 11265, loss = 1.40667144\n",
      "Iteration 11266, loss = 1.40665241\n",
      "Iteration 11267, loss = 1.40663357\n",
      "Iteration 11268, loss = 1.40661484\n",
      "Iteration 11269, loss = 1.40659638\n",
      "Iteration 11270, loss = 1.40657847\n",
      "Iteration 11271, loss = 1.40656079\n",
      "Iteration 11272, loss = 1.40654288\n",
      "Iteration 11273, loss = 1.40652474\n",
      "Iteration 11274, loss = 1.40650638\n",
      "Iteration 11275, loss = 1.40648784\n",
      "Iteration 11276, loss = 1.40646914\n",
      "Iteration 11277, loss = 1.40645029\n",
      "Iteration 11278, loss = 1.40643131\n",
      "Iteration 11279, loss = 1.40641223\n",
      "Iteration 11280, loss = 1.40639304\n",
      "Iteration 11281, loss = 1.40637376\n",
      "Iteration 11282, loss = 1.40635441\n",
      "Iteration 11283, loss = 1.40633499\n",
      "Iteration 11284, loss = 1.40631552\n",
      "Iteration 11285, loss = 1.40630132\n",
      "Iteration 11286, loss = 1.40627768\n",
      "Iteration 11287, loss = 1.40625922\n",
      "Iteration 11288, loss = 1.40624062\n",
      "Iteration 11289, loss = 1.40622188\n",
      "Iteration 11290, loss = 1.40620304\n",
      "Iteration 11291, loss = 1.40618410\n",
      "Iteration 11292, loss = 1.40616544\n",
      "Iteration 11293, loss = 1.40614671\n",
      "Iteration 11294, loss = 1.40612824\n",
      "Iteration 11295, loss = 1.40610963\n",
      "Iteration 11296, loss = 1.40609086\n",
      "Iteration 11297, loss = 1.40607198\n",
      "Iteration 11298, loss = 1.40605299\n",
      "Iteration 11299, loss = 1.40603578\n",
      "Iteration 11300, loss = 1.40601641\n",
      "Iteration 11301, loss = 1.40599711\n",
      "Iteration 11302, loss = 1.40597874\n",
      "Iteration 11303, loss = 1.40596021\n",
      "Iteration 11304, loss = 1.40594152\n",
      "Iteration 11305, loss = 1.40592270\n",
      "Iteration 11306, loss = 1.40590374\n",
      "Iteration 11307, loss = 1.40588467\n",
      "Iteration 11308, loss = 1.40586598\n",
      "Iteration 11309, loss = 1.40584706\n",
      "Iteration 11310, loss = 1.40582917\n",
      "Iteration 11311, loss = 1.40581045\n",
      "Iteration 11312, loss = 1.40579224\n",
      "Iteration 11313, loss = 1.40577383\n",
      "Iteration 11314, loss = 1.40575526\n",
      "Iteration 11315, loss = 1.40573655\n",
      "Iteration 11316, loss = 1.40571770\n",
      "Iteration 11317, loss = 1.40569875\n",
      "Iteration 11318, loss = 1.40567972\n",
      "Iteration 11319, loss = 1.40566164\n",
      "Iteration 11320, loss = 1.40564209\n",
      "Iteration 11321, loss = 1.40562378\n",
      "Iteration 11322, loss = 1.40560544\n",
      "Iteration 11323, loss = 1.40558695\n",
      "Iteration 11324, loss = 1.40556832\n",
      "Iteration 11325, loss = 1.40554956\n",
      "Iteration 11326, loss = 1.40553067\n",
      "Iteration 11327, loss = 1.40551539\n",
      "Iteration 11328, loss = 1.40549505\n",
      "Iteration 11329, loss = 1.40547812\n",
      "Iteration 11330, loss = 1.40546089\n",
      "Iteration 11331, loss = 1.40544339\n",
      "Iteration 11332, loss = 1.40542563\n",
      "Iteration 11333, loss = 1.40540765\n",
      "Iteration 11334, loss = 1.40538946\n",
      "Iteration 11335, loss = 1.40537108\n",
      "Iteration 11336, loss = 1.40535253\n",
      "Iteration 11337, loss = 1.40533384\n",
      "Iteration 11338, loss = 1.40531500\n",
      "Iteration 11339, loss = 1.40529605\n",
      "Iteration 11340, loss = 1.40527699\n",
      "Iteration 11341, loss = 1.40525784\n",
      "Iteration 11342, loss = 1.40523860\n",
      "Iteration 11343, loss = 1.40521928\n",
      "Iteration 11344, loss = 1.40519989\n",
      "Iteration 11345, loss = 1.40518047\n",
      "Iteration 11346, loss = 1.40516109\n",
      "Iteration 11347, loss = 1.40514785\n",
      "Iteration 11348, loss = 1.40512416\n",
      "Iteration 11349, loss = 1.40510564\n",
      "Iteration 11350, loss = 1.40508766\n",
      "Iteration 11351, loss = 1.40506959\n",
      "Iteration 11352, loss = 1.40505135\n",
      "Iteration 11353, loss = 1.40503294\n",
      "Iteration 11354, loss = 1.40501437\n",
      "Iteration 11355, loss = 1.40499566\n",
      "Iteration 11356, loss = 1.40497684\n",
      "Iteration 11357, loss = 1.40495793\n",
      "Iteration 11358, loss = 1.40493896\n",
      "Iteration 11359, loss = 1.40491991\n",
      "Iteration 11360, loss = 1.40490080\n",
      "Iteration 11361, loss = 1.40488163\n",
      "Iteration 11362, loss = 1.40486693\n",
      "Iteration 11363, loss = 1.40484503\n",
      "Iteration 11364, loss = 1.40482747\n",
      "Iteration 11365, loss = 1.40480975\n",
      "Iteration 11366, loss = 1.40479181\n",
      "Iteration 11367, loss = 1.40477369\n",
      "Iteration 11368, loss = 1.40475541\n",
      "Iteration 11369, loss = 1.40473696\n",
      "Iteration 11370, loss = 1.40471838\n",
      "Iteration 11371, loss = 1.40469967\n",
      "Iteration 11372, loss = 1.40468085\n",
      "Iteration 11373, loss = 1.40466193\n",
      "Iteration 11374, loss = 1.40464293\n",
      "Iteration 11375, loss = 1.40462386\n",
      "Iteration 11376, loss = 1.40460520\n",
      "Iteration 11377, loss = 1.40458643\n",
      "Iteration 11378, loss = 1.40456795\n",
      "Iteration 11379, loss = 1.40454965\n",
      "Iteration 11380, loss = 1.40453121\n",
      "Iteration 11381, loss = 1.40451264\n",
      "Iteration 11382, loss = 1.40449398\n",
      "Iteration 11383, loss = 1.40447632\n",
      "Iteration 11384, loss = 1.40445722\n",
      "Iteration 11385, loss = 1.40443907\n",
      "Iteration 11386, loss = 1.40442076\n",
      "Iteration 11387, loss = 1.40440231\n",
      "Iteration 11388, loss = 1.40438372\n",
      "Iteration 11389, loss = 1.40436501\n",
      "Iteration 11390, loss = 1.40434621\n",
      "Iteration 11391, loss = 1.40432870\n",
      "Iteration 11392, loss = 1.40430912\n",
      "Iteration 11393, loss = 1.40429075\n",
      "Iteration 11394, loss = 1.40427227\n",
      "Iteration 11395, loss = 1.40425379\n",
      "Iteration 11396, loss = 1.40423579\n",
      "Iteration 11397, loss = 1.40421774\n",
      "Iteration 11398, loss = 1.40419951\n",
      "Iteration 11399, loss = 1.40418113\n",
      "Iteration 11400, loss = 1.40416262\n",
      "Iteration 11401, loss = 1.40414492\n",
      "Iteration 11402, loss = 1.40412583\n",
      "Iteration 11403, loss = 1.40410751\n",
      "Iteration 11404, loss = 1.40408906\n",
      "Iteration 11405, loss = 1.40407058\n",
      "Iteration 11406, loss = 1.40405238\n",
      "Iteration 11407, loss = 1.40403415\n",
      "Iteration 11408, loss = 1.40401577\n",
      "Iteration 11409, loss = 1.40399723\n",
      "Iteration 11410, loss = 1.40398044\n",
      "Iteration 11411, loss = 1.40396061\n",
      "Iteration 11412, loss = 1.40394246\n",
      "Iteration 11413, loss = 1.40392415\n",
      "Iteration 11414, loss = 1.40390569\n",
      "Iteration 11415, loss = 1.40388713\n",
      "Iteration 11416, loss = 1.40386849\n",
      "Iteration 11417, loss = 1.40385015\n",
      "Iteration 11418, loss = 1.40383150\n",
      "Iteration 11419, loss = 1.40381311\n",
      "Iteration 11420, loss = 1.40379462\n",
      "Iteration 11421, loss = 1.40377605\n",
      "Iteration 11422, loss = 1.40375808\n",
      "Iteration 11423, loss = 1.40374065\n",
      "Iteration 11424, loss = 1.40372372\n",
      "Iteration 11425, loss = 1.40370653\n",
      "Iteration 11426, loss = 1.40368907\n",
      "Iteration 11427, loss = 1.40367138\n",
      "Iteration 11428, loss = 1.40365347\n",
      "Iteration 11429, loss = 1.40363538\n",
      "Iteration 11430, loss = 1.40361714\n",
      "Iteration 11431, loss = 1.40359874\n",
      "Iteration 11432, loss = 1.40358022\n",
      "Iteration 11433, loss = 1.40356158\n",
      "Iteration 11434, loss = 1.40354284\n",
      "Iteration 11435, loss = 1.40352402\n",
      "Iteration 11436, loss = 1.40350512\n",
      "Iteration 11437, loss = 1.40348615\n",
      "Iteration 11438, loss = 1.40346712\n",
      "Iteration 11439, loss = 1.40344805\n",
      "Iteration 11440, loss = 1.40344068\n",
      "Iteration 11441, loss = 1.40341203\n",
      "Iteration 11442, loss = 1.40339484\n",
      "Iteration 11443, loss = 1.40337742\n",
      "Iteration 11444, loss = 1.40335980\n",
      "Iteration 11445, loss = 1.40334199\n",
      "Iteration 11446, loss = 1.40332402\n",
      "Iteration 11447, loss = 1.40330588\n",
      "Iteration 11448, loss = 1.40328760\n",
      "Iteration 11449, loss = 1.40326920\n",
      "Iteration 11450, loss = 1.40325069\n",
      "Iteration 11451, loss = 1.40323208\n",
      "Iteration 11452, loss = 1.40321340\n",
      "Iteration 11453, loss = 1.40319464\n",
      "Iteration 11454, loss = 1.40317583\n",
      "Iteration 11455, loss = 1.40315700\n",
      "Iteration 11456, loss = 1.40313813\n",
      "Iteration 11457, loss = 1.40312223\n",
      "Iteration 11458, loss = 1.40310144\n",
      "Iteration 11459, loss = 1.40308378\n",
      "Iteration 11460, loss = 1.40306614\n",
      "Iteration 11461, loss = 1.40304837\n",
      "Iteration 11462, loss = 1.40303043\n",
      "Iteration 11463, loss = 1.40301233\n",
      "Iteration 11464, loss = 1.40299408\n",
      "Iteration 11465, loss = 1.40297569\n",
      "Iteration 11466, loss = 1.40295718\n",
      "Iteration 11467, loss = 1.40293857\n",
      "Iteration 11468, loss = 1.40292064\n",
      "Iteration 11469, loss = 1.40290181\n",
      "Iteration 11470, loss = 1.40288360\n",
      "Iteration 11471, loss = 1.40286529\n",
      "Iteration 11472, loss = 1.40284691\n",
      "Iteration 11473, loss = 1.40282845\n",
      "Iteration 11474, loss = 1.40280993\n",
      "Iteration 11475, loss = 1.40279276\n",
      "Iteration 11476, loss = 1.40277337\n",
      "Iteration 11477, loss = 1.40275528\n",
      "Iteration 11478, loss = 1.40273708\n",
      "Iteration 11479, loss = 1.40271877\n",
      "Iteration 11480, loss = 1.40270036\n",
      "Iteration 11481, loss = 1.40268563\n",
      "Iteration 11482, loss = 1.40266541\n",
      "Iteration 11483, loss = 1.40264874\n",
      "Iteration 11484, loss = 1.40263179\n",
      "Iteration 11485, loss = 1.40261459\n",
      "Iteration 11486, loss = 1.40259716\n",
      "Iteration 11487, loss = 1.40257952\n",
      "Iteration 11488, loss = 1.40256170\n",
      "Iteration 11489, loss = 1.40254370\n",
      "Iteration 11490, loss = 1.40252556\n",
      "Iteration 11491, loss = 1.40250727\n",
      "Iteration 11492, loss = 1.40248887\n",
      "Iteration 11493, loss = 1.40247035\n",
      "Iteration 11494, loss = 1.40245174\n",
      "Iteration 11495, loss = 1.40243304\n",
      "Iteration 11496, loss = 1.40241426\n",
      "Iteration 11497, loss = 1.40239542\n",
      "Iteration 11498, loss = 1.40237652\n",
      "Iteration 11499, loss = 1.40235759\n",
      "Iteration 11500, loss = 1.40233869\n",
      "Iteration 11501, loss = 1.40232644\n",
      "Iteration 11502, loss = 1.40230260\n",
      "Iteration 11503, loss = 1.40228527\n",
      "Iteration 11504, loss = 1.40226781\n",
      "Iteration 11505, loss = 1.40225016\n",
      "Iteration 11506, loss = 1.40223234\n",
      "Iteration 11507, loss = 1.40221434\n",
      "Iteration 11508, loss = 1.40219619\n",
      "Iteration 11509, loss = 1.40217790\n",
      "Iteration 11510, loss = 1.40215951\n",
      "Iteration 11511, loss = 1.40214106\n",
      "Iteration 11512, loss = 1.40212255\n",
      "Iteration 11513, loss = 1.40210399\n",
      "Iteration 11514, loss = 1.40208537\n",
      "Iteration 11515, loss = 1.40206675\n",
      "Iteration 11516, loss = 1.40204877\n",
      "Iteration 11517, loss = 1.40203074\n",
      "Iteration 11518, loss = 1.40201262\n",
      "Iteration 11519, loss = 1.40199512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11520, loss = 1.40197665\n",
      "Iteration 11521, loss = 1.40195882\n",
      "Iteration 11522, loss = 1.40194084\n",
      "Iteration 11523, loss = 1.40192273\n",
      "Iteration 11524, loss = 1.40190450\n",
      "Iteration 11525, loss = 1.40188982\n",
      "Iteration 11526, loss = 1.40186978\n",
      "Iteration 11527, loss = 1.40185311\n",
      "Iteration 11528, loss = 1.40183619\n",
      "Iteration 11529, loss = 1.40181904\n",
      "Iteration 11530, loss = 1.40180167\n",
      "Iteration 11531, loss = 1.40178411\n",
      "Iteration 11532, loss = 1.40176637\n",
      "Iteration 11533, loss = 1.40174847\n",
      "Iteration 11534, loss = 1.40173043\n",
      "Iteration 11535, loss = 1.40171226\n",
      "Iteration 11536, loss = 1.40169398\n",
      "Iteration 11537, loss = 1.40167560\n",
      "Iteration 11538, loss = 1.40165717\n",
      "Iteration 11539, loss = 1.40163866\n",
      "Iteration 11540, loss = 1.40162009\n",
      "Iteration 11541, loss = 1.40160146\n",
      "Iteration 11542, loss = 1.40158279\n",
      "Iteration 11543, loss = 1.40156408\n",
      "Iteration 11544, loss = 1.40154927\n",
      "Iteration 11545, loss = 1.40152772\n",
      "Iteration 11546, loss = 1.40151036\n",
      "Iteration 11547, loss = 1.40149297\n",
      "Iteration 11548, loss = 1.40147539\n",
      "Iteration 11549, loss = 1.40145765\n",
      "Iteration 11550, loss = 1.40143976\n",
      "Iteration 11551, loss = 1.40142174\n",
      "Iteration 11552, loss = 1.40140359\n",
      "Iteration 11553, loss = 1.40138533\n",
      "Iteration 11554, loss = 1.40136696\n",
      "Iteration 11555, loss = 1.40134851\n",
      "Iteration 11556, loss = 1.40133303\n",
      "Iteration 11557, loss = 1.40131273\n",
      "Iteration 11558, loss = 1.40129500\n",
      "Iteration 11559, loss = 1.40127764\n",
      "Iteration 11560, loss = 1.40126010\n",
      "Iteration 11561, loss = 1.40124238\n",
      "Iteration 11562, loss = 1.40122450\n",
      "Iteration 11563, loss = 1.40120647\n",
      "Iteration 11564, loss = 1.40118832\n",
      "Iteration 11565, loss = 1.40117010\n",
      "Iteration 11566, loss = 1.40115185\n",
      "Iteration 11567, loss = 1.40113354\n",
      "Iteration 11568, loss = 1.40111760\n",
      "Iteration 11569, loss = 1.40109737\n",
      "Iteration 11570, loss = 1.40107946\n",
      "Iteration 11571, loss = 1.40106145\n",
      "Iteration 11572, loss = 1.40104337\n",
      "Iteration 11573, loss = 1.40102522\n",
      "Iteration 11574, loss = 1.40100873\n",
      "Iteration 11575, loss = 1.40099008\n",
      "Iteration 11576, loss = 1.40097302\n",
      "Iteration 11577, loss = 1.40095577\n",
      "Iteration 11578, loss = 1.40093833\n",
      "Iteration 11579, loss = 1.40092072\n",
      "Iteration 11580, loss = 1.40090295\n",
      "Iteration 11581, loss = 1.40088504\n",
      "Iteration 11582, loss = 1.40086701\n",
      "Iteration 11583, loss = 1.40084887\n",
      "Iteration 11584, loss = 1.40083063\n",
      "Iteration 11585, loss = 1.40081231\n",
      "Iteration 11586, loss = 1.40079393\n",
      "Iteration 11587, loss = 1.40077590\n",
      "Iteration 11588, loss = 1.40075782\n",
      "Iteration 11589, loss = 1.40074006\n",
      "Iteration 11590, loss = 1.40072241\n",
      "Iteration 11591, loss = 1.40070463\n",
      "Iteration 11592, loss = 1.40068674\n",
      "Iteration 11593, loss = 1.40066873\n",
      "Iteration 11594, loss = 1.40065063\n",
      "Iteration 11595, loss = 1.40063244\n",
      "Iteration 11596, loss = 1.40061419\n",
      "Iteration 11597, loss = 1.40059586\n",
      "Iteration 11598, loss = 1.40058397\n",
      "Iteration 11599, loss = 1.40056183\n",
      "Iteration 11600, loss = 1.40054591\n",
      "Iteration 11601, loss = 1.40052968\n",
      "Iteration 11602, loss = 1.40051316\n",
      "Iteration 11603, loss = 1.40049637\n",
      "Iteration 11604, loss = 1.40047933\n",
      "Iteration 11605, loss = 1.40046207\n",
      "Iteration 11606, loss = 1.40044461\n",
      "Iteration 11607, loss = 1.40042699\n",
      "Iteration 11608, loss = 1.40040921\n",
      "Iteration 11609, loss = 1.40039130\n",
      "Iteration 11610, loss = 1.40037328\n",
      "Iteration 11611, loss = 1.40035516\n",
      "Iteration 11612, loss = 1.40033695\n",
      "Iteration 11613, loss = 1.40031865\n",
      "Iteration 11614, loss = 1.40030027\n",
      "Iteration 11615, loss = 1.40028182\n",
      "Iteration 11616, loss = 1.40026333\n",
      "Iteration 11617, loss = 1.40024480\n",
      "Iteration 11618, loss = 1.40022622\n",
      "Iteration 11619, loss = 1.40020761\n",
      "Iteration 11620, loss = 1.40018904\n",
      "Iteration 11621, loss = 1.40017051\n",
      "Iteration 11622, loss = 1.40015726\n",
      "Iteration 11623, loss = 1.40013463\n",
      "Iteration 11624, loss = 1.40011762\n",
      "Iteration 11625, loss = 1.40010056\n",
      "Iteration 11626, loss = 1.40008338\n",
      "Iteration 11627, loss = 1.40006606\n",
      "Iteration 11628, loss = 1.40004859\n",
      "Iteration 11629, loss = 1.40003099\n",
      "Iteration 11630, loss = 1.40001328\n",
      "Iteration 11631, loss = 1.39999544\n",
      "Iteration 11632, loss = 1.39997750\n",
      "Iteration 11633, loss = 1.39995945\n",
      "Iteration 11634, loss = 1.39994132\n",
      "Iteration 11635, loss = 1.39992313\n",
      "Iteration 11636, loss = 1.39990492\n",
      "Iteration 11637, loss = 1.39988669\n",
      "Iteration 11638, loss = 1.39986843\n",
      "Iteration 11639, loss = 1.39985293\n",
      "Iteration 11640, loss = 1.39983330\n",
      "Iteration 11641, loss = 1.39981639\n",
      "Iteration 11642, loss = 1.39979939\n",
      "Iteration 11643, loss = 1.39978221\n",
      "Iteration 11644, loss = 1.39976486\n",
      "Iteration 11645, loss = 1.39974735\n",
      "Iteration 11646, loss = 1.39972971\n",
      "Iteration 11647, loss = 1.39971195\n",
      "Iteration 11648, loss = 1.39969408\n",
      "Iteration 11649, loss = 1.39967611\n",
      "Iteration 11650, loss = 1.39965806\n",
      "Iteration 11651, loss = 1.39963993\n",
      "Iteration 11652, loss = 1.39962174\n",
      "Iteration 11653, loss = 1.39960350\n",
      "Iteration 11654, loss = 1.39958768\n",
      "Iteration 11655, loss = 1.39956839\n",
      "Iteration 11656, loss = 1.39955140\n",
      "Iteration 11657, loss = 1.39953424\n",
      "Iteration 11658, loss = 1.39951691\n",
      "Iteration 11659, loss = 1.39949943\n",
      "Iteration 11660, loss = 1.39948181\n",
      "Iteration 11661, loss = 1.39946407\n",
      "Iteration 11662, loss = 1.39944621\n",
      "Iteration 11663, loss = 1.39942826\n",
      "Iteration 11664, loss = 1.39941022\n",
      "Iteration 11665, loss = 1.39939347\n",
      "Iteration 11666, loss = 1.39937552\n",
      "Iteration 11667, loss = 1.39935731\n",
      "Iteration 11668, loss = 1.39933997\n",
      "Iteration 11669, loss = 1.39932249\n",
      "Iteration 11670, loss = 1.39930488\n",
      "Iteration 11671, loss = 1.39928715\n",
      "Iteration 11672, loss = 1.39926933\n",
      "Iteration 11673, loss = 1.39925145\n",
      "Iteration 11674, loss = 1.39923351\n",
      "Iteration 11675, loss = 1.39921552\n",
      "Iteration 11676, loss = 1.39919778\n",
      "Iteration 11677, loss = 1.39917989\n",
      "Iteration 11678, loss = 1.39916220\n",
      "Iteration 11679, loss = 1.39914484\n",
      "Iteration 11680, loss = 1.39912709\n",
      "Iteration 11681, loss = 1.39910968\n",
      "Iteration 11682, loss = 1.39909215\n",
      "Iteration 11683, loss = 1.39907451\n",
      "Iteration 11684, loss = 1.39905674\n",
      "Iteration 11685, loss = 1.39903886\n",
      "Iteration 11686, loss = 1.39902110\n",
      "Iteration 11687, loss = 1.39900361\n",
      "Iteration 11688, loss = 1.39898620\n",
      "Iteration 11689, loss = 1.39896868\n",
      "Iteration 11690, loss = 1.39895103\n",
      "Iteration 11691, loss = 1.39893328\n",
      "Iteration 11692, loss = 1.39891545\n",
      "Iteration 11693, loss = 1.39889806\n",
      "Iteration 11694, loss = 1.39888056\n",
      "Iteration 11695, loss = 1.39886294\n",
      "Iteration 11696, loss = 1.39884574\n",
      "Iteration 11697, loss = 1.39882811\n",
      "Iteration 11698, loss = 1.39881087\n",
      "Iteration 11699, loss = 1.39879348\n",
      "Iteration 11700, loss = 1.39877597\n",
      "Iteration 11701, loss = 1.39875833\n",
      "Iteration 11702, loss = 1.39874060\n",
      "Iteration 11703, loss = 1.39872280\n",
      "Iteration 11704, loss = 1.39870822\n",
      "Iteration 11705, loss = 1.39868830\n",
      "Iteration 11706, loss = 1.39867157\n",
      "Iteration 11707, loss = 1.39865464\n",
      "Iteration 11708, loss = 1.39863753\n",
      "Iteration 11709, loss = 1.39862025\n",
      "Iteration 11710, loss = 1.39860282\n",
      "Iteration 11711, loss = 1.39858527\n",
      "Iteration 11712, loss = 1.39856760\n",
      "Iteration 11713, loss = 1.39854982\n",
      "Iteration 11714, loss = 1.39853196\n",
      "Iteration 11715, loss = 1.39851404\n",
      "Iteration 11716, loss = 1.39849609\n",
      "Iteration 11717, loss = 1.39847812\n",
      "Iteration 11718, loss = 1.39846145\n",
      "Iteration 11719, loss = 1.39844378\n",
      "Iteration 11720, loss = 1.39842589\n",
      "Iteration 11721, loss = 1.39840913\n",
      "Iteration 11722, loss = 1.39839224\n",
      "Iteration 11723, loss = 1.39837523\n",
      "Iteration 11724, loss = 1.39835804\n",
      "Iteration 11725, loss = 1.39834069\n",
      "Iteration 11726, loss = 1.39832319\n",
      "Iteration 11727, loss = 1.39830556\n",
      "Iteration 11728, loss = 1.39828781\n",
      "Iteration 11729, loss = 1.39826996\n",
      "Iteration 11730, loss = 1.39825204\n",
      "Iteration 11731, loss = 1.39823411\n",
      "Iteration 11732, loss = 1.39821616\n",
      "Iteration 11733, loss = 1.39820380\n",
      "Iteration 11734, loss = 1.39818160\n",
      "Iteration 11735, loss = 1.39816490\n",
      "Iteration 11736, loss = 1.39814807\n",
      "Iteration 11737, loss = 1.39813107\n",
      "Iteration 11738, loss = 1.39811391\n",
      "Iteration 11739, loss = 1.39809660\n",
      "Iteration 11740, loss = 1.39807917\n",
      "Iteration 11741, loss = 1.39806162\n",
      "Iteration 11742, loss = 1.39804397\n",
      "Iteration 11743, loss = 1.39802623\n",
      "Iteration 11744, loss = 1.39800841\n",
      "Iteration 11745, loss = 1.39799055\n",
      "Iteration 11746, loss = 1.39797268\n",
      "Iteration 11747, loss = 1.39795480\n",
      "Iteration 11748, loss = 1.39794424\n",
      "Iteration 11749, loss = 1.39792045\n",
      "Iteration 11750, loss = 1.39790391\n",
      "Iteration 11751, loss = 1.39788730\n",
      "Iteration 11752, loss = 1.39787052\n",
      "Iteration 11753, loss = 1.39785356\n",
      "Iteration 11754, loss = 1.39783648\n",
      "Iteration 11755, loss = 1.39781925\n",
      "Iteration 11756, loss = 1.39780186\n",
      "Iteration 11757, loss = 1.39778434\n",
      "Iteration 11758, loss = 1.39776669\n",
      "Iteration 11759, loss = 1.39774894\n",
      "Iteration 11760, loss = 1.39773111\n",
      "Iteration 11761, loss = 1.39771325\n",
      "Iteration 11762, loss = 1.39769537\n",
      "Iteration 11763, loss = 1.39767750\n",
      "Iteration 11764, loss = 1.39765964\n",
      "Iteration 11765, loss = 1.39764177\n",
      "Iteration 11766, loss = 1.39762503\n",
      "Iteration 11767, loss = 1.39760761\n",
      "Iteration 11768, loss = 1.39758971\n",
      "Iteration 11769, loss = 1.39757296\n",
      "Iteration 11770, loss = 1.39755614\n",
      "Iteration 11771, loss = 1.39753923\n",
      "Iteration 11772, loss = 1.39752218\n",
      "Iteration 11773, loss = 1.39750499\n",
      "Iteration 11774, loss = 1.39748765\n",
      "Iteration 11775, loss = 1.39747019\n",
      "Iteration 11776, loss = 1.39745261\n",
      "Iteration 11777, loss = 1.39743494\n",
      "Iteration 11778, loss = 1.39741721\n",
      "Iteration 11779, loss = 1.39740110\n",
      "Iteration 11780, loss = 1.39738310\n",
      "Iteration 11781, loss = 1.39736656\n",
      "Iteration 11782, loss = 1.39734984\n",
      "Iteration 11783, loss = 1.39733294\n",
      "Iteration 11784, loss = 1.39731589\n",
      "Iteration 11785, loss = 1.39729870\n",
      "Iteration 11786, loss = 1.39728139\n",
      "Iteration 11787, loss = 1.39726398\n",
      "Iteration 11788, loss = 1.39724648\n",
      "Iteration 11789, loss = 1.39722889\n",
      "Iteration 11790, loss = 1.39721123\n",
      "Iteration 11791, loss = 1.39719351\n",
      "Iteration 11792, loss = 1.39717574\n",
      "Iteration 11793, loss = 1.39715793\n",
      "Iteration 11794, loss = 1.39714008\n",
      "Iteration 11795, loss = 1.39712491\n",
      "Iteration 11796, loss = 1.39710525\n",
      "Iteration 11797, loss = 1.39708821\n",
      "Iteration 11798, loss = 1.39707137\n",
      "Iteration 11799, loss = 1.39705443\n",
      "Iteration 11800, loss = 1.39703736\n",
      "Iteration 11801, loss = 1.39702019\n",
      "Iteration 11802, loss = 1.39700291\n",
      "Iteration 11803, loss = 1.39698554\n",
      "Iteration 11804, loss = 1.39696810\n",
      "Iteration 11805, loss = 1.39695058\n",
      "Iteration 11806, loss = 1.39693301\n",
      "Iteration 11807, loss = 1.39691543\n",
      "Iteration 11808, loss = 1.39689917\n",
      "Iteration 11809, loss = 1.39688115\n",
      "Iteration 11810, loss = 1.39686375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11811, loss = 1.39684688\n",
      "Iteration 11812, loss = 1.39682988\n",
      "Iteration 11813, loss = 1.39681273\n",
      "Iteration 11814, loss = 1.39679546\n",
      "Iteration 11815, loss = 1.39677807\n",
      "Iteration 11816, loss = 1.39676185\n",
      "Iteration 11817, loss = 1.39674367\n",
      "Iteration 11818, loss = 1.39672664\n",
      "Iteration 11819, loss = 1.39670948\n",
      "Iteration 11820, loss = 1.39669221\n",
      "Iteration 11821, loss = 1.39667485\n",
      "Iteration 11822, loss = 1.39665743\n",
      "Iteration 11823, loss = 1.39663993\n",
      "Iteration 11824, loss = 1.39662305\n",
      "Iteration 11825, loss = 1.39660553\n",
      "Iteration 11826, loss = 1.39658856\n",
      "Iteration 11827, loss = 1.39657147\n",
      "Iteration 11828, loss = 1.39655429\n",
      "Iteration 11829, loss = 1.39653699\n",
      "Iteration 11830, loss = 1.39651969\n",
      "Iteration 11831, loss = 1.39650269\n",
      "Iteration 11832, loss = 1.39648565\n",
      "Iteration 11833, loss = 1.39646847\n",
      "Iteration 11834, loss = 1.39645118\n",
      "Iteration 11835, loss = 1.39643381\n",
      "Iteration 11836, loss = 1.39641638\n",
      "Iteration 11837, loss = 1.39640101\n",
      "Iteration 11838, loss = 1.39638237\n",
      "Iteration 11839, loss = 1.39636575\n",
      "Iteration 11840, loss = 1.39634897\n",
      "Iteration 11841, loss = 1.39633204\n",
      "Iteration 11842, loss = 1.39631497\n",
      "Iteration 11843, loss = 1.39629778\n",
      "Iteration 11844, loss = 1.39628048\n",
      "Iteration 11845, loss = 1.39626310\n",
      "Iteration 11846, loss = 1.39624569\n",
      "Iteration 11847, loss = 1.39622824\n",
      "Iteration 11848, loss = 1.39621075\n",
      "Iteration 11849, loss = 1.39619322\n",
      "Iteration 11850, loss = 1.39617616\n",
      "Iteration 11851, loss = 1.39615955\n",
      "Iteration 11852, loss = 1.39614191\n",
      "Iteration 11853, loss = 1.39612524\n",
      "Iteration 11854, loss = 1.39610847\n",
      "Iteration 11855, loss = 1.39609157\n",
      "Iteration 11856, loss = 1.39607455\n",
      "Iteration 11857, loss = 1.39605741\n",
      "Iteration 11858, loss = 1.39604018\n",
      "Iteration 11859, loss = 1.39602285\n",
      "Iteration 11860, loss = 1.39600544\n",
      "Iteration 11861, loss = 1.39598797\n",
      "Iteration 11862, loss = 1.39597047\n",
      "Iteration 11863, loss = 1.39595430\n",
      "Iteration 11864, loss = 1.39593654\n",
      "Iteration 11865, loss = 1.39591914\n",
      "Iteration 11866, loss = 1.39590234\n",
      "Iteration 11867, loss = 1.39588546\n",
      "Iteration 11868, loss = 1.39586846\n",
      "Iteration 11869, loss = 1.39585134\n",
      "Iteration 11870, loss = 1.39583410\n",
      "Iteration 11871, loss = 1.39581759\n",
      "Iteration 11872, loss = 1.39580024\n",
      "Iteration 11873, loss = 1.39578354\n",
      "Iteration 11874, loss = 1.39576669\n",
      "Iteration 11875, loss = 1.39574971\n",
      "Iteration 11876, loss = 1.39573260\n",
      "Iteration 11877, loss = 1.39571539\n",
      "Iteration 11878, loss = 1.39569809\n",
      "Iteration 11879, loss = 1.39568076\n",
      "Iteration 11880, loss = 1.39566340\n",
      "Iteration 11881, loss = 1.39564826\n",
      "Iteration 11882, loss = 1.39563002\n",
      "Iteration 11883, loss = 1.39561246\n",
      "Iteration 11884, loss = 1.39559581\n",
      "Iteration 11885, loss = 1.39557907\n",
      "Iteration 11886, loss = 1.39556221\n",
      "Iteration 11887, loss = 1.39554522\n",
      "Iteration 11888, loss = 1.39552810\n",
      "Iteration 11889, loss = 1.39551086\n",
      "Iteration 11890, loss = 1.39549353\n",
      "Iteration 11891, loss = 1.39547617\n",
      "Iteration 11892, loss = 1.39545879\n",
      "Iteration 11893, loss = 1.39544185\n",
      "Iteration 11894, loss = 1.39542455\n",
      "Iteration 11895, loss = 1.39540762\n",
      "Iteration 11896, loss = 1.39539061\n",
      "Iteration 11897, loss = 1.39537353\n",
      "Iteration 11898, loss = 1.39535639\n",
      "Iteration 11899, loss = 1.39534073\n",
      "Iteration 11900, loss = 1.39532272\n",
      "Iteration 11901, loss = 1.39530613\n",
      "Iteration 11902, loss = 1.39528939\n",
      "Iteration 11903, loss = 1.39527252\n",
      "Iteration 11904, loss = 1.39525554\n",
      "Iteration 11905, loss = 1.39523845\n",
      "Iteration 11906, loss = 1.39522125\n",
      "Iteration 11907, loss = 1.39520396\n",
      "Iteration 11908, loss = 1.39518733\n",
      "Iteration 11909, loss = 1.39516971\n",
      "Iteration 11910, loss = 1.39515273\n",
      "Iteration 11911, loss = 1.39513564\n",
      "Iteration 11912, loss = 1.39511926\n",
      "Iteration 11913, loss = 1.39510180\n",
      "Iteration 11914, loss = 1.39508500\n",
      "Iteration 11915, loss = 1.39506809\n",
      "Iteration 11916, loss = 1.39505107\n",
      "Iteration 11917, loss = 1.39503397\n",
      "Iteration 11918, loss = 1.39501681\n",
      "Iteration 11919, loss = 1.39500051\n",
      "Iteration 11920, loss = 1.39498299\n",
      "Iteration 11921, loss = 1.39496633\n",
      "Iteration 11922, loss = 1.39494956\n",
      "Iteration 11923, loss = 1.39493266\n",
      "Iteration 11924, loss = 1.39491563\n",
      "Iteration 11925, loss = 1.39489918\n",
      "Iteration 11926, loss = 1.39488188\n",
      "Iteration 11927, loss = 1.39486511\n",
      "Iteration 11928, loss = 1.39484821\n",
      "Iteration 11929, loss = 1.39483121\n",
      "Iteration 11930, loss = 1.39481411\n",
      "Iteration 11931, loss = 1.39479700\n",
      "Iteration 11932, loss = 1.39478043\n",
      "Iteration 11933, loss = 1.39476308\n",
      "Iteration 11934, loss = 1.39474626\n",
      "Iteration 11935, loss = 1.39472934\n",
      "Iteration 11936, loss = 1.39471234\n",
      "Iteration 11937, loss = 1.39469528\n",
      "Iteration 11938, loss = 1.39467873\n",
      "Iteration 11939, loss = 1.39466150\n",
      "Iteration 11940, loss = 1.39464474\n",
      "Iteration 11941, loss = 1.39462790\n",
      "Iteration 11942, loss = 1.39461092\n",
      "Iteration 11943, loss = 1.39459383\n",
      "Iteration 11944, loss = 1.39457775\n",
      "Iteration 11945, loss = 1.39456026\n",
      "Iteration 11946, loss = 1.39454375\n",
      "Iteration 11947, loss = 1.39452710\n",
      "Iteration 11948, loss = 1.39451030\n",
      "Iteration 11949, loss = 1.39449338\n",
      "Iteration 11950, loss = 1.39447636\n",
      "Iteration 11951, loss = 1.39445926\n",
      "Iteration 11952, loss = 1.39444231\n",
      "Iteration 11953, loss = 1.39442607\n",
      "Iteration 11954, loss = 1.39440992\n",
      "Iteration 11955, loss = 1.39439358\n",
      "Iteration 11956, loss = 1.39437707\n",
      "Iteration 11957, loss = 1.39436043\n",
      "Iteration 11958, loss = 1.39434367\n",
      "Iteration 11959, loss = 1.39432676\n",
      "Iteration 11960, loss = 1.39430973\n",
      "Iteration 11961, loss = 1.39429269\n",
      "Iteration 11962, loss = 1.39427559\n",
      "Iteration 11963, loss = 1.39425846\n",
      "Iteration 11964, loss = 1.39424128\n",
      "Iteration 11965, loss = 1.39422413\n",
      "Iteration 11966, loss = 1.39420695\n",
      "Iteration 11967, loss = 1.39418985\n",
      "Iteration 11968, loss = 1.39417292\n",
      "Iteration 11969, loss = 1.39415603\n",
      "Iteration 11970, loss = 1.39413908\n",
      "Iteration 11971, loss = 1.39412207\n",
      "Iteration 11972, loss = 1.39410577\n",
      "Iteration 11973, loss = 1.39408838\n",
      "Iteration 11974, loss = 1.39407169\n",
      "Iteration 11975, loss = 1.39405493\n",
      "Iteration 11976, loss = 1.39403807\n",
      "Iteration 11977, loss = 1.39402253\n",
      "Iteration 11978, loss = 1.39400496\n",
      "Iteration 11979, loss = 1.39398867\n",
      "Iteration 11980, loss = 1.39397222\n",
      "Iteration 11981, loss = 1.39395563\n",
      "Iteration 11982, loss = 1.39393890\n",
      "Iteration 11983, loss = 1.39392205\n",
      "Iteration 11984, loss = 1.39390509\n",
      "Iteration 11985, loss = 1.39388803\n",
      "Iteration 11986, loss = 1.39387089\n",
      "Iteration 11987, loss = 1.39385366\n",
      "Iteration 11988, loss = 1.39383706\n",
      "Iteration 11989, loss = 1.39381975\n",
      "Iteration 11990, loss = 1.39380330\n",
      "Iteration 11991, loss = 1.39378683\n",
      "Iteration 11992, loss = 1.39377023\n",
      "Iteration 11993, loss = 1.39375352\n",
      "Iteration 11994, loss = 1.39373673\n",
      "Iteration 11995, loss = 1.39371987\n",
      "Iteration 11996, loss = 1.39370354\n",
      "Iteration 11997, loss = 1.39368657\n",
      "Iteration 11998, loss = 1.39367011\n",
      "Iteration 11999, loss = 1.39365355\n",
      "Iteration 12000, loss = 1.39363686\n",
      "Iteration 12001, loss = 1.39362005\n",
      "Iteration 12002, loss = 1.39360315\n",
      "Iteration 12003, loss = 1.39358618\n",
      "Iteration 12004, loss = 1.39357063\n",
      "Iteration 12005, loss = 1.39355262\n",
      "Iteration 12006, loss = 1.39353597\n",
      "Iteration 12007, loss = 1.39351926\n",
      "Iteration 12008, loss = 1.39350249\n",
      "Iteration 12009, loss = 1.39348564\n",
      "Iteration 12010, loss = 1.39346903\n",
      "Iteration 12011, loss = 1.39345226\n",
      "Iteration 12012, loss = 1.39343573\n",
      "Iteration 12013, loss = 1.39341909\n",
      "Iteration 12014, loss = 1.39340235\n",
      "Iteration 12015, loss = 1.39338551\n",
      "Iteration 12016, loss = 1.39336859\n",
      "Iteration 12017, loss = 1.39335160\n",
      "Iteration 12018, loss = 1.39333459\n",
      "Iteration 12019, loss = 1.39331830\n",
      "Iteration 12020, loss = 1.39330107\n",
      "Iteration 12021, loss = 1.39328451\n",
      "Iteration 12022, loss = 1.39326787\n",
      "Iteration 12023, loss = 1.39325114\n",
      "Iteration 12024, loss = 1.39323468\n",
      "Iteration 12025, loss = 1.39321825\n",
      "Iteration 12026, loss = 1.39320206\n",
      "Iteration 12027, loss = 1.39318571\n",
      "Iteration 12028, loss = 1.39316922\n",
      "Iteration 12029, loss = 1.39315260\n",
      "Iteration 12030, loss = 1.39313586\n",
      "Iteration 12031, loss = 1.39311900\n",
      "Iteration 12032, loss = 1.39310205\n",
      "Iteration 12033, loss = 1.39308502\n",
      "Iteration 12034, loss = 1.39306817\n",
      "Iteration 12035, loss = 1.39305136\n",
      "Iteration 12036, loss = 1.39303472\n",
      "Iteration 12037, loss = 1.39301843\n",
      "Iteration 12038, loss = 1.39300172\n",
      "Iteration 12039, loss = 1.39298534\n",
      "Iteration 12040, loss = 1.39296887\n",
      "Iteration 12041, loss = 1.39295229\n",
      "Iteration 12042, loss = 1.39293558\n",
      "Iteration 12043, loss = 1.39291879\n",
      "Iteration 12044, loss = 1.39290195\n",
      "Iteration 12045, loss = 1.39288511\n",
      "Iteration 12046, loss = 1.39286823\n",
      "Iteration 12047, loss = 1.39285226\n",
      "Iteration 12048, loss = 1.39283476\n",
      "Iteration 12049, loss = 1.39281815\n",
      "Iteration 12050, loss = 1.39280149\n",
      "Iteration 12051, loss = 1.39278506\n",
      "Iteration 12052, loss = 1.39276861\n",
      "Iteration 12053, loss = 1.39275232\n",
      "Iteration 12054, loss = 1.39273591\n",
      "Iteration 12055, loss = 1.39271937\n",
      "Iteration 12056, loss = 1.39270274\n",
      "Iteration 12057, loss = 1.39268600\n",
      "Iteration 12058, loss = 1.39266919\n",
      "Iteration 12059, loss = 1.39265232\n",
      "Iteration 12060, loss = 1.39263738\n",
      "Iteration 12061, loss = 1.39261935\n",
      "Iteration 12062, loss = 1.39260297\n",
      "Iteration 12063, loss = 1.39258689\n",
      "Iteration 12064, loss = 1.39257064\n",
      "Iteration 12065, loss = 1.39255422\n",
      "Iteration 12066, loss = 1.39253767\n",
      "Iteration 12067, loss = 1.39252098\n",
      "Iteration 12068, loss = 1.39250419\n",
      "Iteration 12069, loss = 1.39248733\n",
      "Iteration 12070, loss = 1.39247044\n",
      "Iteration 12071, loss = 1.39245353\n",
      "Iteration 12072, loss = 1.39243660\n",
      "Iteration 12073, loss = 1.39242459\n",
      "Iteration 12074, loss = 1.39240446\n",
      "Iteration 12075, loss = 1.39238927\n",
      "Iteration 12076, loss = 1.39237391\n",
      "Iteration 12077, loss = 1.39235831\n",
      "Iteration 12078, loss = 1.39234250\n",
      "Iteration 12079, loss = 1.39232651\n",
      "Iteration 12080, loss = 1.39231034\n",
      "Iteration 12081, loss = 1.39229402\n",
      "Iteration 12082, loss = 1.39227756\n",
      "Iteration 12083, loss = 1.39226098\n",
      "Iteration 12084, loss = 1.39224429\n",
      "Iteration 12085, loss = 1.39222749\n",
      "Iteration 12086, loss = 1.39221061\n",
      "Iteration 12087, loss = 1.39219365\n",
      "Iteration 12088, loss = 1.39217663\n",
      "Iteration 12089, loss = 1.39215960\n",
      "Iteration 12090, loss = 1.39214258\n",
      "Iteration 12091, loss = 1.39212559\n",
      "Iteration 12092, loss = 1.39210856\n",
      "Iteration 12093, loss = 1.39209153\n",
      "Iteration 12094, loss = 1.39207458\n",
      "Iteration 12095, loss = 1.39205764\n",
      "Iteration 12096, loss = 1.39204562\n",
      "Iteration 12097, loss = 1.39202456\n",
      "Iteration 12098, loss = 1.39200854\n",
      "Iteration 12099, loss = 1.39199262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12100, loss = 1.39197666\n",
      "Iteration 12101, loss = 1.39196060\n",
      "Iteration 12102, loss = 1.39194441\n",
      "Iteration 12103, loss = 1.39192813\n",
      "Iteration 12104, loss = 1.39191171\n",
      "Iteration 12105, loss = 1.39189517\n",
      "Iteration 12106, loss = 1.39187852\n",
      "Iteration 12107, loss = 1.39186176\n",
      "Iteration 12108, loss = 1.39184490\n",
      "Iteration 12109, loss = 1.39182804\n",
      "Iteration 12110, loss = 1.39181119\n",
      "Iteration 12111, loss = 1.39179430\n",
      "Iteration 12112, loss = 1.39177740\n",
      "Iteration 12113, loss = 1.39176054\n",
      "Iteration 12114, loss = 1.39174368\n",
      "Iteration 12115, loss = 1.39172882\n",
      "Iteration 12116, loss = 1.39171038\n",
      "Iteration 12117, loss = 1.39169389\n",
      "Iteration 12118, loss = 1.39167734\n",
      "Iteration 12119, loss = 1.39166074\n",
      "Iteration 12120, loss = 1.39164428\n",
      "Iteration 12121, loss = 1.39162795\n",
      "Iteration 12122, loss = 1.39161175\n",
      "Iteration 12123, loss = 1.39159542\n",
      "Iteration 12124, loss = 1.39157897\n",
      "Iteration 12125, loss = 1.39156241\n",
      "Iteration 12126, loss = 1.39154578\n",
      "Iteration 12127, loss = 1.39153004\n",
      "Iteration 12128, loss = 1.39151315\n",
      "Iteration 12129, loss = 1.39149709\n",
      "Iteration 12130, loss = 1.39148088\n",
      "Iteration 12131, loss = 1.39146454\n",
      "Iteration 12132, loss = 1.39144807\n",
      "Iteration 12133, loss = 1.39143151\n",
      "Iteration 12134, loss = 1.39141489\n",
      "Iteration 12135, loss = 1.39139821\n",
      "Iteration 12136, loss = 1.39138151\n",
      "Iteration 12137, loss = 1.39136685\n",
      "Iteration 12138, loss = 1.39134954\n",
      "Iteration 12139, loss = 1.39133259\n",
      "Iteration 12140, loss = 1.39131657\n",
      "Iteration 12141, loss = 1.39130047\n",
      "Iteration 12142, loss = 1.39128425\n",
      "Iteration 12143, loss = 1.39126788\n",
      "Iteration 12144, loss = 1.39125138\n",
      "Iteration 12145, loss = 1.39123476\n",
      "Iteration 12146, loss = 1.39121811\n",
      "Iteration 12147, loss = 1.39120144\n",
      "Iteration 12148, loss = 1.39118476\n",
      "Iteration 12149, loss = 1.39116922\n",
      "Iteration 12150, loss = 1.39115179\n",
      "Iteration 12151, loss = 1.39113545\n",
      "Iteration 12152, loss = 1.39111906\n",
      "Iteration 12153, loss = 1.39110262\n",
      "Iteration 12154, loss = 1.39108613\n",
      "Iteration 12155, loss = 1.39106959\n",
      "Iteration 12156, loss = 1.39105536\n",
      "Iteration 12157, loss = 1.39103769\n",
      "Iteration 12158, loss = 1.39102228\n",
      "Iteration 12159, loss = 1.39100667\n",
      "Iteration 12160, loss = 1.39099088\n",
      "Iteration 12161, loss = 1.39097494\n",
      "Iteration 12162, loss = 1.39095885\n",
      "Iteration 12163, loss = 1.39094262\n",
      "Iteration 12164, loss = 1.39092628\n",
      "Iteration 12165, loss = 1.39090983\n",
      "Iteration 12166, loss = 1.39089328\n",
      "Iteration 12167, loss = 1.39087666\n",
      "Iteration 12168, loss = 1.39085996\n",
      "Iteration 12169, loss = 1.39084320\n",
      "Iteration 12170, loss = 1.39082653\n",
      "Iteration 12171, loss = 1.39080988\n",
      "Iteration 12172, loss = 1.39079319\n",
      "Iteration 12173, loss = 1.39077668\n",
      "Iteration 12174, loss = 1.39076015\n",
      "Iteration 12175, loss = 1.39074377\n",
      "Iteration 12176, loss = 1.39072751\n",
      "Iteration 12177, loss = 1.39071133\n",
      "Iteration 12178, loss = 1.39069522\n",
      "Iteration 12179, loss = 1.39067902\n",
      "Iteration 12180, loss = 1.39066275\n",
      "Iteration 12181, loss = 1.39064640\n",
      "Iteration 12182, loss = 1.39063001\n",
      "Iteration 12183, loss = 1.39061356\n",
      "Iteration 12184, loss = 1.39059709\n",
      "Iteration 12185, loss = 1.39058190\n",
      "Iteration 12186, loss = 1.39056461\n",
      "Iteration 12187, loss = 1.39054857\n",
      "Iteration 12188, loss = 1.39053245\n",
      "Iteration 12189, loss = 1.39051623\n",
      "Iteration 12190, loss = 1.39049991\n",
      "Iteration 12191, loss = 1.39048351\n",
      "Iteration 12192, loss = 1.39046703\n",
      "Iteration 12193, loss = 1.39045048\n",
      "Iteration 12194, loss = 1.39043440\n",
      "Iteration 12195, loss = 1.39041864\n",
      "Iteration 12196, loss = 1.39040331\n",
      "Iteration 12197, loss = 1.39038780\n",
      "Iteration 12198, loss = 1.39037211\n",
      "Iteration 12199, loss = 1.39035624\n",
      "Iteration 12200, loss = 1.39034023\n",
      "Iteration 12201, loss = 1.39032408\n",
      "Iteration 12202, loss = 1.39030780\n",
      "Iteration 12203, loss = 1.39029142\n",
      "Iteration 12204, loss = 1.39027493\n",
      "Iteration 12205, loss = 1.39025836\n",
      "Iteration 12206, loss = 1.39024183\n",
      "Iteration 12207, loss = 1.39022530\n",
      "Iteration 12208, loss = 1.39020876\n",
      "Iteration 12209, loss = 1.39019219\n",
      "Iteration 12210, loss = 1.39017661\n",
      "Iteration 12211, loss = 1.39015944\n",
      "Iteration 12212, loss = 1.39014362\n",
      "Iteration 12213, loss = 1.39012773\n",
      "Iteration 12214, loss = 1.39011174\n",
      "Iteration 12215, loss = 1.39009567\n",
      "Iteration 12216, loss = 1.39007953\n",
      "Iteration 12217, loss = 1.39006331\n",
      "Iteration 12218, loss = 1.39004703\n",
      "Iteration 12219, loss = 1.39003070\n",
      "Iteration 12220, loss = 1.39001437\n",
      "Iteration 12221, loss = 1.38999800\n",
      "Iteration 12222, loss = 1.38998161\n",
      "Iteration 12223, loss = 1.38996521\n",
      "Iteration 12224, loss = 1.38994932\n",
      "Iteration 12225, loss = 1.38993265\n",
      "Iteration 12226, loss = 1.38991649\n",
      "Iteration 12227, loss = 1.38990030\n",
      "Iteration 12228, loss = 1.38988407\n",
      "Iteration 12229, loss = 1.38986776\n",
      "Iteration 12230, loss = 1.38985143\n",
      "Iteration 12231, loss = 1.38983573\n",
      "Iteration 12232, loss = 1.38981910\n",
      "Iteration 12233, loss = 1.38980307\n",
      "Iteration 12234, loss = 1.38978694\n",
      "Iteration 12235, loss = 1.38977072\n",
      "Iteration 12236, loss = 1.38975440\n",
      "Iteration 12237, loss = 1.38973805\n",
      "Iteration 12238, loss = 1.38972167\n",
      "Iteration 12239, loss = 1.38970586\n",
      "Iteration 12240, loss = 1.38968927\n",
      "Iteration 12241, loss = 1.38967323\n",
      "Iteration 12242, loss = 1.38965711\n",
      "Iteration 12243, loss = 1.38964091\n",
      "Iteration 12244, loss = 1.38962465\n",
      "Iteration 12245, loss = 1.38960836\n",
      "Iteration 12246, loss = 1.38959204\n",
      "Iteration 12247, loss = 1.38957742\n",
      "Iteration 12248, loss = 1.38955980\n",
      "Iteration 12249, loss = 1.38954384\n",
      "Iteration 12250, loss = 1.38952779\n",
      "Iteration 12251, loss = 1.38951166\n",
      "Iteration 12252, loss = 1.38949615\n",
      "Iteration 12253, loss = 1.38948047\n",
      "Iteration 12254, loss = 1.38946466\n",
      "Iteration 12255, loss = 1.38944872\n",
      "Iteration 12256, loss = 1.38943264\n",
      "Iteration 12257, loss = 1.38941646\n",
      "Iteration 12258, loss = 1.38940019\n",
      "Iteration 12259, loss = 1.38938385\n",
      "Iteration 12260, loss = 1.38936750\n",
      "Iteration 12261, loss = 1.38935116\n",
      "Iteration 12262, loss = 1.38933485\n",
      "Iteration 12263, loss = 1.38931884\n",
      "Iteration 12264, loss = 1.38930281\n",
      "Iteration 12265, loss = 1.38928671\n",
      "Iteration 12266, loss = 1.38927055\n",
      "Iteration 12267, loss = 1.38925435\n",
      "Iteration 12268, loss = 1.38923851\n",
      "Iteration 12269, loss = 1.38922224\n",
      "Iteration 12270, loss = 1.38920631\n",
      "Iteration 12271, loss = 1.38919031\n",
      "Iteration 12272, loss = 1.38917427\n",
      "Iteration 12273, loss = 1.38915817\n",
      "Iteration 12274, loss = 1.38914200\n",
      "Iteration 12275, loss = 1.38912577\n",
      "Iteration 12276, loss = 1.38911182\n",
      "Iteration 12277, loss = 1.38909390\n",
      "Iteration 12278, loss = 1.38907826\n",
      "Iteration 12279, loss = 1.38906249\n",
      "Iteration 12280, loss = 1.38904659\n",
      "Iteration 12281, loss = 1.38903056\n",
      "Iteration 12282, loss = 1.38901443\n",
      "Iteration 12283, loss = 1.38899823\n",
      "Iteration 12284, loss = 1.38898198\n",
      "Iteration 12285, loss = 1.38896568\n",
      "Iteration 12286, loss = 1.38895060\n",
      "Iteration 12287, loss = 1.38893397\n",
      "Iteration 12288, loss = 1.38891846\n",
      "Iteration 12289, loss = 1.38890291\n",
      "Iteration 12290, loss = 1.38888722\n",
      "Iteration 12291, loss = 1.38887139\n",
      "Iteration 12292, loss = 1.38885543\n",
      "Iteration 12293, loss = 1.38883937\n",
      "Iteration 12294, loss = 1.38882320\n",
      "Iteration 12295, loss = 1.38880698\n",
      "Iteration 12296, loss = 1.38879077\n",
      "Iteration 12297, loss = 1.38877455\n",
      "Iteration 12298, loss = 1.38875830\n",
      "Iteration 12299, loss = 1.38874202\n",
      "Iteration 12300, loss = 1.38872577\n",
      "Iteration 12301, loss = 1.38870952\n",
      "Iteration 12302, loss = 1.38869692\n",
      "Iteration 12303, loss = 1.38867769\n",
      "Iteration 12304, loss = 1.38866212\n",
      "Iteration 12305, loss = 1.38864655\n",
      "Iteration 12306, loss = 1.38863097\n",
      "Iteration 12307, loss = 1.38861531\n",
      "Iteration 12308, loss = 1.38859950\n",
      "Iteration 12309, loss = 1.38858354\n",
      "Iteration 12310, loss = 1.38856745\n",
      "Iteration 12311, loss = 1.38855123\n",
      "Iteration 12312, loss = 1.38853499\n",
      "Iteration 12313, loss = 1.38851876\n",
      "Iteration 12314, loss = 1.38850253\n",
      "Iteration 12315, loss = 1.38848630\n",
      "Iteration 12316, loss = 1.38847006\n",
      "Iteration 12317, loss = 1.38845387\n",
      "Iteration 12318, loss = 1.38843795\n",
      "Iteration 12319, loss = 1.38842202\n",
      "Iteration 12320, loss = 1.38840606\n",
      "Iteration 12321, loss = 1.38839007\n",
      "Iteration 12322, loss = 1.38837489\n",
      "Iteration 12323, loss = 1.38835857\n",
      "Iteration 12324, loss = 1.38834312\n",
      "Iteration 12325, loss = 1.38832762\n",
      "Iteration 12326, loss = 1.38831197\n",
      "Iteration 12327, loss = 1.38829619\n",
      "Iteration 12328, loss = 1.38828028\n",
      "Iteration 12329, loss = 1.38826426\n",
      "Iteration 12330, loss = 1.38824813\n",
      "Iteration 12331, loss = 1.38823193\n",
      "Iteration 12332, loss = 1.38821567\n",
      "Iteration 12333, loss = 1.38819943\n",
      "Iteration 12334, loss = 1.38818321\n",
      "Iteration 12335, loss = 1.38816706\n",
      "Iteration 12336, loss = 1.38815118\n",
      "Iteration 12337, loss = 1.38813531\n",
      "Iteration 12338, loss = 1.38811940\n",
      "Iteration 12339, loss = 1.38810344\n",
      "Iteration 12340, loss = 1.38808766\n",
      "Iteration 12341, loss = 1.38807180\n",
      "Iteration 12342, loss = 1.38805611\n",
      "Iteration 12343, loss = 1.38804036\n",
      "Iteration 12344, loss = 1.38802452\n",
      "Iteration 12345, loss = 1.38800860\n",
      "Iteration 12346, loss = 1.38799261\n",
      "Iteration 12347, loss = 1.38797654\n",
      "Iteration 12348, loss = 1.38796045\n",
      "Iteration 12349, loss = 1.38794438\n",
      "Iteration 12350, loss = 1.38792840\n",
      "Iteration 12351, loss = 1.38791253\n",
      "Iteration 12352, loss = 1.38789676\n",
      "Iteration 12353, loss = 1.38788093\n",
      "Iteration 12354, loss = 1.38786503\n",
      "Iteration 12355, loss = 1.38784907\n",
      "Iteration 12356, loss = 1.38783306\n",
      "Iteration 12357, loss = 1.38781702\n",
      "Iteration 12358, loss = 1.38780230\n",
      "Iteration 12359, loss = 1.38778533\n",
      "Iteration 12360, loss = 1.38776969\n",
      "Iteration 12361, loss = 1.38775392\n",
      "Iteration 12362, loss = 1.38773803\n",
      "Iteration 12363, loss = 1.38772221\n",
      "Iteration 12364, loss = 1.38770681\n",
      "Iteration 12365, loss = 1.38769146\n",
      "Iteration 12366, loss = 1.38767596\n",
      "Iteration 12367, loss = 1.38766030\n",
      "Iteration 12368, loss = 1.38764451\n",
      "Iteration 12369, loss = 1.38762862\n",
      "Iteration 12370, loss = 1.38761266\n",
      "Iteration 12371, loss = 1.38759665\n",
      "Iteration 12372, loss = 1.38758063\n",
      "Iteration 12373, loss = 1.38756460\n",
      "Iteration 12374, loss = 1.38754858\n",
      "Iteration 12375, loss = 1.38753255\n",
      "Iteration 12376, loss = 1.38751960\n",
      "Iteration 12377, loss = 1.38750113\n",
      "Iteration 12378, loss = 1.38748574\n",
      "Iteration 12379, loss = 1.38747028\n",
      "Iteration 12380, loss = 1.38745478\n",
      "Iteration 12381, loss = 1.38743919\n",
      "Iteration 12382, loss = 1.38742353\n",
      "Iteration 12383, loss = 1.38740777\n",
      "Iteration 12384, loss = 1.38739188\n",
      "Iteration 12385, loss = 1.38737587\n",
      "Iteration 12386, loss = 1.38735985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12387, loss = 1.38734387\n",
      "Iteration 12388, loss = 1.38732786\n",
      "Iteration 12389, loss = 1.38731187\n",
      "Iteration 12390, loss = 1.38729589\n",
      "Iteration 12391, loss = 1.38727990\n",
      "Iteration 12392, loss = 1.38726388\n",
      "Iteration 12393, loss = 1.38724979\n",
      "Iteration 12394, loss = 1.38723237\n",
      "Iteration 12395, loss = 1.38721655\n",
      "Iteration 12396, loss = 1.38720119\n",
      "Iteration 12397, loss = 1.38718587\n",
      "Iteration 12398, loss = 1.38717042\n",
      "Iteration 12399, loss = 1.38715484\n",
      "Iteration 12400, loss = 1.38713909\n",
      "Iteration 12401, loss = 1.38712322\n",
      "Iteration 12402, loss = 1.38710724\n",
      "Iteration 12403, loss = 1.38709124\n",
      "Iteration 12404, loss = 1.38707522\n",
      "Iteration 12405, loss = 1.38705921\n",
      "Iteration 12406, loss = 1.38704320\n",
      "Iteration 12407, loss = 1.38702757\n",
      "Iteration 12408, loss = 1.38701169\n",
      "Iteration 12409, loss = 1.38699613\n",
      "Iteration 12410, loss = 1.38698052\n",
      "Iteration 12411, loss = 1.38696488\n",
      "Iteration 12412, loss = 1.38694916\n",
      "Iteration 12413, loss = 1.38693332\n",
      "Iteration 12414, loss = 1.38691741\n",
      "Iteration 12415, loss = 1.38690179\n",
      "Iteration 12416, loss = 1.38688599\n",
      "Iteration 12417, loss = 1.38687042\n",
      "Iteration 12418, loss = 1.38685475\n",
      "Iteration 12419, loss = 1.38683902\n",
      "Iteration 12420, loss = 1.38682326\n",
      "Iteration 12421, loss = 1.38680748\n",
      "Iteration 12422, loss = 1.38679165\n",
      "Iteration 12423, loss = 1.38677579\n",
      "Iteration 12424, loss = 1.38675992\n",
      "Iteration 12425, loss = 1.38674476\n",
      "Iteration 12426, loss = 1.38672852\n",
      "Iteration 12427, loss = 1.38671300\n",
      "Iteration 12428, loss = 1.38669741\n",
      "Iteration 12429, loss = 1.38668175\n",
      "Iteration 12430, loss = 1.38666599\n",
      "Iteration 12431, loss = 1.38665012\n",
      "Iteration 12432, loss = 1.38663423\n",
      "Iteration 12433, loss = 1.38661835\n",
      "Iteration 12434, loss = 1.38660270\n",
      "Iteration 12435, loss = 1.38658691\n",
      "Iteration 12436, loss = 1.38657130\n",
      "Iteration 12437, loss = 1.38655562\n",
      "Iteration 12438, loss = 1.38653989\n",
      "Iteration 12439, loss = 1.38652412\n",
      "Iteration 12440, loss = 1.38650833\n",
      "Iteration 12441, loss = 1.38649323\n",
      "Iteration 12442, loss = 1.38647715\n",
      "Iteration 12443, loss = 1.38646181\n",
      "Iteration 12444, loss = 1.38644633\n",
      "Iteration 12445, loss = 1.38643073\n",
      "Iteration 12446, loss = 1.38641502\n",
      "Iteration 12447, loss = 1.38639921\n",
      "Iteration 12448, loss = 1.38638337\n",
      "Iteration 12449, loss = 1.38636827\n",
      "Iteration 12450, loss = 1.38635212\n",
      "Iteration 12451, loss = 1.38633663\n",
      "Iteration 12452, loss = 1.38632109\n",
      "Iteration 12453, loss = 1.38630546\n",
      "Iteration 12454, loss = 1.38628976\n",
      "Iteration 12455, loss = 1.38627402\n",
      "Iteration 12456, loss = 1.38625825\n",
      "Iteration 12457, loss = 1.38624245\n",
      "Iteration 12458, loss = 1.38622667\n",
      "Iteration 12459, loss = 1.38621339\n",
      "Iteration 12460, loss = 1.38619585\n",
      "Iteration 12461, loss = 1.38618100\n",
      "Iteration 12462, loss = 1.38616598\n",
      "Iteration 12463, loss = 1.38615076\n",
      "Iteration 12464, loss = 1.38613536\n",
      "Iteration 12465, loss = 1.38611980\n",
      "Iteration 12466, loss = 1.38610409\n",
      "Iteration 12467, loss = 1.38608827\n",
      "Iteration 12468, loss = 1.38607240\n",
      "Iteration 12469, loss = 1.38605655\n",
      "Iteration 12470, loss = 1.38604070\n",
      "Iteration 12471, loss = 1.38602489\n",
      "Iteration 12472, loss = 1.38600907\n",
      "Iteration 12473, loss = 1.38599325\n",
      "Iteration 12474, loss = 1.38597743\n",
      "Iteration 12475, loss = 1.38596159\n",
      "Iteration 12476, loss = 1.38594681\n",
      "Iteration 12477, loss = 1.38593016\n",
      "Iteration 12478, loss = 1.38591453\n",
      "Iteration 12479, loss = 1.38589891\n",
      "Iteration 12480, loss = 1.38588370\n",
      "Iteration 12481, loss = 1.38586816\n",
      "Iteration 12482, loss = 1.38585300\n",
      "Iteration 12483, loss = 1.38583783\n",
      "Iteration 12484, loss = 1.38582256\n",
      "Iteration 12485, loss = 1.38580724\n",
      "Iteration 12486, loss = 1.38579183\n",
      "Iteration 12487, loss = 1.38577628\n",
      "Iteration 12488, loss = 1.38576063\n",
      "Iteration 12489, loss = 1.38574486\n",
      "Iteration 12490, loss = 1.38572900\n",
      "Iteration 12491, loss = 1.38571310\n",
      "Iteration 12492, loss = 1.38569720\n",
      "Iteration 12493, loss = 1.38568134\n",
      "Iteration 12494, loss = 1.38566554\n",
      "Iteration 12495, loss = 1.38565005\n",
      "Iteration 12496, loss = 1.38563453\n",
      "Iteration 12497, loss = 1.38561895\n",
      "Iteration 12498, loss = 1.38560335\n",
      "Iteration 12499, loss = 1.38558770\n",
      "Iteration 12500, loss = 1.38557200\n",
      "Iteration 12501, loss = 1.38555626\n",
      "Iteration 12502, loss = 1.38554208\n",
      "Iteration 12503, loss = 1.38552574\n",
      "Iteration 12504, loss = 1.38551104\n",
      "Iteration 12505, loss = 1.38549612\n",
      "Iteration 12506, loss = 1.38548101\n",
      "Iteration 12507, loss = 1.38546571\n",
      "Iteration 12508, loss = 1.38545027\n",
      "Iteration 12509, loss = 1.38543470\n",
      "Iteration 12510, loss = 1.38541902\n",
      "Iteration 12511, loss = 1.38540323\n",
      "Iteration 12512, loss = 1.38538739\n",
      "Iteration 12513, loss = 1.38537159\n",
      "Iteration 12514, loss = 1.38535579\n",
      "Iteration 12515, loss = 1.38534005\n",
      "Iteration 12516, loss = 1.38532435\n",
      "Iteration 12517, loss = 1.38530863\n",
      "Iteration 12518, loss = 1.38529291\n",
      "Iteration 12519, loss = 1.38527718\n",
      "Iteration 12520, loss = 1.38526314\n",
      "Iteration 12521, loss = 1.38524601\n",
      "Iteration 12522, loss = 1.38523086\n",
      "Iteration 12523, loss = 1.38521571\n",
      "Iteration 12524, loss = 1.38520049\n",
      "Iteration 12525, loss = 1.38518520\n",
      "Iteration 12526, loss = 1.38516988\n",
      "Iteration 12527, loss = 1.38515451\n",
      "Iteration 12528, loss = 1.38513905\n",
      "Iteration 12529, loss = 1.38512350\n",
      "Iteration 12530, loss = 1.38510786\n",
      "Iteration 12531, loss = 1.38509217\n",
      "Iteration 12532, loss = 1.38507645\n",
      "Iteration 12533, loss = 1.38506074\n",
      "Iteration 12534, loss = 1.38504501\n",
      "Iteration 12535, loss = 1.38502925\n",
      "Iteration 12536, loss = 1.38501608\n",
      "Iteration 12537, loss = 1.38499829\n",
      "Iteration 12538, loss = 1.38498309\n",
      "Iteration 12539, loss = 1.38496786\n",
      "Iteration 12540, loss = 1.38495250\n",
      "Iteration 12541, loss = 1.38493702\n",
      "Iteration 12542, loss = 1.38492143\n",
      "Iteration 12543, loss = 1.38490574\n",
      "Iteration 12544, loss = 1.38489001\n",
      "Iteration 12545, loss = 1.38487639\n",
      "Iteration 12546, loss = 1.38485902\n",
      "Iteration 12547, loss = 1.38484370\n",
      "Iteration 12548, loss = 1.38482832\n",
      "Iteration 12549, loss = 1.38481288\n",
      "Iteration 12550, loss = 1.38479739\n",
      "Iteration 12551, loss = 1.38478187\n",
      "Iteration 12552, loss = 1.38476633\n",
      "Iteration 12553, loss = 1.38475090\n",
      "Iteration 12554, loss = 1.38473557\n",
      "Iteration 12555, loss = 1.38472037\n",
      "Iteration 12556, loss = 1.38470510\n",
      "Iteration 12557, loss = 1.38468973\n",
      "Iteration 12558, loss = 1.38467425\n",
      "Iteration 12559, loss = 1.38465869\n",
      "Iteration 12560, loss = 1.38464313\n",
      "Iteration 12561, loss = 1.38462758\n",
      "Iteration 12562, loss = 1.38461201\n",
      "Iteration 12563, loss = 1.38459641\n",
      "Iteration 12564, loss = 1.38458081\n",
      "Iteration 12565, loss = 1.38456711\n",
      "Iteration 12566, loss = 1.38455030\n",
      "Iteration 12567, loss = 1.38453551\n",
      "Iteration 12568, loss = 1.38452068\n",
      "Iteration 12569, loss = 1.38450573\n",
      "Iteration 12570, loss = 1.38449060\n",
      "Iteration 12571, loss = 1.38447530\n",
      "Iteration 12572, loss = 1.38445986\n",
      "Iteration 12573, loss = 1.38444431\n",
      "Iteration 12574, loss = 1.38442869\n",
      "Iteration 12575, loss = 1.38441311\n",
      "Iteration 12576, loss = 1.38439751\n",
      "Iteration 12577, loss = 1.38438193\n",
      "Iteration 12578, loss = 1.38436634\n",
      "Iteration 12579, loss = 1.38435074\n",
      "Iteration 12580, loss = 1.38433513\n",
      "Iteration 12581, loss = 1.38431949\n",
      "Iteration 12582, loss = 1.38430601\n",
      "Iteration 12583, loss = 1.38428862\n",
      "Iteration 12584, loss = 1.38427340\n",
      "Iteration 12585, loss = 1.38425814\n",
      "Iteration 12586, loss = 1.38424286\n",
      "Iteration 12587, loss = 1.38422755\n",
      "Iteration 12588, loss = 1.38421218\n",
      "Iteration 12589, loss = 1.38419674\n",
      "Iteration 12590, loss = 1.38418125\n",
      "Iteration 12591, loss = 1.38416570\n",
      "Iteration 12592, loss = 1.38415191\n",
      "Iteration 12593, loss = 1.38413490\n",
      "Iteration 12594, loss = 1.38411963\n",
      "Iteration 12595, loss = 1.38410432\n",
      "Iteration 12596, loss = 1.38408896\n",
      "Iteration 12597, loss = 1.38407356\n",
      "Iteration 12598, loss = 1.38405811\n",
      "Iteration 12599, loss = 1.38404261\n",
      "Iteration 12600, loss = 1.38402968\n",
      "Iteration 12601, loss = 1.38401242\n",
      "Iteration 12602, loss = 1.38399780\n",
      "Iteration 12603, loss = 1.38398305\n",
      "Iteration 12604, loss = 1.38396815\n",
      "Iteration 12605, loss = 1.38395310\n",
      "Iteration 12606, loss = 1.38393791\n",
      "Iteration 12607, loss = 1.38392258\n",
      "Iteration 12608, loss = 1.38390712\n",
      "Iteration 12609, loss = 1.38389162\n",
      "Iteration 12610, loss = 1.38387604\n",
      "Iteration 12611, loss = 1.38386046\n",
      "Iteration 12612, loss = 1.38384496\n",
      "Iteration 12613, loss = 1.38382949\n",
      "Iteration 12614, loss = 1.38381402\n",
      "Iteration 12615, loss = 1.38379856\n",
      "Iteration 12616, loss = 1.38378307\n",
      "Iteration 12617, loss = 1.38376755\n",
      "Iteration 12618, loss = 1.38375201\n",
      "Iteration 12619, loss = 1.38373792\n",
      "Iteration 12620, loss = 1.38372116\n",
      "Iteration 12621, loss = 1.38370586\n",
      "Iteration 12622, loss = 1.38369060\n",
      "Iteration 12623, loss = 1.38367530\n",
      "Iteration 12624, loss = 1.38365995\n",
      "Iteration 12625, loss = 1.38364456\n",
      "Iteration 12626, loss = 1.38362914\n",
      "Iteration 12627, loss = 1.38361370\n",
      "Iteration 12628, loss = 1.38359823\n",
      "Iteration 12629, loss = 1.38358380\n",
      "Iteration 12630, loss = 1.38356764\n",
      "Iteration 12631, loss = 1.38355247\n",
      "Iteration 12632, loss = 1.38353721\n",
      "Iteration 12633, loss = 1.38352188\n",
      "Iteration 12634, loss = 1.38350650\n",
      "Iteration 12635, loss = 1.38349108\n",
      "Iteration 12636, loss = 1.38347705\n",
      "Iteration 12637, loss = 1.38346149\n",
      "Iteration 12638, loss = 1.38344722\n",
      "Iteration 12639, loss = 1.38343272\n",
      "Iteration 12640, loss = 1.38341800\n",
      "Iteration 12641, loss = 1.38340309\n",
      "Iteration 12642, loss = 1.38338803\n",
      "Iteration 12643, loss = 1.38337286\n",
      "Iteration 12644, loss = 1.38335758\n",
      "Iteration 12645, loss = 1.38334219\n",
      "Iteration 12646, loss = 1.38332673\n",
      "Iteration 12647, loss = 1.38331124\n",
      "Iteration 12648, loss = 1.38329574\n",
      "Iteration 12649, loss = 1.38328032\n",
      "Iteration 12650, loss = 1.38326487\n",
      "Iteration 12651, loss = 1.38324943\n",
      "Iteration 12652, loss = 1.38323395\n",
      "Iteration 12653, loss = 1.38321845\n",
      "Iteration 12654, loss = 1.38320296\n",
      "Iteration 12655, loss = 1.38318749\n",
      "Iteration 12656, loss = 1.38317202\n",
      "Iteration 12657, loss = 1.38315905\n",
      "Iteration 12658, loss = 1.38314227\n",
      "Iteration 12659, loss = 1.38312635\n",
      "Iteration 12660, loss = 1.38311142\n",
      "Iteration 12661, loss = 1.38309645\n",
      "Iteration 12662, loss = 1.38308147\n",
      "Iteration 12663, loss = 1.38306647\n",
      "Iteration 12664, loss = 1.38305136\n",
      "Iteration 12665, loss = 1.38303616\n",
      "Iteration 12666, loss = 1.38302091\n",
      "Iteration 12667, loss = 1.38300557\n",
      "Iteration 12668, loss = 1.38299015\n",
      "Iteration 12669, loss = 1.38297470\n",
      "Iteration 12670, loss = 1.38295931\n",
      "Iteration 12671, loss = 1.38294392\n",
      "Iteration 12672, loss = 1.38292851\n",
      "Iteration 12673, loss = 1.38291309\n",
      "Iteration 12674, loss = 1.38289840\n",
      "Iteration 12675, loss = 1.38288253\n",
      "Iteration 12676, loss = 1.38286742\n",
      "Iteration 12677, loss = 1.38285227\n",
      "Iteration 12678, loss = 1.38283711\n",
      "Iteration 12679, loss = 1.38282189\n",
      "Iteration 12680, loss = 1.38280661\n",
      "Iteration 12681, loss = 1.38279129\n",
      "Iteration 12682, loss = 1.38277594\n",
      "Iteration 12683, loss = 1.38276057\n",
      "Iteration 12684, loss = 1.38274681\n",
      "Iteration 12685, loss = 1.38273022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12686, loss = 1.38271520\n",
      "Iteration 12687, loss = 1.38270010\n",
      "Iteration 12688, loss = 1.38268492\n",
      "Iteration 12689, loss = 1.38266966\n",
      "Iteration 12690, loss = 1.38265438\n",
      "Iteration 12691, loss = 1.38263909\n",
      "Iteration 12692, loss = 1.38262379\n",
      "Iteration 12693, loss = 1.38260856\n",
      "Iteration 12694, loss = 1.38259348\n",
      "Iteration 12695, loss = 1.38257849\n",
      "Iteration 12696, loss = 1.38256345\n",
      "Iteration 12697, loss = 1.38254834\n",
      "Iteration 12698, loss = 1.38253315\n",
      "Iteration 12699, loss = 1.38251792\n",
      "Iteration 12700, loss = 1.38250344\n",
      "Iteration 12701, loss = 1.38248809\n",
      "Iteration 12702, loss = 1.38247345\n",
      "Iteration 12703, loss = 1.38245865\n",
      "Iteration 12704, loss = 1.38244370\n",
      "Iteration 12705, loss = 1.38242862\n",
      "Iteration 12706, loss = 1.38241343\n",
      "Iteration 12707, loss = 1.38239816\n",
      "Iteration 12708, loss = 1.38238287\n",
      "Iteration 12709, loss = 1.38236760\n",
      "Iteration 12710, loss = 1.38235235\n",
      "Iteration 12711, loss = 1.38233709\n",
      "Iteration 12712, loss = 1.38232179\n",
      "Iteration 12713, loss = 1.38230803\n",
      "Iteration 12714, loss = 1.38229145\n",
      "Iteration 12715, loss = 1.38227641\n",
      "Iteration 12716, loss = 1.38226131\n",
      "Iteration 12717, loss = 1.38224618\n",
      "Iteration 12718, loss = 1.38223101\n",
      "Iteration 12719, loss = 1.38221581\n",
      "Iteration 12720, loss = 1.38220057\n",
      "Iteration 12721, loss = 1.38218533\n",
      "Iteration 12722, loss = 1.38217007\n",
      "Iteration 12723, loss = 1.38215634\n",
      "Iteration 12724, loss = 1.38213984\n",
      "Iteration 12725, loss = 1.38212491\n",
      "Iteration 12726, loss = 1.38210991\n",
      "Iteration 12727, loss = 1.38209481\n",
      "Iteration 12728, loss = 1.38207966\n",
      "Iteration 12729, loss = 1.38206443\n",
      "Iteration 12730, loss = 1.38204920\n",
      "Iteration 12731, loss = 1.38203519\n",
      "Iteration 12732, loss = 1.38201909\n",
      "Iteration 12733, loss = 1.38200418\n",
      "Iteration 12734, loss = 1.38198921\n",
      "Iteration 12735, loss = 1.38197416\n",
      "Iteration 12736, loss = 1.38195907\n",
      "Iteration 12737, loss = 1.38194392\n",
      "Iteration 12738, loss = 1.38192875\n",
      "Iteration 12739, loss = 1.38191357\n",
      "Iteration 12740, loss = 1.38189838\n",
      "Iteration 12741, loss = 1.38188317\n",
      "Iteration 12742, loss = 1.38186907\n",
      "Iteration 12743, loss = 1.38185302\n",
      "Iteration 12744, loss = 1.38183819\n",
      "Iteration 12745, loss = 1.38182329\n",
      "Iteration 12746, loss = 1.38180826\n",
      "Iteration 12747, loss = 1.38179315\n",
      "Iteration 12748, loss = 1.38177800\n",
      "Iteration 12749, loss = 1.38176282\n",
      "Iteration 12750, loss = 1.38174762\n",
      "Iteration 12751, loss = 1.38173323\n",
      "Iteration 12752, loss = 1.38171755\n",
      "Iteration 12753, loss = 1.38170263\n",
      "Iteration 12754, loss = 1.38168768\n",
      "Iteration 12755, loss = 1.38167267\n",
      "Iteration 12756, loss = 1.38165760\n",
      "Iteration 12757, loss = 1.38164244\n",
      "Iteration 12758, loss = 1.38162727\n",
      "Iteration 12759, loss = 1.38161212\n",
      "Iteration 12760, loss = 1.38159696\n",
      "Iteration 12761, loss = 1.38158359\n",
      "Iteration 12762, loss = 1.38156711\n",
      "Iteration 12763, loss = 1.38155255\n",
      "Iteration 12764, loss = 1.38153786\n",
      "Iteration 12765, loss = 1.38152302\n",
      "Iteration 12766, loss = 1.38150804\n",
      "Iteration 12767, loss = 1.38149294\n",
      "Iteration 12768, loss = 1.38147774\n",
      "Iteration 12769, loss = 1.38146253\n",
      "Iteration 12770, loss = 1.38144732\n",
      "Iteration 12771, loss = 1.38143214\n",
      "Iteration 12772, loss = 1.38141902\n",
      "Iteration 12773, loss = 1.38140250\n",
      "Iteration 12774, loss = 1.38138804\n",
      "Iteration 12775, loss = 1.38137354\n",
      "Iteration 12776, loss = 1.38135899\n",
      "Iteration 12777, loss = 1.38134429\n",
      "Iteration 12778, loss = 1.38132941\n",
      "Iteration 12779, loss = 1.38131438\n",
      "Iteration 12780, loss = 1.38129925\n",
      "Iteration 12781, loss = 1.38128412\n",
      "Iteration 12782, loss = 1.38126906\n",
      "Iteration 12783, loss = 1.38125397\n",
      "Iteration 12784, loss = 1.38123885\n",
      "Iteration 12785, loss = 1.38122372\n",
      "Iteration 12786, loss = 1.38120858\n",
      "Iteration 12787, loss = 1.38119340\n",
      "Iteration 12788, loss = 1.38117821\n",
      "Iteration 12789, loss = 1.38116304\n",
      "Iteration 12790, loss = 1.38114784\n",
      "Iteration 12791, loss = 1.38113266\n",
      "Iteration 12792, loss = 1.38111836\n",
      "Iteration 12793, loss = 1.38110256\n",
      "Iteration 12794, loss = 1.38108764\n",
      "Iteration 12795, loss = 1.38107270\n",
      "Iteration 12796, loss = 1.38105776\n",
      "Iteration 12797, loss = 1.38104282\n",
      "Iteration 12798, loss = 1.38102780\n",
      "Iteration 12799, loss = 1.38101272\n",
      "Iteration 12800, loss = 1.38099757\n",
      "Iteration 12801, loss = 1.38098239\n",
      "Iteration 12802, loss = 1.38096720\n",
      "Iteration 12803, loss = 1.38095564\n",
      "Iteration 12804, loss = 1.38093772\n",
      "Iteration 12805, loss = 1.38092347\n",
      "Iteration 12806, loss = 1.38090910\n",
      "Iteration 12807, loss = 1.38089453\n",
      "Iteration 12808, loss = 1.38087978\n",
      "Iteration 12809, loss = 1.38086486\n",
      "Iteration 12810, loss = 1.38084979\n",
      "Iteration 12811, loss = 1.38083465\n",
      "Iteration 12812, loss = 1.38081949\n",
      "Iteration 12813, loss = 1.38080440\n",
      "Iteration 12814, loss = 1.38078937\n",
      "Iteration 12815, loss = 1.38077430\n",
      "Iteration 12816, loss = 1.38075923\n",
      "Iteration 12817, loss = 1.38074416\n",
      "Iteration 12818, loss = 1.38072907\n",
      "Iteration 12819, loss = 1.38071397\n",
      "Iteration 12820, loss = 1.38069886\n",
      "Iteration 12821, loss = 1.38068394\n",
      "Iteration 12822, loss = 1.38066881\n",
      "Iteration 12823, loss = 1.38065390\n",
      "Iteration 12824, loss = 1.38063904\n",
      "Iteration 12825, loss = 1.38062415\n",
      "Iteration 12826, loss = 1.38060922\n",
      "Iteration 12827, loss = 1.38059426\n",
      "Iteration 12828, loss = 1.38057924\n",
      "Iteration 12829, loss = 1.38056415\n",
      "Iteration 12830, loss = 1.38054904\n",
      "Iteration 12831, loss = 1.38053499\n",
      "Iteration 12832, loss = 1.38051922\n",
      "Iteration 12833, loss = 1.38050444\n",
      "Iteration 12834, loss = 1.38048959\n",
      "Iteration 12835, loss = 1.38047470\n",
      "Iteration 12836, loss = 1.38045980\n",
      "Iteration 12837, loss = 1.38044485\n",
      "Iteration 12838, loss = 1.38042985\n",
      "Iteration 12839, loss = 1.38041602\n",
      "Iteration 12840, loss = 1.38040037\n",
      "Iteration 12841, loss = 1.38038592\n",
      "Iteration 12842, loss = 1.38037135\n",
      "Iteration 12843, loss = 1.38035663\n",
      "Iteration 12844, loss = 1.38034179\n",
      "Iteration 12845, loss = 1.38032685\n",
      "Iteration 12846, loss = 1.38031187\n",
      "Iteration 12847, loss = 1.38029684\n",
      "Iteration 12848, loss = 1.38028177\n",
      "Iteration 12849, loss = 1.38026670\n",
      "Iteration 12850, loss = 1.38025164\n",
      "Iteration 12851, loss = 1.38023662\n",
      "Iteration 12852, loss = 1.38022212\n",
      "Iteration 12853, loss = 1.38020687\n",
      "Iteration 12854, loss = 1.38019209\n",
      "Iteration 12855, loss = 1.38017729\n",
      "Iteration 12856, loss = 1.38016246\n",
      "Iteration 12857, loss = 1.38014760\n",
      "Iteration 12858, loss = 1.38013269\n",
      "Iteration 12859, loss = 1.38011776\n",
      "Iteration 12860, loss = 1.38010279\n",
      "Iteration 12861, loss = 1.38008779\n",
      "Iteration 12862, loss = 1.38007363\n",
      "Iteration 12863, loss = 1.38005817\n",
      "Iteration 12864, loss = 1.38004353\n",
      "Iteration 12865, loss = 1.38002882\n",
      "Iteration 12866, loss = 1.38001403\n",
      "Iteration 12867, loss = 1.37999913\n",
      "Iteration 12868, loss = 1.37998417\n",
      "Iteration 12869, loss = 1.37996919\n",
      "Iteration 12870, loss = 1.37995421\n",
      "Iteration 12871, loss = 1.37993922\n",
      "Iteration 12872, loss = 1.37992434\n",
      "Iteration 12873, loss = 1.37990958\n",
      "Iteration 12874, loss = 1.37989493\n",
      "Iteration 12875, loss = 1.37988024\n",
      "Iteration 12876, loss = 1.37986546\n",
      "Iteration 12877, loss = 1.37985058\n",
      "Iteration 12878, loss = 1.37983568\n",
      "Iteration 12879, loss = 1.37982075\n",
      "Iteration 12880, loss = 1.37980580\n",
      "Iteration 12881, loss = 1.37979087\n",
      "Iteration 12882, loss = 1.37977597\n",
      "Iteration 12883, loss = 1.37976132\n",
      "Iteration 12884, loss = 1.37974669\n",
      "Iteration 12885, loss = 1.37973199\n",
      "Iteration 12886, loss = 1.37971720\n",
      "Iteration 12887, loss = 1.37970232\n",
      "Iteration 12888, loss = 1.37968744\n",
      "Iteration 12889, loss = 1.37967254\n",
      "Iteration 12890, loss = 1.37965761\n",
      "Iteration 12891, loss = 1.37964266\n",
      "Iteration 12892, loss = 1.37962769\n",
      "Iteration 12893, loss = 1.37961462\n",
      "Iteration 12894, loss = 1.37959810\n",
      "Iteration 12895, loss = 1.37958348\n",
      "Iteration 12896, loss = 1.37956883\n",
      "Iteration 12897, loss = 1.37955411\n",
      "Iteration 12898, loss = 1.37953930\n",
      "Iteration 12899, loss = 1.37952442\n",
      "Iteration 12900, loss = 1.37950946\n",
      "Iteration 12901, loss = 1.37949454\n",
      "Iteration 12902, loss = 1.37947965\n",
      "Iteration 12903, loss = 1.37946475\n",
      "Iteration 12904, loss = 1.37945210\n",
      "Iteration 12905, loss = 1.37943565\n",
      "Iteration 12906, loss = 1.37942175\n",
      "Iteration 12907, loss = 1.37940764\n",
      "Iteration 12908, loss = 1.37939331\n",
      "Iteration 12909, loss = 1.37937879\n",
      "Iteration 12910, loss = 1.37936410\n",
      "Iteration 12911, loss = 1.37934925\n",
      "Iteration 12912, loss = 1.37933428\n",
      "Iteration 12913, loss = 1.37931934\n",
      "Iteration 12914, loss = 1.37930444\n",
      "Iteration 12915, loss = 1.37928951\n",
      "Iteration 12916, loss = 1.37927464\n",
      "Iteration 12917, loss = 1.37925976\n",
      "Iteration 12918, loss = 1.37924485\n",
      "Iteration 12919, loss = 1.37922994\n",
      "Iteration 12920, loss = 1.37921501\n",
      "Iteration 12921, loss = 1.37920005\n",
      "Iteration 12922, loss = 1.37918511\n",
      "Iteration 12923, loss = 1.37917126\n",
      "Iteration 12924, loss = 1.37915548\n",
      "Iteration 12925, loss = 1.37914090\n",
      "Iteration 12926, loss = 1.37912635\n",
      "Iteration 12927, loss = 1.37911176\n",
      "Iteration 12928, loss = 1.37909713\n",
      "Iteration 12929, loss = 1.37908243\n",
      "Iteration 12930, loss = 1.37906765\n",
      "Iteration 12931, loss = 1.37905281\n",
      "Iteration 12932, loss = 1.37903791\n",
      "Iteration 12933, loss = 1.37902294\n",
      "Iteration 12934, loss = 1.37900794\n",
      "Iteration 12935, loss = 1.37899301\n",
      "Iteration 12936, loss = 1.37897810\n",
      "Iteration 12937, loss = 1.37896569\n",
      "Iteration 12938, loss = 1.37894892\n",
      "Iteration 12939, loss = 1.37893478\n",
      "Iteration 12940, loss = 1.37892059\n",
      "Iteration 12941, loss = 1.37890624\n",
      "Iteration 12942, loss = 1.37889174\n",
      "Iteration 12943, loss = 1.37887713\n",
      "Iteration 12944, loss = 1.37886237\n",
      "Iteration 12945, loss = 1.37884757\n",
      "Iteration 12946, loss = 1.37883277\n",
      "Iteration 12947, loss = 1.37881797\n",
      "Iteration 12948, loss = 1.37880318\n",
      "Iteration 12949, loss = 1.37878835\n",
      "Iteration 12950, loss = 1.37877349\n",
      "Iteration 12951, loss = 1.37875862\n",
      "Iteration 12952, loss = 1.37874374\n",
      "Iteration 12953, loss = 1.37872885\n",
      "Iteration 12954, loss = 1.37871393\n",
      "Iteration 12955, loss = 1.37869902\n",
      "Iteration 12956, loss = 1.37868412\n",
      "Iteration 12957, loss = 1.37866968\n",
      "Iteration 12958, loss = 1.37865451\n",
      "Iteration 12959, loss = 1.37863994\n",
      "Iteration 12960, loss = 1.37862537\n",
      "Iteration 12961, loss = 1.37861075\n",
      "Iteration 12962, loss = 1.37859606\n",
      "Iteration 12963, loss = 1.37858131\n",
      "Iteration 12964, loss = 1.37856649\n",
      "Iteration 12965, loss = 1.37855161\n",
      "Iteration 12966, loss = 1.37853671\n",
      "Iteration 12967, loss = 1.37852181\n",
      "Iteration 12968, loss = 1.37850730\n",
      "Iteration 12969, loss = 1.37849250\n",
      "Iteration 12970, loss = 1.37847803\n",
      "Iteration 12971, loss = 1.37846350\n",
      "Iteration 12972, loss = 1.37844885\n",
      "Iteration 12973, loss = 1.37843408\n",
      "Iteration 12974, loss = 1.37841918\n",
      "Iteration 12975, loss = 1.37840432\n",
      "Iteration 12976, loss = 1.37838961\n",
      "Iteration 12977, loss = 1.37837514\n",
      "Iteration 12978, loss = 1.37836071\n",
      "Iteration 12979, loss = 1.37834619\n",
      "Iteration 12980, loss = 1.37833159\n",
      "Iteration 12981, loss = 1.37831693\n",
      "Iteration 12982, loss = 1.37830226\n",
      "Iteration 12983, loss = 1.37828754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12984, loss = 1.37827282\n",
      "Iteration 12985, loss = 1.37825807\n",
      "Iteration 12986, loss = 1.37824329\n",
      "Iteration 12987, loss = 1.37822850\n",
      "Iteration 12988, loss = 1.37821369\n",
      "Iteration 12989, loss = 1.37819886\n",
      "Iteration 12990, loss = 1.37818536\n",
      "Iteration 12991, loss = 1.37817020\n",
      "Iteration 12992, loss = 1.37815645\n",
      "Iteration 12993, loss = 1.37814251\n",
      "Iteration 12994, loss = 1.37812834\n",
      "Iteration 12995, loss = 1.37811398\n",
      "Iteration 12996, loss = 1.37809944\n",
      "Iteration 12997, loss = 1.37808474\n",
      "Iteration 12998, loss = 1.37806989\n",
      "Iteration 12999, loss = 1.37805498\n",
      "Iteration 13000, loss = 1.37804020\n",
      "Iteration 13001, loss = 1.37802538\n",
      "Iteration 13002, loss = 1.37801054\n",
      "Iteration 13003, loss = 1.37799575\n",
      "Iteration 13004, loss = 1.37798098\n",
      "Iteration 13005, loss = 1.37796623\n",
      "Iteration 13006, loss = 1.37795144\n",
      "Iteration 13007, loss = 1.37793665\n",
      "Iteration 13008, loss = 1.37792185\n",
      "Iteration 13009, loss = 1.37790706\n",
      "Iteration 13010, loss = 1.37789227\n",
      "Iteration 13011, loss = 1.37787905\n",
      "Iteration 13012, loss = 1.37786285\n",
      "Iteration 13013, loss = 1.37784823\n",
      "Iteration 13014, loss = 1.37783365\n",
      "Iteration 13015, loss = 1.37781903\n",
      "Iteration 13016, loss = 1.37780440\n",
      "Iteration 13017, loss = 1.37778976\n",
      "Iteration 13018, loss = 1.37777511\n",
      "Iteration 13019, loss = 1.37776043\n",
      "Iteration 13020, loss = 1.37774569\n",
      "Iteration 13021, loss = 1.37773090\n",
      "Iteration 13022, loss = 1.37771628\n",
      "Iteration 13023, loss = 1.37770163\n",
      "Iteration 13024, loss = 1.37768716\n",
      "Iteration 13025, loss = 1.37767268\n",
      "Iteration 13026, loss = 1.37765815\n",
      "Iteration 13027, loss = 1.37764354\n",
      "Iteration 13028, loss = 1.37762882\n",
      "Iteration 13029, loss = 1.37761403\n",
      "Iteration 13030, loss = 1.37760099\n",
      "Iteration 13031, loss = 1.37758515\n",
      "Iteration 13032, loss = 1.37757104\n",
      "Iteration 13033, loss = 1.37755679\n",
      "Iteration 13034, loss = 1.37754239\n",
      "Iteration 13035, loss = 1.37752788\n",
      "Iteration 13036, loss = 1.37751330\n",
      "Iteration 13037, loss = 1.37749867\n",
      "Iteration 13038, loss = 1.37748398\n",
      "Iteration 13039, loss = 1.37746923\n",
      "Iteration 13040, loss = 1.37745443\n",
      "Iteration 13041, loss = 1.37743968\n",
      "Iteration 13042, loss = 1.37742497\n",
      "Iteration 13043, loss = 1.37741111\n",
      "Iteration 13044, loss = 1.37739585\n",
      "Iteration 13045, loss = 1.37738139\n",
      "Iteration 13046, loss = 1.37736688\n",
      "Iteration 13047, loss = 1.37735234\n",
      "Iteration 13048, loss = 1.37733779\n",
      "Iteration 13049, loss = 1.37732322\n",
      "Iteration 13050, loss = 1.37730861\n",
      "Iteration 13051, loss = 1.37729395\n",
      "Iteration 13052, loss = 1.37727931\n",
      "Iteration 13053, loss = 1.37726468\n",
      "Iteration 13054, loss = 1.37724999\n",
      "Iteration 13055, loss = 1.37723526\n",
      "Iteration 13056, loss = 1.37722055\n",
      "Iteration 13057, loss = 1.37720629\n",
      "Iteration 13058, loss = 1.37719202\n",
      "Iteration 13059, loss = 1.37717763\n",
      "Iteration 13060, loss = 1.37716316\n",
      "Iteration 13061, loss = 1.37714862\n",
      "Iteration 13062, loss = 1.37713399\n",
      "Iteration 13063, loss = 1.37711931\n",
      "Iteration 13064, loss = 1.37710463\n",
      "Iteration 13065, loss = 1.37709000\n",
      "Iteration 13066, loss = 1.37707534\n",
      "Iteration 13067, loss = 1.37706263\n",
      "Iteration 13068, loss = 1.37704624\n",
      "Iteration 13069, loss = 1.37703179\n",
      "Iteration 13070, loss = 1.37701736\n",
      "Iteration 13071, loss = 1.37700293\n",
      "Iteration 13072, loss = 1.37698840\n",
      "Iteration 13073, loss = 1.37697373\n",
      "Iteration 13074, loss = 1.37695908\n",
      "Iteration 13075, loss = 1.37694445\n",
      "Iteration 13076, loss = 1.37692986\n",
      "Iteration 13077, loss = 1.37691608\n",
      "Iteration 13078, loss = 1.37690085\n",
      "Iteration 13079, loss = 1.37688655\n",
      "Iteration 13080, loss = 1.37687223\n",
      "Iteration 13081, loss = 1.37685784\n",
      "Iteration 13082, loss = 1.37684335\n",
      "Iteration 13083, loss = 1.37682876\n",
      "Iteration 13084, loss = 1.37681417\n",
      "Iteration 13085, loss = 1.37679958\n",
      "Iteration 13086, loss = 1.37678502\n",
      "Iteration 13087, loss = 1.37677040\n",
      "Iteration 13088, loss = 1.37675576\n",
      "Iteration 13089, loss = 1.37674110\n",
      "Iteration 13090, loss = 1.37672801\n",
      "Iteration 13091, loss = 1.37671250\n",
      "Iteration 13092, loss = 1.37669886\n",
      "Iteration 13093, loss = 1.37668507\n",
      "Iteration 13094, loss = 1.37667106\n",
      "Iteration 13095, loss = 1.37665685\n",
      "Iteration 13096, loss = 1.37664245\n",
      "Iteration 13097, loss = 1.37662790\n",
      "Iteration 13098, loss = 1.37661324\n",
      "Iteration 13099, loss = 1.37659860\n",
      "Iteration 13100, loss = 1.37658397\n",
      "Iteration 13101, loss = 1.37656935\n",
      "Iteration 13102, loss = 1.37655477\n",
      "Iteration 13103, loss = 1.37654022\n",
      "Iteration 13104, loss = 1.37652563\n",
      "Iteration 13105, loss = 1.37651103\n",
      "Iteration 13106, loss = 1.37649641\n",
      "Iteration 13107, loss = 1.37648178\n",
      "Iteration 13108, loss = 1.37646715\n",
      "Iteration 13109, loss = 1.37645250\n",
      "Iteration 13110, loss = 1.37643782\n",
      "Iteration 13111, loss = 1.37642311\n",
      "Iteration 13112, loss = 1.37640837\n",
      "Iteration 13113, loss = 1.37639368\n",
      "Iteration 13114, loss = 1.37638226\n",
      "Iteration 13115, loss = 1.37636488\n",
      "Iteration 13116, loss = 1.37635100\n",
      "Iteration 13117, loss = 1.37633707\n",
      "Iteration 13118, loss = 1.37632305\n",
      "Iteration 13119, loss = 1.37630904\n",
      "Iteration 13120, loss = 1.37629493\n",
      "Iteration 13121, loss = 1.37628067\n",
      "Iteration 13122, loss = 1.37626624\n",
      "Iteration 13123, loss = 1.37625165\n",
      "Iteration 13124, loss = 1.37623693\n",
      "Iteration 13125, loss = 1.37622219\n",
      "Iteration 13126, loss = 1.37620749\n",
      "Iteration 13127, loss = 1.37619279\n",
      "Iteration 13128, loss = 1.37617818\n",
      "Iteration 13129, loss = 1.37616361\n",
      "Iteration 13130, loss = 1.37614902\n",
      "Iteration 13131, loss = 1.37613439\n",
      "Iteration 13132, loss = 1.37611979\n",
      "Iteration 13133, loss = 1.37610517\n",
      "Iteration 13134, loss = 1.37609052\n",
      "Iteration 13135, loss = 1.37607584\n",
      "Iteration 13136, loss = 1.37606329\n",
      "Iteration 13137, loss = 1.37604673\n",
      "Iteration 13138, loss = 1.37603235\n",
      "Iteration 13139, loss = 1.37601792\n",
      "Iteration 13140, loss = 1.37600348\n",
      "Iteration 13141, loss = 1.37598904\n",
      "Iteration 13142, loss = 1.37597456\n",
      "Iteration 13143, loss = 1.37596009\n",
      "Iteration 13144, loss = 1.37594559\n",
      "Iteration 13145, loss = 1.37593105\n",
      "Iteration 13146, loss = 1.37591648\n",
      "Iteration 13147, loss = 1.37590188\n",
      "Iteration 13148, loss = 1.37588725\n",
      "Iteration 13149, loss = 1.37587315\n",
      "Iteration 13150, loss = 1.37585896\n",
      "Iteration 13151, loss = 1.37584464\n",
      "Iteration 13152, loss = 1.37583022\n",
      "Iteration 13153, loss = 1.37581576\n",
      "Iteration 13154, loss = 1.37580126\n",
      "Iteration 13155, loss = 1.37578676\n",
      "Iteration 13156, loss = 1.37577227\n",
      "Iteration 13157, loss = 1.37575808\n",
      "Iteration 13158, loss = 1.37574355\n",
      "Iteration 13159, loss = 1.37572936\n",
      "Iteration 13160, loss = 1.37571510\n",
      "Iteration 13161, loss = 1.37570077\n",
      "Iteration 13162, loss = 1.37568634\n",
      "Iteration 13163, loss = 1.37567184\n",
      "Iteration 13164, loss = 1.37565734\n",
      "Iteration 13165, loss = 1.37564287\n",
      "Iteration 13166, loss = 1.37562842\n",
      "Iteration 13167, loss = 1.37561393\n",
      "Iteration 13168, loss = 1.37559941\n",
      "Iteration 13169, loss = 1.37558489\n",
      "Iteration 13170, loss = 1.37557035\n",
      "Iteration 13171, loss = 1.37555673\n",
      "Iteration 13172, loss = 1.37554164\n",
      "Iteration 13173, loss = 1.37552749\n",
      "Iteration 13174, loss = 1.37551330\n",
      "Iteration 13175, loss = 1.37549901\n",
      "Iteration 13176, loss = 1.37548464\n",
      "Iteration 13177, loss = 1.37547021\n",
      "Iteration 13178, loss = 1.37545574\n",
      "Iteration 13179, loss = 1.37544120\n",
      "Iteration 13180, loss = 1.37542666\n",
      "Iteration 13181, loss = 1.37541218\n",
      "Iteration 13182, loss = 1.37539822\n",
      "Iteration 13183, loss = 1.37538399\n",
      "Iteration 13184, loss = 1.37537050\n",
      "Iteration 13185, loss = 1.37535694\n",
      "Iteration 13186, loss = 1.37534320\n",
      "Iteration 13187, loss = 1.37532925\n",
      "Iteration 13188, loss = 1.37531512\n",
      "Iteration 13189, loss = 1.37530086\n",
      "Iteration 13190, loss = 1.37528647\n",
      "Iteration 13191, loss = 1.37527194\n",
      "Iteration 13192, loss = 1.37525734\n",
      "Iteration 13193, loss = 1.37524281\n",
      "Iteration 13194, loss = 1.37522825\n",
      "Iteration 13195, loss = 1.37521378\n",
      "Iteration 13196, loss = 1.37519935\n",
      "Iteration 13197, loss = 1.37518490\n",
      "Iteration 13198, loss = 1.37517042\n",
      "Iteration 13199, loss = 1.37515590\n",
      "Iteration 13200, loss = 1.37514136\n",
      "Iteration 13201, loss = 1.37512679\n",
      "Iteration 13202, loss = 1.37511220\n",
      "Iteration 13203, loss = 1.37509762\n",
      "Iteration 13204, loss = 1.37508304\n",
      "Iteration 13205, loss = 1.37506850\n",
      "Iteration 13206, loss = 1.37505394\n",
      "Iteration 13207, loss = 1.37504048\n",
      "Iteration 13208, loss = 1.37502500\n",
      "Iteration 13209, loss = 1.37501066\n",
      "Iteration 13210, loss = 1.37499638\n",
      "Iteration 13211, loss = 1.37498220\n",
      "Iteration 13212, loss = 1.37496791\n",
      "Iteration 13213, loss = 1.37495356\n",
      "Iteration 13214, loss = 1.37493914\n",
      "Iteration 13215, loss = 1.37492467\n",
      "Iteration 13216, loss = 1.37491014\n",
      "Iteration 13217, loss = 1.37489560\n",
      "Iteration 13218, loss = 1.37488112\n",
      "Iteration 13219, loss = 1.37486685\n",
      "Iteration 13220, loss = 1.37485259\n",
      "Iteration 13221, loss = 1.37483847\n",
      "Iteration 13222, loss = 1.37482430\n",
      "Iteration 13223, loss = 1.37481002\n",
      "Iteration 13224, loss = 1.37479569\n",
      "Iteration 13225, loss = 1.37478128\n",
      "Iteration 13226, loss = 1.37476683\n",
      "Iteration 13227, loss = 1.37475241\n",
      "Iteration 13228, loss = 1.37473800\n",
      "Iteration 13229, loss = 1.37472361\n",
      "Iteration 13230, loss = 1.37470918\n",
      "Iteration 13231, loss = 1.37469473\n",
      "Iteration 13232, loss = 1.37468027\n",
      "Iteration 13233, loss = 1.37466809\n",
      "Iteration 13234, loss = 1.37465217\n",
      "Iteration 13235, loss = 1.37463862\n",
      "Iteration 13236, loss = 1.37462498\n",
      "Iteration 13237, loss = 1.37461121\n",
      "Iteration 13238, loss = 1.37459724\n",
      "Iteration 13239, loss = 1.37458306\n",
      "Iteration 13240, loss = 1.37456871\n",
      "Iteration 13241, loss = 1.37455430\n",
      "Iteration 13242, loss = 1.37453984\n",
      "Iteration 13243, loss = 1.37452541\n",
      "Iteration 13244, loss = 1.37451098\n",
      "Iteration 13245, loss = 1.37449651\n",
      "Iteration 13246, loss = 1.37448211\n",
      "Iteration 13247, loss = 1.37446775\n",
      "Iteration 13248, loss = 1.37445338\n",
      "Iteration 13249, loss = 1.37443900\n",
      "Iteration 13250, loss = 1.37442457\n",
      "Iteration 13251, loss = 1.37441010\n",
      "Iteration 13252, loss = 1.37439561\n",
      "Iteration 13253, loss = 1.37438115\n",
      "Iteration 13254, loss = 1.37436669\n",
      "Iteration 13255, loss = 1.37435221\n",
      "Iteration 13256, loss = 1.37433771\n",
      "Iteration 13257, loss = 1.37432320\n",
      "Iteration 13258, loss = 1.37430890\n",
      "Iteration 13259, loss = 1.37429477\n",
      "Iteration 13260, loss = 1.37428063\n",
      "Iteration 13261, loss = 1.37426646\n",
      "Iteration 13262, loss = 1.37425225\n",
      "Iteration 13263, loss = 1.37423797\n",
      "Iteration 13264, loss = 1.37422360\n",
      "Iteration 13265, loss = 1.37420921\n",
      "Iteration 13266, loss = 1.37419476\n",
      "Iteration 13267, loss = 1.37418028\n",
      "Iteration 13268, loss = 1.37416582\n",
      "Iteration 13269, loss = 1.37415139\n",
      "Iteration 13270, loss = 1.37413699\n",
      "Iteration 13271, loss = 1.37412517\n",
      "Iteration 13272, loss = 1.37410893\n",
      "Iteration 13273, loss = 1.37409532\n",
      "Iteration 13274, loss = 1.37408167\n",
      "Iteration 13275, loss = 1.37406780\n",
      "Iteration 13276, loss = 1.37405373\n",
      "Iteration 13277, loss = 1.37403949\n",
      "Iteration 13278, loss = 1.37402514\n",
      "Iteration 13279, loss = 1.37401073\n",
      "Iteration 13280, loss = 1.37399642\n",
      "Iteration 13281, loss = 1.37398212\n",
      "Iteration 13282, loss = 1.37396781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13283, loss = 1.37395352\n",
      "Iteration 13284, loss = 1.37393920\n",
      "Iteration 13285, loss = 1.37392485\n",
      "Iteration 13286, loss = 1.37391049\n",
      "Iteration 13287, loss = 1.37389614\n",
      "Iteration 13288, loss = 1.37388180\n",
      "Iteration 13289, loss = 1.37386743\n",
      "Iteration 13290, loss = 1.37385300\n",
      "Iteration 13291, loss = 1.37383854\n",
      "Iteration 13292, loss = 1.37382409\n",
      "Iteration 13293, loss = 1.37380964\n",
      "Iteration 13294, loss = 1.37379522\n",
      "Iteration 13295, loss = 1.37378081\n",
      "Iteration 13296, loss = 1.37376750\n",
      "Iteration 13297, loss = 1.37375233\n",
      "Iteration 13298, loss = 1.37373829\n",
      "Iteration 13299, loss = 1.37372417\n",
      "Iteration 13300, loss = 1.37371000\n",
      "Iteration 13301, loss = 1.37369576\n",
      "Iteration 13302, loss = 1.37368145\n",
      "Iteration 13303, loss = 1.37366712\n",
      "Iteration 13304, loss = 1.37365276\n",
      "Iteration 13305, loss = 1.37363835\n",
      "Iteration 13306, loss = 1.37362397\n",
      "Iteration 13307, loss = 1.37360961\n",
      "Iteration 13308, loss = 1.37359580\n",
      "Iteration 13309, loss = 1.37358124\n",
      "Iteration 13310, loss = 1.37356720\n",
      "Iteration 13311, loss = 1.37355303\n",
      "Iteration 13312, loss = 1.37353875\n",
      "Iteration 13313, loss = 1.37352447\n",
      "Iteration 13314, loss = 1.37351021\n",
      "Iteration 13315, loss = 1.37349591\n",
      "Iteration 13316, loss = 1.37348159\n",
      "Iteration 13317, loss = 1.37346727\n",
      "Iteration 13318, loss = 1.37345451\n",
      "Iteration 13319, loss = 1.37343895\n",
      "Iteration 13320, loss = 1.37342501\n",
      "Iteration 13321, loss = 1.37341100\n",
      "Iteration 13322, loss = 1.37339687\n",
      "Iteration 13323, loss = 1.37338263\n",
      "Iteration 13324, loss = 1.37336841\n",
      "Iteration 13325, loss = 1.37335417\n",
      "Iteration 13326, loss = 1.37333994\n",
      "Iteration 13327, loss = 1.37332568\n",
      "Iteration 13328, loss = 1.37331140\n",
      "Iteration 13329, loss = 1.37329707\n",
      "Iteration 13330, loss = 1.37328278\n",
      "Iteration 13331, loss = 1.37326846\n",
      "Iteration 13332, loss = 1.37325414\n",
      "Iteration 13333, loss = 1.37323981\n",
      "Iteration 13334, loss = 1.37322723\n",
      "Iteration 13335, loss = 1.37321147\n",
      "Iteration 13336, loss = 1.37319757\n",
      "Iteration 13337, loss = 1.37318363\n",
      "Iteration 13338, loss = 1.37316956\n",
      "Iteration 13339, loss = 1.37315539\n",
      "Iteration 13340, loss = 1.37314107\n",
      "Iteration 13341, loss = 1.37312676\n",
      "Iteration 13342, loss = 1.37311242\n",
      "Iteration 13343, loss = 1.37309816\n",
      "Iteration 13344, loss = 1.37308389\n",
      "Iteration 13345, loss = 1.37306960\n",
      "Iteration 13346, loss = 1.37305557\n",
      "Iteration 13347, loss = 1.37304124\n",
      "Iteration 13348, loss = 1.37302715\n",
      "Iteration 13349, loss = 1.37301309\n",
      "Iteration 13350, loss = 1.37299899\n",
      "Iteration 13351, loss = 1.37298483\n",
      "Iteration 13352, loss = 1.37297060\n",
      "Iteration 13353, loss = 1.37295632\n",
      "Iteration 13354, loss = 1.37294202\n",
      "Iteration 13355, loss = 1.37292773\n",
      "Iteration 13356, loss = 1.37291346\n",
      "Iteration 13357, loss = 1.37289921\n",
      "Iteration 13358, loss = 1.37288610\n",
      "Iteration 13359, loss = 1.37287094\n",
      "Iteration 13360, loss = 1.37285707\n",
      "Iteration 13361, loss = 1.37284312\n",
      "Iteration 13362, loss = 1.37282909\n",
      "Iteration 13363, loss = 1.37281490\n",
      "Iteration 13364, loss = 1.37280068\n",
      "Iteration 13365, loss = 1.37278644\n",
      "Iteration 13366, loss = 1.37277222\n",
      "Iteration 13367, loss = 1.37275800\n",
      "Iteration 13368, loss = 1.37274377\n",
      "Iteration 13369, loss = 1.37272953\n",
      "Iteration 13370, loss = 1.37271526\n",
      "Iteration 13371, loss = 1.37270095\n",
      "Iteration 13372, loss = 1.37268717\n",
      "Iteration 13373, loss = 1.37267259\n",
      "Iteration 13374, loss = 1.37265863\n",
      "Iteration 13375, loss = 1.37264468\n",
      "Iteration 13376, loss = 1.37263071\n",
      "Iteration 13377, loss = 1.37261661\n",
      "Iteration 13378, loss = 1.37260240\n",
      "Iteration 13379, loss = 1.37258818\n",
      "Iteration 13380, loss = 1.37257389\n",
      "Iteration 13381, loss = 1.37255960\n",
      "Iteration 13382, loss = 1.37254538\n",
      "Iteration 13383, loss = 1.37253117\n",
      "Iteration 13384, loss = 1.37251693\n",
      "Iteration 13385, loss = 1.37250268\n",
      "Iteration 13386, loss = 1.37248841\n",
      "Iteration 13387, loss = 1.37247434\n",
      "Iteration 13388, loss = 1.37246044\n",
      "Iteration 13389, loss = 1.37244716\n",
      "Iteration 13390, loss = 1.37243381\n",
      "Iteration 13391, loss = 1.37242025\n",
      "Iteration 13392, loss = 1.37240647\n",
      "Iteration 13393, loss = 1.37239248\n",
      "Iteration 13394, loss = 1.37237831\n",
      "Iteration 13395, loss = 1.37236397\n",
      "Iteration 13396, loss = 1.37234963\n",
      "Iteration 13397, loss = 1.37233542\n",
      "Iteration 13398, loss = 1.37232116\n",
      "Iteration 13399, loss = 1.37230688\n",
      "Iteration 13400, loss = 1.37229261\n",
      "Iteration 13401, loss = 1.37227841\n",
      "Iteration 13402, loss = 1.37226420\n",
      "Iteration 13403, loss = 1.37224996\n",
      "Iteration 13404, loss = 1.37223570\n",
      "Iteration 13405, loss = 1.37222141\n",
      "Iteration 13406, loss = 1.37220709\n",
      "Iteration 13407, loss = 1.37219274\n",
      "Iteration 13408, loss = 1.37217846\n",
      "Iteration 13409, loss = 1.37216422\n",
      "Iteration 13410, loss = 1.37214994\n",
      "Iteration 13411, loss = 1.37213559\n",
      "Iteration 13412, loss = 1.37212119\n",
      "Iteration 13413, loss = 1.37210678\n",
      "Iteration 13414, loss = 1.37209244\n",
      "Iteration 13415, loss = 1.37207853\n",
      "Iteration 13416, loss = 1.37206462\n",
      "Iteration 13417, loss = 1.37205068\n",
      "Iteration 13418, loss = 1.37203667\n",
      "Iteration 13419, loss = 1.37202260\n",
      "Iteration 13420, loss = 1.37200846\n",
      "Iteration 13421, loss = 1.37199422\n",
      "Iteration 13422, loss = 1.37197993\n",
      "Iteration 13423, loss = 1.37196563\n",
      "Iteration 13424, loss = 1.37195132\n",
      "Iteration 13425, loss = 1.37193707\n",
      "Iteration 13426, loss = 1.37192285\n",
      "Iteration 13427, loss = 1.37190861\n",
      "Iteration 13428, loss = 1.37189432\n",
      "Iteration 13429, loss = 1.37188229\n",
      "Iteration 13430, loss = 1.37186643\n",
      "Iteration 13431, loss = 1.37185289\n",
      "Iteration 13432, loss = 1.37183948\n",
      "Iteration 13433, loss = 1.37182584\n",
      "Iteration 13434, loss = 1.37181199\n",
      "Iteration 13435, loss = 1.37179794\n",
      "Iteration 13436, loss = 1.37178378\n",
      "Iteration 13437, loss = 1.37176958\n",
      "Iteration 13438, loss = 1.37175534\n",
      "Iteration 13439, loss = 1.37174114\n",
      "Iteration 13440, loss = 1.37172697\n",
      "Iteration 13441, loss = 1.37171285\n",
      "Iteration 13442, loss = 1.37169870\n",
      "Iteration 13443, loss = 1.37168452\n",
      "Iteration 13444, loss = 1.37167031\n",
      "Iteration 13445, loss = 1.37165607\n",
      "Iteration 13446, loss = 1.37164180\n",
      "Iteration 13447, loss = 1.37162754\n",
      "Iteration 13448, loss = 1.37161327\n",
      "Iteration 13449, loss = 1.37159903\n",
      "Iteration 13450, loss = 1.37158477\n",
      "Iteration 13451, loss = 1.37157046\n",
      "Iteration 13452, loss = 1.37155612\n",
      "Iteration 13453, loss = 1.37154185\n",
      "Iteration 13454, loss = 1.37152756\n",
      "Iteration 13455, loss = 1.37151332\n",
      "Iteration 13456, loss = 1.37150139\n",
      "Iteration 13457, loss = 1.37148511\n",
      "Iteration 13458, loss = 1.37147114\n",
      "Iteration 13459, loss = 1.37145716\n",
      "Iteration 13460, loss = 1.37144314\n",
      "Iteration 13461, loss = 1.37142907\n",
      "Iteration 13462, loss = 1.37141495\n",
      "Iteration 13463, loss = 1.37140079\n",
      "Iteration 13464, loss = 1.37138658\n",
      "Iteration 13465, loss = 1.37137240\n",
      "Iteration 13466, loss = 1.37135824\n",
      "Iteration 13467, loss = 1.37134404\n",
      "Iteration 13468, loss = 1.37132976\n",
      "Iteration 13469, loss = 1.37131618\n",
      "Iteration 13470, loss = 1.37130161\n",
      "Iteration 13471, loss = 1.37128772\n",
      "Iteration 13472, loss = 1.37127377\n",
      "Iteration 13473, loss = 1.37125977\n",
      "Iteration 13474, loss = 1.37124573\n",
      "Iteration 13475, loss = 1.37123165\n",
      "Iteration 13476, loss = 1.37121750\n",
      "Iteration 13477, loss = 1.37120331\n",
      "Iteration 13478, loss = 1.37118915\n",
      "Iteration 13479, loss = 1.37117502\n",
      "Iteration 13480, loss = 1.37116087\n",
      "Iteration 13481, loss = 1.37114671\n",
      "Iteration 13482, loss = 1.37113253\n",
      "Iteration 13483, loss = 1.37111837\n",
      "Iteration 13484, loss = 1.37110440\n",
      "Iteration 13485, loss = 1.37109024\n",
      "Iteration 13486, loss = 1.37107640\n",
      "Iteration 13487, loss = 1.37106251\n",
      "Iteration 13488, loss = 1.37104855\n",
      "Iteration 13489, loss = 1.37103444\n",
      "Iteration 13490, loss = 1.37102028\n",
      "Iteration 13491, loss = 1.37100612\n",
      "Iteration 13492, loss = 1.37099194\n",
      "Iteration 13493, loss = 1.37097776\n",
      "Iteration 13494, loss = 1.37096362\n",
      "Iteration 13495, loss = 1.37094997\n",
      "Iteration 13496, loss = 1.37093556\n",
      "Iteration 13497, loss = 1.37092164\n",
      "Iteration 13498, loss = 1.37090778\n",
      "Iteration 13499, loss = 1.37089384\n",
      "Iteration 13500, loss = 1.37087982\n",
      "Iteration 13501, loss = 1.37086574\n",
      "Iteration 13502, loss = 1.37085164\n",
      "Iteration 13503, loss = 1.37083753\n",
      "Iteration 13504, loss = 1.37082340\n",
      "Iteration 13505, loss = 1.37080926\n",
      "Iteration 13506, loss = 1.37079511\n",
      "Iteration 13507, loss = 1.37078097\n",
      "Iteration 13508, loss = 1.37076681\n",
      "Iteration 13509, loss = 1.37075264\n",
      "Iteration 13510, loss = 1.37073850\n",
      "Iteration 13511, loss = 1.37072429\n",
      "Iteration 13512, loss = 1.37071007\n",
      "Iteration 13513, loss = 1.37069726\n",
      "Iteration 13514, loss = 1.37068217\n",
      "Iteration 13515, loss = 1.37066849\n",
      "Iteration 13516, loss = 1.37065465\n",
      "Iteration 13517, loss = 1.37064061\n",
      "Iteration 13518, loss = 1.37062651\n",
      "Iteration 13519, loss = 1.37061244\n",
      "Iteration 13520, loss = 1.37059835\n",
      "Iteration 13521, loss = 1.37058424\n",
      "Iteration 13522, loss = 1.37057016\n",
      "Iteration 13523, loss = 1.37055605\n",
      "Iteration 13524, loss = 1.37054190\n",
      "Iteration 13525, loss = 1.37052773\n",
      "Iteration 13526, loss = 1.37051354\n",
      "Iteration 13527, loss = 1.37049998\n",
      "Iteration 13528, loss = 1.37048549\n",
      "Iteration 13529, loss = 1.37047163\n",
      "Iteration 13530, loss = 1.37045772\n",
      "Iteration 13531, loss = 1.37044384\n",
      "Iteration 13532, loss = 1.37042989\n",
      "Iteration 13533, loss = 1.37041579\n",
      "Iteration 13534, loss = 1.37040163\n",
      "Iteration 13535, loss = 1.37038749\n",
      "Iteration 13536, loss = 1.37037332\n",
      "Iteration 13537, loss = 1.37035920\n",
      "Iteration 13538, loss = 1.37034507\n",
      "Iteration 13539, loss = 1.37033099\n",
      "Iteration 13540, loss = 1.37031689\n",
      "Iteration 13541, loss = 1.37030275\n",
      "Iteration 13542, loss = 1.37028919\n",
      "Iteration 13543, loss = 1.37027462\n",
      "Iteration 13544, loss = 1.37026079\n",
      "Iteration 13545, loss = 1.37024698\n",
      "Iteration 13546, loss = 1.37023311\n",
      "Iteration 13547, loss = 1.37021911\n",
      "Iteration 13548, loss = 1.37020503\n",
      "Iteration 13549, loss = 1.37019091\n",
      "Iteration 13550, loss = 1.37017677\n",
      "Iteration 13551, loss = 1.37016261\n",
      "Iteration 13552, loss = 1.37014851\n",
      "Iteration 13553, loss = 1.37013438\n",
      "Iteration 13554, loss = 1.37012027\n",
      "Iteration 13555, loss = 1.37010726\n",
      "Iteration 13556, loss = 1.37009220\n",
      "Iteration 13557, loss = 1.37007830\n",
      "Iteration 13558, loss = 1.37006450\n",
      "Iteration 13559, loss = 1.37005067\n",
      "Iteration 13560, loss = 1.37003672\n",
      "Iteration 13561, loss = 1.37002264\n",
      "Iteration 13562, loss = 1.37000849\n",
      "Iteration 13563, loss = 1.36999438\n",
      "Iteration 13564, loss = 1.36998034\n",
      "Iteration 13565, loss = 1.36996628\n",
      "Iteration 13566, loss = 1.36995219\n",
      "Iteration 13567, loss = 1.36993806\n",
      "Iteration 13568, loss = 1.36992393\n",
      "Iteration 13569, loss = 1.36990983\n",
      "Iteration 13570, loss = 1.36989572\n",
      "Iteration 13571, loss = 1.36988161\n",
      "Iteration 13572, loss = 1.36986773\n",
      "Iteration 13573, loss = 1.36985396\n",
      "Iteration 13574, loss = 1.36984025\n",
      "Iteration 13575, loss = 1.36982639\n",
      "Iteration 13576, loss = 1.36981237\n",
      "Iteration 13577, loss = 1.36979827\n",
      "Iteration 13578, loss = 1.36978419\n",
      "Iteration 13579, loss = 1.36977014\n",
      "Iteration 13580, loss = 1.36975605\n",
      "Iteration 13581, loss = 1.36974195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13582, loss = 1.36972789\n",
      "Iteration 13583, loss = 1.36971382\n",
      "Iteration 13584, loss = 1.36969971\n",
      "Iteration 13585, loss = 1.36968557\n",
      "Iteration 13586, loss = 1.36967145\n",
      "Iteration 13587, loss = 1.36965732\n",
      "Iteration 13588, loss = 1.36964407\n",
      "Iteration 13589, loss = 1.36962943\n",
      "Iteration 13590, loss = 1.36961565\n",
      "Iteration 13591, loss = 1.36960181\n",
      "Iteration 13592, loss = 1.36958794\n",
      "Iteration 13593, loss = 1.36957404\n",
      "Iteration 13594, loss = 1.36956003\n",
      "Iteration 13595, loss = 1.36954599\n",
      "Iteration 13596, loss = 1.36953191\n",
      "Iteration 13597, loss = 1.36951785\n",
      "Iteration 13598, loss = 1.36950376\n",
      "Iteration 13599, loss = 1.36948966\n",
      "Iteration 13600, loss = 1.36947556\n",
      "Iteration 13601, loss = 1.36946144\n",
      "Iteration 13602, loss = 1.36944735\n",
      "Iteration 13603, loss = 1.36943327\n",
      "Iteration 13604, loss = 1.36941916\n",
      "Iteration 13605, loss = 1.36940780\n",
      "Iteration 13606, loss = 1.36939163\n",
      "Iteration 13607, loss = 1.36937850\n",
      "Iteration 13608, loss = 1.36936525\n",
      "Iteration 13609, loss = 1.36935180\n",
      "Iteration 13610, loss = 1.36933811\n",
      "Iteration 13611, loss = 1.36932420\n",
      "Iteration 13612, loss = 1.36931009\n",
      "Iteration 13613, loss = 1.36929588\n",
      "Iteration 13614, loss = 1.36928174\n",
      "Iteration 13615, loss = 1.36926771\n",
      "Iteration 13616, loss = 1.36925370\n",
      "Iteration 13617, loss = 1.36923972\n",
      "Iteration 13618, loss = 1.36922575\n",
      "Iteration 13619, loss = 1.36921174\n",
      "Iteration 13620, loss = 1.36919769\n",
      "Iteration 13621, loss = 1.36918363\n",
      "Iteration 13622, loss = 1.36916955\n",
      "Iteration 13623, loss = 1.36915543\n",
      "Iteration 13624, loss = 1.36914126\n",
      "Iteration 13625, loss = 1.36912710\n",
      "Iteration 13626, loss = 1.36911295\n",
      "Iteration 13627, loss = 1.36909879\n",
      "Iteration 13628, loss = 1.36908461\n",
      "Iteration 13629, loss = 1.36907046\n",
      "Iteration 13630, loss = 1.36905632\n",
      "Iteration 13631, loss = 1.36904216\n",
      "Iteration 13632, loss = 1.36902805\n",
      "Iteration 13633, loss = 1.36901391\n",
      "Iteration 13634, loss = 1.36899975\n",
      "Iteration 13635, loss = 1.36898617\n",
      "Iteration 13636, loss = 1.36897167\n",
      "Iteration 13637, loss = 1.36895784\n",
      "Iteration 13638, loss = 1.36894396\n",
      "Iteration 13639, loss = 1.36893010\n",
      "Iteration 13640, loss = 1.36891617\n",
      "Iteration 13641, loss = 1.36890217\n",
      "Iteration 13642, loss = 1.36888816\n",
      "Iteration 13643, loss = 1.36887415\n",
      "Iteration 13644, loss = 1.36886013\n",
      "Iteration 13645, loss = 1.36884608\n",
      "Iteration 13646, loss = 1.36883197\n",
      "Iteration 13647, loss = 1.36881789\n",
      "Iteration 13648, loss = 1.36880380\n",
      "Iteration 13649, loss = 1.36878971\n",
      "Iteration 13650, loss = 1.36877559\n",
      "Iteration 13651, loss = 1.36876318\n",
      "Iteration 13652, loss = 1.36874778\n",
      "Iteration 13653, loss = 1.36873409\n",
      "Iteration 13654, loss = 1.36872032\n",
      "Iteration 13655, loss = 1.36870646\n",
      "Iteration 13656, loss = 1.36869250\n",
      "Iteration 13657, loss = 1.36867851\n",
      "Iteration 13658, loss = 1.36866451\n",
      "Iteration 13659, loss = 1.36865045\n",
      "Iteration 13660, loss = 1.36863645\n",
      "Iteration 13661, loss = 1.36862246\n",
      "Iteration 13662, loss = 1.36860842\n",
      "Iteration 13663, loss = 1.36859440\n",
      "Iteration 13664, loss = 1.36858034\n",
      "Iteration 13665, loss = 1.36856626\n",
      "Iteration 13666, loss = 1.36855214\n",
      "Iteration 13667, loss = 1.36853801\n",
      "Iteration 13668, loss = 1.36852484\n",
      "Iteration 13669, loss = 1.36851021\n",
      "Iteration 13670, loss = 1.36849652\n",
      "Iteration 13671, loss = 1.36848277\n",
      "Iteration 13672, loss = 1.36846895\n",
      "Iteration 13673, loss = 1.36845505\n",
      "Iteration 13674, loss = 1.36844106\n",
      "Iteration 13675, loss = 1.36842702\n",
      "Iteration 13676, loss = 1.36841296\n",
      "Iteration 13677, loss = 1.36839893\n",
      "Iteration 13678, loss = 1.36838491\n",
      "Iteration 13679, loss = 1.36837092\n",
      "Iteration 13680, loss = 1.36835692\n",
      "Iteration 13681, loss = 1.36834291\n",
      "Iteration 13682, loss = 1.36832888\n",
      "Iteration 13683, loss = 1.36831481\n",
      "Iteration 13684, loss = 1.36830070\n",
      "Iteration 13685, loss = 1.36828801\n",
      "Iteration 13686, loss = 1.36827318\n",
      "Iteration 13687, loss = 1.36825991\n",
      "Iteration 13688, loss = 1.36824676\n",
      "Iteration 13689, loss = 1.36823345\n",
      "Iteration 13690, loss = 1.36821988\n",
      "Iteration 13691, loss = 1.36820607\n",
      "Iteration 13692, loss = 1.36819207\n",
      "Iteration 13693, loss = 1.36817789\n",
      "Iteration 13694, loss = 1.36816383\n",
      "Iteration 13695, loss = 1.36814984\n",
      "Iteration 13696, loss = 1.36813584\n",
      "Iteration 13697, loss = 1.36812182\n",
      "Iteration 13698, loss = 1.36810780\n",
      "Iteration 13699, loss = 1.36809385\n",
      "Iteration 13700, loss = 1.36807988\n",
      "Iteration 13701, loss = 1.36806588\n",
      "Iteration 13702, loss = 1.36805185\n",
      "Iteration 13703, loss = 1.36803778\n",
      "Iteration 13704, loss = 1.36802371\n",
      "Iteration 13705, loss = 1.36800964\n",
      "Iteration 13706, loss = 1.36799552\n",
      "Iteration 13707, loss = 1.36798135\n",
      "Iteration 13708, loss = 1.36796718\n",
      "Iteration 13709, loss = 1.36795311\n",
      "Iteration 13710, loss = 1.36793901\n",
      "Iteration 13711, loss = 1.36792493\n",
      "Iteration 13712, loss = 1.36791082\n",
      "Iteration 13713, loss = 1.36789672\n",
      "Iteration 13714, loss = 1.36788262\n",
      "Iteration 13715, loss = 1.36786852\n",
      "Iteration 13716, loss = 1.36785441\n",
      "Iteration 13717, loss = 1.36784030\n",
      "Iteration 13718, loss = 1.36782708\n",
      "Iteration 13719, loss = 1.36781241\n",
      "Iteration 13720, loss = 1.36779873\n",
      "Iteration 13721, loss = 1.36778502\n",
      "Iteration 13722, loss = 1.36777117\n",
      "Iteration 13723, loss = 1.36775725\n",
      "Iteration 13724, loss = 1.36774325\n",
      "Iteration 13725, loss = 1.36772927\n",
      "Iteration 13726, loss = 1.36771522\n",
      "Iteration 13727, loss = 1.36770115\n",
      "Iteration 13728, loss = 1.36768719\n",
      "Iteration 13729, loss = 1.36767320\n",
      "Iteration 13730, loss = 1.36765918\n",
      "Iteration 13731, loss = 1.36764514\n",
      "Iteration 13732, loss = 1.36763111\n",
      "Iteration 13733, loss = 1.36761734\n",
      "Iteration 13734, loss = 1.36760326\n",
      "Iteration 13735, loss = 1.36758953\n",
      "Iteration 13736, loss = 1.36757581\n",
      "Iteration 13737, loss = 1.36756200\n",
      "Iteration 13738, loss = 1.36754813\n",
      "Iteration 13739, loss = 1.36753421\n",
      "Iteration 13740, loss = 1.36752022\n",
      "Iteration 13741, loss = 1.36750618\n",
      "Iteration 13742, loss = 1.36749215\n",
      "Iteration 13743, loss = 1.36747818\n",
      "Iteration 13744, loss = 1.36746426\n",
      "Iteration 13745, loss = 1.36745031\n",
      "Iteration 13746, loss = 1.36743629\n",
      "Iteration 13747, loss = 1.36742225\n",
      "Iteration 13748, loss = 1.36740820\n",
      "Iteration 13749, loss = 1.36739414\n",
      "Iteration 13750, loss = 1.36738009\n",
      "Iteration 13751, loss = 1.36736607\n",
      "Iteration 13752, loss = 1.36735205\n",
      "Iteration 13753, loss = 1.36733855\n",
      "Iteration 13754, loss = 1.36732431\n",
      "Iteration 13755, loss = 1.36731066\n",
      "Iteration 13756, loss = 1.36729699\n",
      "Iteration 13757, loss = 1.36728316\n",
      "Iteration 13758, loss = 1.36726917\n",
      "Iteration 13759, loss = 1.36725517\n",
      "Iteration 13760, loss = 1.36724122\n",
      "Iteration 13761, loss = 1.36722730\n",
      "Iteration 13762, loss = 1.36721335\n",
      "Iteration 13763, loss = 1.36719939\n",
      "Iteration 13764, loss = 1.36718541\n",
      "Iteration 13765, loss = 1.36717141\n",
      "Iteration 13766, loss = 1.36715736\n",
      "Iteration 13767, loss = 1.36714331\n",
      "Iteration 13768, loss = 1.36712930\n",
      "Iteration 13769, loss = 1.36711526\n",
      "Iteration 13770, loss = 1.36710149\n",
      "Iteration 13771, loss = 1.36708757\n",
      "Iteration 13772, loss = 1.36707388\n",
      "Iteration 13773, loss = 1.36706009\n",
      "Iteration 13774, loss = 1.36704633\n",
      "Iteration 13775, loss = 1.36703252\n",
      "Iteration 13776, loss = 1.36701858\n",
      "Iteration 13777, loss = 1.36700459\n",
      "Iteration 13778, loss = 1.36699058\n",
      "Iteration 13779, loss = 1.36697662\n",
      "Iteration 13780, loss = 1.36696264\n",
      "Iteration 13781, loss = 1.36694869\n",
      "Iteration 13782, loss = 1.36693470\n",
      "Iteration 13783, loss = 1.36692069\n",
      "Iteration 13784, loss = 1.36690667\n",
      "Iteration 13785, loss = 1.36689263\n",
      "Iteration 13786, loss = 1.36687858\n",
      "Iteration 13787, loss = 1.36686458\n",
      "Iteration 13788, loss = 1.36685057\n",
      "Iteration 13789, loss = 1.36683652\n",
      "Iteration 13790, loss = 1.36682400\n",
      "Iteration 13791, loss = 1.36680881\n",
      "Iteration 13792, loss = 1.36679517\n",
      "Iteration 13793, loss = 1.36678147\n",
      "Iteration 13794, loss = 1.36676772\n",
      "Iteration 13795, loss = 1.36675388\n",
      "Iteration 13796, loss = 1.36673991\n",
      "Iteration 13797, loss = 1.36672588\n",
      "Iteration 13798, loss = 1.36671188\n",
      "Iteration 13799, loss = 1.36669797\n",
      "Iteration 13800, loss = 1.36668407\n",
      "Iteration 13801, loss = 1.36667013\n",
      "Iteration 13802, loss = 1.36665615\n",
      "Iteration 13803, loss = 1.36664213\n",
      "Iteration 13804, loss = 1.36662812\n",
      "Iteration 13805, loss = 1.36661406\n",
      "Iteration 13806, loss = 1.36660003\n",
      "Iteration 13807, loss = 1.36658600\n",
      "Iteration 13808, loss = 1.36657264\n",
      "Iteration 13809, loss = 1.36655861\n",
      "Iteration 13810, loss = 1.36654548\n",
      "Iteration 13811, loss = 1.36653245\n",
      "Iteration 13812, loss = 1.36651913\n",
      "Iteration 13813, loss = 1.36650556\n",
      "Iteration 13814, loss = 1.36649175\n",
      "Iteration 13815, loss = 1.36647773\n",
      "Iteration 13816, loss = 1.36646354\n",
      "Iteration 13817, loss = 1.36644950\n",
      "Iteration 13818, loss = 1.36643553\n",
      "Iteration 13819, loss = 1.36642158\n",
      "Iteration 13820, loss = 1.36640762\n",
      "Iteration 13821, loss = 1.36639364\n",
      "Iteration 13822, loss = 1.36637962\n",
      "Iteration 13823, loss = 1.36636566\n",
      "Iteration 13824, loss = 1.36635169\n",
      "Iteration 13825, loss = 1.36633769\n",
      "Iteration 13826, loss = 1.36632365\n",
      "Iteration 13827, loss = 1.36630958\n",
      "Iteration 13828, loss = 1.36629553\n",
      "Iteration 13829, loss = 1.36628146\n",
      "Iteration 13830, loss = 1.36626734\n",
      "Iteration 13831, loss = 1.36625318\n",
      "Iteration 13832, loss = 1.36623910\n",
      "Iteration 13833, loss = 1.36622502\n",
      "Iteration 13834, loss = 1.36621093\n",
      "Iteration 13835, loss = 1.36619681\n",
      "Iteration 13836, loss = 1.36618273\n",
      "Iteration 13837, loss = 1.36616859\n",
      "Iteration 13838, loss = 1.36615450\n",
      "Iteration 13839, loss = 1.36614044\n",
      "Iteration 13840, loss = 1.36612633\n",
      "Iteration 13841, loss = 1.36611223\n",
      "Iteration 13842, loss = 1.36609816\n",
      "Iteration 13843, loss = 1.36608404\n",
      "Iteration 13844, loss = 1.36607152\n",
      "Iteration 13845, loss = 1.36605617\n",
      "Iteration 13846, loss = 1.36604248\n",
      "Iteration 13847, loss = 1.36602890\n",
      "Iteration 13848, loss = 1.36601517\n",
      "Iteration 13849, loss = 1.36600128\n",
      "Iteration 13850, loss = 1.36598730\n",
      "Iteration 13851, loss = 1.36597323\n",
      "Iteration 13852, loss = 1.36595907\n",
      "Iteration 13853, loss = 1.36594510\n",
      "Iteration 13854, loss = 1.36593111\n",
      "Iteration 13855, loss = 1.36591714\n",
      "Iteration 13856, loss = 1.36590315\n",
      "Iteration 13857, loss = 1.36588913\n",
      "Iteration 13858, loss = 1.36587508\n",
      "Iteration 13859, loss = 1.36586103\n",
      "Iteration 13860, loss = 1.36584726\n",
      "Iteration 13861, loss = 1.36583323\n",
      "Iteration 13862, loss = 1.36581952\n",
      "Iteration 13863, loss = 1.36580573\n",
      "Iteration 13864, loss = 1.36579188\n",
      "Iteration 13865, loss = 1.36577795\n",
      "Iteration 13866, loss = 1.36576413\n",
      "Iteration 13867, loss = 1.36575024\n",
      "Iteration 13868, loss = 1.36573621\n",
      "Iteration 13869, loss = 1.36572217\n",
      "Iteration 13870, loss = 1.36570821\n",
      "Iteration 13871, loss = 1.36569422\n",
      "Iteration 13872, loss = 1.36568022\n",
      "Iteration 13873, loss = 1.36566622\n",
      "Iteration 13874, loss = 1.36565219\n",
      "Iteration 13875, loss = 1.36563817\n",
      "Iteration 13876, loss = 1.36562416\n",
      "Iteration 13877, loss = 1.36561012\n",
      "Iteration 13878, loss = 1.36559606\n",
      "Iteration 13879, loss = 1.36558204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13880, loss = 1.36556799\n",
      "Iteration 13881, loss = 1.36555392\n",
      "Iteration 13882, loss = 1.36554127\n",
      "Iteration 13883, loss = 1.36552630\n",
      "Iteration 13884, loss = 1.36551280\n",
      "Iteration 13885, loss = 1.36549910\n",
      "Iteration 13886, loss = 1.36548520\n",
      "Iteration 13887, loss = 1.36547130\n",
      "Iteration 13888, loss = 1.36545734\n",
      "Iteration 13889, loss = 1.36544334\n",
      "Iteration 13890, loss = 1.36542935\n",
      "Iteration 13891, loss = 1.36541539\n",
      "Iteration 13892, loss = 1.36540148\n",
      "Iteration 13893, loss = 1.36538752\n",
      "Iteration 13894, loss = 1.36537352\n",
      "Iteration 13895, loss = 1.36535949\n",
      "Iteration 13896, loss = 1.36534545\n",
      "Iteration 13897, loss = 1.36533141\n",
      "Iteration 13898, loss = 1.36531735\n",
      "Iteration 13899, loss = 1.36530325\n",
      "Iteration 13900, loss = 1.36528917\n",
      "Iteration 13901, loss = 1.36527631\n",
      "Iteration 13902, loss = 1.36526142\n",
      "Iteration 13903, loss = 1.36524780\n",
      "Iteration 13904, loss = 1.36523405\n",
      "Iteration 13905, loss = 1.36522019\n",
      "Iteration 13906, loss = 1.36520630\n",
      "Iteration 13907, loss = 1.36519235\n",
      "Iteration 13908, loss = 1.36517832\n",
      "Iteration 13909, loss = 1.36516431\n",
      "Iteration 13910, loss = 1.36515035\n",
      "Iteration 13911, loss = 1.36513640\n",
      "Iteration 13912, loss = 1.36512248\n",
      "Iteration 13913, loss = 1.36510851\n",
      "Iteration 13914, loss = 1.36509452\n",
      "Iteration 13915, loss = 1.36508050\n",
      "Iteration 13916, loss = 1.36506647\n",
      "Iteration 13917, loss = 1.36505244\n",
      "Iteration 13918, loss = 1.36503842\n",
      "Iteration 13919, loss = 1.36502435\n",
      "Iteration 13920, loss = 1.36501035\n",
      "Iteration 13921, loss = 1.36499634\n",
      "Iteration 13922, loss = 1.36498369\n",
      "Iteration 13923, loss = 1.36496860\n",
      "Iteration 13924, loss = 1.36495493\n",
      "Iteration 13925, loss = 1.36494112\n",
      "Iteration 13926, loss = 1.36492732\n",
      "Iteration 13927, loss = 1.36491348\n",
      "Iteration 13928, loss = 1.36489952\n",
      "Iteration 13929, loss = 1.36488557\n",
      "Iteration 13930, loss = 1.36487157\n",
      "Iteration 13931, loss = 1.36485763\n",
      "Iteration 13932, loss = 1.36484368\n",
      "Iteration 13933, loss = 1.36482971\n",
      "Iteration 13934, loss = 1.36481571\n",
      "Iteration 13935, loss = 1.36480169\n",
      "Iteration 13936, loss = 1.36478767\n",
      "Iteration 13937, loss = 1.36477363\n",
      "Iteration 13938, loss = 1.36475960\n",
      "Iteration 13939, loss = 1.36474558\n",
      "Iteration 13940, loss = 1.36473153\n",
      "Iteration 13941, loss = 1.36471751\n",
      "Iteration 13942, loss = 1.36470347\n",
      "Iteration 13943, loss = 1.36469067\n",
      "Iteration 13944, loss = 1.36467568\n",
      "Iteration 13945, loss = 1.36466196\n",
      "Iteration 13946, loss = 1.36464820\n",
      "Iteration 13947, loss = 1.36463440\n",
      "Iteration 13948, loss = 1.36462057\n",
      "Iteration 13949, loss = 1.36460663\n",
      "Iteration 13950, loss = 1.36459270\n",
      "Iteration 13951, loss = 1.36457873\n",
      "Iteration 13952, loss = 1.36456474\n",
      "Iteration 13953, loss = 1.36455078\n",
      "Iteration 13954, loss = 1.36453680\n",
      "Iteration 13955, loss = 1.36452282\n",
      "Iteration 13956, loss = 1.36450883\n",
      "Iteration 13957, loss = 1.36449479\n",
      "Iteration 13958, loss = 1.36448075\n",
      "Iteration 13959, loss = 1.36446674\n",
      "Iteration 13960, loss = 1.36445270\n",
      "Iteration 13961, loss = 1.36443863\n",
      "Iteration 13962, loss = 1.36442458\n",
      "Iteration 13963, loss = 1.36441054\n",
      "Iteration 13964, loss = 1.36439810\n",
      "Iteration 13965, loss = 1.36438272\n",
      "Iteration 13966, loss = 1.36436903\n",
      "Iteration 13967, loss = 1.36435543\n",
      "Iteration 13968, loss = 1.36434171\n",
      "Iteration 13969, loss = 1.36432776\n",
      "Iteration 13970, loss = 1.36431372\n",
      "Iteration 13971, loss = 1.36429972\n",
      "Iteration 13972, loss = 1.36428581\n",
      "Iteration 13973, loss = 1.36427188\n",
      "Iteration 13974, loss = 1.36425794\n",
      "Iteration 13975, loss = 1.36424396\n",
      "Iteration 13976, loss = 1.36422999\n",
      "Iteration 13977, loss = 1.36421597\n",
      "Iteration 13978, loss = 1.36420197\n",
      "Iteration 13979, loss = 1.36418793\n",
      "Iteration 13980, loss = 1.36417388\n",
      "Iteration 13981, loss = 1.36415983\n",
      "Iteration 13982, loss = 1.36414576\n",
      "Iteration 13983, loss = 1.36413175\n",
      "Iteration 13984, loss = 1.36411770\n",
      "Iteration 13985, loss = 1.36410400\n",
      "Iteration 13986, loss = 1.36408989\n",
      "Iteration 13987, loss = 1.36407624\n",
      "Iteration 13988, loss = 1.36406255\n",
      "Iteration 13989, loss = 1.36404873\n",
      "Iteration 13990, loss = 1.36403479\n",
      "Iteration 13991, loss = 1.36402078\n",
      "Iteration 13992, loss = 1.36400689\n",
      "Iteration 13993, loss = 1.36399296\n",
      "Iteration 13994, loss = 1.36397897\n",
      "Iteration 13995, loss = 1.36396503\n",
      "Iteration 13996, loss = 1.36395106\n",
      "Iteration 13997, loss = 1.36393707\n",
      "Iteration 13998, loss = 1.36392307\n",
      "Iteration 13999, loss = 1.36390907\n",
      "Iteration 14000, loss = 1.36389504\n",
      "Iteration 14001, loss = 1.36388101\n",
      "Iteration 14002, loss = 1.36386700\n",
      "Iteration 14003, loss = 1.36385295\n",
      "Iteration 14004, loss = 1.36383889\n",
      "Iteration 14005, loss = 1.36382485\n",
      "Iteration 14006, loss = 1.36381079\n",
      "Iteration 14007, loss = 1.36379746\n",
      "Iteration 14008, loss = 1.36378292\n",
      "Iteration 14009, loss = 1.36376922\n",
      "Iteration 14010, loss = 1.36375549\n",
      "Iteration 14011, loss = 1.36374171\n",
      "Iteration 14012, loss = 1.36372790\n",
      "Iteration 14013, loss = 1.36371391\n",
      "Iteration 14014, loss = 1.36369988\n",
      "Iteration 14015, loss = 1.36368588\n",
      "Iteration 14016, loss = 1.36367193\n",
      "Iteration 14017, loss = 1.36365800\n",
      "Iteration 14018, loss = 1.36364404\n",
      "Iteration 14019, loss = 1.36363003\n",
      "Iteration 14020, loss = 1.36361600\n",
      "Iteration 14021, loss = 1.36360201\n",
      "Iteration 14022, loss = 1.36358798\n",
      "Iteration 14023, loss = 1.36357390\n",
      "Iteration 14024, loss = 1.36355984\n",
      "Iteration 14025, loss = 1.36354581\n",
      "Iteration 14026, loss = 1.36353180\n",
      "Iteration 14027, loss = 1.36351774\n",
      "Iteration 14028, loss = 1.36350363\n",
      "Iteration 14029, loss = 1.36348953\n",
      "Iteration 14030, loss = 1.36347653\n",
      "Iteration 14031, loss = 1.36346181\n",
      "Iteration 14032, loss = 1.36344818\n",
      "Iteration 14033, loss = 1.36343459\n",
      "Iteration 14034, loss = 1.36342077\n",
      "Iteration 14035, loss = 1.36340673\n",
      "Iteration 14036, loss = 1.36339260\n",
      "Iteration 14037, loss = 1.36337862\n",
      "Iteration 14038, loss = 1.36336470\n",
      "Iteration 14039, loss = 1.36335079\n",
      "Iteration 14040, loss = 1.36333685\n",
      "Iteration 14041, loss = 1.36332286\n",
      "Iteration 14042, loss = 1.36330885\n",
      "Iteration 14043, loss = 1.36329483\n",
      "Iteration 14044, loss = 1.36328080\n",
      "Iteration 14045, loss = 1.36326681\n",
      "Iteration 14046, loss = 1.36325275\n",
      "Iteration 14047, loss = 1.36323865\n",
      "Iteration 14048, loss = 1.36322460\n",
      "Iteration 14049, loss = 1.36321057\n",
      "Iteration 14050, loss = 1.36319650\n",
      "Iteration 14051, loss = 1.36318247\n",
      "Iteration 14052, loss = 1.36316850\n",
      "Iteration 14053, loss = 1.36315465\n",
      "Iteration 14054, loss = 1.36314111\n",
      "Iteration 14055, loss = 1.36312742\n",
      "Iteration 14056, loss = 1.36311358\n",
      "Iteration 14057, loss = 1.36309953\n",
      "Iteration 14058, loss = 1.36308548\n",
      "Iteration 14059, loss = 1.36307147\n",
      "Iteration 14060, loss = 1.36305749\n",
      "Iteration 14061, loss = 1.36304359\n",
      "Iteration 14062, loss = 1.36302964\n",
      "Iteration 14063, loss = 1.36301566\n",
      "Iteration 14064, loss = 1.36300165\n",
      "Iteration 14065, loss = 1.36298761\n",
      "Iteration 14066, loss = 1.36297354\n",
      "Iteration 14067, loss = 1.36295953\n",
      "Iteration 14068, loss = 1.36294554\n",
      "Iteration 14069, loss = 1.36293147\n",
      "Iteration 14070, loss = 1.36291739\n",
      "Iteration 14071, loss = 1.36290334\n",
      "Iteration 14072, loss = 1.36288927\n",
      "Iteration 14073, loss = 1.36287521\n",
      "Iteration 14074, loss = 1.36286119\n",
      "Iteration 14075, loss = 1.36284714\n",
      "Iteration 14076, loss = 1.36283407\n",
      "Iteration 14077, loss = 1.36281934\n",
      "Iteration 14078, loss = 1.36280572\n",
      "Iteration 14079, loss = 1.36279206\n",
      "Iteration 14080, loss = 1.36277823\n",
      "Iteration 14081, loss = 1.36276422\n",
      "Iteration 14082, loss = 1.36275024\n",
      "Iteration 14083, loss = 1.36273622\n",
      "Iteration 14084, loss = 1.36272223\n",
      "Iteration 14085, loss = 1.36270833\n",
      "Iteration 14086, loss = 1.36269438\n",
      "Iteration 14087, loss = 1.36268040\n",
      "Iteration 14088, loss = 1.36266637\n",
      "Iteration 14089, loss = 1.36265231\n",
      "Iteration 14090, loss = 1.36263826\n",
      "Iteration 14091, loss = 1.36262423\n",
      "Iteration 14092, loss = 1.36261017\n",
      "Iteration 14093, loss = 1.36259609\n",
      "Iteration 14094, loss = 1.36258200\n",
      "Iteration 14095, loss = 1.36256792\n",
      "Iteration 14096, loss = 1.36255385\n",
      "Iteration 14097, loss = 1.36253977\n",
      "Iteration 14098, loss = 1.36252571\n",
      "Iteration 14099, loss = 1.36251159\n",
      "Iteration 14100, loss = 1.36249901\n",
      "Iteration 14101, loss = 1.36248385\n",
      "Iteration 14102, loss = 1.36247020\n",
      "Iteration 14103, loss = 1.36245644\n",
      "Iteration 14104, loss = 1.36244262\n",
      "Iteration 14105, loss = 1.36242868\n",
      "Iteration 14106, loss = 1.36241463\n",
      "Iteration 14107, loss = 1.36240054\n",
      "Iteration 14108, loss = 1.36238652\n",
      "Iteration 14109, loss = 1.36237257\n",
      "Iteration 14110, loss = 1.36235861\n",
      "Iteration 14111, loss = 1.36234465\n",
      "Iteration 14112, loss = 1.36233063\n",
      "Iteration 14113, loss = 1.36231657\n",
      "Iteration 14114, loss = 1.36230255\n",
      "Iteration 14115, loss = 1.36228853\n",
      "Iteration 14116, loss = 1.36227445\n",
      "Iteration 14117, loss = 1.36226033\n",
      "Iteration 14118, loss = 1.36224622\n",
      "Iteration 14119, loss = 1.36223221\n",
      "Iteration 14120, loss = 1.36221814\n",
      "Iteration 14121, loss = 1.36220404\n",
      "Iteration 14122, loss = 1.36218995\n",
      "Iteration 14123, loss = 1.36217588\n",
      "Iteration 14124, loss = 1.36216238\n",
      "Iteration 14125, loss = 1.36214811\n",
      "Iteration 14126, loss = 1.36213451\n",
      "Iteration 14127, loss = 1.36212073\n",
      "Iteration 14128, loss = 1.36210675\n",
      "Iteration 14129, loss = 1.36209264\n",
      "Iteration 14130, loss = 1.36207858\n",
      "Iteration 14131, loss = 1.36206466\n",
      "Iteration 14132, loss = 1.36205074\n",
      "Iteration 14133, loss = 1.36203679\n",
      "Iteration 14134, loss = 1.36202278\n",
      "Iteration 14135, loss = 1.36200878\n",
      "Iteration 14136, loss = 1.36199474\n",
      "Iteration 14137, loss = 1.36198068\n",
      "Iteration 14138, loss = 1.36196665\n",
      "Iteration 14139, loss = 1.36195260\n",
      "Iteration 14140, loss = 1.36193853\n",
      "Iteration 14141, loss = 1.36192449\n",
      "Iteration 14142, loss = 1.36191041\n",
      "Iteration 14143, loss = 1.36189633\n",
      "Iteration 14144, loss = 1.36188223\n",
      "Iteration 14145, loss = 1.36186815\n",
      "Iteration 14146, loss = 1.36185404\n",
      "Iteration 14147, loss = 1.36183995\n",
      "Iteration 14148, loss = 1.36182589\n",
      "Iteration 14149, loss = 1.36181352\n",
      "Iteration 14150, loss = 1.36179807\n",
      "Iteration 14151, loss = 1.36178442\n",
      "Iteration 14152, loss = 1.36177074\n",
      "Iteration 14153, loss = 1.36175684\n",
      "Iteration 14154, loss = 1.36174282\n",
      "Iteration 14155, loss = 1.36172872\n",
      "Iteration 14156, loss = 1.36171470\n",
      "Iteration 14157, loss = 1.36170075\n",
      "Iteration 14158, loss = 1.36168678\n",
      "Iteration 14159, loss = 1.36167280\n",
      "Iteration 14160, loss = 1.36165882\n",
      "Iteration 14161, loss = 1.36164480\n",
      "Iteration 14162, loss = 1.36163074\n",
      "Iteration 14163, loss = 1.36161668\n",
      "Iteration 14164, loss = 1.36160261\n",
      "Iteration 14165, loss = 1.36158854\n",
      "Iteration 14166, loss = 1.36157442\n",
      "Iteration 14167, loss = 1.36156034\n",
      "Iteration 14168, loss = 1.36154625\n",
      "Iteration 14169, loss = 1.36153216\n",
      "Iteration 14170, loss = 1.36151806\n",
      "Iteration 14171, loss = 1.36150391\n",
      "Iteration 14172, loss = 1.36148982\n",
      "Iteration 14173, loss = 1.36147574\n",
      "Iteration 14174, loss = 1.36146303\n",
      "Iteration 14175, loss = 1.36144780\n",
      "Iteration 14176, loss = 1.36143405\n",
      "Iteration 14177, loss = 1.36142032\n",
      "Iteration 14178, loss = 1.36140653\n",
      "Iteration 14179, loss = 1.36139254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14180, loss = 1.36137840\n",
      "Iteration 14181, loss = 1.36136433\n",
      "Iteration 14182, loss = 1.36135034\n",
      "Iteration 14183, loss = 1.36133639\n",
      "Iteration 14184, loss = 1.36132237\n",
      "Iteration 14185, loss = 1.36130833\n",
      "Iteration 14186, loss = 1.36129427\n",
      "Iteration 14187, loss = 1.36128021\n",
      "Iteration 14188, loss = 1.36126613\n",
      "Iteration 14189, loss = 1.36125207\n",
      "Iteration 14190, loss = 1.36123798\n",
      "Iteration 14191, loss = 1.36122385\n",
      "Iteration 14192, loss = 1.36120981\n",
      "Iteration 14193, loss = 1.36119573\n",
      "Iteration 14194, loss = 1.36118160\n",
      "Iteration 14195, loss = 1.36116749\n",
      "Iteration 14196, loss = 1.36115338\n",
      "Iteration 14197, loss = 1.36113927\n",
      "Iteration 14198, loss = 1.36112518\n",
      "Iteration 14199, loss = 1.36111106\n",
      "Iteration 14200, loss = 1.36109713\n",
      "Iteration 14201, loss = 1.36108330\n",
      "Iteration 14202, loss = 1.36106955\n",
      "Iteration 14203, loss = 1.36105575\n",
      "Iteration 14204, loss = 1.36104175\n",
      "Iteration 14205, loss = 1.36102775\n",
      "Iteration 14206, loss = 1.36101379\n",
      "Iteration 14207, loss = 1.36099980\n",
      "Iteration 14208, loss = 1.36098581\n",
      "Iteration 14209, loss = 1.36097183\n",
      "Iteration 14210, loss = 1.36095781\n",
      "Iteration 14211, loss = 1.36094379\n",
      "Iteration 14212, loss = 1.36092972\n",
      "Iteration 14213, loss = 1.36091561\n",
      "Iteration 14214, loss = 1.36090151\n",
      "Iteration 14215, loss = 1.36088740\n",
      "Iteration 14216, loss = 1.36087331\n",
      "Iteration 14217, loss = 1.36085918\n",
      "Iteration 14218, loss = 1.36084506\n",
      "Iteration 14219, loss = 1.36083095\n",
      "Iteration 14220, loss = 1.36081682\n",
      "Iteration 14221, loss = 1.36080271\n",
      "Iteration 14222, loss = 1.36078857\n",
      "Iteration 14223, loss = 1.36077440\n",
      "Iteration 14224, loss = 1.36076027\n",
      "Iteration 14225, loss = 1.36074613\n",
      "Iteration 14226, loss = 1.36073200\n",
      "Iteration 14227, loss = 1.36071885\n",
      "Iteration 14228, loss = 1.36070426\n",
      "Iteration 14229, loss = 1.36069056\n",
      "Iteration 14230, loss = 1.36067674\n",
      "Iteration 14231, loss = 1.36066270\n",
      "Iteration 14232, loss = 1.36064855\n",
      "Iteration 14233, loss = 1.36063444\n",
      "Iteration 14234, loss = 1.36062043\n",
      "Iteration 14235, loss = 1.36060645\n",
      "Iteration 14236, loss = 1.36059247\n",
      "Iteration 14237, loss = 1.36057843\n",
      "Iteration 14238, loss = 1.36056440\n",
      "Iteration 14239, loss = 1.36055032\n",
      "Iteration 14240, loss = 1.36053621\n",
      "Iteration 14241, loss = 1.36052208\n",
      "Iteration 14242, loss = 1.36050800\n",
      "Iteration 14243, loss = 1.36049393\n",
      "Iteration 14244, loss = 1.36047980\n",
      "Iteration 14245, loss = 1.36046565\n",
      "Iteration 14246, loss = 1.36045148\n",
      "Iteration 14247, loss = 1.36043736\n",
      "Iteration 14248, loss = 1.36042325\n",
      "Iteration 14249, loss = 1.36040913\n",
      "Iteration 14250, loss = 1.36039498\n",
      "Iteration 14251, loss = 1.36038081\n",
      "Iteration 14252, loss = 1.36036667\n",
      "Iteration 14253, loss = 1.36035248\n",
      "Iteration 14254, loss = 1.36033923\n",
      "Iteration 14255, loss = 1.36032490\n",
      "Iteration 14256, loss = 1.36031124\n",
      "Iteration 14257, loss = 1.36029740\n",
      "Iteration 14258, loss = 1.36028335\n",
      "Iteration 14259, loss = 1.36026917\n",
      "Iteration 14260, loss = 1.36025491\n",
      "Iteration 14261, loss = 1.36024086\n",
      "Iteration 14262, loss = 1.36022685\n",
      "Iteration 14263, loss = 1.36021286\n",
      "Iteration 14264, loss = 1.36019885\n",
      "Iteration 14265, loss = 1.36018478\n",
      "Iteration 14266, loss = 1.36017072\n",
      "Iteration 14267, loss = 1.36015666\n",
      "Iteration 14268, loss = 1.36014256\n",
      "Iteration 14269, loss = 1.36012843\n",
      "Iteration 14270, loss = 1.36011428\n",
      "Iteration 14271, loss = 1.36010011\n",
      "Iteration 14272, loss = 1.36008596\n",
      "Iteration 14273, loss = 1.36007181\n",
      "Iteration 14274, loss = 1.36005769\n",
      "Iteration 14275, loss = 1.36004357\n",
      "Iteration 14276, loss = 1.36002943\n",
      "Iteration 14277, loss = 1.36001523\n",
      "Iteration 14278, loss = 1.36000101\n",
      "Iteration 14279, loss = 1.35998688\n",
      "Iteration 14280, loss = 1.35997273\n",
      "Iteration 14281, loss = 1.35995941\n",
      "Iteration 14282, loss = 1.35994485\n",
      "Iteration 14283, loss = 1.35993102\n",
      "Iteration 14284, loss = 1.35991711\n",
      "Iteration 14285, loss = 1.35990308\n",
      "Iteration 14286, loss = 1.35988912\n",
      "Iteration 14287, loss = 1.35987513\n",
      "Iteration 14288, loss = 1.35986099\n",
      "Iteration 14289, loss = 1.35984690\n",
      "Iteration 14290, loss = 1.35983280\n",
      "Iteration 14291, loss = 1.35981876\n",
      "Iteration 14292, loss = 1.35980472\n",
      "Iteration 14293, loss = 1.35979064\n",
      "Iteration 14294, loss = 1.35977652\n",
      "Iteration 14295, loss = 1.35976242\n",
      "Iteration 14296, loss = 1.35974828\n",
      "Iteration 14297, loss = 1.35973416\n",
      "Iteration 14298, loss = 1.35971997\n",
      "Iteration 14299, loss = 1.35970576\n",
      "Iteration 14300, loss = 1.35969160\n",
      "Iteration 14301, loss = 1.35967744\n",
      "Iteration 14302, loss = 1.35966328\n",
      "Iteration 14303, loss = 1.35964909\n",
      "Iteration 14304, loss = 1.35963487\n",
      "Iteration 14305, loss = 1.35962067\n",
      "Iteration 14306, loss = 1.35960651\n",
      "Iteration 14307, loss = 1.35959231\n",
      "Iteration 14308, loss = 1.35957815\n",
      "Iteration 14309, loss = 1.35956399\n",
      "Iteration 14310, loss = 1.35954991\n",
      "Iteration 14311, loss = 1.35953600\n",
      "Iteration 14312, loss = 1.35952215\n",
      "Iteration 14313, loss = 1.35950817\n",
      "Iteration 14314, loss = 1.35949424\n",
      "Iteration 14315, loss = 1.35948022\n",
      "Iteration 14316, loss = 1.35946614\n",
      "Iteration 14317, loss = 1.35945207\n",
      "Iteration 14318, loss = 1.35943797\n",
      "Iteration 14319, loss = 1.35942391\n",
      "Iteration 14320, loss = 1.35940983\n",
      "Iteration 14321, loss = 1.35939572\n",
      "Iteration 14322, loss = 1.35938157\n",
      "Iteration 14323, loss = 1.35936743\n",
      "Iteration 14324, loss = 1.35935324\n",
      "Iteration 14325, loss = 1.35933910\n",
      "Iteration 14326, loss = 1.35932501\n",
      "Iteration 14327, loss = 1.35931084\n",
      "Iteration 14328, loss = 1.35929663\n",
      "Iteration 14329, loss = 1.35928245\n",
      "Iteration 14330, loss = 1.35926824\n",
      "Iteration 14331, loss = 1.35925406\n",
      "Iteration 14332, loss = 1.35923992\n",
      "Iteration 14333, loss = 1.35922573\n",
      "Iteration 14334, loss = 1.35921152\n",
      "Iteration 14335, loss = 1.35919730\n",
      "Iteration 14336, loss = 1.35918308\n",
      "Iteration 14337, loss = 1.35916890\n",
      "Iteration 14338, loss = 1.35915470\n",
      "Iteration 14339, loss = 1.35914049\n",
      "Iteration 14340, loss = 1.35912787\n",
      "Iteration 14341, loss = 1.35911258\n",
      "Iteration 14342, loss = 1.35909886\n",
      "Iteration 14343, loss = 1.35908496\n",
      "Iteration 14344, loss = 1.35907083\n",
      "Iteration 14345, loss = 1.35905653\n",
      "Iteration 14346, loss = 1.35904240\n",
      "Iteration 14347, loss = 1.35902842\n",
      "Iteration 14348, loss = 1.35901440\n",
      "Iteration 14349, loss = 1.35900034\n",
      "Iteration 14350, loss = 1.35898623\n",
      "Iteration 14351, loss = 1.35897208\n",
      "Iteration 14352, loss = 1.35895791\n",
      "Iteration 14353, loss = 1.35894377\n",
      "Iteration 14354, loss = 1.35892963\n",
      "Iteration 14355, loss = 1.35891545\n",
      "Iteration 14356, loss = 1.35890125\n",
      "Iteration 14357, loss = 1.35888703\n",
      "Iteration 14358, loss = 1.35887285\n",
      "Iteration 14359, loss = 1.35885862\n",
      "Iteration 14360, loss = 1.35884439\n",
      "Iteration 14361, loss = 1.35883014\n",
      "Iteration 14362, loss = 1.35881596\n",
      "Iteration 14363, loss = 1.35880181\n",
      "Iteration 14364, loss = 1.35878759\n",
      "Iteration 14365, loss = 1.35877328\n",
      "Iteration 14366, loss = 1.35875908\n",
      "Iteration 14367, loss = 1.35874489\n",
      "Iteration 14368, loss = 1.35873068\n",
      "Iteration 14369, loss = 1.35871647\n",
      "Iteration 14370, loss = 1.35870326\n",
      "Iteration 14371, loss = 1.35868847\n",
      "Iteration 14372, loss = 1.35867475\n",
      "Iteration 14373, loss = 1.35866083\n",
      "Iteration 14374, loss = 1.35864669\n",
      "Iteration 14375, loss = 1.35863244\n",
      "Iteration 14376, loss = 1.35861824\n",
      "Iteration 14377, loss = 1.35860415\n",
      "Iteration 14378, loss = 1.35859007\n",
      "Iteration 14379, loss = 1.35857600\n",
      "Iteration 14380, loss = 1.35856190\n",
      "Iteration 14381, loss = 1.35854775\n",
      "Iteration 14382, loss = 1.35853358\n",
      "Iteration 14383, loss = 1.35851940\n",
      "Iteration 14384, loss = 1.35850521\n",
      "Iteration 14385, loss = 1.35849100\n",
      "Iteration 14386, loss = 1.35847678\n",
      "Iteration 14387, loss = 1.35846256\n",
      "Iteration 14388, loss = 1.35844831\n",
      "Iteration 14389, loss = 1.35843408\n",
      "Iteration 14390, loss = 1.35841988\n",
      "Iteration 14391, loss = 1.35840569\n",
      "Iteration 14392, loss = 1.35839145\n",
      "Iteration 14393, loss = 1.35837718\n",
      "Iteration 14394, loss = 1.35836291\n",
      "Iteration 14395, loss = 1.35834865\n",
      "Iteration 14396, loss = 1.35833442\n",
      "Iteration 14397, loss = 1.35832020\n",
      "Iteration 14398, loss = 1.35830601\n",
      "Iteration 14399, loss = 1.35829179\n",
      "Iteration 14400, loss = 1.35827749\n",
      "Iteration 14401, loss = 1.35826386\n",
      "Iteration 14402, loss = 1.35824945\n",
      "Iteration 14403, loss = 1.35823567\n",
      "Iteration 14404, loss = 1.35822166\n",
      "Iteration 14405, loss = 1.35820748\n",
      "Iteration 14406, loss = 1.35819329\n",
      "Iteration 14407, loss = 1.35817909\n",
      "Iteration 14408, loss = 1.35816499\n",
      "Iteration 14409, loss = 1.35815087\n",
      "Iteration 14410, loss = 1.35813674\n",
      "Iteration 14411, loss = 1.35812259\n",
      "Iteration 14412, loss = 1.35810840\n",
      "Iteration 14413, loss = 1.35809418\n",
      "Iteration 14414, loss = 1.35808001\n",
      "Iteration 14415, loss = 1.35806579\n",
      "Iteration 14416, loss = 1.35805158\n",
      "Iteration 14417, loss = 1.35803731\n",
      "Iteration 14418, loss = 1.35802303\n",
      "Iteration 14419, loss = 1.35800881\n",
      "Iteration 14420, loss = 1.35799460\n",
      "Iteration 14421, loss = 1.35798044\n",
      "Iteration 14422, loss = 1.35796612\n",
      "Iteration 14423, loss = 1.35795185\n",
      "Iteration 14424, loss = 1.35793760\n",
      "Iteration 14425, loss = 1.35792336\n",
      "Iteration 14426, loss = 1.35790910\n",
      "Iteration 14427, loss = 1.35789486\n",
      "Iteration 14428, loss = 1.35788059\n",
      "Iteration 14429, loss = 1.35786635\n",
      "Iteration 14430, loss = 1.35785209\n",
      "Iteration 14431, loss = 1.35783784\n",
      "Iteration 14432, loss = 1.35782358\n",
      "Iteration 14433, loss = 1.35781008\n",
      "Iteration 14434, loss = 1.35779540\n",
      "Iteration 14435, loss = 1.35778164\n",
      "Iteration 14436, loss = 1.35776764\n",
      "Iteration 14437, loss = 1.35775336\n",
      "Iteration 14438, loss = 1.35773911\n",
      "Iteration 14439, loss = 1.35772489\n",
      "Iteration 14440, loss = 1.35771072\n",
      "Iteration 14441, loss = 1.35769651\n",
      "Iteration 14442, loss = 1.35768232\n",
      "Iteration 14443, loss = 1.35766810\n",
      "Iteration 14444, loss = 1.35765389\n",
      "Iteration 14445, loss = 1.35763964\n",
      "Iteration 14446, loss = 1.35762535\n",
      "Iteration 14447, loss = 1.35761104\n",
      "Iteration 14448, loss = 1.35759672\n",
      "Iteration 14449, loss = 1.35758239\n",
      "Iteration 14450, loss = 1.35756806\n",
      "Iteration 14451, loss = 1.35755369\n",
      "Iteration 14452, loss = 1.35753937\n",
      "Iteration 14453, loss = 1.35752505\n",
      "Iteration 14454, loss = 1.35751070\n",
      "Iteration 14455, loss = 1.35749634\n",
      "Iteration 14456, loss = 1.35748204\n",
      "Iteration 14457, loss = 1.35746769\n",
      "Iteration 14458, loss = 1.35745332\n",
      "Iteration 14459, loss = 1.35743901\n",
      "Iteration 14460, loss = 1.35742467\n",
      "Iteration 14461, loss = 1.35741033\n",
      "Iteration 14462, loss = 1.35739598\n",
      "Iteration 14463, loss = 1.35738161\n",
      "Iteration 14464, loss = 1.35736725\n",
      "Iteration 14465, loss = 1.35735289\n",
      "Iteration 14466, loss = 1.35733925\n",
      "Iteration 14467, loss = 1.35732458\n",
      "Iteration 14468, loss = 1.35731068\n",
      "Iteration 14469, loss = 1.35729659\n",
      "Iteration 14470, loss = 1.35728226\n",
      "Iteration 14471, loss = 1.35726789\n",
      "Iteration 14472, loss = 1.35725365\n",
      "Iteration 14473, loss = 1.35723943\n",
      "Iteration 14474, loss = 1.35722515\n",
      "Iteration 14475, loss = 1.35721088\n",
      "Iteration 14476, loss = 1.35719661\n",
      "Iteration 14477, loss = 1.35718239\n",
      "Iteration 14478, loss = 1.35716807\n",
      "Iteration 14479, loss = 1.35715375\n",
      "Iteration 14480, loss = 1.35713940\n",
      "Iteration 14481, loss = 1.35712504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14482, loss = 1.35711068\n",
      "Iteration 14483, loss = 1.35709629\n",
      "Iteration 14484, loss = 1.35708189\n",
      "Iteration 14485, loss = 1.35706753\n",
      "Iteration 14486, loss = 1.35705317\n",
      "Iteration 14487, loss = 1.35703878\n",
      "Iteration 14488, loss = 1.35702438\n",
      "Iteration 14489, loss = 1.35700998\n",
      "Iteration 14490, loss = 1.35699560\n",
      "Iteration 14491, loss = 1.35698124\n",
      "Iteration 14492, loss = 1.35696683\n",
      "Iteration 14493, loss = 1.35695246\n",
      "Iteration 14494, loss = 1.35693810\n",
      "Iteration 14495, loss = 1.35692377\n",
      "Iteration 14496, loss = 1.35690939\n",
      "Iteration 14497, loss = 1.35689495\n",
      "Iteration 14498, loss = 1.35688055\n",
      "Iteration 14499, loss = 1.35686665\n",
      "Iteration 14500, loss = 1.35685217\n",
      "Iteration 14501, loss = 1.35683831\n",
      "Iteration 14502, loss = 1.35682423\n",
      "Iteration 14503, loss = 1.35680989\n",
      "Iteration 14504, loss = 1.35679542\n",
      "Iteration 14505, loss = 1.35678114\n",
      "Iteration 14506, loss = 1.35676689\n",
      "Iteration 14507, loss = 1.35675260\n",
      "Iteration 14508, loss = 1.35673834\n",
      "Iteration 14509, loss = 1.35672406\n",
      "Iteration 14510, loss = 1.35670975\n",
      "Iteration 14511, loss = 1.35669541\n",
      "Iteration 14512, loss = 1.35668104\n",
      "Iteration 14513, loss = 1.35666672\n",
      "Iteration 14514, loss = 1.35665233\n",
      "Iteration 14515, loss = 1.35663795\n",
      "Iteration 14516, loss = 1.35662353\n",
      "Iteration 14517, loss = 1.35660909\n",
      "Iteration 14518, loss = 1.35659468\n",
      "Iteration 14519, loss = 1.35658030\n",
      "Iteration 14520, loss = 1.35656589\n",
      "Iteration 14521, loss = 1.35655143\n",
      "Iteration 14522, loss = 1.35653704\n",
      "Iteration 14523, loss = 1.35652265\n",
      "Iteration 14524, loss = 1.35650822\n",
      "Iteration 14525, loss = 1.35649375\n",
      "Iteration 14526, loss = 1.35647936\n",
      "Iteration 14527, loss = 1.35646498\n",
      "Iteration 14528, loss = 1.35645052\n",
      "Iteration 14529, loss = 1.35643610\n",
      "Iteration 14530, loss = 1.35642167\n",
      "Iteration 14531, loss = 1.35640728\n",
      "Iteration 14532, loss = 1.35639284\n",
      "Iteration 14533, loss = 1.35637890\n",
      "Iteration 14534, loss = 1.35636442\n",
      "Iteration 14535, loss = 1.35635051\n",
      "Iteration 14536, loss = 1.35633634\n",
      "Iteration 14537, loss = 1.35632188\n",
      "Iteration 14538, loss = 1.35630737\n",
      "Iteration 14539, loss = 1.35629315\n",
      "Iteration 14540, loss = 1.35627890\n",
      "Iteration 14541, loss = 1.35626464\n",
      "Iteration 14542, loss = 1.35625035\n",
      "Iteration 14543, loss = 1.35623602\n",
      "Iteration 14544, loss = 1.35622166\n",
      "Iteration 14545, loss = 1.35620727\n",
      "Iteration 14546, loss = 1.35619288\n",
      "Iteration 14547, loss = 1.35617848\n",
      "Iteration 14548, loss = 1.35616404\n",
      "Iteration 14549, loss = 1.35614962\n",
      "Iteration 14550, loss = 1.35613523\n",
      "Iteration 14551, loss = 1.35612081\n",
      "Iteration 14552, loss = 1.35610636\n",
      "Iteration 14553, loss = 1.35609195\n",
      "Iteration 14554, loss = 1.35607748\n",
      "Iteration 14555, loss = 1.35606301\n",
      "Iteration 14556, loss = 1.35604860\n",
      "Iteration 14557, loss = 1.35603417\n",
      "Iteration 14558, loss = 1.35601971\n",
      "Iteration 14559, loss = 1.35600523\n",
      "Iteration 14560, loss = 1.35599079\n",
      "Iteration 14561, loss = 1.35597636\n",
      "Iteration 14562, loss = 1.35596197\n",
      "Iteration 14563, loss = 1.35594752\n",
      "Iteration 14564, loss = 1.35593306\n",
      "Iteration 14565, loss = 1.35591861\n",
      "Iteration 14566, loss = 1.35590418\n",
      "Iteration 14567, loss = 1.35588971\n",
      "Iteration 14568, loss = 1.35587527\n",
      "Iteration 14569, loss = 1.35586187\n",
      "Iteration 14570, loss = 1.35584682\n",
      "Iteration 14571, loss = 1.35583291\n",
      "Iteration 14572, loss = 1.35581868\n",
      "Iteration 14573, loss = 1.35580420\n",
      "Iteration 14574, loss = 1.35578973\n",
      "Iteration 14575, loss = 1.35577538\n",
      "Iteration 14576, loss = 1.35576108\n",
      "Iteration 14577, loss = 1.35574675\n",
      "Iteration 14578, loss = 1.35573242\n",
      "Iteration 14579, loss = 1.35571807\n",
      "Iteration 14580, loss = 1.35570370\n",
      "Iteration 14581, loss = 1.35568932\n",
      "Iteration 14582, loss = 1.35567490\n",
      "Iteration 14583, loss = 1.35566046\n",
      "Iteration 14584, loss = 1.35564600\n",
      "Iteration 14585, loss = 1.35563158\n",
      "Iteration 14586, loss = 1.35561714\n",
      "Iteration 14587, loss = 1.35560267\n",
      "Iteration 14588, loss = 1.35558818\n",
      "Iteration 14589, loss = 1.35557369\n",
      "Iteration 14590, loss = 1.35555922\n",
      "Iteration 14591, loss = 1.35554474\n",
      "Iteration 14592, loss = 1.35553023\n",
      "Iteration 14593, loss = 1.35551575\n",
      "Iteration 14594, loss = 1.35550125\n",
      "Iteration 14595, loss = 1.35548678\n",
      "Iteration 14596, loss = 1.35547235\n",
      "Iteration 14597, loss = 1.35545786\n",
      "Iteration 14598, loss = 1.35544338\n",
      "Iteration 14599, loss = 1.35542890\n",
      "Iteration 14600, loss = 1.35541440\n",
      "Iteration 14601, loss = 1.35539995\n",
      "Iteration 14602, loss = 1.35538547\n",
      "Iteration 14603, loss = 1.35537098\n",
      "Iteration 14604, loss = 1.35535670\n",
      "Iteration 14605, loss = 1.35534242\n",
      "Iteration 14606, loss = 1.35532842\n",
      "Iteration 14607, loss = 1.35531413\n",
      "Iteration 14608, loss = 1.35529964\n",
      "Iteration 14609, loss = 1.35528522\n",
      "Iteration 14610, loss = 1.35527085\n",
      "Iteration 14611, loss = 1.35525649\n",
      "Iteration 14612, loss = 1.35524216\n",
      "Iteration 14613, loss = 1.35522781\n",
      "Iteration 14614, loss = 1.35521343\n",
      "Iteration 14615, loss = 1.35519903\n",
      "Iteration 14616, loss = 1.35518460\n",
      "Iteration 14617, loss = 1.35517018\n",
      "Iteration 14618, loss = 1.35515571\n",
      "Iteration 14619, loss = 1.35514121\n",
      "Iteration 14620, loss = 1.35512673\n",
      "Iteration 14621, loss = 1.35511228\n",
      "Iteration 14622, loss = 1.35509780\n",
      "Iteration 14623, loss = 1.35508331\n",
      "Iteration 14624, loss = 1.35506878\n",
      "Iteration 14625, loss = 1.35505423\n",
      "Iteration 14626, loss = 1.35503978\n",
      "Iteration 14627, loss = 1.35502528\n",
      "Iteration 14628, loss = 1.35501081\n",
      "Iteration 14629, loss = 1.35499627\n",
      "Iteration 14630, loss = 1.35498173\n",
      "Iteration 14631, loss = 1.35496725\n",
      "Iteration 14632, loss = 1.35495278\n",
      "Iteration 14633, loss = 1.35493827\n",
      "Iteration 14634, loss = 1.35492374\n",
      "Iteration 14635, loss = 1.35490923\n",
      "Iteration 14636, loss = 1.35489473\n",
      "Iteration 14637, loss = 1.35488025\n",
      "Iteration 14638, loss = 1.35486573\n",
      "Iteration 14639, loss = 1.35485121\n",
      "Iteration 14640, loss = 1.35483671\n",
      "Iteration 14641, loss = 1.35482222\n",
      "Iteration 14642, loss = 1.35480906\n",
      "Iteration 14643, loss = 1.35479374\n",
      "Iteration 14644, loss = 1.35477965\n",
      "Iteration 14645, loss = 1.35476523\n",
      "Iteration 14646, loss = 1.35475069\n",
      "Iteration 14647, loss = 1.35473626\n",
      "Iteration 14648, loss = 1.35472189\n",
      "Iteration 14649, loss = 1.35470757\n",
      "Iteration 14650, loss = 1.35469321\n",
      "Iteration 14651, loss = 1.35467883\n",
      "Iteration 14652, loss = 1.35466441\n",
      "Iteration 14653, loss = 1.35464996\n",
      "Iteration 14654, loss = 1.35463549\n",
      "Iteration 14655, loss = 1.35462100\n",
      "Iteration 14656, loss = 1.35460651\n",
      "Iteration 14657, loss = 1.35459201\n",
      "Iteration 14658, loss = 1.35457748\n",
      "Iteration 14659, loss = 1.35456302\n",
      "Iteration 14660, loss = 1.35454848\n",
      "Iteration 14661, loss = 1.35453397\n",
      "Iteration 14662, loss = 1.35451945\n",
      "Iteration 14663, loss = 1.35450498\n",
      "Iteration 14664, loss = 1.35449043\n",
      "Iteration 14665, loss = 1.35447584\n",
      "Iteration 14666, loss = 1.35446129\n",
      "Iteration 14667, loss = 1.35444676\n",
      "Iteration 14668, loss = 1.35443224\n",
      "Iteration 14669, loss = 1.35441770\n",
      "Iteration 14670, loss = 1.35440314\n",
      "Iteration 14671, loss = 1.35438858\n",
      "Iteration 14672, loss = 1.35437406\n",
      "Iteration 14673, loss = 1.35435951\n",
      "Iteration 14674, loss = 1.35434497\n",
      "Iteration 14675, loss = 1.35433043\n",
      "Iteration 14676, loss = 1.35431587\n",
      "Iteration 14677, loss = 1.35430135\n",
      "Iteration 14678, loss = 1.35428680\n",
      "Iteration 14679, loss = 1.35427332\n",
      "Iteration 14680, loss = 1.35425810\n",
      "Iteration 14681, loss = 1.35424392\n",
      "Iteration 14682, loss = 1.35422955\n",
      "Iteration 14683, loss = 1.35421499\n",
      "Iteration 14684, loss = 1.35420053\n",
      "Iteration 14685, loss = 1.35418618\n",
      "Iteration 14686, loss = 1.35417182\n",
      "Iteration 14687, loss = 1.35415742\n",
      "Iteration 14688, loss = 1.35414298\n",
      "Iteration 14689, loss = 1.35412853\n",
      "Iteration 14690, loss = 1.35411400\n",
      "Iteration 14691, loss = 1.35409946\n",
      "Iteration 14692, loss = 1.35408497\n",
      "Iteration 14693, loss = 1.35407050\n",
      "Iteration 14694, loss = 1.35405598\n",
      "Iteration 14695, loss = 1.35404140\n",
      "Iteration 14696, loss = 1.35402688\n",
      "Iteration 14697, loss = 1.35401234\n",
      "Iteration 14698, loss = 1.35399781\n",
      "Iteration 14699, loss = 1.35398326\n",
      "Iteration 14700, loss = 1.35396870\n",
      "Iteration 14701, loss = 1.35395416\n",
      "Iteration 14702, loss = 1.35393956\n",
      "Iteration 14703, loss = 1.35392502\n",
      "Iteration 14704, loss = 1.35391043\n",
      "Iteration 14705, loss = 1.35389585\n",
      "Iteration 14706, loss = 1.35388136\n",
      "Iteration 14707, loss = 1.35386678\n",
      "Iteration 14708, loss = 1.35385220\n",
      "Iteration 14709, loss = 1.35383763\n",
      "Iteration 14710, loss = 1.35382307\n",
      "Iteration 14711, loss = 1.35380853\n",
      "Iteration 14712, loss = 1.35379395\n",
      "Iteration 14713, loss = 1.35377944\n",
      "Iteration 14714, loss = 1.35376488\n",
      "Iteration 14715, loss = 1.35375030\n",
      "Iteration 14716, loss = 1.35373572\n",
      "Iteration 14717, loss = 1.35372174\n",
      "Iteration 14718, loss = 1.35370710\n",
      "Iteration 14719, loss = 1.35369289\n",
      "Iteration 14720, loss = 1.35367855\n",
      "Iteration 14721, loss = 1.35366398\n",
      "Iteration 14722, loss = 1.35364944\n",
      "Iteration 14723, loss = 1.35363496\n",
      "Iteration 14724, loss = 1.35362059\n",
      "Iteration 14725, loss = 1.35360622\n",
      "Iteration 14726, loss = 1.35359180\n",
      "Iteration 14727, loss = 1.35357732\n",
      "Iteration 14728, loss = 1.35356284\n",
      "Iteration 14729, loss = 1.35354832\n",
      "Iteration 14730, loss = 1.35353378\n",
      "Iteration 14731, loss = 1.35351920\n",
      "Iteration 14732, loss = 1.35350460\n",
      "Iteration 14733, loss = 1.35349005\n",
      "Iteration 14734, loss = 1.35347552\n",
      "Iteration 14735, loss = 1.35346094\n",
      "Iteration 14736, loss = 1.35344632\n",
      "Iteration 14737, loss = 1.35343174\n",
      "Iteration 14738, loss = 1.35341714\n",
      "Iteration 14739, loss = 1.35340253\n",
      "Iteration 14740, loss = 1.35338797\n",
      "Iteration 14741, loss = 1.35337340\n",
      "Iteration 14742, loss = 1.35335881\n",
      "Iteration 14743, loss = 1.35334423\n",
      "Iteration 14744, loss = 1.35332961\n",
      "Iteration 14745, loss = 1.35331500\n",
      "Iteration 14746, loss = 1.35330043\n",
      "Iteration 14747, loss = 1.35328584\n",
      "Iteration 14748, loss = 1.35327122\n",
      "Iteration 14749, loss = 1.35325663\n",
      "Iteration 14750, loss = 1.35324204\n",
      "Iteration 14751, loss = 1.35322744\n",
      "Iteration 14752, loss = 1.35321282\n",
      "Iteration 14753, loss = 1.35319823\n",
      "Iteration 14754, loss = 1.35318384\n",
      "Iteration 14755, loss = 1.35316961\n",
      "Iteration 14756, loss = 1.35315554\n",
      "Iteration 14757, loss = 1.35314109\n",
      "Iteration 14758, loss = 1.35312640\n",
      "Iteration 14759, loss = 1.35311186\n",
      "Iteration 14760, loss = 1.35309738\n",
      "Iteration 14761, loss = 1.35308287\n",
      "Iteration 14762, loss = 1.35306835\n",
      "Iteration 14763, loss = 1.35305391\n",
      "Iteration 14764, loss = 1.35303945\n",
      "Iteration 14765, loss = 1.35302495\n",
      "Iteration 14766, loss = 1.35301042\n",
      "Iteration 14767, loss = 1.35299585\n",
      "Iteration 14768, loss = 1.35298127\n",
      "Iteration 14769, loss = 1.35296666\n",
      "Iteration 14770, loss = 1.35295206\n",
      "Iteration 14771, loss = 1.35293748\n",
      "Iteration 14772, loss = 1.35292290\n",
      "Iteration 14773, loss = 1.35290828\n",
      "Iteration 14774, loss = 1.35289367\n",
      "Iteration 14775, loss = 1.35287904\n",
      "Iteration 14776, loss = 1.35286445\n",
      "Iteration 14777, loss = 1.35284981\n",
      "Iteration 14778, loss = 1.35283519\n",
      "Iteration 14779, loss = 1.35282061\n",
      "Iteration 14780, loss = 1.35280605\n",
      "Iteration 14781, loss = 1.35279143\n",
      "Iteration 14782, loss = 1.35277677\n",
      "Iteration 14783, loss = 1.35276217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14784, loss = 1.35274758\n",
      "Iteration 14785, loss = 1.35273296\n",
      "Iteration 14786, loss = 1.35271834\n",
      "Iteration 14787, loss = 1.35270375\n",
      "Iteration 14788, loss = 1.35268914\n",
      "Iteration 14789, loss = 1.35267448\n",
      "Iteration 14790, loss = 1.35265986\n",
      "Iteration 14791, loss = 1.35264527\n",
      "Iteration 14792, loss = 1.35263063\n",
      "Iteration 14793, loss = 1.35261774\n",
      "Iteration 14794, loss = 1.35260197\n",
      "Iteration 14795, loss = 1.35258781\n",
      "Iteration 14796, loss = 1.35257329\n",
      "Iteration 14797, loss = 1.35255858\n",
      "Iteration 14798, loss = 1.35254398\n",
      "Iteration 14799, loss = 1.35252960\n",
      "Iteration 14800, loss = 1.35251519\n",
      "Iteration 14801, loss = 1.35250073\n",
      "Iteration 14802, loss = 1.35248622\n",
      "Iteration 14803, loss = 1.35247168\n",
      "Iteration 14804, loss = 1.35245711\n",
      "Iteration 14805, loss = 1.35244251\n",
      "Iteration 14806, loss = 1.35242796\n",
      "Iteration 14807, loss = 1.35241342\n",
      "Iteration 14808, loss = 1.35239884\n",
      "Iteration 14809, loss = 1.35238420\n",
      "Iteration 14810, loss = 1.35236957\n",
      "Iteration 14811, loss = 1.35235493\n",
      "Iteration 14812, loss = 1.35234037\n",
      "Iteration 14813, loss = 1.35232575\n",
      "Iteration 14814, loss = 1.35231110\n",
      "Iteration 14815, loss = 1.35229647\n",
      "Iteration 14816, loss = 1.35228185\n",
      "Iteration 14817, loss = 1.35226726\n",
      "Iteration 14818, loss = 1.35225260\n",
      "Iteration 14819, loss = 1.35223796\n",
      "Iteration 14820, loss = 1.35222335\n",
      "Iteration 14821, loss = 1.35220873\n",
      "Iteration 14822, loss = 1.35219411\n",
      "Iteration 14823, loss = 1.35217945\n",
      "Iteration 14824, loss = 1.35216481\n",
      "Iteration 14825, loss = 1.35215019\n",
      "Iteration 14826, loss = 1.35213554\n",
      "Iteration 14827, loss = 1.35212092\n",
      "Iteration 14828, loss = 1.35210624\n",
      "Iteration 14829, loss = 1.35209166\n",
      "Iteration 14830, loss = 1.35207699\n",
      "Iteration 14831, loss = 1.35206385\n",
      "Iteration 14832, loss = 1.35204832\n",
      "Iteration 14833, loss = 1.35203417\n",
      "Iteration 14834, loss = 1.35201969\n",
      "Iteration 14835, loss = 1.35200506\n",
      "Iteration 14836, loss = 1.35199037\n",
      "Iteration 14837, loss = 1.35197584\n",
      "Iteration 14838, loss = 1.35196135\n",
      "Iteration 14839, loss = 1.35194685\n",
      "Iteration 14840, loss = 1.35193237\n",
      "Iteration 14841, loss = 1.35191784\n",
      "Iteration 14842, loss = 1.35190329\n",
      "Iteration 14843, loss = 1.35188872\n",
      "Iteration 14844, loss = 1.35187411\n",
      "Iteration 14845, loss = 1.35185952\n",
      "Iteration 14846, loss = 1.35184491\n",
      "Iteration 14847, loss = 1.35183026\n",
      "Iteration 14848, loss = 1.35181561\n",
      "Iteration 14849, loss = 1.35180101\n",
      "Iteration 14850, loss = 1.35178640\n",
      "Iteration 14851, loss = 1.35177174\n",
      "Iteration 14852, loss = 1.35175709\n",
      "Iteration 14853, loss = 1.35174243\n",
      "Iteration 14854, loss = 1.35172772\n",
      "Iteration 14855, loss = 1.35171316\n",
      "Iteration 14856, loss = 1.35169849\n",
      "Iteration 14857, loss = 1.35168387\n",
      "Iteration 14858, loss = 1.35166921\n",
      "Iteration 14859, loss = 1.35165454\n",
      "Iteration 14860, loss = 1.35163994\n",
      "Iteration 14861, loss = 1.35162527\n",
      "Iteration 14862, loss = 1.35161063\n",
      "Iteration 14863, loss = 1.35159599\n",
      "Iteration 14864, loss = 1.35158139\n",
      "Iteration 14865, loss = 1.35156671\n",
      "Iteration 14866, loss = 1.35155201\n",
      "Iteration 14867, loss = 1.35153739\n",
      "Iteration 14868, loss = 1.35152273\n",
      "Iteration 14869, loss = 1.35150932\n",
      "Iteration 14870, loss = 1.35149407\n",
      "Iteration 14871, loss = 1.35147995\n",
      "Iteration 14872, loss = 1.35146547\n",
      "Iteration 14873, loss = 1.35145069\n",
      "Iteration 14874, loss = 1.35143599\n",
      "Iteration 14875, loss = 1.35142155\n",
      "Iteration 14876, loss = 1.35140713\n",
      "Iteration 14877, loss = 1.35139266\n",
      "Iteration 14878, loss = 1.35137814\n",
      "Iteration 14879, loss = 1.35136359\n",
      "Iteration 14880, loss = 1.35134901\n",
      "Iteration 14881, loss = 1.35133439\n",
      "Iteration 14882, loss = 1.35131981\n",
      "Iteration 14883, loss = 1.35130524\n",
      "Iteration 14884, loss = 1.35129062\n",
      "Iteration 14885, loss = 1.35127595\n",
      "Iteration 14886, loss = 1.35126129\n",
      "Iteration 14887, loss = 1.35124665\n",
      "Iteration 14888, loss = 1.35123201\n",
      "Iteration 14889, loss = 1.35121739\n",
      "Iteration 14890, loss = 1.35120271\n",
      "Iteration 14891, loss = 1.35118803\n",
      "Iteration 14892, loss = 1.35117336\n",
      "Iteration 14893, loss = 1.35115872\n",
      "Iteration 14894, loss = 1.35114407\n",
      "Iteration 14895, loss = 1.35112941\n",
      "Iteration 14896, loss = 1.35111477\n",
      "Iteration 14897, loss = 1.35110008\n",
      "Iteration 14898, loss = 1.35108545\n",
      "Iteration 14899, loss = 1.35107079\n",
      "Iteration 14900, loss = 1.35105615\n",
      "Iteration 14901, loss = 1.35104149\n",
      "Iteration 14902, loss = 1.35102682\n",
      "Iteration 14903, loss = 1.35101217\n",
      "Iteration 14904, loss = 1.35099747\n",
      "Iteration 14905, loss = 1.35098280\n",
      "Iteration 14906, loss = 1.35096861\n",
      "Iteration 14907, loss = 1.35095408\n",
      "Iteration 14908, loss = 1.35093996\n",
      "Iteration 14909, loss = 1.35092548\n",
      "Iteration 14910, loss = 1.35091074\n",
      "Iteration 14911, loss = 1.35089605\n",
      "Iteration 14912, loss = 1.35088153\n",
      "Iteration 14913, loss = 1.35086711\n",
      "Iteration 14914, loss = 1.35085263\n",
      "Iteration 14915, loss = 1.35083812\n",
      "Iteration 14916, loss = 1.35082358\n",
      "Iteration 14917, loss = 1.35080901\n",
      "Iteration 14918, loss = 1.35079440\n",
      "Iteration 14919, loss = 1.35077978\n",
      "Iteration 14920, loss = 1.35076513\n",
      "Iteration 14921, loss = 1.35075054\n",
      "Iteration 14922, loss = 1.35073595\n",
      "Iteration 14923, loss = 1.35072132\n",
      "Iteration 14924, loss = 1.35070664\n",
      "Iteration 14925, loss = 1.35069192\n",
      "Iteration 14926, loss = 1.35067724\n",
      "Iteration 14927, loss = 1.35066258\n",
      "Iteration 14928, loss = 1.35064792\n",
      "Iteration 14929, loss = 1.35063325\n",
      "Iteration 14930, loss = 1.35061858\n",
      "Iteration 14931, loss = 1.35060394\n",
      "Iteration 14932, loss = 1.35058931\n",
      "Iteration 14933, loss = 1.35057461\n",
      "Iteration 14934, loss = 1.35055991\n",
      "Iteration 14935, loss = 1.35054525\n",
      "Iteration 14936, loss = 1.35053060\n",
      "Iteration 14937, loss = 1.35051593\n",
      "Iteration 14938, loss = 1.35050126\n",
      "Iteration 14939, loss = 1.35048658\n",
      "Iteration 14940, loss = 1.35047196\n",
      "Iteration 14941, loss = 1.35045730\n",
      "Iteration 14942, loss = 1.35044313\n",
      "Iteration 14943, loss = 1.35042842\n",
      "Iteration 14944, loss = 1.35041431\n",
      "Iteration 14945, loss = 1.35039989\n",
      "Iteration 14946, loss = 1.35038521\n",
      "Iteration 14947, loss = 1.35037045\n",
      "Iteration 14948, loss = 1.35035601\n",
      "Iteration 14949, loss = 1.35034157\n",
      "Iteration 14950, loss = 1.35032709\n",
      "Iteration 14951, loss = 1.35031258\n",
      "Iteration 14952, loss = 1.35029803\n",
      "Iteration 14953, loss = 1.35028346\n",
      "Iteration 14954, loss = 1.35026885\n",
      "Iteration 14955, loss = 1.35025421\n",
      "Iteration 14956, loss = 1.35023957\n",
      "Iteration 14957, loss = 1.35022492\n",
      "Iteration 14958, loss = 1.35021023\n",
      "Iteration 14959, loss = 1.35019554\n",
      "Iteration 14960, loss = 1.35018088\n",
      "Iteration 14961, loss = 1.35016615\n",
      "Iteration 14962, loss = 1.35015141\n",
      "Iteration 14963, loss = 1.35013671\n",
      "Iteration 14964, loss = 1.35012199\n",
      "Iteration 14965, loss = 1.35010728\n",
      "Iteration 14966, loss = 1.35009253\n",
      "Iteration 14967, loss = 1.35007780\n",
      "Iteration 14968, loss = 1.35006311\n",
      "Iteration 14969, loss = 1.35004838\n",
      "Iteration 14970, loss = 1.35003364\n",
      "Iteration 14971, loss = 1.35001891\n",
      "Iteration 14972, loss = 1.35000417\n",
      "Iteration 14973, loss = 1.34998948\n",
      "Iteration 14974, loss = 1.34997476\n",
      "Iteration 14975, loss = 1.34996002\n",
      "Iteration 14976, loss = 1.34994523\n",
      "Iteration 14977, loss = 1.34993052\n",
      "Iteration 14978, loss = 1.34991664\n",
      "Iteration 14979, loss = 1.34990136\n",
      "Iteration 14980, loss = 1.34988715\n",
      "Iteration 14981, loss = 1.34987277\n",
      "Iteration 14982, loss = 1.34985804\n",
      "Iteration 14983, loss = 1.34984325\n",
      "Iteration 14984, loss = 1.34982873\n",
      "Iteration 14985, loss = 1.34981420\n",
      "Iteration 14986, loss = 1.34979963\n",
      "Iteration 14987, loss = 1.34978502\n",
      "Iteration 14988, loss = 1.34977040\n",
      "Iteration 14989, loss = 1.34975575\n",
      "Iteration 14990, loss = 1.34974110\n",
      "Iteration 14991, loss = 1.34972643\n",
      "Iteration 14992, loss = 1.34971174\n",
      "Iteration 14993, loss = 1.34969704\n",
      "Iteration 14994, loss = 1.34968234\n",
      "Iteration 14995, loss = 1.34966762\n",
      "Iteration 14996, loss = 1.34965290\n",
      "Iteration 14997, loss = 1.34963818\n",
      "Iteration 14998, loss = 1.34962344\n",
      "Iteration 14999, loss = 1.34960869\n",
      "Iteration 15000, loss = 1.34959392\n",
      "Iteration 15001, loss = 1.34957922\n",
      "Iteration 15002, loss = 1.34956448\n",
      "Iteration 15003, loss = 1.34954970\n",
      "Iteration 15004, loss = 1.34953502\n",
      "Iteration 15005, loss = 1.34952031\n",
      "Iteration 15006, loss = 1.34950556\n",
      "Iteration 15007, loss = 1.34949081\n",
      "Iteration 15008, loss = 1.34947609\n",
      "Iteration 15009, loss = 1.34946136\n",
      "Iteration 15010, loss = 1.34944664\n",
      "Iteration 15011, loss = 1.34943194\n",
      "Iteration 15012, loss = 1.34941717\n",
      "Iteration 15013, loss = 1.34940393\n",
      "Iteration 15014, loss = 1.34938820\n",
      "Iteration 15015, loss = 1.34937399\n",
      "Iteration 15016, loss = 1.34935954\n",
      "Iteration 15017, loss = 1.34934484\n",
      "Iteration 15018, loss = 1.34933008\n",
      "Iteration 15019, loss = 1.34931557\n",
      "Iteration 15020, loss = 1.34930105\n",
      "Iteration 15021, loss = 1.34928649\n",
      "Iteration 15022, loss = 1.34927190\n",
      "Iteration 15023, loss = 1.34925729\n",
      "Iteration 15024, loss = 1.34924267\n",
      "Iteration 15025, loss = 1.34922804\n",
      "Iteration 15026, loss = 1.34921339\n",
      "Iteration 15027, loss = 1.34919871\n",
      "Iteration 15028, loss = 1.34918403\n",
      "Iteration 15029, loss = 1.34916933\n",
      "Iteration 15030, loss = 1.34915461\n",
      "Iteration 15031, loss = 1.34913990\n",
      "Iteration 15032, loss = 1.34912517\n",
      "Iteration 15033, loss = 1.34911044\n",
      "Iteration 15034, loss = 1.34909571\n",
      "Iteration 15035, loss = 1.34908101\n",
      "Iteration 15036, loss = 1.34906627\n",
      "Iteration 15037, loss = 1.34905149\n",
      "Iteration 15038, loss = 1.34903681\n",
      "Iteration 15039, loss = 1.34902209\n",
      "Iteration 15040, loss = 1.34900734\n",
      "Iteration 15041, loss = 1.34899266\n",
      "Iteration 15042, loss = 1.34897794\n",
      "Iteration 15043, loss = 1.34896323\n",
      "Iteration 15044, loss = 1.34894852\n",
      "Iteration 15045, loss = 1.34893380\n",
      "Iteration 15046, loss = 1.34891907\n",
      "Iteration 15047, loss = 1.34890656\n",
      "Iteration 15048, loss = 1.34889010\n",
      "Iteration 15049, loss = 1.34887590\n",
      "Iteration 15050, loss = 1.34886170\n",
      "Iteration 15051, loss = 1.34884711\n",
      "Iteration 15052, loss = 1.34883239\n",
      "Iteration 15053, loss = 1.34881764\n",
      "Iteration 15054, loss = 1.34880304\n",
      "Iteration 15055, loss = 1.34878852\n",
      "Iteration 15056, loss = 1.34877401\n",
      "Iteration 15057, loss = 1.34875946\n",
      "Iteration 15058, loss = 1.34874487\n",
      "Iteration 15059, loss = 1.34873025\n",
      "Iteration 15060, loss = 1.34871561\n",
      "Iteration 15061, loss = 1.34870094\n",
      "Iteration 15062, loss = 1.34868624\n",
      "Iteration 15063, loss = 1.34867155\n",
      "Iteration 15064, loss = 1.34865685\n",
      "Iteration 15065, loss = 1.34864212\n",
      "Iteration 15066, loss = 1.34862739\n",
      "Iteration 15067, loss = 1.34861270\n",
      "Iteration 15068, loss = 1.34859799\n",
      "Iteration 15069, loss = 1.34858325\n",
      "Iteration 15070, loss = 1.34856852\n",
      "Iteration 15071, loss = 1.34855379\n",
      "Iteration 15072, loss = 1.34853912\n",
      "Iteration 15073, loss = 1.34852443\n",
      "Iteration 15074, loss = 1.34850970\n",
      "Iteration 15075, loss = 1.34849499\n",
      "Iteration 15076, loss = 1.34848030\n",
      "Iteration 15077, loss = 1.34846557\n",
      "Iteration 15078, loss = 1.34845089\n",
      "Iteration 15079, loss = 1.34843618\n",
      "Iteration 15080, loss = 1.34842361\n",
      "Iteration 15081, loss = 1.34840721\n",
      "Iteration 15082, loss = 1.34839289\n",
      "Iteration 15083, loss = 1.34837864\n",
      "Iteration 15084, loss = 1.34836407\n",
      "Iteration 15085, loss = 1.34834941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15086, loss = 1.34833484\n",
      "Iteration 15087, loss = 1.34832035\n",
      "Iteration 15088, loss = 1.34830584\n",
      "Iteration 15089, loss = 1.34829129\n",
      "Iteration 15090, loss = 1.34827671\n",
      "Iteration 15091, loss = 1.34826210\n",
      "Iteration 15092, loss = 1.34824750\n",
      "Iteration 15093, loss = 1.34823289\n",
      "Iteration 15094, loss = 1.34821826\n",
      "Iteration 15095, loss = 1.34820359\n",
      "Iteration 15096, loss = 1.34818894\n",
      "Iteration 15097, loss = 1.34817426\n",
      "Iteration 15098, loss = 1.34815958\n",
      "Iteration 15099, loss = 1.34814488\n",
      "Iteration 15100, loss = 1.34813021\n",
      "Iteration 15101, loss = 1.34811549\n",
      "Iteration 15102, loss = 1.34810083\n",
      "Iteration 15103, loss = 1.34808611\n",
      "Iteration 15104, loss = 1.34807143\n",
      "Iteration 15105, loss = 1.34805671\n",
      "Iteration 15106, loss = 1.34804209\n",
      "Iteration 15107, loss = 1.34802738\n",
      "Iteration 15108, loss = 1.34801266\n",
      "Iteration 15109, loss = 1.34799799\n",
      "Iteration 15110, loss = 1.34798330\n",
      "Iteration 15111, loss = 1.34796861\n",
      "Iteration 15112, loss = 1.34795603\n",
      "Iteration 15113, loss = 1.34793985\n",
      "Iteration 15114, loss = 1.34792567\n",
      "Iteration 15115, loss = 1.34791134\n",
      "Iteration 15116, loss = 1.34789676\n",
      "Iteration 15117, loss = 1.34788205\n",
      "Iteration 15118, loss = 1.34786749\n",
      "Iteration 15119, loss = 1.34785309\n",
      "Iteration 15120, loss = 1.34783865\n",
      "Iteration 15121, loss = 1.34782416\n",
      "Iteration 15122, loss = 1.34780964\n",
      "Iteration 15123, loss = 1.34779509\n",
      "Iteration 15124, loss = 1.34778050\n",
      "Iteration 15125, loss = 1.34776589\n",
      "Iteration 15126, loss = 1.34775125\n",
      "Iteration 15127, loss = 1.34773661\n",
      "Iteration 15128, loss = 1.34772196\n",
      "Iteration 15129, loss = 1.34770729\n",
      "Iteration 15130, loss = 1.34769262\n",
      "Iteration 15131, loss = 1.34767800\n",
      "Iteration 15132, loss = 1.34766338\n",
      "Iteration 15133, loss = 1.34764868\n",
      "Iteration 15134, loss = 1.34763396\n",
      "Iteration 15135, loss = 1.34761925\n",
      "Iteration 15136, loss = 1.34760455\n",
      "Iteration 15137, loss = 1.34758993\n",
      "Iteration 15138, loss = 1.34757532\n",
      "Iteration 15139, loss = 1.34756058\n",
      "Iteration 15140, loss = 1.34754588\n",
      "Iteration 15141, loss = 1.34753122\n",
      "Iteration 15142, loss = 1.34751662\n",
      "Iteration 15143, loss = 1.34750216\n",
      "Iteration 15144, loss = 1.34748777\n",
      "Iteration 15145, loss = 1.34747348\n",
      "Iteration 15146, loss = 1.34745919\n",
      "Iteration 15147, loss = 1.34744468\n",
      "Iteration 15148, loss = 1.34743006\n",
      "Iteration 15149, loss = 1.34741560\n",
      "Iteration 15150, loss = 1.34740118\n",
      "Iteration 15151, loss = 1.34738672\n",
      "Iteration 15152, loss = 1.34737223\n",
      "Iteration 15153, loss = 1.34735770\n",
      "Iteration 15154, loss = 1.34734318\n",
      "Iteration 15155, loss = 1.34732866\n",
      "Iteration 15156, loss = 1.34731407\n",
      "Iteration 15157, loss = 1.34729945\n",
      "Iteration 15158, loss = 1.34728484\n",
      "Iteration 15159, loss = 1.34727024\n",
      "Iteration 15160, loss = 1.34725561\n",
      "Iteration 15161, loss = 1.34724099\n",
      "Iteration 15162, loss = 1.34722631\n",
      "Iteration 15163, loss = 1.34721167\n",
      "Iteration 15164, loss = 1.34719699\n",
      "Iteration 15165, loss = 1.34718234\n",
      "Iteration 15166, loss = 1.34716770\n",
      "Iteration 15167, loss = 1.34715305\n",
      "Iteration 15168, loss = 1.34713839\n",
      "Iteration 15169, loss = 1.34712379\n",
      "Iteration 15170, loss = 1.34710920\n",
      "Iteration 15171, loss = 1.34709458\n",
      "Iteration 15172, loss = 1.34707985\n",
      "Iteration 15173, loss = 1.34706525\n",
      "Iteration 15174, loss = 1.34705280\n",
      "Iteration 15175, loss = 1.34703650\n",
      "Iteration 15176, loss = 1.34702227\n",
      "Iteration 15177, loss = 1.34700808\n",
      "Iteration 15178, loss = 1.34699362\n",
      "Iteration 15179, loss = 1.34697907\n",
      "Iteration 15180, loss = 1.34696465\n",
      "Iteration 15181, loss = 1.34695029\n",
      "Iteration 15182, loss = 1.34693588\n",
      "Iteration 15183, loss = 1.34692143\n",
      "Iteration 15184, loss = 1.34690695\n",
      "Iteration 15185, loss = 1.34689243\n",
      "Iteration 15186, loss = 1.34687789\n",
      "Iteration 15187, loss = 1.34686331\n",
      "Iteration 15188, loss = 1.34684875\n",
      "Iteration 15189, loss = 1.34683417\n",
      "Iteration 15190, loss = 1.34681956\n",
      "Iteration 15191, loss = 1.34680494\n",
      "Iteration 15192, loss = 1.34679037\n",
      "Iteration 15193, loss = 1.34677576\n",
      "Iteration 15194, loss = 1.34676115\n",
      "Iteration 15195, loss = 1.34674653\n",
      "Iteration 15196, loss = 1.34673187\n",
      "Iteration 15197, loss = 1.34671725\n",
      "Iteration 15198, loss = 1.34670267\n",
      "Iteration 15199, loss = 1.34668809\n",
      "Iteration 15200, loss = 1.34667346\n",
      "Iteration 15201, loss = 1.34665885\n",
      "Iteration 15202, loss = 1.34664428\n",
      "Iteration 15203, loss = 1.34662969\n",
      "Iteration 15204, loss = 1.34661556\n",
      "Iteration 15205, loss = 1.34660087\n",
      "Iteration 15206, loss = 1.34658684\n",
      "Iteration 15207, loss = 1.34657278\n",
      "Iteration 15208, loss = 1.34655838\n",
      "Iteration 15209, loss = 1.34654376\n",
      "Iteration 15210, loss = 1.34652924\n",
      "Iteration 15211, loss = 1.34651488\n",
      "Iteration 15212, loss = 1.34650052\n",
      "Iteration 15213, loss = 1.34648613\n",
      "Iteration 15214, loss = 1.34647169\n",
      "Iteration 15215, loss = 1.34645722\n",
      "Iteration 15216, loss = 1.34644272\n",
      "Iteration 15217, loss = 1.34642819\n",
      "Iteration 15218, loss = 1.34641366\n",
      "Iteration 15219, loss = 1.34639908\n",
      "Iteration 15220, loss = 1.34638450\n",
      "Iteration 15221, loss = 1.34636994\n",
      "Iteration 15222, loss = 1.34635534\n",
      "Iteration 15223, loss = 1.34634077\n",
      "Iteration 15224, loss = 1.34632617\n",
      "Iteration 15225, loss = 1.34631153\n",
      "Iteration 15226, loss = 1.34629697\n",
      "Iteration 15227, loss = 1.34628231\n",
      "Iteration 15228, loss = 1.34626768\n",
      "Iteration 15229, loss = 1.34625305\n",
      "Iteration 15230, loss = 1.34623839\n",
      "Iteration 15231, loss = 1.34622377\n",
      "Iteration 15232, loss = 1.34620910\n",
      "Iteration 15233, loss = 1.34619444\n",
      "Iteration 15234, loss = 1.34618124\n",
      "Iteration 15235, loss = 1.34616571\n",
      "Iteration 15236, loss = 1.34615162\n",
      "Iteration 15237, loss = 1.34613742\n",
      "Iteration 15238, loss = 1.34612294\n",
      "Iteration 15239, loss = 1.34610823\n",
      "Iteration 15240, loss = 1.34609377\n",
      "Iteration 15241, loss = 1.34607937\n",
      "Iteration 15242, loss = 1.34606496\n",
      "Iteration 15243, loss = 1.34605050\n",
      "Iteration 15244, loss = 1.34603601\n",
      "Iteration 15245, loss = 1.34602149\n",
      "Iteration 15246, loss = 1.34600694\n",
      "Iteration 15247, loss = 1.34599237\n",
      "Iteration 15248, loss = 1.34597777\n",
      "Iteration 15249, loss = 1.34596321\n",
      "Iteration 15250, loss = 1.34594862\n",
      "Iteration 15251, loss = 1.34593398\n",
      "Iteration 15252, loss = 1.34591930\n",
      "Iteration 15253, loss = 1.34590469\n",
      "Iteration 15254, loss = 1.34589006\n",
      "Iteration 15255, loss = 1.34587539\n",
      "Iteration 15256, loss = 1.34586074\n",
      "Iteration 15257, loss = 1.34584613\n",
      "Iteration 15258, loss = 1.34583149\n",
      "Iteration 15259, loss = 1.34581683\n",
      "Iteration 15260, loss = 1.34580219\n",
      "Iteration 15261, loss = 1.34578761\n",
      "Iteration 15262, loss = 1.34577493\n",
      "Iteration 15263, loss = 1.34575883\n",
      "Iteration 15264, loss = 1.34574468\n",
      "Iteration 15265, loss = 1.34573053\n",
      "Iteration 15266, loss = 1.34571616\n",
      "Iteration 15267, loss = 1.34570158\n",
      "Iteration 15268, loss = 1.34568712\n",
      "Iteration 15269, loss = 1.34567277\n",
      "Iteration 15270, loss = 1.34565837\n",
      "Iteration 15271, loss = 1.34564394\n",
      "Iteration 15272, loss = 1.34562948\n",
      "Iteration 15273, loss = 1.34561498\n",
      "Iteration 15274, loss = 1.34560046\n",
      "Iteration 15275, loss = 1.34558592\n",
      "Iteration 15276, loss = 1.34557137\n",
      "Iteration 15277, loss = 1.34555682\n",
      "Iteration 15278, loss = 1.34554224\n",
      "Iteration 15279, loss = 1.34552765\n",
      "Iteration 15280, loss = 1.34551307\n",
      "Iteration 15281, loss = 1.34549847\n",
      "Iteration 15282, loss = 1.34548392\n",
      "Iteration 15283, loss = 1.34546926\n",
      "Iteration 15284, loss = 1.34545467\n",
      "Iteration 15285, loss = 1.34544010\n",
      "Iteration 15286, loss = 1.34542549\n",
      "Iteration 15287, loss = 1.34541088\n",
      "Iteration 15288, loss = 1.34539634\n",
      "Iteration 15289, loss = 1.34538177\n",
      "Iteration 15290, loss = 1.34536883\n",
      "Iteration 15291, loss = 1.34535314\n",
      "Iteration 15292, loss = 1.34533903\n",
      "Iteration 15293, loss = 1.34532488\n",
      "Iteration 15294, loss = 1.34531055\n",
      "Iteration 15295, loss = 1.34529610\n",
      "Iteration 15296, loss = 1.34528178\n",
      "Iteration 15297, loss = 1.34526747\n",
      "Iteration 15298, loss = 1.34525313\n",
      "Iteration 15299, loss = 1.34523874\n",
      "Iteration 15300, loss = 1.34522432\n",
      "Iteration 15301, loss = 1.34520987\n",
      "Iteration 15302, loss = 1.34519539\n",
      "Iteration 15303, loss = 1.34518088\n",
      "Iteration 15304, loss = 1.34516636\n",
      "Iteration 15305, loss = 1.34515186\n",
      "Iteration 15306, loss = 1.34513733\n",
      "Iteration 15307, loss = 1.34512279\n",
      "Iteration 15308, loss = 1.34510819\n",
      "Iteration 15309, loss = 1.34509363\n",
      "Iteration 15310, loss = 1.34507915\n",
      "Iteration 15311, loss = 1.34506453\n",
      "Iteration 15312, loss = 1.34504999\n",
      "Iteration 15313, loss = 1.34503546\n",
      "Iteration 15314, loss = 1.34502085\n",
      "Iteration 15315, loss = 1.34500632\n",
      "Iteration 15316, loss = 1.34499183\n",
      "Iteration 15317, loss = 1.34497728\n",
      "Iteration 15318, loss = 1.34496391\n",
      "Iteration 15319, loss = 1.34494877\n",
      "Iteration 15320, loss = 1.34493473\n",
      "Iteration 15321, loss = 1.34492063\n",
      "Iteration 15322, loss = 1.34490633\n",
      "Iteration 15323, loss = 1.34489198\n",
      "Iteration 15324, loss = 1.34487760\n",
      "Iteration 15325, loss = 1.34486333\n",
      "Iteration 15326, loss = 1.34484902\n",
      "Iteration 15327, loss = 1.34483468\n",
      "Iteration 15328, loss = 1.34482030\n",
      "Iteration 15329, loss = 1.34480588\n",
      "Iteration 15330, loss = 1.34479149\n",
      "Iteration 15331, loss = 1.34477705\n",
      "Iteration 15332, loss = 1.34476256\n",
      "Iteration 15333, loss = 1.34474806\n",
      "Iteration 15334, loss = 1.34473358\n",
      "Iteration 15335, loss = 1.34471907\n",
      "Iteration 15336, loss = 1.34470458\n",
      "Iteration 15337, loss = 1.34469004\n",
      "Iteration 15338, loss = 1.34467553\n",
      "Iteration 15339, loss = 1.34466100\n",
      "Iteration 15340, loss = 1.34464645\n",
      "Iteration 15341, loss = 1.34463203\n",
      "Iteration 15342, loss = 1.34461751\n",
      "Iteration 15343, loss = 1.34460298\n",
      "Iteration 15344, loss = 1.34458846\n",
      "Iteration 15345, loss = 1.34457633\n",
      "Iteration 15346, loss = 1.34455980\n",
      "Iteration 15347, loss = 1.34454577\n",
      "Iteration 15348, loss = 1.34453188\n",
      "Iteration 15349, loss = 1.34451775\n",
      "Iteration 15350, loss = 1.34450334\n",
      "Iteration 15351, loss = 1.34448906\n",
      "Iteration 15352, loss = 1.34447476\n",
      "Iteration 15353, loss = 1.34446049\n",
      "Iteration 15354, loss = 1.34444620\n",
      "Iteration 15355, loss = 1.34443188\n",
      "Iteration 15356, loss = 1.34441753\n",
      "Iteration 15357, loss = 1.34440315\n",
      "Iteration 15358, loss = 1.34438877\n",
      "Iteration 15359, loss = 1.34437434\n",
      "Iteration 15360, loss = 1.34435990\n",
      "Iteration 15361, loss = 1.34434550\n",
      "Iteration 15362, loss = 1.34433104\n",
      "Iteration 15363, loss = 1.34431657\n",
      "Iteration 15364, loss = 1.34430208\n",
      "Iteration 15365, loss = 1.34428760\n",
      "Iteration 15366, loss = 1.34427315\n",
      "Iteration 15367, loss = 1.34425867\n",
      "Iteration 15368, loss = 1.34424419\n",
      "Iteration 15369, loss = 1.34422976\n",
      "Iteration 15370, loss = 1.34421529\n",
      "Iteration 15371, loss = 1.34420079\n",
      "Iteration 15372, loss = 1.34418867\n",
      "Iteration 15373, loss = 1.34417249\n",
      "Iteration 15374, loss = 1.34415851\n",
      "Iteration 15375, loss = 1.34414439\n",
      "Iteration 15376, loss = 1.34413012\n",
      "Iteration 15377, loss = 1.34411594\n",
      "Iteration 15378, loss = 1.34410177\n",
      "Iteration 15379, loss = 1.34408754\n",
      "Iteration 15380, loss = 1.34407330\n",
      "Iteration 15381, loss = 1.34405905\n",
      "Iteration 15382, loss = 1.34404476\n",
      "Iteration 15383, loss = 1.34403044\n",
      "Iteration 15384, loss = 1.34401609\n",
      "Iteration 15385, loss = 1.34400172\n",
      "Iteration 15386, loss = 1.34398735\n",
      "Iteration 15387, loss = 1.34397295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15388, loss = 1.34395851\n",
      "Iteration 15389, loss = 1.34394412\n",
      "Iteration 15390, loss = 1.34392972\n",
      "Iteration 15391, loss = 1.34391525\n",
      "Iteration 15392, loss = 1.34390086\n",
      "Iteration 15393, loss = 1.34388647\n",
      "Iteration 15394, loss = 1.34387202\n",
      "Iteration 15395, loss = 1.34385757\n",
      "Iteration 15396, loss = 1.34384326\n",
      "Iteration 15397, loss = 1.34382882\n",
      "Iteration 15398, loss = 1.34381599\n",
      "Iteration 15399, loss = 1.34380040\n",
      "Iteration 15400, loss = 1.34378671\n",
      "Iteration 15401, loss = 1.34377282\n",
      "Iteration 15402, loss = 1.34375872\n",
      "Iteration 15403, loss = 1.34374448\n",
      "Iteration 15404, loss = 1.34373019\n",
      "Iteration 15405, loss = 1.34371605\n",
      "Iteration 15406, loss = 1.34370189\n",
      "Iteration 15407, loss = 1.34368769\n",
      "Iteration 15408, loss = 1.34367346\n",
      "Iteration 15409, loss = 1.34365921\n",
      "Iteration 15410, loss = 1.34364491\n",
      "Iteration 15411, loss = 1.34363060\n",
      "Iteration 15412, loss = 1.34361627\n",
      "Iteration 15413, loss = 1.34360195\n",
      "Iteration 15414, loss = 1.34358756\n",
      "Iteration 15415, loss = 1.34357318\n",
      "Iteration 15416, loss = 1.34355882\n",
      "Iteration 15417, loss = 1.34354443\n",
      "Iteration 15418, loss = 1.34353006\n",
      "Iteration 15419, loss = 1.34351567\n",
      "Iteration 15420, loss = 1.34350127\n",
      "Iteration 15421, loss = 1.34348690\n",
      "Iteration 15422, loss = 1.34347254\n",
      "Iteration 15423, loss = 1.34345822\n",
      "Iteration 15424, loss = 1.34344379\n",
      "Iteration 15425, loss = 1.34343166\n",
      "Iteration 15426, loss = 1.34341568\n",
      "Iteration 15427, loss = 1.34340187\n",
      "Iteration 15428, loss = 1.34338795\n",
      "Iteration 15429, loss = 1.34337391\n",
      "Iteration 15430, loss = 1.34335971\n",
      "Iteration 15431, loss = 1.34334549\n",
      "Iteration 15432, loss = 1.34333135\n",
      "Iteration 15433, loss = 1.34331725\n",
      "Iteration 15434, loss = 1.34330311\n",
      "Iteration 15435, loss = 1.34328892\n",
      "Iteration 15436, loss = 1.34327471\n",
      "Iteration 15437, loss = 1.34326046\n",
      "Iteration 15438, loss = 1.34324619\n",
      "Iteration 15439, loss = 1.34323189\n",
      "Iteration 15440, loss = 1.34321758\n",
      "Iteration 15441, loss = 1.34320326\n",
      "Iteration 15442, loss = 1.34318893\n",
      "Iteration 15443, loss = 1.34317457\n",
      "Iteration 15444, loss = 1.34316024\n",
      "Iteration 15445, loss = 1.34314587\n",
      "Iteration 15446, loss = 1.34313154\n",
      "Iteration 15447, loss = 1.34311725\n",
      "Iteration 15448, loss = 1.34310291\n",
      "Iteration 15449, loss = 1.34308857\n",
      "Iteration 15450, loss = 1.34307453\n",
      "Iteration 15451, loss = 1.34306056\n",
      "Iteration 15452, loss = 1.34304691\n",
      "Iteration 15453, loss = 1.34303295\n",
      "Iteration 15454, loss = 1.34301889\n",
      "Iteration 15455, loss = 1.34300471\n",
      "Iteration 15456, loss = 1.34299063\n",
      "Iteration 15457, loss = 1.34297660\n",
      "Iteration 15458, loss = 1.34296253\n",
      "Iteration 15459, loss = 1.34294843\n",
      "Iteration 15460, loss = 1.34293428\n",
      "Iteration 15461, loss = 1.34292011\n",
      "Iteration 15462, loss = 1.34290591\n",
      "Iteration 15463, loss = 1.34289170\n",
      "Iteration 15464, loss = 1.34287744\n",
      "Iteration 15465, loss = 1.34286318\n",
      "Iteration 15466, loss = 1.34284892\n",
      "Iteration 15467, loss = 1.34283464\n",
      "Iteration 15468, loss = 1.34282037\n",
      "Iteration 15469, loss = 1.34280609\n",
      "Iteration 15470, loss = 1.34279178\n",
      "Iteration 15471, loss = 1.34277749\n",
      "Iteration 15472, loss = 1.34276323\n",
      "Iteration 15473, loss = 1.34274902\n",
      "Iteration 15474, loss = 1.34273473\n",
      "Iteration 15475, loss = 1.34272044\n",
      "Iteration 15476, loss = 1.34270686\n",
      "Iteration 15477, loss = 1.34269233\n",
      "Iteration 15478, loss = 1.34267871\n",
      "Iteration 15479, loss = 1.34266497\n",
      "Iteration 15480, loss = 1.34265099\n",
      "Iteration 15481, loss = 1.34263690\n",
      "Iteration 15482, loss = 1.34262291\n",
      "Iteration 15483, loss = 1.34260894\n",
      "Iteration 15484, loss = 1.34259493\n",
      "Iteration 15485, loss = 1.34258088\n",
      "Iteration 15486, loss = 1.34256679\n",
      "Iteration 15487, loss = 1.34255267\n",
      "Iteration 15488, loss = 1.34253851\n",
      "Iteration 15489, loss = 1.34252435\n",
      "Iteration 15490, loss = 1.34251014\n",
      "Iteration 15491, loss = 1.34249593\n",
      "Iteration 15492, loss = 1.34248168\n",
      "Iteration 15493, loss = 1.34246747\n",
      "Iteration 15494, loss = 1.34245320\n",
      "Iteration 15495, loss = 1.34243896\n",
      "Iteration 15496, loss = 1.34242472\n",
      "Iteration 15497, loss = 1.34241051\n",
      "Iteration 15498, loss = 1.34239622\n",
      "Iteration 15499, loss = 1.34238199\n",
      "Iteration 15500, loss = 1.34236774\n",
      "Iteration 15501, loss = 1.34235355\n",
      "Iteration 15502, loss = 1.34234175\n",
      "Iteration 15503, loss = 1.34232559\n",
      "Iteration 15504, loss = 1.34231181\n",
      "Iteration 15505, loss = 1.34229802\n",
      "Iteration 15506, loss = 1.34228416\n",
      "Iteration 15507, loss = 1.34227019\n",
      "Iteration 15508, loss = 1.34225624\n",
      "Iteration 15509, loss = 1.34224226\n",
      "Iteration 15510, loss = 1.34222827\n",
      "Iteration 15511, loss = 1.34221425\n",
      "Iteration 15512, loss = 1.34220018\n",
      "Iteration 15513, loss = 1.34218609\n",
      "Iteration 15514, loss = 1.34217199\n",
      "Iteration 15515, loss = 1.34215784\n",
      "Iteration 15516, loss = 1.34214368\n",
      "Iteration 15517, loss = 1.34212953\n",
      "Iteration 15518, loss = 1.34211536\n",
      "Iteration 15519, loss = 1.34210120\n",
      "Iteration 15520, loss = 1.34208701\n",
      "Iteration 15521, loss = 1.34207290\n",
      "Iteration 15522, loss = 1.34205869\n",
      "Iteration 15523, loss = 1.34204454\n",
      "Iteration 15524, loss = 1.34203037\n",
      "Iteration 15525, loss = 1.34201622\n",
      "Iteration 15526, loss = 1.34200215\n",
      "Iteration 15527, loss = 1.34198968\n",
      "Iteration 15528, loss = 1.34197429\n",
      "Iteration 15529, loss = 1.34196065\n",
      "Iteration 15530, loss = 1.34194704\n",
      "Iteration 15531, loss = 1.34193326\n",
      "Iteration 15532, loss = 1.34191937\n",
      "Iteration 15533, loss = 1.34190540\n",
      "Iteration 15534, loss = 1.34189153\n",
      "Iteration 15535, loss = 1.34187761\n",
      "Iteration 15536, loss = 1.34186365\n",
      "Iteration 15537, loss = 1.34184966\n",
      "Iteration 15538, loss = 1.34183563\n",
      "Iteration 15539, loss = 1.34182157\n",
      "Iteration 15540, loss = 1.34180749\n",
      "Iteration 15541, loss = 1.34179342\n",
      "Iteration 15542, loss = 1.34177935\n",
      "Iteration 15543, loss = 1.34176522\n",
      "Iteration 15544, loss = 1.34175111\n",
      "Iteration 15545, loss = 1.34173696\n",
      "Iteration 15546, loss = 1.34172283\n",
      "Iteration 15547, loss = 1.34170870\n",
      "Iteration 15548, loss = 1.34169454\n",
      "Iteration 15549, loss = 1.34168044\n",
      "Iteration 15550, loss = 1.34166636\n",
      "Iteration 15551, loss = 1.34165224\n",
      "Iteration 15552, loss = 1.34163808\n",
      "Iteration 15553, loss = 1.34162496\n",
      "Iteration 15554, loss = 1.34161042\n",
      "Iteration 15555, loss = 1.34159692\n",
      "Iteration 15556, loss = 1.34158334\n",
      "Iteration 15557, loss = 1.34156947\n",
      "Iteration 15558, loss = 1.34155548\n",
      "Iteration 15559, loss = 1.34154166\n",
      "Iteration 15560, loss = 1.34152786\n",
      "Iteration 15561, loss = 1.34151401\n",
      "Iteration 15562, loss = 1.34150011\n",
      "Iteration 15563, loss = 1.34148618\n",
      "Iteration 15564, loss = 1.34147221\n",
      "Iteration 15565, loss = 1.34145821\n",
      "Iteration 15566, loss = 1.34144419\n",
      "Iteration 15567, loss = 1.34143013\n",
      "Iteration 15568, loss = 1.34141607\n",
      "Iteration 15569, loss = 1.34140202\n",
      "Iteration 15570, loss = 1.34138791\n",
      "Iteration 15571, loss = 1.34137381\n",
      "Iteration 15572, loss = 1.34135974\n",
      "Iteration 15573, loss = 1.34134561\n",
      "Iteration 15574, loss = 1.34133160\n",
      "Iteration 15575, loss = 1.34131749\n",
      "Iteration 15576, loss = 1.34130338\n",
      "Iteration 15577, loss = 1.34128942\n",
      "Iteration 15578, loss = 1.34127568\n",
      "Iteration 15579, loss = 1.34126201\n",
      "Iteration 15580, loss = 1.34124862\n",
      "Iteration 15581, loss = 1.34123504\n",
      "Iteration 15582, loss = 1.34122129\n",
      "Iteration 15583, loss = 1.34120737\n",
      "Iteration 15584, loss = 1.34119352\n",
      "Iteration 15585, loss = 1.34117980\n",
      "Iteration 15586, loss = 1.34116603\n",
      "Iteration 15587, loss = 1.34115221\n",
      "Iteration 15588, loss = 1.34113834\n",
      "Iteration 15589, loss = 1.34112444\n",
      "Iteration 15590, loss = 1.34111051\n",
      "Iteration 15591, loss = 1.34109654\n",
      "Iteration 15592, loss = 1.34108255\n",
      "Iteration 15593, loss = 1.34106855\n",
      "Iteration 15594, loss = 1.34105451\n",
      "Iteration 15595, loss = 1.34104054\n",
      "Iteration 15596, loss = 1.34102647\n",
      "Iteration 15597, loss = 1.34101240\n",
      "Iteration 15598, loss = 1.34099837\n",
      "Iteration 15599, loss = 1.34098440\n",
      "Iteration 15600, loss = 1.34097035\n",
      "Iteration 15601, loss = 1.34095626\n",
      "Iteration 15602, loss = 1.34094222\n",
      "Iteration 15603, loss = 1.34092821\n",
      "Iteration 15604, loss = 1.34091555\n",
      "Iteration 15605, loss = 1.34090072\n",
      "Iteration 15606, loss = 1.34088742\n",
      "Iteration 15607, loss = 1.34087392\n",
      "Iteration 15608, loss = 1.34086012\n",
      "Iteration 15609, loss = 1.34084620\n",
      "Iteration 15610, loss = 1.34083243\n",
      "Iteration 15611, loss = 1.34081874\n",
      "Iteration 15612, loss = 1.34080499\n",
      "Iteration 15613, loss = 1.34079120\n",
      "Iteration 15614, loss = 1.34077736\n",
      "Iteration 15615, loss = 1.34076352\n",
      "Iteration 15616, loss = 1.34074961\n",
      "Iteration 15617, loss = 1.34073569\n",
      "Iteration 15618, loss = 1.34072176\n",
      "Iteration 15619, loss = 1.34070782\n",
      "Iteration 15620, loss = 1.34069383\n",
      "Iteration 15621, loss = 1.34067990\n",
      "Iteration 15622, loss = 1.34066587\n",
      "Iteration 15623, loss = 1.34065193\n",
      "Iteration 15624, loss = 1.34063792\n",
      "Iteration 15625, loss = 1.34062397\n",
      "Iteration 15626, loss = 1.34061008\n",
      "Iteration 15627, loss = 1.34059610\n",
      "Iteration 15628, loss = 1.34058213\n",
      "Iteration 15629, loss = 1.34057065\n",
      "Iteration 15630, loss = 1.34055479\n",
      "Iteration 15631, loss = 1.34054141\n",
      "Iteration 15632, loss = 1.34052799\n",
      "Iteration 15633, loss = 1.34051435\n",
      "Iteration 15634, loss = 1.34050076\n",
      "Iteration 15635, loss = 1.34048706\n",
      "Iteration 15636, loss = 1.34047338\n",
      "Iteration 15637, loss = 1.34045969\n",
      "Iteration 15638, loss = 1.34044596\n",
      "Iteration 15639, loss = 1.34043218\n",
      "Iteration 15640, loss = 1.34041837\n",
      "Iteration 15641, loss = 1.34040452\n",
      "Iteration 15642, loss = 1.34039064\n",
      "Iteration 15643, loss = 1.34037683\n",
      "Iteration 15644, loss = 1.34036292\n",
      "Iteration 15645, loss = 1.34034904\n",
      "Iteration 15646, loss = 1.34033514\n",
      "Iteration 15647, loss = 1.34032125\n",
      "Iteration 15648, loss = 1.34030733\n",
      "Iteration 15649, loss = 1.34029343\n",
      "Iteration 15650, loss = 1.34027960\n",
      "Iteration 15651, loss = 1.34026565\n",
      "Iteration 15652, loss = 1.34025172\n",
      "Iteration 15653, loss = 1.34023781\n",
      "Iteration 15654, loss = 1.34022450\n",
      "Iteration 15655, loss = 1.34021054\n",
      "Iteration 15656, loss = 1.34019729\n",
      "Iteration 15657, loss = 1.34018405\n",
      "Iteration 15658, loss = 1.34017048\n",
      "Iteration 15659, loss = 1.34015679\n",
      "Iteration 15660, loss = 1.34014311\n",
      "Iteration 15661, loss = 1.34012938\n",
      "Iteration 15662, loss = 1.34011573\n",
      "Iteration 15663, loss = 1.34010208\n",
      "Iteration 15664, loss = 1.34008839\n",
      "Iteration 15665, loss = 1.34007466\n",
      "Iteration 15666, loss = 1.34006089\n",
      "Iteration 15667, loss = 1.34004712\n",
      "Iteration 15668, loss = 1.34003328\n",
      "Iteration 15669, loss = 1.34001946\n",
      "Iteration 15670, loss = 1.34000559\n",
      "Iteration 15671, loss = 1.33999173\n",
      "Iteration 15672, loss = 1.33997788\n",
      "Iteration 15673, loss = 1.33996397\n",
      "Iteration 15674, loss = 1.33995013\n",
      "Iteration 15675, loss = 1.33993626\n",
      "Iteration 15676, loss = 1.33992239\n",
      "Iteration 15677, loss = 1.33990857\n",
      "Iteration 15678, loss = 1.33989470\n",
      "Iteration 15679, loss = 1.33988136\n",
      "Iteration 15680, loss = 1.33986771\n",
      "Iteration 15681, loss = 1.33985449\n",
      "Iteration 15682, loss = 1.33984131\n",
      "Iteration 15683, loss = 1.33982783\n",
      "Iteration 15684, loss = 1.33981413\n",
      "Iteration 15685, loss = 1.33980041\n",
      "Iteration 15686, loss = 1.33978685\n",
      "Iteration 15687, loss = 1.33977327\n",
      "Iteration 15688, loss = 1.33975969\n",
      "Iteration 15689, loss = 1.33974606\n",
      "Iteration 15690, loss = 1.33973239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15691, loss = 1.33971868\n",
      "Iteration 15692, loss = 1.33970494\n",
      "Iteration 15693, loss = 1.33969117\n",
      "Iteration 15694, loss = 1.33967738\n",
      "Iteration 15695, loss = 1.33966360\n",
      "Iteration 15696, loss = 1.33964979\n",
      "Iteration 15697, loss = 1.33963596\n",
      "Iteration 15698, loss = 1.33962220\n",
      "Iteration 15699, loss = 1.33960839\n",
      "Iteration 15700, loss = 1.33959460\n",
      "Iteration 15701, loss = 1.33958079\n",
      "Iteration 15702, loss = 1.33956701\n",
      "Iteration 15703, loss = 1.33955325\n",
      "Iteration 15704, loss = 1.33953947\n",
      "Iteration 15705, loss = 1.33952631\n",
      "Iteration 15706, loss = 1.33951258\n",
      "Iteration 15707, loss = 1.33949950\n",
      "Iteration 15708, loss = 1.33948630\n",
      "Iteration 15709, loss = 1.33947286\n",
      "Iteration 15710, loss = 1.33945921\n",
      "Iteration 15711, loss = 1.33944560\n",
      "Iteration 15712, loss = 1.33943210\n",
      "Iteration 15713, loss = 1.33941860\n",
      "Iteration 15714, loss = 1.33940507\n",
      "Iteration 15715, loss = 1.33939149\n",
      "Iteration 15716, loss = 1.33937788\n",
      "Iteration 15717, loss = 1.33936423\n",
      "Iteration 15718, loss = 1.33935055\n",
      "Iteration 15719, loss = 1.33933686\n",
      "Iteration 15720, loss = 1.33932313\n",
      "Iteration 15721, loss = 1.33930939\n",
      "Iteration 15722, loss = 1.33929563\n",
      "Iteration 15723, loss = 1.33928186\n",
      "Iteration 15724, loss = 1.33926812\n",
      "Iteration 15725, loss = 1.33925434\n",
      "Iteration 15726, loss = 1.33924057\n",
      "Iteration 15727, loss = 1.33922683\n",
      "Iteration 15728, loss = 1.33921305\n",
      "Iteration 15729, loss = 1.33919930\n",
      "Iteration 15730, loss = 1.33918754\n",
      "Iteration 15731, loss = 1.33917249\n",
      "Iteration 15732, loss = 1.33915949\n",
      "Iteration 15733, loss = 1.33914628\n",
      "Iteration 15734, loss = 1.33913284\n",
      "Iteration 15735, loss = 1.33911930\n",
      "Iteration 15736, loss = 1.33910585\n",
      "Iteration 15737, loss = 1.33909243\n",
      "Iteration 15738, loss = 1.33907901\n",
      "Iteration 15739, loss = 1.33906553\n",
      "Iteration 15740, loss = 1.33905202\n",
      "Iteration 15741, loss = 1.33903846\n",
      "Iteration 15742, loss = 1.33902487\n",
      "Iteration 15743, loss = 1.33901125\n",
      "Iteration 15744, loss = 1.33899760\n",
      "Iteration 15745, loss = 1.33898395\n",
      "Iteration 15746, loss = 1.33897036\n",
      "Iteration 15747, loss = 1.33895664\n",
      "Iteration 15748, loss = 1.33894292\n",
      "Iteration 15749, loss = 1.33892925\n",
      "Iteration 15750, loss = 1.33891560\n",
      "Iteration 15751, loss = 1.33890196\n",
      "Iteration 15752, loss = 1.33888829\n",
      "Iteration 15753, loss = 1.33887460\n",
      "Iteration 15754, loss = 1.33886089\n",
      "Iteration 15755, loss = 1.33884735\n",
      "Iteration 15756, loss = 1.33883600\n",
      "Iteration 15757, loss = 1.33882053\n",
      "Iteration 15758, loss = 1.33880756\n",
      "Iteration 15759, loss = 1.33879439\n",
      "Iteration 15760, loss = 1.33878104\n",
      "Iteration 15761, loss = 1.33876765\n",
      "Iteration 15762, loss = 1.33875433\n",
      "Iteration 15763, loss = 1.33874099\n",
      "Iteration 15764, loss = 1.33872760\n",
      "Iteration 15765, loss = 1.33871419\n",
      "Iteration 15766, loss = 1.33870073\n",
      "Iteration 15767, loss = 1.33868724\n",
      "Iteration 15768, loss = 1.33867370\n",
      "Iteration 15769, loss = 1.33866014\n",
      "Iteration 15770, loss = 1.33864655\n",
      "Iteration 15771, loss = 1.33863297\n",
      "Iteration 15772, loss = 1.33861934\n",
      "Iteration 15773, loss = 1.33860569\n",
      "Iteration 15774, loss = 1.33859205\n",
      "Iteration 15775, loss = 1.33857844\n",
      "Iteration 15776, loss = 1.33856483\n",
      "Iteration 15777, loss = 1.33855119\n",
      "Iteration 15778, loss = 1.33853757\n",
      "Iteration 15779, loss = 1.33852395\n",
      "Iteration 15780, loss = 1.33851033\n",
      "Iteration 15781, loss = 1.33849673\n",
      "Iteration 15782, loss = 1.33848625\n",
      "Iteration 15783, loss = 1.33847025\n",
      "Iteration 15784, loss = 1.33845747\n",
      "Iteration 15785, loss = 1.33844442\n",
      "Iteration 15786, loss = 1.33843106\n",
      "Iteration 15787, loss = 1.33841759\n",
      "Iteration 15788, loss = 1.33840420\n",
      "Iteration 15789, loss = 1.33839096\n",
      "Iteration 15790, loss = 1.33837766\n",
      "Iteration 15791, loss = 1.33836432\n",
      "Iteration 15792, loss = 1.33835093\n",
      "Iteration 15793, loss = 1.33833750\n",
      "Iteration 15794, loss = 1.33832404\n",
      "Iteration 15795, loss = 1.33831055\n",
      "Iteration 15796, loss = 1.33829705\n",
      "Iteration 15797, loss = 1.33828350\n",
      "Iteration 15798, loss = 1.33826997\n",
      "Iteration 15799, loss = 1.33825639\n",
      "Iteration 15800, loss = 1.33824281\n",
      "Iteration 15801, loss = 1.33822929\n",
      "Iteration 15802, loss = 1.33821571\n",
      "Iteration 15803, loss = 1.33820208\n",
      "Iteration 15804, loss = 1.33818862\n",
      "Iteration 15805, loss = 1.33817509\n",
      "Iteration 15806, loss = 1.33816159\n",
      "Iteration 15807, loss = 1.33814800\n",
      "Iteration 15808, loss = 1.33813514\n",
      "Iteration 15809, loss = 1.33812240\n",
      "Iteration 15810, loss = 1.33810938\n",
      "Iteration 15811, loss = 1.33809612\n",
      "Iteration 15812, loss = 1.33808276\n",
      "Iteration 15813, loss = 1.33806936\n",
      "Iteration 15814, loss = 1.33805617\n",
      "Iteration 15815, loss = 1.33804292\n",
      "Iteration 15816, loss = 1.33802962\n",
      "Iteration 15817, loss = 1.33801629\n",
      "Iteration 15818, loss = 1.33800291\n",
      "Iteration 15819, loss = 1.33798950\n",
      "Iteration 15820, loss = 1.33797608\n",
      "Iteration 15821, loss = 1.33796260\n",
      "Iteration 15822, loss = 1.33794919\n",
      "Iteration 15823, loss = 1.33793572\n",
      "Iteration 15824, loss = 1.33792220\n",
      "Iteration 15825, loss = 1.33790873\n",
      "Iteration 15826, loss = 1.33789525\n",
      "Iteration 15827, loss = 1.33788186\n",
      "Iteration 15828, loss = 1.33786838\n",
      "Iteration 15829, loss = 1.33785489\n",
      "Iteration 15830, loss = 1.33784142\n",
      "Iteration 15831, loss = 1.33782791\n",
      "Iteration 15832, loss = 1.33781443\n",
      "Iteration 15833, loss = 1.33780102\n",
      "Iteration 15834, loss = 1.33778990\n",
      "Iteration 15835, loss = 1.33777483\n",
      "Iteration 15836, loss = 1.33776197\n",
      "Iteration 15837, loss = 1.33774889\n",
      "Iteration 15838, loss = 1.33773582\n",
      "Iteration 15839, loss = 1.33772260\n",
      "Iteration 15840, loss = 1.33770936\n",
      "Iteration 15841, loss = 1.33769617\n",
      "Iteration 15842, loss = 1.33768298\n",
      "Iteration 15843, loss = 1.33766975\n",
      "Iteration 15844, loss = 1.33765648\n",
      "Iteration 15845, loss = 1.33764316\n",
      "Iteration 15846, loss = 1.33762982\n",
      "Iteration 15847, loss = 1.33761646\n",
      "Iteration 15848, loss = 1.33760308\n",
      "Iteration 15849, loss = 1.33758967\n",
      "Iteration 15850, loss = 1.33757628\n",
      "Iteration 15851, loss = 1.33756284\n",
      "Iteration 15852, loss = 1.33754941\n",
      "Iteration 15853, loss = 1.33753597\n",
      "Iteration 15854, loss = 1.33752252\n",
      "Iteration 15855, loss = 1.33750917\n",
      "Iteration 15856, loss = 1.33749574\n",
      "Iteration 15857, loss = 1.33748229\n",
      "Iteration 15858, loss = 1.33746892\n",
      "Iteration 15859, loss = 1.33745548\n",
      "Iteration 15860, loss = 1.33744534\n",
      "Iteration 15861, loss = 1.33742952\n",
      "Iteration 15862, loss = 1.33741689\n",
      "Iteration 15863, loss = 1.33740393\n",
      "Iteration 15864, loss = 1.33739076\n",
      "Iteration 15865, loss = 1.33737752\n",
      "Iteration 15866, loss = 1.33736443\n",
      "Iteration 15867, loss = 1.33735136\n",
      "Iteration 15868, loss = 1.33733826\n",
      "Iteration 15869, loss = 1.33732510\n",
      "Iteration 15870, loss = 1.33731191\n",
      "Iteration 15871, loss = 1.33729867\n",
      "Iteration 15872, loss = 1.33728542\n",
      "Iteration 15873, loss = 1.33727211\n",
      "Iteration 15874, loss = 1.33725879\n",
      "Iteration 15875, loss = 1.33724546\n",
      "Iteration 15876, loss = 1.33723219\n",
      "Iteration 15877, loss = 1.33721881\n",
      "Iteration 15878, loss = 1.33720542\n",
      "Iteration 15879, loss = 1.33719204\n",
      "Iteration 15880, loss = 1.33717871\n",
      "Iteration 15881, loss = 1.33716541\n",
      "Iteration 15882, loss = 1.33715206\n",
      "Iteration 15883, loss = 1.33713866\n",
      "Iteration 15884, loss = 1.33712532\n",
      "Iteration 15885, loss = 1.33711198\n",
      "Iteration 15886, loss = 1.33710052\n",
      "Iteration 15887, loss = 1.33708600\n",
      "Iteration 15888, loss = 1.33707331\n",
      "Iteration 15889, loss = 1.33706043\n",
      "Iteration 15890, loss = 1.33704735\n",
      "Iteration 15891, loss = 1.33703428\n",
      "Iteration 15892, loss = 1.33702127\n",
      "Iteration 15893, loss = 1.33700820\n",
      "Iteration 15894, loss = 1.33699514\n",
      "Iteration 15895, loss = 1.33698204\n",
      "Iteration 15896, loss = 1.33696890\n",
      "Iteration 15897, loss = 1.33695572\n",
      "Iteration 15898, loss = 1.33694251\n",
      "Iteration 15899, loss = 1.33692931\n",
      "Iteration 15900, loss = 1.33691603\n",
      "Iteration 15901, loss = 1.33690280\n",
      "Iteration 15902, loss = 1.33688950\n",
      "Iteration 15903, loss = 1.33687623\n",
      "Iteration 15904, loss = 1.33686299\n",
      "Iteration 15905, loss = 1.33684968\n",
      "Iteration 15906, loss = 1.33683636\n",
      "Iteration 15907, loss = 1.33682312\n",
      "Iteration 15908, loss = 1.33680990\n",
      "Iteration 15909, loss = 1.33679661\n",
      "Iteration 15910, loss = 1.33678337\n",
      "Iteration 15911, loss = 1.33677014\n",
      "Iteration 15912, loss = 1.33675689\n",
      "Iteration 15913, loss = 1.33674418\n",
      "Iteration 15914, loss = 1.33673170\n",
      "Iteration 15915, loss = 1.33671901\n",
      "Iteration 15916, loss = 1.33670612\n",
      "Iteration 15917, loss = 1.33669312\n",
      "Iteration 15918, loss = 1.33668007\n",
      "Iteration 15919, loss = 1.33666708\n",
      "Iteration 15920, loss = 1.33665412\n",
      "Iteration 15921, loss = 1.33664110\n",
      "Iteration 15922, loss = 1.33662806\n",
      "Iteration 15923, loss = 1.33661495\n",
      "Iteration 15924, loss = 1.33660182\n",
      "Iteration 15925, loss = 1.33658867\n",
      "Iteration 15926, loss = 1.33657550\n",
      "Iteration 15927, loss = 1.33656229\n",
      "Iteration 15928, loss = 1.33654907\n",
      "Iteration 15929, loss = 1.33653586\n",
      "Iteration 15930, loss = 1.33652263\n",
      "Iteration 15931, loss = 1.33650938\n",
      "Iteration 15932, loss = 1.33649617\n",
      "Iteration 15933, loss = 1.33648293\n",
      "Iteration 15934, loss = 1.33646966\n",
      "Iteration 15935, loss = 1.33645645\n",
      "Iteration 15936, loss = 1.33644327\n",
      "Iteration 15937, loss = 1.33643002\n",
      "Iteration 15938, loss = 1.33641679\n",
      "Iteration 15939, loss = 1.33640525\n",
      "Iteration 15940, loss = 1.33639117\n",
      "Iteration 15941, loss = 1.33637876\n",
      "Iteration 15942, loss = 1.33636602\n",
      "Iteration 15943, loss = 1.33635310\n",
      "Iteration 15944, loss = 1.33634008\n",
      "Iteration 15945, loss = 1.33632716\n",
      "Iteration 15946, loss = 1.33631424\n",
      "Iteration 15947, loss = 1.33630134\n",
      "Iteration 15948, loss = 1.33628840\n",
      "Iteration 15949, loss = 1.33627541\n",
      "Iteration 15950, loss = 1.33626239\n",
      "Iteration 15951, loss = 1.33624933\n",
      "Iteration 15952, loss = 1.33623624\n",
      "Iteration 15953, loss = 1.33622312\n",
      "Iteration 15954, loss = 1.33621000\n",
      "Iteration 15955, loss = 1.33619683\n",
      "Iteration 15956, loss = 1.33618366\n",
      "Iteration 15957, loss = 1.33617055\n",
      "Iteration 15958, loss = 1.33615738\n",
      "Iteration 15959, loss = 1.33614423\n",
      "Iteration 15960, loss = 1.33613111\n",
      "Iteration 15961, loss = 1.33611795\n",
      "Iteration 15962, loss = 1.33610486\n",
      "Iteration 15963, loss = 1.33609178\n",
      "Iteration 15964, loss = 1.33607863\n",
      "Iteration 15965, loss = 1.33606550\n",
      "Iteration 15966, loss = 1.33605431\n",
      "Iteration 15967, loss = 1.33603983\n",
      "Iteration 15968, loss = 1.33602751\n",
      "Iteration 15969, loss = 1.33601493\n",
      "Iteration 15970, loss = 1.33600216\n",
      "Iteration 15971, loss = 1.33598934\n",
      "Iteration 15972, loss = 1.33597655\n",
      "Iteration 15973, loss = 1.33596365\n",
      "Iteration 15974, loss = 1.33595077\n",
      "Iteration 15975, loss = 1.33593791\n",
      "Iteration 15976, loss = 1.33592501\n",
      "Iteration 15977, loss = 1.33591206\n",
      "Iteration 15978, loss = 1.33589908\n",
      "Iteration 15979, loss = 1.33588607\n",
      "Iteration 15980, loss = 1.33587302\n",
      "Iteration 15981, loss = 1.33585996\n",
      "Iteration 15982, loss = 1.33584687\n",
      "Iteration 15983, loss = 1.33583377\n",
      "Iteration 15984, loss = 1.33582066\n",
      "Iteration 15985, loss = 1.33580757\n",
      "Iteration 15986, loss = 1.33579445\n",
      "Iteration 15987, loss = 1.33578140\n",
      "Iteration 15988, loss = 1.33576833\n",
      "Iteration 15989, loss = 1.33575521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15990, loss = 1.33574210\n",
      "Iteration 15991, loss = 1.33572904\n",
      "Iteration 15992, loss = 1.33571603\n",
      "Iteration 15993, loss = 1.33570630\n",
      "Iteration 15994, loss = 1.33569055\n",
      "Iteration 15995, loss = 1.33567830\n",
      "Iteration 15996, loss = 1.33566585\n",
      "Iteration 15997, loss = 1.33565313\n",
      "Iteration 15998, loss = 1.33564034\n",
      "Iteration 15999, loss = 1.33562748\n",
      "Iteration 16000, loss = 1.33561470\n",
      "Iteration 16001, loss = 1.33560188\n",
      "Iteration 16002, loss = 1.33558910\n",
      "Iteration 16003, loss = 1.33557627\n",
      "Iteration 16004, loss = 1.33556340\n",
      "Iteration 16005, loss = 1.33555049\n",
      "Iteration 16006, loss = 1.33553755\n",
      "Iteration 16007, loss = 1.33552458\n",
      "Iteration 16008, loss = 1.33551160\n",
      "Iteration 16009, loss = 1.33549867\n",
      "Iteration 16010, loss = 1.33548564\n",
      "Iteration 16011, loss = 1.33547261\n",
      "Iteration 16012, loss = 1.33545961\n",
      "Iteration 16013, loss = 1.33544659\n",
      "Iteration 16014, loss = 1.33543359\n",
      "Iteration 16015, loss = 1.33542058\n",
      "Iteration 16016, loss = 1.33540755\n",
      "Iteration 16017, loss = 1.33539463\n",
      "Iteration 16018, loss = 1.33538162\n",
      "Iteration 16019, loss = 1.33536856\n",
      "Iteration 16020, loss = 1.33535637\n",
      "Iteration 16021, loss = 1.33534339\n",
      "Iteration 16022, loss = 1.33533129\n",
      "Iteration 16023, loss = 1.33531893\n",
      "Iteration 16024, loss = 1.33530626\n",
      "Iteration 16025, loss = 1.33529342\n",
      "Iteration 16026, loss = 1.33528058\n",
      "Iteration 16027, loss = 1.33526778\n",
      "Iteration 16028, loss = 1.33525507\n",
      "Iteration 16029, loss = 1.33524237\n",
      "Iteration 16030, loss = 1.33522963\n",
      "Iteration 16031, loss = 1.33521684\n",
      "Iteration 16032, loss = 1.33520401\n",
      "Iteration 16033, loss = 1.33519115\n",
      "Iteration 16034, loss = 1.33517830\n",
      "Iteration 16035, loss = 1.33516538\n",
      "Iteration 16036, loss = 1.33515245\n",
      "Iteration 16037, loss = 1.33513955\n",
      "Iteration 16038, loss = 1.33512660\n",
      "Iteration 16039, loss = 1.33511368\n",
      "Iteration 16040, loss = 1.33510073\n",
      "Iteration 16041, loss = 1.33508779\n",
      "Iteration 16042, loss = 1.33507485\n",
      "Iteration 16043, loss = 1.33506188\n",
      "Iteration 16044, loss = 1.33504899\n",
      "Iteration 16045, loss = 1.33503610\n",
      "Iteration 16046, loss = 1.33502313\n",
      "Iteration 16047, loss = 1.33501024\n",
      "Iteration 16048, loss = 1.33500000\n",
      "Iteration 16049, loss = 1.33498512\n",
      "Iteration 16050, loss = 1.33497303\n",
      "Iteration 16051, loss = 1.33496073\n",
      "Iteration 16052, loss = 1.33494821\n",
      "Iteration 16053, loss = 1.33493554\n",
      "Iteration 16054, loss = 1.33492275\n",
      "Iteration 16055, loss = 1.33490999\n",
      "Iteration 16056, loss = 1.33489740\n",
      "Iteration 16057, loss = 1.33488477\n",
      "Iteration 16058, loss = 1.33487208\n",
      "Iteration 16059, loss = 1.33485936\n",
      "Iteration 16060, loss = 1.33484660\n",
      "Iteration 16061, loss = 1.33483381\n",
      "Iteration 16062, loss = 1.33482100\n",
      "Iteration 16063, loss = 1.33480815\n",
      "Iteration 16064, loss = 1.33479529\n",
      "Iteration 16065, loss = 1.33478241\n",
      "Iteration 16066, loss = 1.33476954\n",
      "Iteration 16067, loss = 1.33475668\n",
      "Iteration 16068, loss = 1.33474379\n",
      "Iteration 16069, loss = 1.33473096\n",
      "Iteration 16070, loss = 1.33471808\n",
      "Iteration 16071, loss = 1.33470518\n",
      "Iteration 16072, loss = 1.33469237\n",
      "Iteration 16073, loss = 1.33467957\n",
      "Iteration 16074, loss = 1.33466677\n",
      "Iteration 16075, loss = 1.33465519\n",
      "Iteration 16076, loss = 1.33464175\n",
      "Iteration 16077, loss = 1.33462970\n",
      "Iteration 16078, loss = 1.33461742\n",
      "Iteration 16079, loss = 1.33460487\n",
      "Iteration 16080, loss = 1.33459229\n",
      "Iteration 16081, loss = 1.33457971\n",
      "Iteration 16082, loss = 1.33456722\n",
      "Iteration 16083, loss = 1.33455471\n",
      "Iteration 16084, loss = 1.33454214\n",
      "Iteration 16085, loss = 1.33452953\n",
      "Iteration 16086, loss = 1.33451688\n",
      "Iteration 16087, loss = 1.33450418\n",
      "Iteration 16088, loss = 1.33449146\n",
      "Iteration 16089, loss = 1.33447871\n",
      "Iteration 16090, loss = 1.33446593\n",
      "Iteration 16091, loss = 1.33445313\n",
      "Iteration 16092, loss = 1.33444036\n",
      "Iteration 16093, loss = 1.33442753\n",
      "Iteration 16094, loss = 1.33441473\n",
      "Iteration 16095, loss = 1.33440201\n",
      "Iteration 16096, loss = 1.33438917\n",
      "Iteration 16097, loss = 1.33437633\n",
      "Iteration 16098, loss = 1.33436359\n",
      "Iteration 16099, loss = 1.33435077\n",
      "Iteration 16100, loss = 1.33433796\n",
      "Iteration 16101, loss = 1.33432529\n",
      "Iteration 16102, loss = 1.33431255\n",
      "Iteration 16103, loss = 1.33430196\n",
      "Iteration 16104, loss = 1.33428767\n",
      "Iteration 16105, loss = 1.33427567\n",
      "Iteration 16106, loss = 1.33426343\n",
      "Iteration 16107, loss = 1.33425102\n",
      "Iteration 16108, loss = 1.33423844\n",
      "Iteration 16109, loss = 1.33422593\n",
      "Iteration 16110, loss = 1.33421348\n",
      "Iteration 16111, loss = 1.33420105\n",
      "Iteration 16112, loss = 1.33418856\n",
      "Iteration 16113, loss = 1.33417603\n",
      "Iteration 16114, loss = 1.33416346\n",
      "Iteration 16115, loss = 1.33415085\n",
      "Iteration 16116, loss = 1.33413820\n",
      "Iteration 16117, loss = 1.33412553\n",
      "Iteration 16118, loss = 1.33411285\n",
      "Iteration 16119, loss = 1.33410018\n",
      "Iteration 16120, loss = 1.33408745\n",
      "Iteration 16121, loss = 1.33407472\n",
      "Iteration 16122, loss = 1.33406199\n",
      "Iteration 16123, loss = 1.33404929\n",
      "Iteration 16124, loss = 1.33403661\n",
      "Iteration 16125, loss = 1.33402386\n",
      "Iteration 16126, loss = 1.33401118\n",
      "Iteration 16127, loss = 1.33399846\n",
      "Iteration 16128, loss = 1.33398574\n",
      "Iteration 16129, loss = 1.33397312\n",
      "Iteration 16130, loss = 1.33396047\n",
      "Iteration 16131, loss = 1.33394859\n",
      "Iteration 16132, loss = 1.33393585\n",
      "Iteration 16133, loss = 1.33392387\n",
      "Iteration 16134, loss = 1.33391165\n",
      "Iteration 16135, loss = 1.33389921\n",
      "Iteration 16136, loss = 1.33388679\n",
      "Iteration 16137, loss = 1.33387443\n",
      "Iteration 16138, loss = 1.33386205\n",
      "Iteration 16139, loss = 1.33384968\n",
      "Iteration 16140, loss = 1.33383723\n",
      "Iteration 16141, loss = 1.33382475\n",
      "Iteration 16142, loss = 1.33381223\n",
      "Iteration 16143, loss = 1.33379969\n",
      "Iteration 16144, loss = 1.33378710\n",
      "Iteration 16145, loss = 1.33377449\n",
      "Iteration 16146, loss = 1.33376186\n",
      "Iteration 16147, loss = 1.33374926\n",
      "Iteration 16148, loss = 1.33373659\n",
      "Iteration 16149, loss = 1.33372395\n",
      "Iteration 16150, loss = 1.33371132\n",
      "Iteration 16151, loss = 1.33369872\n",
      "Iteration 16152, loss = 1.33368607\n",
      "Iteration 16153, loss = 1.33367349\n",
      "Iteration 16154, loss = 1.33366089\n",
      "Iteration 16155, loss = 1.33364823\n",
      "Iteration 16156, loss = 1.33363562\n",
      "Iteration 16157, loss = 1.33362302\n",
      "Iteration 16158, loss = 1.33361044\n",
      "Iteration 16159, loss = 1.33359876\n",
      "Iteration 16160, loss = 1.33358615\n",
      "Iteration 16161, loss = 1.33357443\n",
      "Iteration 16162, loss = 1.33356237\n",
      "Iteration 16163, loss = 1.33355001\n",
      "Iteration 16164, loss = 1.33353750\n",
      "Iteration 16165, loss = 1.33352501\n",
      "Iteration 16166, loss = 1.33351269\n",
      "Iteration 16167, loss = 1.33350041\n",
      "Iteration 16168, loss = 1.33348808\n",
      "Iteration 16169, loss = 1.33347570\n",
      "Iteration 16170, loss = 1.33346329\n",
      "Iteration 16171, loss = 1.33345083\n",
      "Iteration 16172, loss = 1.33343836\n",
      "Iteration 16173, loss = 1.33342584\n",
      "Iteration 16174, loss = 1.33341333\n",
      "Iteration 16175, loss = 1.33340079\n",
      "Iteration 16176, loss = 1.33338824\n",
      "Iteration 16177, loss = 1.33337570\n",
      "Iteration 16178, loss = 1.33336313\n",
      "Iteration 16179, loss = 1.33335059\n",
      "Iteration 16180, loss = 1.33333800\n",
      "Iteration 16181, loss = 1.33332545\n",
      "Iteration 16182, loss = 1.33331286\n",
      "Iteration 16183, loss = 1.33330035\n",
      "Iteration 16184, loss = 1.33328786\n",
      "Iteration 16185, loss = 1.33327527\n",
      "Iteration 16186, loss = 1.33326281\n",
      "Iteration 16187, loss = 1.33325031\n",
      "Iteration 16188, loss = 1.33324101\n",
      "Iteration 16189, loss = 1.33322585\n",
      "Iteration 16190, loss = 1.33321390\n",
      "Iteration 16191, loss = 1.33320193\n",
      "Iteration 16192, loss = 1.33318987\n",
      "Iteration 16193, loss = 1.33317771\n",
      "Iteration 16194, loss = 1.33316552\n",
      "Iteration 16195, loss = 1.33315330\n",
      "Iteration 16196, loss = 1.33314100\n",
      "Iteration 16197, loss = 1.33312870\n",
      "Iteration 16198, loss = 1.33311639\n",
      "Iteration 16199, loss = 1.33310403\n",
      "Iteration 16200, loss = 1.33309164\n",
      "Iteration 16201, loss = 1.33307924\n",
      "Iteration 16202, loss = 1.33306678\n",
      "Iteration 16203, loss = 1.33305431\n",
      "Iteration 16204, loss = 1.33304186\n",
      "Iteration 16205, loss = 1.33302937\n",
      "Iteration 16206, loss = 1.33301685\n",
      "Iteration 16207, loss = 1.33300441\n",
      "Iteration 16208, loss = 1.33299197\n",
      "Iteration 16209, loss = 1.33297956\n",
      "Iteration 16210, loss = 1.33296706\n",
      "Iteration 16211, loss = 1.33295456\n",
      "Iteration 16212, loss = 1.33294209\n",
      "Iteration 16213, loss = 1.33292960\n",
      "Iteration 16214, loss = 1.33291716\n",
      "Iteration 16215, loss = 1.33290474\n",
      "Iteration 16216, loss = 1.33289347\n",
      "Iteration 16217, loss = 1.33288057\n",
      "Iteration 16218, loss = 1.33286882\n",
      "Iteration 16219, loss = 1.33285686\n",
      "Iteration 16220, loss = 1.33284479\n",
      "Iteration 16221, loss = 1.33283259\n",
      "Iteration 16222, loss = 1.33282047\n",
      "Iteration 16223, loss = 1.33280831\n",
      "Iteration 16224, loss = 1.33279612\n",
      "Iteration 16225, loss = 1.33278394\n",
      "Iteration 16226, loss = 1.33277170\n",
      "Iteration 16227, loss = 1.33275943\n",
      "Iteration 16228, loss = 1.33274711\n",
      "Iteration 16229, loss = 1.33273477\n",
      "Iteration 16230, loss = 1.33272241\n",
      "Iteration 16231, loss = 1.33271000\n",
      "Iteration 16232, loss = 1.33269760\n",
      "Iteration 16233, loss = 1.33268520\n",
      "Iteration 16234, loss = 1.33267282\n",
      "Iteration 16235, loss = 1.33266039\n",
      "Iteration 16236, loss = 1.33264797\n",
      "Iteration 16237, loss = 1.33263557\n",
      "Iteration 16238, loss = 1.33262315\n",
      "Iteration 16239, loss = 1.33261071\n",
      "Iteration 16240, loss = 1.33259830\n",
      "Iteration 16241, loss = 1.33258593\n",
      "Iteration 16242, loss = 1.33257352\n",
      "Iteration 16243, loss = 1.33256109\n",
      "Iteration 16244, loss = 1.33254867\n",
      "Iteration 16245, loss = 1.33253830\n",
      "Iteration 16246, loss = 1.33252474\n",
      "Iteration 16247, loss = 1.33251318\n",
      "Iteration 16248, loss = 1.33250134\n",
      "Iteration 16249, loss = 1.33248928\n",
      "Iteration 16250, loss = 1.33247705\n",
      "Iteration 16251, loss = 1.33246475\n",
      "Iteration 16252, loss = 1.33245259\n",
      "Iteration 16253, loss = 1.33244046\n",
      "Iteration 16254, loss = 1.33242831\n",
      "Iteration 16255, loss = 1.33241611\n",
      "Iteration 16256, loss = 1.33240386\n",
      "Iteration 16257, loss = 1.33239158\n",
      "Iteration 16258, loss = 1.33237925\n",
      "Iteration 16259, loss = 1.33236691\n",
      "Iteration 16260, loss = 1.33235456\n",
      "Iteration 16261, loss = 1.33234219\n",
      "Iteration 16262, loss = 1.33232981\n",
      "Iteration 16263, loss = 1.33231738\n",
      "Iteration 16264, loss = 1.33230494\n",
      "Iteration 16265, loss = 1.33229252\n",
      "Iteration 16266, loss = 1.33228022\n",
      "Iteration 16267, loss = 1.33226784\n",
      "Iteration 16268, loss = 1.33225537\n",
      "Iteration 16269, loss = 1.33224295\n",
      "Iteration 16270, loss = 1.33223052\n",
      "Iteration 16271, loss = 1.33221816\n",
      "Iteration 16272, loss = 1.33220580\n",
      "Iteration 16273, loss = 1.33219431\n",
      "Iteration 16274, loss = 1.33218165\n",
      "Iteration 16275, loss = 1.33216998\n",
      "Iteration 16276, loss = 1.33215824\n",
      "Iteration 16277, loss = 1.33214642\n",
      "Iteration 16278, loss = 1.33213442\n",
      "Iteration 16279, loss = 1.33212221\n",
      "Iteration 16280, loss = 1.33211001\n",
      "Iteration 16281, loss = 1.33209788\n",
      "Iteration 16282, loss = 1.33208573\n",
      "Iteration 16283, loss = 1.33207359\n",
      "Iteration 16284, loss = 1.33206142\n",
      "Iteration 16285, loss = 1.33204920\n",
      "Iteration 16286, loss = 1.33203695\n",
      "Iteration 16287, loss = 1.33202468\n",
      "Iteration 16288, loss = 1.33201243\n",
      "Iteration 16289, loss = 1.33200015\n",
      "Iteration 16290, loss = 1.33198792\n",
      "Iteration 16291, loss = 1.33197561\n",
      "Iteration 16292, loss = 1.33196322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16293, loss = 1.33195093\n",
      "Iteration 16294, loss = 1.33193862\n",
      "Iteration 16295, loss = 1.33192638\n",
      "Iteration 16296, loss = 1.33191411\n",
      "Iteration 16297, loss = 1.33190175\n",
      "Iteration 16298, loss = 1.33188941\n",
      "Iteration 16299, loss = 1.33187711\n",
      "Iteration 16300, loss = 1.33186485\n",
      "Iteration 16301, loss = 1.33185267\n",
      "Iteration 16302, loss = 1.33184038\n",
      "Iteration 16303, loss = 1.33183206\n",
      "Iteration 16304, loss = 1.33181662\n",
      "Iteration 16305, loss = 1.33180499\n",
      "Iteration 16306, loss = 1.33179314\n",
      "Iteration 16307, loss = 1.33178127\n",
      "Iteration 16308, loss = 1.33176929\n",
      "Iteration 16309, loss = 1.33175730\n",
      "Iteration 16310, loss = 1.33174537\n",
      "Iteration 16311, loss = 1.33173340\n",
      "Iteration 16312, loss = 1.33172137\n",
      "Iteration 16313, loss = 1.33170929\n",
      "Iteration 16314, loss = 1.33169718\n",
      "Iteration 16315, loss = 1.33168503\n",
      "Iteration 16316, loss = 1.33167285\n",
      "Iteration 16317, loss = 1.33166069\n",
      "Iteration 16318, loss = 1.33164849\n",
      "Iteration 16319, loss = 1.33163627\n",
      "Iteration 16320, loss = 1.33162403\n",
      "Iteration 16321, loss = 1.33161180\n",
      "Iteration 16322, loss = 1.33159964\n",
      "Iteration 16323, loss = 1.33158743\n",
      "Iteration 16324, loss = 1.33157522\n",
      "Iteration 16325, loss = 1.33156300\n",
      "Iteration 16326, loss = 1.33155075\n",
      "Iteration 16327, loss = 1.33153856\n",
      "Iteration 16328, loss = 1.33152639\n",
      "Iteration 16329, loss = 1.33151418\n",
      "Iteration 16330, loss = 1.33150205\n",
      "Iteration 16331, loss = 1.33149120\n",
      "Iteration 16332, loss = 1.33147846\n",
      "Iteration 16333, loss = 1.33146696\n",
      "Iteration 16334, loss = 1.33145528\n",
      "Iteration 16335, loss = 1.33144348\n",
      "Iteration 16336, loss = 1.33143158\n",
      "Iteration 16337, loss = 1.33141968\n",
      "Iteration 16338, loss = 1.33140785\n",
      "Iteration 16339, loss = 1.33139597\n",
      "Iteration 16340, loss = 1.33138405\n",
      "Iteration 16341, loss = 1.33137206\n",
      "Iteration 16342, loss = 1.33136003\n",
      "Iteration 16343, loss = 1.33134797\n",
      "Iteration 16344, loss = 1.33133590\n",
      "Iteration 16345, loss = 1.33132379\n",
      "Iteration 16346, loss = 1.33131168\n",
      "Iteration 16347, loss = 1.33129956\n",
      "Iteration 16348, loss = 1.33128742\n",
      "Iteration 16349, loss = 1.33127526\n",
      "Iteration 16350, loss = 1.33126311\n",
      "Iteration 16351, loss = 1.33125097\n",
      "Iteration 16352, loss = 1.33123882\n",
      "Iteration 16353, loss = 1.33122670\n",
      "Iteration 16354, loss = 1.33121456\n",
      "Iteration 16355, loss = 1.33120246\n",
      "Iteration 16356, loss = 1.33119036\n",
      "Iteration 16357, loss = 1.33117817\n",
      "Iteration 16358, loss = 1.33116603\n",
      "Iteration 16359, loss = 1.33115399\n",
      "Iteration 16360, loss = 1.33114515\n",
      "Iteration 16361, loss = 1.33113060\n",
      "Iteration 16362, loss = 1.33111935\n",
      "Iteration 16363, loss = 1.33110782\n",
      "Iteration 16364, loss = 1.33109607\n",
      "Iteration 16365, loss = 1.33108416\n",
      "Iteration 16366, loss = 1.33107231\n",
      "Iteration 16367, loss = 1.33106052\n",
      "Iteration 16368, loss = 1.33104875\n",
      "Iteration 16369, loss = 1.33103693\n",
      "Iteration 16370, loss = 1.33102505\n",
      "Iteration 16371, loss = 1.33101313\n",
      "Iteration 16372, loss = 1.33100117\n",
      "Iteration 16373, loss = 1.33098918\n",
      "Iteration 16374, loss = 1.33097715\n",
      "Iteration 16375, loss = 1.33096516\n",
      "Iteration 16376, loss = 1.33095309\n",
      "Iteration 16377, loss = 1.33094099\n",
      "Iteration 16378, loss = 1.33092892\n",
      "Iteration 16379, loss = 1.33091688\n",
      "Iteration 16380, loss = 1.33090486\n",
      "Iteration 16381, loss = 1.33089275\n",
      "Iteration 16382, loss = 1.33088063\n",
      "Iteration 16383, loss = 1.33086856\n",
      "Iteration 16384, loss = 1.33085652\n",
      "Iteration 16385, loss = 1.33084447\n",
      "Iteration 16386, loss = 1.33083243\n",
      "Iteration 16387, loss = 1.33082051\n",
      "Iteration 16388, loss = 1.33080852\n",
      "Iteration 16389, loss = 1.33079993\n",
      "Iteration 16390, loss = 1.33078532\n",
      "Iteration 16391, loss = 1.33077410\n",
      "Iteration 16392, loss = 1.33076261\n",
      "Iteration 16393, loss = 1.33075087\n",
      "Iteration 16394, loss = 1.33073909\n",
      "Iteration 16395, loss = 1.33072743\n",
      "Iteration 16396, loss = 1.33071581\n",
      "Iteration 16397, loss = 1.33070413\n",
      "Iteration 16398, loss = 1.33069239\n",
      "Iteration 16399, loss = 1.33068060\n",
      "Iteration 16400, loss = 1.33066877\n",
      "Iteration 16401, loss = 1.33065689\n",
      "Iteration 16402, loss = 1.33064497\n",
      "Iteration 16403, loss = 1.33063303\n",
      "Iteration 16404, loss = 1.33062110\n",
      "Iteration 16405, loss = 1.33060911\n",
      "Iteration 16406, loss = 1.33059709\n",
      "Iteration 16407, loss = 1.33058512\n",
      "Iteration 16408, loss = 1.33057311\n",
      "Iteration 16409, loss = 1.33056108\n",
      "Iteration 16410, loss = 1.33054910\n",
      "Iteration 16411, loss = 1.33053718\n",
      "Iteration 16412, loss = 1.33052516\n",
      "Iteration 16413, loss = 1.33051316\n",
      "Iteration 16414, loss = 1.33050131\n",
      "Iteration 16415, loss = 1.33048943\n",
      "Iteration 16416, loss = 1.33047746\n",
      "Iteration 16417, loss = 1.33046550\n",
      "Iteration 16418, loss = 1.33045618\n",
      "Iteration 16419, loss = 1.33044243\n",
      "Iteration 16420, loss = 1.33043124\n",
      "Iteration 16421, loss = 1.33041999\n",
      "Iteration 16422, loss = 1.33040857\n",
      "Iteration 16423, loss = 1.33039690\n",
      "Iteration 16424, loss = 1.33038512\n",
      "Iteration 16425, loss = 1.33037353\n",
      "Iteration 16426, loss = 1.33036195\n",
      "Iteration 16427, loss = 1.33035032\n",
      "Iteration 16428, loss = 1.33033863\n",
      "Iteration 16429, loss = 1.33032690\n",
      "Iteration 16430, loss = 1.33031512\n",
      "Iteration 16431, loss = 1.33030331\n",
      "Iteration 16432, loss = 1.33029146\n",
      "Iteration 16433, loss = 1.33027959\n",
      "Iteration 16434, loss = 1.33026771\n",
      "Iteration 16435, loss = 1.33025579\n",
      "Iteration 16436, loss = 1.33024386\n",
      "Iteration 16437, loss = 1.33023194\n",
      "Iteration 16438, loss = 1.33022000\n",
      "Iteration 16439, loss = 1.33020805\n",
      "Iteration 16440, loss = 1.33019611\n",
      "Iteration 16441, loss = 1.33018427\n",
      "Iteration 16442, loss = 1.33017239\n",
      "Iteration 16443, loss = 1.33016044\n",
      "Iteration 16444, loss = 1.33014866\n",
      "Iteration 16445, loss = 1.33013677\n",
      "Iteration 16446, loss = 1.33012486\n",
      "Iteration 16447, loss = 1.33011543\n",
      "Iteration 16448, loss = 1.33010206\n",
      "Iteration 16449, loss = 1.33009102\n",
      "Iteration 16450, loss = 1.33007976\n",
      "Iteration 16451, loss = 1.33006830\n",
      "Iteration 16452, loss = 1.33005668\n",
      "Iteration 16453, loss = 1.33004500\n",
      "Iteration 16454, loss = 1.33003342\n",
      "Iteration 16455, loss = 1.33002187\n",
      "Iteration 16456, loss = 1.33001026\n",
      "Iteration 16457, loss = 1.32999860\n",
      "Iteration 16458, loss = 1.32998690\n",
      "Iteration 16459, loss = 1.32997513\n",
      "Iteration 16460, loss = 1.32996334\n",
      "Iteration 16461, loss = 1.32995152\n",
      "Iteration 16462, loss = 1.32993973\n",
      "Iteration 16463, loss = 1.32992787\n",
      "Iteration 16464, loss = 1.32991595\n",
      "Iteration 16465, loss = 1.32990407\n",
      "Iteration 16466, loss = 1.32989217\n",
      "Iteration 16467, loss = 1.32988029\n",
      "Iteration 16468, loss = 1.32986844\n",
      "Iteration 16469, loss = 1.32985657\n",
      "Iteration 16470, loss = 1.32984466\n",
      "Iteration 16471, loss = 1.32983277\n",
      "Iteration 16472, loss = 1.32982098\n",
      "Iteration 16473, loss = 1.32980912\n",
      "Iteration 16474, loss = 1.32979716\n",
      "Iteration 16475, loss = 1.32978539\n",
      "Iteration 16476, loss = 1.32977522\n",
      "Iteration 16477, loss = 1.32976269\n",
      "Iteration 16478, loss = 1.32975177\n",
      "Iteration 16479, loss = 1.32974060\n",
      "Iteration 16480, loss = 1.32972923\n",
      "Iteration 16481, loss = 1.32971770\n",
      "Iteration 16482, loss = 1.32970602\n",
      "Iteration 16483, loss = 1.32969435\n",
      "Iteration 16484, loss = 1.32968266\n",
      "Iteration 16485, loss = 1.32967113\n",
      "Iteration 16486, loss = 1.32965955\n",
      "Iteration 16487, loss = 1.32964792\n",
      "Iteration 16488, loss = 1.32963624\n",
      "Iteration 16489, loss = 1.32962453\n",
      "Iteration 16490, loss = 1.32961279\n",
      "Iteration 16491, loss = 1.32960103\n",
      "Iteration 16492, loss = 1.32958924\n",
      "Iteration 16493, loss = 1.32957743\n",
      "Iteration 16494, loss = 1.32956562\n",
      "Iteration 16495, loss = 1.32955386\n",
      "Iteration 16496, loss = 1.32954205\n",
      "Iteration 16497, loss = 1.32953021\n",
      "Iteration 16498, loss = 1.32951844\n",
      "Iteration 16499, loss = 1.32950670\n",
      "Iteration 16500, loss = 1.32949491\n",
      "Iteration 16501, loss = 1.32948314\n",
      "Iteration 16502, loss = 1.32947135\n",
      "Iteration 16503, loss = 1.32945961\n",
      "Iteration 16504, loss = 1.32944786\n",
      "Iteration 16505, loss = 1.32943611\n",
      "Iteration 16506, loss = 1.32942782\n",
      "Iteration 16507, loss = 1.32941369\n",
      "Iteration 16508, loss = 1.32940278\n",
      "Iteration 16509, loss = 1.32939162\n",
      "Iteration 16510, loss = 1.32938029\n",
      "Iteration 16511, loss = 1.32936887\n",
      "Iteration 16512, loss = 1.32935735\n",
      "Iteration 16513, loss = 1.32934582\n",
      "Iteration 16514, loss = 1.32933429\n",
      "Iteration 16515, loss = 1.32932284\n",
      "Iteration 16516, loss = 1.32931134\n",
      "Iteration 16517, loss = 1.32929979\n",
      "Iteration 16518, loss = 1.32928819\n",
      "Iteration 16519, loss = 1.32927656\n",
      "Iteration 16520, loss = 1.32926490\n",
      "Iteration 16521, loss = 1.32925321\n",
      "Iteration 16522, loss = 1.32924151\n",
      "Iteration 16523, loss = 1.32922984\n",
      "Iteration 16524, loss = 1.32921815\n",
      "Iteration 16525, loss = 1.32920641\n",
      "Iteration 16526, loss = 1.32919466\n",
      "Iteration 16527, loss = 1.32918301\n",
      "Iteration 16528, loss = 1.32917139\n",
      "Iteration 16529, loss = 1.32915968\n",
      "Iteration 16530, loss = 1.32914798\n",
      "Iteration 16531, loss = 1.32913633\n",
      "Iteration 16532, loss = 1.32912467\n",
      "Iteration 16533, loss = 1.32911296\n",
      "Iteration 16534, loss = 1.32910226\n",
      "Iteration 16535, loss = 1.32909046\n",
      "Iteration 16536, loss = 1.32907958\n",
      "Iteration 16537, loss = 1.32906856\n",
      "Iteration 16538, loss = 1.32905737\n",
      "Iteration 16539, loss = 1.32904607\n",
      "Iteration 16540, loss = 1.32903458\n",
      "Iteration 16541, loss = 1.32902315\n",
      "Iteration 16542, loss = 1.32901177\n",
      "Iteration 16543, loss = 1.32900036\n",
      "Iteration 16544, loss = 1.32898895\n",
      "Iteration 16545, loss = 1.32897748\n",
      "Iteration 16546, loss = 1.32896598\n",
      "Iteration 16547, loss = 1.32895443\n",
      "Iteration 16548, loss = 1.32894288\n",
      "Iteration 16549, loss = 1.32893128\n",
      "Iteration 16550, loss = 1.32891964\n",
      "Iteration 16551, loss = 1.32890805\n",
      "Iteration 16552, loss = 1.32889644\n",
      "Iteration 16553, loss = 1.32888479\n",
      "Iteration 16554, loss = 1.32887315\n",
      "Iteration 16555, loss = 1.32886158\n",
      "Iteration 16556, loss = 1.32885003\n",
      "Iteration 16557, loss = 1.32883839\n",
      "Iteration 16558, loss = 1.32882687\n",
      "Iteration 16559, loss = 1.32881530\n",
      "Iteration 16560, loss = 1.32880369\n",
      "Iteration 16561, loss = 1.32879207\n",
      "Iteration 16562, loss = 1.32878052\n",
      "Iteration 16563, loss = 1.32876901\n",
      "Iteration 16564, loss = 1.32875864\n",
      "Iteration 16565, loss = 1.32874669\n",
      "Iteration 16566, loss = 1.32873589\n",
      "Iteration 16567, loss = 1.32872494\n",
      "Iteration 16568, loss = 1.32871384\n",
      "Iteration 16569, loss = 1.32870258\n",
      "Iteration 16570, loss = 1.32869139\n",
      "Iteration 16571, loss = 1.32868014\n",
      "Iteration 16572, loss = 1.32866887\n",
      "Iteration 16573, loss = 1.32865758\n",
      "Iteration 16574, loss = 1.32864624\n",
      "Iteration 16575, loss = 1.32863484\n",
      "Iteration 16576, loss = 1.32862341\n",
      "Iteration 16577, loss = 1.32861193\n",
      "Iteration 16578, loss = 1.32860043\n",
      "Iteration 16579, loss = 1.32858892\n",
      "Iteration 16580, loss = 1.32857738\n",
      "Iteration 16581, loss = 1.32856586\n",
      "Iteration 16582, loss = 1.32855434\n",
      "Iteration 16583, loss = 1.32854276\n",
      "Iteration 16584, loss = 1.32853120\n",
      "Iteration 16585, loss = 1.32851972\n",
      "Iteration 16586, loss = 1.32850826\n",
      "Iteration 16587, loss = 1.32849674\n",
      "Iteration 16588, loss = 1.32848516\n",
      "Iteration 16589, loss = 1.32847364\n",
      "Iteration 16590, loss = 1.32846213\n",
      "Iteration 16591, loss = 1.32845074\n",
      "Iteration 16592, loss = 1.32843928\n",
      "Iteration 16593, loss = 1.32842879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16594, loss = 1.32841709\n",
      "Iteration 16595, loss = 1.32840635\n",
      "Iteration 16596, loss = 1.32839540\n",
      "Iteration 16597, loss = 1.32838435\n",
      "Iteration 16598, loss = 1.32837329\n",
      "Iteration 16599, loss = 1.32836219\n",
      "Iteration 16600, loss = 1.32835102\n",
      "Iteration 16601, loss = 1.32833982\n",
      "Iteration 16602, loss = 1.32832860\n",
      "Iteration 16603, loss = 1.32831734\n",
      "Iteration 16604, loss = 1.32830602\n",
      "Iteration 16605, loss = 1.32829467\n",
      "Iteration 16606, loss = 1.32828327\n",
      "Iteration 16607, loss = 1.32827186\n",
      "Iteration 16608, loss = 1.32826046\n",
      "Iteration 16609, loss = 1.32824900\n",
      "Iteration 16610, loss = 1.32823754\n",
      "Iteration 16611, loss = 1.32822608\n",
      "Iteration 16612, loss = 1.32821461\n",
      "Iteration 16613, loss = 1.32820319\n",
      "Iteration 16614, loss = 1.32819182\n",
      "Iteration 16615, loss = 1.32818039\n",
      "Iteration 16616, loss = 1.32816897\n",
      "Iteration 16617, loss = 1.32815757\n",
      "Iteration 16618, loss = 1.32814611\n",
      "Iteration 16619, loss = 1.32813471\n",
      "Iteration 16620, loss = 1.32812335\n",
      "Iteration 16621, loss = 1.32811195\n",
      "Iteration 16622, loss = 1.32810112\n",
      "Iteration 16623, loss = 1.32809003\n",
      "Iteration 16624, loss = 1.32807945\n",
      "Iteration 16625, loss = 1.32806874\n",
      "Iteration 16626, loss = 1.32805782\n",
      "Iteration 16627, loss = 1.32804676\n",
      "Iteration 16628, loss = 1.32803558\n",
      "Iteration 16629, loss = 1.32802444\n",
      "Iteration 16630, loss = 1.32801332\n",
      "Iteration 16631, loss = 1.32800221\n",
      "Iteration 16632, loss = 1.32799104\n",
      "Iteration 16633, loss = 1.32797982\n",
      "Iteration 16634, loss = 1.32796856\n",
      "Iteration 16635, loss = 1.32795728\n",
      "Iteration 16636, loss = 1.32794596\n",
      "Iteration 16637, loss = 1.32793466\n",
      "Iteration 16638, loss = 1.32792331\n",
      "Iteration 16639, loss = 1.32791191\n",
      "Iteration 16640, loss = 1.32790054\n",
      "Iteration 16641, loss = 1.32788918\n",
      "Iteration 16642, loss = 1.32787784\n",
      "Iteration 16643, loss = 1.32786644\n",
      "Iteration 16644, loss = 1.32785502\n",
      "Iteration 16645, loss = 1.32784383\n",
      "Iteration 16646, loss = 1.32783314\n",
      "Iteration 16647, loss = 1.32782265\n",
      "Iteration 16648, loss = 1.32781202\n",
      "Iteration 16649, loss = 1.32780121\n",
      "Iteration 16650, loss = 1.32779019\n",
      "Iteration 16651, loss = 1.32777912\n",
      "Iteration 16652, loss = 1.32776795\n",
      "Iteration 16653, loss = 1.32775682\n",
      "Iteration 16654, loss = 1.32774570\n",
      "Iteration 16655, loss = 1.32773458\n",
      "Iteration 16656, loss = 1.32772340\n",
      "Iteration 16657, loss = 1.32771219\n",
      "Iteration 16658, loss = 1.32770093\n",
      "Iteration 16659, loss = 1.32768964\n",
      "Iteration 16660, loss = 1.32767833\n",
      "Iteration 16661, loss = 1.32766703\n",
      "Iteration 16662, loss = 1.32765577\n",
      "Iteration 16663, loss = 1.32764444\n",
      "Iteration 16664, loss = 1.32763303\n",
      "Iteration 16665, loss = 1.32762177\n",
      "Iteration 16666, loss = 1.32761059\n",
      "Iteration 16667, loss = 1.32759928\n",
      "Iteration 16668, loss = 1.32758801\n",
      "Iteration 16669, loss = 1.32757674\n",
      "Iteration 16670, loss = 1.32756544\n",
      "Iteration 16671, loss = 1.32755420\n",
      "Iteration 16672, loss = 1.32754627\n",
      "Iteration 16673, loss = 1.32753272\n",
      "Iteration 16674, loss = 1.32752249\n",
      "Iteration 16675, loss = 1.32751204\n",
      "Iteration 16676, loss = 1.32750136\n",
      "Iteration 16677, loss = 1.32749045\n",
      "Iteration 16678, loss = 1.32747944\n",
      "Iteration 16679, loss = 1.32746828\n",
      "Iteration 16680, loss = 1.32745721\n",
      "Iteration 16681, loss = 1.32744619\n",
      "Iteration 16682, loss = 1.32743518\n",
      "Iteration 16683, loss = 1.32742411\n",
      "Iteration 16684, loss = 1.32741298\n",
      "Iteration 16685, loss = 1.32740181\n",
      "Iteration 16686, loss = 1.32739060\n",
      "Iteration 16687, loss = 1.32737936\n",
      "Iteration 16688, loss = 1.32736809\n",
      "Iteration 16689, loss = 1.32735681\n",
      "Iteration 16690, loss = 1.32734553\n",
      "Iteration 16691, loss = 1.32733421\n",
      "Iteration 16692, loss = 1.32732454\n",
      "Iteration 16693, loss = 1.32731240\n",
      "Iteration 16694, loss = 1.32730183\n",
      "Iteration 16695, loss = 1.32729123\n",
      "Iteration 16696, loss = 1.32728054\n",
      "Iteration 16697, loss = 1.32726980\n",
      "Iteration 16698, loss = 1.32725898\n",
      "Iteration 16699, loss = 1.32724801\n",
      "Iteration 16700, loss = 1.32723694\n",
      "Iteration 16701, loss = 1.32722581\n",
      "Iteration 16702, loss = 1.32721468\n",
      "Iteration 16703, loss = 1.32720351\n",
      "Iteration 16704, loss = 1.32719248\n",
      "Iteration 16705, loss = 1.32718143\n",
      "Iteration 16706, loss = 1.32717030\n",
      "Iteration 16707, loss = 1.32715918\n",
      "Iteration 16708, loss = 1.32714804\n",
      "Iteration 16709, loss = 1.32713695\n",
      "Iteration 16710, loss = 1.32712592\n",
      "Iteration 16711, loss = 1.32711481\n",
      "Iteration 16712, loss = 1.32710363\n",
      "Iteration 16713, loss = 1.32709249\n",
      "Iteration 16714, loss = 1.32708143\n",
      "Iteration 16715, loss = 1.32707030\n",
      "Iteration 16716, loss = 1.32705916\n",
      "Iteration 16717, loss = 1.32704800\n",
      "Iteration 16718, loss = 1.32704028\n",
      "Iteration 16719, loss = 1.32702664\n",
      "Iteration 16720, loss = 1.32701652\n",
      "Iteration 16721, loss = 1.32700630\n",
      "Iteration 16722, loss = 1.32699587\n",
      "Iteration 16723, loss = 1.32698522\n",
      "Iteration 16724, loss = 1.32697434\n",
      "Iteration 16725, loss = 1.32696328\n",
      "Iteration 16726, loss = 1.32695225\n",
      "Iteration 16727, loss = 1.32694130\n",
      "Iteration 16728, loss = 1.32693042\n",
      "Iteration 16729, loss = 1.32691947\n",
      "Iteration 16730, loss = 1.32690847\n",
      "Iteration 16731, loss = 1.32689743\n",
      "Iteration 16732, loss = 1.32688634\n",
      "Iteration 16733, loss = 1.32687523\n",
      "Iteration 16734, loss = 1.32686408\n",
      "Iteration 16735, loss = 1.32685294\n",
      "Iteration 16736, loss = 1.32684178\n",
      "Iteration 16737, loss = 1.32683061\n",
      "Iteration 16738, loss = 1.32682093\n",
      "Iteration 16739, loss = 1.32680896\n",
      "Iteration 16740, loss = 1.32679845\n",
      "Iteration 16741, loss = 1.32678791\n",
      "Iteration 16742, loss = 1.32677744\n",
      "Iteration 16743, loss = 1.32676697\n",
      "Iteration 16744, loss = 1.32675633\n",
      "Iteration 16745, loss = 1.32674556\n",
      "Iteration 16746, loss = 1.32673460\n",
      "Iteration 16747, loss = 1.32672352\n",
      "Iteration 16748, loss = 1.32671250\n",
      "Iteration 16749, loss = 1.32670150\n",
      "Iteration 16750, loss = 1.32669059\n",
      "Iteration 16751, loss = 1.32667960\n",
      "Iteration 16752, loss = 1.32666858\n",
      "Iteration 16753, loss = 1.32665759\n",
      "Iteration 16754, loss = 1.32664657\n",
      "Iteration 16755, loss = 1.32663557\n",
      "Iteration 16756, loss = 1.32662462\n",
      "Iteration 16757, loss = 1.32661366\n",
      "Iteration 16758, loss = 1.32660265\n",
      "Iteration 16759, loss = 1.32659157\n",
      "Iteration 16760, loss = 1.32658060\n",
      "Iteration 16761, loss = 1.32656962\n",
      "Iteration 16762, loss = 1.32656144\n",
      "Iteration 16763, loss = 1.32654851\n",
      "Iteration 16764, loss = 1.32653852\n",
      "Iteration 16765, loss = 1.32652847\n",
      "Iteration 16766, loss = 1.32651823\n",
      "Iteration 16767, loss = 1.32650774\n",
      "Iteration 16768, loss = 1.32649704\n",
      "Iteration 16769, loss = 1.32648612\n",
      "Iteration 16770, loss = 1.32647512\n",
      "Iteration 16771, loss = 1.32646432\n",
      "Iteration 16772, loss = 1.32645356\n",
      "Iteration 16773, loss = 1.32644273\n",
      "Iteration 16774, loss = 1.32643187\n",
      "Iteration 16775, loss = 1.32642095\n",
      "Iteration 16776, loss = 1.32640998\n",
      "Iteration 16777, loss = 1.32639899\n",
      "Iteration 16778, loss = 1.32638797\n",
      "Iteration 16779, loss = 1.32637694\n",
      "Iteration 16780, loss = 1.32636589\n",
      "Iteration 16781, loss = 1.32635484\n",
      "Iteration 16782, loss = 1.32634380\n",
      "Iteration 16783, loss = 1.32633304\n",
      "Iteration 16784, loss = 1.32632238\n",
      "Iteration 16785, loss = 1.32631202\n",
      "Iteration 16786, loss = 1.32630171\n",
      "Iteration 16787, loss = 1.32629133\n",
      "Iteration 16788, loss = 1.32628092\n",
      "Iteration 16789, loss = 1.32627034\n",
      "Iteration 16790, loss = 1.32625965\n",
      "Iteration 16791, loss = 1.32624880\n",
      "Iteration 16792, loss = 1.32623793\n",
      "Iteration 16793, loss = 1.32622711\n",
      "Iteration 16794, loss = 1.32621626\n",
      "Iteration 16795, loss = 1.32620539\n",
      "Iteration 16796, loss = 1.32619454\n",
      "Iteration 16797, loss = 1.32618364\n",
      "Iteration 16798, loss = 1.32617277\n",
      "Iteration 16799, loss = 1.32616192\n",
      "Iteration 16800, loss = 1.32615108\n",
      "Iteration 16801, loss = 1.32614024\n",
      "Iteration 16802, loss = 1.32612932\n",
      "Iteration 16803, loss = 1.32611846\n",
      "Iteration 16804, loss = 1.32610766\n",
      "Iteration 16805, loss = 1.32609815\n",
      "Iteration 16806, loss = 1.32608661\n",
      "Iteration 16807, loss = 1.32607662\n",
      "Iteration 16808, loss = 1.32606677\n",
      "Iteration 16809, loss = 1.32605668\n",
      "Iteration 16810, loss = 1.32604636\n",
      "Iteration 16811, loss = 1.32603580\n",
      "Iteration 16812, loss = 1.32602504\n",
      "Iteration 16813, loss = 1.32601425\n",
      "Iteration 16814, loss = 1.32600355\n",
      "Iteration 16815, loss = 1.32599289\n",
      "Iteration 16816, loss = 1.32598216\n",
      "Iteration 16817, loss = 1.32597137\n",
      "Iteration 16818, loss = 1.32596054\n",
      "Iteration 16819, loss = 1.32594967\n",
      "Iteration 16820, loss = 1.32593876\n",
      "Iteration 16821, loss = 1.32592783\n",
      "Iteration 16822, loss = 1.32591688\n",
      "Iteration 16823, loss = 1.32590592\n",
      "Iteration 16824, loss = 1.32589497\n",
      "Iteration 16825, loss = 1.32588408\n",
      "Iteration 16826, loss = 1.32587329\n",
      "Iteration 16827, loss = 1.32586273\n",
      "Iteration 16828, loss = 1.32585203\n",
      "Iteration 16829, loss = 1.32584192\n",
      "Iteration 16830, loss = 1.32583186\n",
      "Iteration 16831, loss = 1.32582168\n",
      "Iteration 16832, loss = 1.32581136\n",
      "Iteration 16833, loss = 1.32580087\n",
      "Iteration 16834, loss = 1.32579025\n",
      "Iteration 16835, loss = 1.32577954\n",
      "Iteration 16836, loss = 1.32576881\n",
      "Iteration 16837, loss = 1.32575811\n",
      "Iteration 16838, loss = 1.32574754\n",
      "Iteration 16839, loss = 1.32573688\n",
      "Iteration 16840, loss = 1.32572611\n",
      "Iteration 16841, loss = 1.32571540\n",
      "Iteration 16842, loss = 1.32570473\n",
      "Iteration 16843, loss = 1.32569404\n",
      "Iteration 16844, loss = 1.32568330\n",
      "Iteration 16845, loss = 1.32567250\n",
      "Iteration 16846, loss = 1.32566167\n",
      "Iteration 16847, loss = 1.32565091\n",
      "Iteration 16848, loss = 1.32564268\n",
      "Iteration 16849, loss = 1.32563006\n",
      "Iteration 16850, loss = 1.32562001\n",
      "Iteration 16851, loss = 1.32561016\n",
      "Iteration 16852, loss = 1.32560020\n",
      "Iteration 16853, loss = 1.32559008\n",
      "Iteration 16854, loss = 1.32557974\n",
      "Iteration 16855, loss = 1.32556918\n",
      "Iteration 16856, loss = 1.32555848\n",
      "Iteration 16857, loss = 1.32554773\n",
      "Iteration 16858, loss = 1.32553711\n",
      "Iteration 16859, loss = 1.32552648\n",
      "Iteration 16860, loss = 1.32551581\n",
      "Iteration 16861, loss = 1.32550511\n",
      "Iteration 16862, loss = 1.32549445\n",
      "Iteration 16863, loss = 1.32548370\n",
      "Iteration 16864, loss = 1.32547289\n",
      "Iteration 16865, loss = 1.32546208\n",
      "Iteration 16866, loss = 1.32545136\n",
      "Iteration 16867, loss = 1.32544064\n",
      "Iteration 16868, loss = 1.32542987\n",
      "Iteration 16869, loss = 1.32541908\n",
      "Iteration 16870, loss = 1.32540832\n",
      "Iteration 16871, loss = 1.32540139\n",
      "Iteration 16872, loss = 1.32538750\n",
      "Iteration 16873, loss = 1.32537762\n",
      "Iteration 16874, loss = 1.32536777\n",
      "Iteration 16875, loss = 1.32535779\n",
      "Iteration 16876, loss = 1.32534772\n",
      "Iteration 16877, loss = 1.32533742\n",
      "Iteration 16878, loss = 1.32532692\n",
      "Iteration 16879, loss = 1.32531623\n",
      "Iteration 16880, loss = 1.32530568\n",
      "Iteration 16881, loss = 1.32529508\n",
      "Iteration 16882, loss = 1.32528450\n",
      "Iteration 16883, loss = 1.32527390\n",
      "Iteration 16884, loss = 1.32526334\n",
      "Iteration 16885, loss = 1.32525275\n",
      "Iteration 16886, loss = 1.32524203\n",
      "Iteration 16887, loss = 1.32523132\n",
      "Iteration 16888, loss = 1.32522068\n",
      "Iteration 16889, loss = 1.32521003\n",
      "Iteration 16890, loss = 1.32519931\n",
      "Iteration 16891, loss = 1.32519197\n",
      "Iteration 16892, loss = 1.32517860\n",
      "Iteration 16893, loss = 1.32516864\n",
      "Iteration 16894, loss = 1.32515870\n",
      "Iteration 16895, loss = 1.32514867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16896, loss = 1.32513857\n",
      "Iteration 16897, loss = 1.32512834\n",
      "Iteration 16898, loss = 1.32511792\n",
      "Iteration 16899, loss = 1.32510743\n",
      "Iteration 16900, loss = 1.32509689\n",
      "Iteration 16901, loss = 1.32508638\n",
      "Iteration 16902, loss = 1.32507589\n",
      "Iteration 16903, loss = 1.32506533\n",
      "Iteration 16904, loss = 1.32505469\n",
      "Iteration 16905, loss = 1.32504410\n",
      "Iteration 16906, loss = 1.32503350\n",
      "Iteration 16907, loss = 1.32502286\n",
      "Iteration 16908, loss = 1.32501222\n",
      "Iteration 16909, loss = 1.32500157\n",
      "Iteration 16910, loss = 1.32499093\n",
      "Iteration 16911, loss = 1.32498020\n",
      "Iteration 16912, loss = 1.32496953\n",
      "Iteration 16913, loss = 1.32496068\n",
      "Iteration 16914, loss = 1.32494923\n",
      "Iteration 16915, loss = 1.32493954\n",
      "Iteration 16916, loss = 1.32492977\n",
      "Iteration 16917, loss = 1.32491977\n",
      "Iteration 16918, loss = 1.32490957\n",
      "Iteration 16919, loss = 1.32489925\n",
      "Iteration 16920, loss = 1.32488881\n",
      "Iteration 16921, loss = 1.32487842\n",
      "Iteration 16922, loss = 1.32486808\n",
      "Iteration 16923, loss = 1.32485768\n",
      "Iteration 16924, loss = 1.32484722\n",
      "Iteration 16925, loss = 1.32483676\n",
      "Iteration 16926, loss = 1.32482624\n",
      "Iteration 16927, loss = 1.32481566\n",
      "Iteration 16928, loss = 1.32480510\n",
      "Iteration 16929, loss = 1.32479449\n",
      "Iteration 16930, loss = 1.32478394\n",
      "Iteration 16931, loss = 1.32477342\n",
      "Iteration 16932, loss = 1.32476282\n",
      "Iteration 16933, loss = 1.32475219\n",
      "Iteration 16934, loss = 1.32474568\n",
      "Iteration 16935, loss = 1.32473183\n",
      "Iteration 16936, loss = 1.32472214\n",
      "Iteration 16937, loss = 1.32471241\n",
      "Iteration 16938, loss = 1.32470251\n",
      "Iteration 16939, loss = 1.32469249\n",
      "Iteration 16940, loss = 1.32468228\n",
      "Iteration 16941, loss = 1.32467190\n",
      "Iteration 16942, loss = 1.32466154\n",
      "Iteration 16943, loss = 1.32465119\n",
      "Iteration 16944, loss = 1.32464078\n",
      "Iteration 16945, loss = 1.32463038\n",
      "Iteration 16946, loss = 1.32461998\n",
      "Iteration 16947, loss = 1.32460952\n",
      "Iteration 16948, loss = 1.32459901\n",
      "Iteration 16949, loss = 1.32458859\n",
      "Iteration 16950, loss = 1.32457819\n",
      "Iteration 16951, loss = 1.32456768\n",
      "Iteration 16952, loss = 1.32455704\n",
      "Iteration 16953, loss = 1.32454650\n",
      "Iteration 16954, loss = 1.32453604\n",
      "Iteration 16955, loss = 1.32452926\n",
      "Iteration 16956, loss = 1.32451574\n",
      "Iteration 16957, loss = 1.32450611\n",
      "Iteration 16958, loss = 1.32449657\n",
      "Iteration 16959, loss = 1.32448685\n",
      "Iteration 16960, loss = 1.32447697\n",
      "Iteration 16961, loss = 1.32446687\n",
      "Iteration 16962, loss = 1.32445656\n",
      "Iteration 16963, loss = 1.32444610\n",
      "Iteration 16964, loss = 1.32443575\n",
      "Iteration 16965, loss = 1.32442547\n",
      "Iteration 16966, loss = 1.32441513\n",
      "Iteration 16967, loss = 1.32440475\n",
      "Iteration 16968, loss = 1.32439432\n",
      "Iteration 16969, loss = 1.32438384\n",
      "Iteration 16970, loss = 1.32437336\n",
      "Iteration 16971, loss = 1.32436291\n",
      "Iteration 16972, loss = 1.32435246\n",
      "Iteration 16973, loss = 1.32434196\n",
      "Iteration 16974, loss = 1.32433141\n",
      "Iteration 16975, loss = 1.32432088\n",
      "Iteration 16976, loss = 1.32431046\n",
      "Iteration 16977, loss = 1.32430594\n",
      "Iteration 16978, loss = 1.32429122\n",
      "Iteration 16979, loss = 1.32428295\n",
      "Iteration 16980, loss = 1.32427466\n",
      "Iteration 16981, loss = 1.32426607\n",
      "Iteration 16982, loss = 1.32425717\n",
      "Iteration 16983, loss = 1.32424795\n",
      "Iteration 16984, loss = 1.32423841\n",
      "Iteration 16985, loss = 1.32422858\n",
      "Iteration 16986, loss = 1.32421847\n",
      "Iteration 16987, loss = 1.32420809\n",
      "Iteration 16988, loss = 1.32419749\n",
      "Iteration 16989, loss = 1.32418712\n",
      "Iteration 16990, loss = 1.32417686\n",
      "Iteration 16991, loss = 1.32416652\n",
      "Iteration 16992, loss = 1.32415609\n",
      "Iteration 16993, loss = 1.32414559\n",
      "Iteration 16994, loss = 1.32413503\n",
      "Iteration 16995, loss = 1.32412444\n",
      "Iteration 16996, loss = 1.32411381\n",
      "Iteration 16997, loss = 1.32410317\n",
      "Iteration 16998, loss = 1.32409252\n",
      "Iteration 16999, loss = 1.32408191\n",
      "Iteration 17000, loss = 1.32407139\n",
      "Iteration 17001, loss = 1.32406091\n",
      "Iteration 17002, loss = 1.32405038\n",
      "Iteration 17003, loss = 1.32403990\n",
      "Iteration 17004, loss = 1.32402938\n",
      "Iteration 17005, loss = 1.32401890\n",
      "Iteration 17006, loss = 1.32400843\n",
      "Iteration 17007, loss = 1.32399798\n",
      "Iteration 17008, loss = 1.32398766\n",
      "Iteration 17009, loss = 1.32397745\n",
      "Iteration 17010, loss = 1.32396716\n",
      "Iteration 17011, loss = 1.32395676\n",
      "Iteration 17012, loss = 1.32394631\n",
      "Iteration 17013, loss = 1.32393588\n",
      "Iteration 17014, loss = 1.32392551\n",
      "Iteration 17015, loss = 1.32391910\n",
      "Iteration 17016, loss = 1.32390562\n",
      "Iteration 17017, loss = 1.32389598\n",
      "Iteration 17018, loss = 1.32388650\n",
      "Iteration 17019, loss = 1.32387706\n",
      "Iteration 17020, loss = 1.32386743\n",
      "Iteration 17021, loss = 1.32385759\n",
      "Iteration 17022, loss = 1.32384756\n",
      "Iteration 17023, loss = 1.32383735\n",
      "Iteration 17024, loss = 1.32382706\n",
      "Iteration 17025, loss = 1.32381669\n",
      "Iteration 17026, loss = 1.32380653\n",
      "Iteration 17027, loss = 1.32379638\n",
      "Iteration 17028, loss = 1.32378617\n",
      "Iteration 17029, loss = 1.32377593\n",
      "Iteration 17030, loss = 1.32376565\n",
      "Iteration 17031, loss = 1.32375534\n",
      "Iteration 17032, loss = 1.32374501\n",
      "Iteration 17033, loss = 1.32373473\n",
      "Iteration 17034, loss = 1.32372441\n",
      "Iteration 17035, loss = 1.32371908\n",
      "Iteration 17036, loss = 1.32370439\n",
      "Iteration 17037, loss = 1.32369473\n",
      "Iteration 17038, loss = 1.32368501\n",
      "Iteration 17039, loss = 1.32367528\n",
      "Iteration 17040, loss = 1.32366552\n",
      "Iteration 17041, loss = 1.32365574\n",
      "Iteration 17042, loss = 1.32364587\n",
      "Iteration 17043, loss = 1.32363583\n",
      "Iteration 17044, loss = 1.32362564\n",
      "Iteration 17045, loss = 1.32361544\n",
      "Iteration 17046, loss = 1.32360522\n",
      "Iteration 17047, loss = 1.32359506\n",
      "Iteration 17048, loss = 1.32358489\n",
      "Iteration 17049, loss = 1.32357473\n",
      "Iteration 17050, loss = 1.32356450\n",
      "Iteration 17051, loss = 1.32355435\n",
      "Iteration 17052, loss = 1.32354423\n",
      "Iteration 17053, loss = 1.32353401\n",
      "Iteration 17054, loss = 1.32352383\n",
      "Iteration 17055, loss = 1.32351366\n",
      "Iteration 17056, loss = 1.32350355\n",
      "Iteration 17057, loss = 1.32349397\n",
      "Iteration 17058, loss = 1.32348460\n",
      "Iteration 17059, loss = 1.32347525\n",
      "Iteration 17060, loss = 1.32346586\n",
      "Iteration 17061, loss = 1.32345632\n",
      "Iteration 17062, loss = 1.32344656\n",
      "Iteration 17063, loss = 1.32343660\n",
      "Iteration 17064, loss = 1.32342649\n",
      "Iteration 17065, loss = 1.32341643\n",
      "Iteration 17066, loss = 1.32340641\n",
      "Iteration 17067, loss = 1.32339636\n",
      "Iteration 17068, loss = 1.32338627\n",
      "Iteration 17069, loss = 1.32337613\n",
      "Iteration 17070, loss = 1.32336593\n",
      "Iteration 17071, loss = 1.32335569\n",
      "Iteration 17072, loss = 1.32334547\n",
      "Iteration 17073, loss = 1.32333521\n",
      "Iteration 17074, loss = 1.32332497\n",
      "Iteration 17075, loss = 1.32331476\n",
      "Iteration 17076, loss = 1.32330450\n",
      "Iteration 17077, loss = 1.32329434\n",
      "Iteration 17078, loss = 1.32328855\n",
      "Iteration 17079, loss = 1.32327459\n",
      "Iteration 17080, loss = 1.32326526\n",
      "Iteration 17081, loss = 1.32325586\n",
      "Iteration 17082, loss = 1.32324644\n",
      "Iteration 17083, loss = 1.32323686\n",
      "Iteration 17084, loss = 1.32322707\n",
      "Iteration 17085, loss = 1.32321709\n",
      "Iteration 17086, loss = 1.32320704\n",
      "Iteration 17087, loss = 1.32319698\n",
      "Iteration 17088, loss = 1.32318699\n",
      "Iteration 17089, loss = 1.32317703\n",
      "Iteration 17090, loss = 1.32316700\n",
      "Iteration 17091, loss = 1.32315696\n",
      "Iteration 17092, loss = 1.32314699\n",
      "Iteration 17093, loss = 1.32313693\n",
      "Iteration 17094, loss = 1.32312681\n",
      "Iteration 17095, loss = 1.32311666\n",
      "Iteration 17096, loss = 1.32310655\n",
      "Iteration 17097, loss = 1.32309642\n",
      "Iteration 17098, loss = 1.32308863\n",
      "Iteration 17099, loss = 1.32307695\n",
      "Iteration 17100, loss = 1.32306761\n",
      "Iteration 17101, loss = 1.32305831\n",
      "Iteration 17102, loss = 1.32304904\n",
      "Iteration 17103, loss = 1.32303957\n",
      "Iteration 17104, loss = 1.32302989\n",
      "Iteration 17105, loss = 1.32302001\n",
      "Iteration 17106, loss = 1.32300996\n",
      "Iteration 17107, loss = 1.32299996\n",
      "Iteration 17108, loss = 1.32299003\n",
      "Iteration 17109, loss = 1.32298006\n",
      "Iteration 17110, loss = 1.32297004\n",
      "Iteration 17111, loss = 1.32295998\n",
      "Iteration 17112, loss = 1.32294991\n",
      "Iteration 17113, loss = 1.32293979\n",
      "Iteration 17114, loss = 1.32292971\n",
      "Iteration 17115, loss = 1.32291970\n",
      "Iteration 17116, loss = 1.32290963\n",
      "Iteration 17117, loss = 1.32289953\n",
      "Iteration 17118, loss = 1.32288945\n",
      "Iteration 17119, loss = 1.32287943\n",
      "Iteration 17120, loss = 1.32287063\n",
      "Iteration 17121, loss = 1.32286108\n",
      "Iteration 17122, loss = 1.32285316\n",
      "Iteration 17123, loss = 1.32284531\n",
      "Iteration 17124, loss = 1.32283716\n",
      "Iteration 17125, loss = 1.32282870\n",
      "Iteration 17126, loss = 1.32281993\n",
      "Iteration 17127, loss = 1.32281085\n",
      "Iteration 17128, loss = 1.32280147\n",
      "Iteration 17129, loss = 1.32279181\n",
      "Iteration 17130, loss = 1.32278190\n",
      "Iteration 17131, loss = 1.32277174\n",
      "Iteration 17132, loss = 1.32276154\n",
      "Iteration 17133, loss = 1.32275152\n",
      "Iteration 17134, loss = 1.32274155\n",
      "Iteration 17135, loss = 1.32273150\n",
      "Iteration 17136, loss = 1.32272137\n",
      "Iteration 17137, loss = 1.32271118\n",
      "Iteration 17138, loss = 1.32270095\n",
      "Iteration 17139, loss = 1.32269069\n",
      "Iteration 17140, loss = 1.32268041\n",
      "Iteration 17141, loss = 1.32267014\n",
      "Iteration 17142, loss = 1.32265993\n",
      "Iteration 17143, loss = 1.32264970\n",
      "Iteration 17144, loss = 1.32263947\n",
      "Iteration 17145, loss = 1.32262932\n",
      "Iteration 17146, loss = 1.32261915\n",
      "Iteration 17147, loss = 1.32260899\n",
      "Iteration 17148, loss = 1.32259894\n",
      "Iteration 17149, loss = 1.32258898\n",
      "Iteration 17150, loss = 1.32257902\n",
      "Iteration 17151, loss = 1.32256902\n",
      "Iteration 17152, loss = 1.32255901\n",
      "Iteration 17153, loss = 1.32254894\n",
      "Iteration 17154, loss = 1.32253886\n",
      "Iteration 17155, loss = 1.32252892\n",
      "Iteration 17156, loss = 1.32251900\n",
      "Iteration 17157, loss = 1.32251118\n",
      "Iteration 17158, loss = 1.32249974\n",
      "Iteration 17159, loss = 1.32249051\n",
      "Iteration 17160, loss = 1.32248138\n",
      "Iteration 17161, loss = 1.32247224\n",
      "Iteration 17162, loss = 1.32246305\n",
      "Iteration 17163, loss = 1.32245371\n",
      "Iteration 17164, loss = 1.32244417\n",
      "Iteration 17165, loss = 1.32243445\n",
      "Iteration 17166, loss = 1.32242455\n",
      "Iteration 17167, loss = 1.32241456\n",
      "Iteration 17168, loss = 1.32240467\n",
      "Iteration 17169, loss = 1.32239484\n",
      "Iteration 17170, loss = 1.32238500\n",
      "Iteration 17171, loss = 1.32237512\n",
      "Iteration 17172, loss = 1.32236520\n",
      "Iteration 17173, loss = 1.32235525\n",
      "Iteration 17174, loss = 1.32234529\n",
      "Iteration 17175, loss = 1.32233532\n",
      "Iteration 17176, loss = 1.32232534\n",
      "Iteration 17177, loss = 1.32231532\n",
      "Iteration 17178, loss = 1.32231035\n",
      "Iteration 17179, loss = 1.32229608\n",
      "Iteration 17180, loss = 1.32228679\n",
      "Iteration 17181, loss = 1.32227748\n",
      "Iteration 17182, loss = 1.32226816\n",
      "Iteration 17183, loss = 1.32225873\n",
      "Iteration 17184, loss = 1.32224928\n",
      "Iteration 17185, loss = 1.32223969\n",
      "Iteration 17186, loss = 1.32222994\n",
      "Iteration 17187, loss = 1.32222014\n",
      "Iteration 17188, loss = 1.32221033\n",
      "Iteration 17189, loss = 1.32220055\n",
      "Iteration 17190, loss = 1.32219078\n",
      "Iteration 17191, loss = 1.32218100\n",
      "Iteration 17192, loss = 1.32217115\n",
      "Iteration 17193, loss = 1.32216131\n",
      "Iteration 17194, loss = 1.32215147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17195, loss = 1.32214165\n",
      "Iteration 17196, loss = 1.32213189\n",
      "Iteration 17197, loss = 1.32212278\n",
      "Iteration 17198, loss = 1.32211286\n",
      "Iteration 17199, loss = 1.32210377\n",
      "Iteration 17200, loss = 1.32209479\n",
      "Iteration 17201, loss = 1.32208580\n",
      "Iteration 17202, loss = 1.32207663\n",
      "Iteration 17203, loss = 1.32206725\n",
      "Iteration 17204, loss = 1.32205767\n",
      "Iteration 17205, loss = 1.32204792\n",
      "Iteration 17206, loss = 1.32203823\n",
      "Iteration 17207, loss = 1.32202854\n",
      "Iteration 17208, loss = 1.32201889\n",
      "Iteration 17209, loss = 1.32200917\n",
      "Iteration 17210, loss = 1.32199937\n",
      "Iteration 17211, loss = 1.32198955\n",
      "Iteration 17212, loss = 1.32197972\n",
      "Iteration 17213, loss = 1.32196991\n",
      "Iteration 17214, loss = 1.32196008\n",
      "Iteration 17215, loss = 1.32195021\n",
      "Iteration 17216, loss = 1.32194031\n",
      "Iteration 17217, loss = 1.32193049\n",
      "Iteration 17218, loss = 1.32192073\n",
      "Iteration 17219, loss = 1.32191208\n",
      "Iteration 17220, loss = 1.32190167\n",
      "Iteration 17221, loss = 1.32189265\n",
      "Iteration 17222, loss = 1.32188365\n",
      "Iteration 17223, loss = 1.32187462\n",
      "Iteration 17224, loss = 1.32186542\n",
      "Iteration 17225, loss = 1.32185601\n",
      "Iteration 17226, loss = 1.32184642\n",
      "Iteration 17227, loss = 1.32183674\n",
      "Iteration 17228, loss = 1.32182702\n",
      "Iteration 17229, loss = 1.32181739\n",
      "Iteration 17230, loss = 1.32180779\n",
      "Iteration 17231, loss = 1.32179816\n",
      "Iteration 17232, loss = 1.32178849\n",
      "Iteration 17233, loss = 1.32177875\n",
      "Iteration 17234, loss = 1.32176906\n",
      "Iteration 17235, loss = 1.32175934\n",
      "Iteration 17236, loss = 1.32174962\n",
      "Iteration 17237, loss = 1.32173987\n",
      "Iteration 17238, loss = 1.32173015\n",
      "Iteration 17239, loss = 1.32172226\n",
      "Iteration 17240, loss = 1.32171134\n",
      "Iteration 17241, loss = 1.32170241\n",
      "Iteration 17242, loss = 1.32169355\n",
      "Iteration 17243, loss = 1.32168465\n",
      "Iteration 17244, loss = 1.32167553\n",
      "Iteration 17245, loss = 1.32166621\n",
      "Iteration 17246, loss = 1.32165670\n",
      "Iteration 17247, loss = 1.32164701\n",
      "Iteration 17248, loss = 1.32163730\n",
      "Iteration 17249, loss = 1.32162775\n",
      "Iteration 17250, loss = 1.32161814\n",
      "Iteration 17251, loss = 1.32160847\n",
      "Iteration 17252, loss = 1.32159876\n",
      "Iteration 17253, loss = 1.32158902\n",
      "Iteration 17254, loss = 1.32157936\n",
      "Iteration 17255, loss = 1.32156962\n",
      "Iteration 17256, loss = 1.32155990\n",
      "Iteration 17257, loss = 1.32155019\n",
      "Iteration 17258, loss = 1.32154052\n",
      "Iteration 17259, loss = 1.32153082\n",
      "Iteration 17260, loss = 1.32152106\n",
      "Iteration 17261, loss = 1.32151458\n",
      "Iteration 17262, loss = 1.32150231\n",
      "Iteration 17263, loss = 1.32149351\n",
      "Iteration 17264, loss = 1.32148480\n",
      "Iteration 17265, loss = 1.32147593\n",
      "Iteration 17266, loss = 1.32146689\n",
      "Iteration 17267, loss = 1.32145768\n",
      "Iteration 17268, loss = 1.32144827\n",
      "Iteration 17269, loss = 1.32143868\n",
      "Iteration 17270, loss = 1.32142906\n",
      "Iteration 17271, loss = 1.32141944\n",
      "Iteration 17272, loss = 1.32140988\n",
      "Iteration 17273, loss = 1.32140029\n",
      "Iteration 17274, loss = 1.32139068\n",
      "Iteration 17275, loss = 1.32138101\n",
      "Iteration 17276, loss = 1.32137129\n",
      "Iteration 17277, loss = 1.32136160\n",
      "Iteration 17278, loss = 1.32135190\n",
      "Iteration 17279, loss = 1.32134308\n",
      "Iteration 17280, loss = 1.32133321\n",
      "Iteration 17281, loss = 1.32132427\n",
      "Iteration 17282, loss = 1.32131532\n",
      "Iteration 17283, loss = 1.32130631\n",
      "Iteration 17284, loss = 1.32129724\n",
      "Iteration 17285, loss = 1.32128806\n",
      "Iteration 17286, loss = 1.32127869\n",
      "Iteration 17287, loss = 1.32126917\n",
      "Iteration 17288, loss = 1.32125957\n",
      "Iteration 17289, loss = 1.32124998\n",
      "Iteration 17290, loss = 1.32124045\n",
      "Iteration 17291, loss = 1.32123087\n",
      "Iteration 17292, loss = 1.32122130\n",
      "Iteration 17293, loss = 1.32121166\n",
      "Iteration 17294, loss = 1.32120206\n",
      "Iteration 17295, loss = 1.32119243\n",
      "Iteration 17296, loss = 1.32118279\n",
      "Iteration 17297, loss = 1.32117315\n",
      "Iteration 17298, loss = 1.32116358\n",
      "Iteration 17299, loss = 1.32115406\n",
      "Iteration 17300, loss = 1.32114615\n",
      "Iteration 17301, loss = 1.32113543\n",
      "Iteration 17302, loss = 1.32112678\n",
      "Iteration 17303, loss = 1.32111818\n",
      "Iteration 17304, loss = 1.32110944\n",
      "Iteration 17305, loss = 1.32110048\n",
      "Iteration 17306, loss = 1.32109132\n",
      "Iteration 17307, loss = 1.32108198\n",
      "Iteration 17308, loss = 1.32107246\n",
      "Iteration 17309, loss = 1.32106287\n",
      "Iteration 17310, loss = 1.32105351\n",
      "Iteration 17311, loss = 1.32104408\n",
      "Iteration 17312, loss = 1.32103457\n",
      "Iteration 17313, loss = 1.32102503\n",
      "Iteration 17314, loss = 1.32101547\n",
      "Iteration 17315, loss = 1.32100590\n",
      "Iteration 17316, loss = 1.32099636\n",
      "Iteration 17317, loss = 1.32098676\n",
      "Iteration 17318, loss = 1.32097711\n",
      "Iteration 17319, loss = 1.32096743\n",
      "Iteration 17320, loss = 1.32096155\n",
      "Iteration 17321, loss = 1.32094895\n",
      "Iteration 17322, loss = 1.32094009\n",
      "Iteration 17323, loss = 1.32093121\n",
      "Iteration 17324, loss = 1.32092227\n",
      "Iteration 17325, loss = 1.32091323\n",
      "Iteration 17326, loss = 1.32090416\n",
      "Iteration 17327, loss = 1.32089494\n",
      "Iteration 17328, loss = 1.32088556\n",
      "Iteration 17329, loss = 1.32087611\n",
      "Iteration 17330, loss = 1.32086660\n",
      "Iteration 17331, loss = 1.32085705\n",
      "Iteration 17332, loss = 1.32084762\n",
      "Iteration 17333, loss = 1.32083822\n",
      "Iteration 17334, loss = 1.32082879\n",
      "Iteration 17335, loss = 1.32081933\n",
      "Iteration 17336, loss = 1.32080984\n",
      "Iteration 17337, loss = 1.32080035\n",
      "Iteration 17338, loss = 1.32079089\n",
      "Iteration 17339, loss = 1.32078145\n",
      "Iteration 17340, loss = 1.32077232\n",
      "Iteration 17341, loss = 1.32076316\n",
      "Iteration 17342, loss = 1.32075450\n",
      "Iteration 17343, loss = 1.32074593\n",
      "Iteration 17344, loss = 1.32073726\n",
      "Iteration 17345, loss = 1.32072837\n",
      "Iteration 17346, loss = 1.32071928\n",
      "Iteration 17347, loss = 1.32071001\n",
      "Iteration 17348, loss = 1.32070065\n",
      "Iteration 17349, loss = 1.32069127\n",
      "Iteration 17350, loss = 1.32068190\n",
      "Iteration 17351, loss = 1.32067252\n",
      "Iteration 17352, loss = 1.32066307\n",
      "Iteration 17353, loss = 1.32065358\n",
      "Iteration 17354, loss = 1.32064405\n",
      "Iteration 17355, loss = 1.32063449\n",
      "Iteration 17356, loss = 1.32062492\n",
      "Iteration 17357, loss = 1.32061552\n",
      "Iteration 17358, loss = 1.32060614\n",
      "Iteration 17359, loss = 1.32059660\n",
      "Iteration 17360, loss = 1.32058693\n",
      "Iteration 17361, loss = 1.32058138\n",
      "Iteration 17362, loss = 1.32056961\n",
      "Iteration 17363, loss = 1.32056247\n",
      "Iteration 17364, loss = 1.32055521\n",
      "Iteration 17365, loss = 1.32054767\n",
      "Iteration 17366, loss = 1.32053983\n",
      "Iteration 17367, loss = 1.32053168\n",
      "Iteration 17368, loss = 1.32052323\n",
      "Iteration 17369, loss = 1.32051450\n",
      "Iteration 17370, loss = 1.32050549\n",
      "Iteration 17371, loss = 1.32049622\n",
      "Iteration 17372, loss = 1.32048673\n",
      "Iteration 17373, loss = 1.32047704\n",
      "Iteration 17374, loss = 1.32046741\n",
      "Iteration 17375, loss = 1.32045799\n",
      "Iteration 17376, loss = 1.32044850\n",
      "Iteration 17377, loss = 1.32043894\n",
      "Iteration 17378, loss = 1.32042933\n",
      "Iteration 17379, loss = 1.32041967\n",
      "Iteration 17380, loss = 1.32040998\n",
      "Iteration 17381, loss = 1.32040027\n",
      "Iteration 17382, loss = 1.32039060\n",
      "Iteration 17383, loss = 1.32038108\n",
      "Iteration 17384, loss = 1.32037158\n",
      "Iteration 17385, loss = 1.32036204\n",
      "Iteration 17386, loss = 1.32035246\n",
      "Iteration 17387, loss = 1.32034280\n",
      "Iteration 17388, loss = 1.32033321\n",
      "Iteration 17389, loss = 1.32032366\n",
      "Iteration 17390, loss = 1.32031430\n",
      "Iteration 17391, loss = 1.32030491\n",
      "Iteration 17392, loss = 1.32029546\n",
      "Iteration 17393, loss = 1.32028605\n",
      "Iteration 17394, loss = 1.32027671\n",
      "Iteration 17395, loss = 1.32026724\n",
      "Iteration 17396, loss = 1.32025780\n",
      "Iteration 17397, loss = 1.32025144\n",
      "Iteration 17398, loss = 1.32023958\n",
      "Iteration 17399, loss = 1.32023090\n",
      "Iteration 17400, loss = 1.32022218\n",
      "Iteration 17401, loss = 1.32021358\n",
      "Iteration 17402, loss = 1.32020501\n",
      "Iteration 17403, loss = 1.32019629\n",
      "Iteration 17404, loss = 1.32018738\n",
      "Iteration 17405, loss = 1.32017830\n",
      "Iteration 17406, loss = 1.32016906\n",
      "Iteration 17407, loss = 1.32015967\n",
      "Iteration 17408, loss = 1.32015030\n",
      "Iteration 17409, loss = 1.32014099\n",
      "Iteration 17410, loss = 1.32013174\n",
      "Iteration 17411, loss = 1.32012245\n",
      "Iteration 17412, loss = 1.32011310\n",
      "Iteration 17413, loss = 1.32010374\n",
      "Iteration 17414, loss = 1.32009436\n",
      "Iteration 17415, loss = 1.32008499\n",
      "Iteration 17416, loss = 1.32007563\n",
      "Iteration 17417, loss = 1.32007200\n",
      "Iteration 17418, loss = 1.32005758\n",
      "Iteration 17419, loss = 1.32004893\n",
      "Iteration 17420, loss = 1.32004023\n",
      "Iteration 17421, loss = 1.32003149\n",
      "Iteration 17422, loss = 1.32002271\n",
      "Iteration 17423, loss = 1.32001388\n",
      "Iteration 17424, loss = 1.32000489\n",
      "Iteration 17425, loss = 1.31999574\n",
      "Iteration 17426, loss = 1.31998648\n",
      "Iteration 17427, loss = 1.31997727\n",
      "Iteration 17428, loss = 1.31996807\n",
      "Iteration 17429, loss = 1.31995887\n",
      "Iteration 17430, loss = 1.31994960\n",
      "Iteration 17431, loss = 1.31994030\n",
      "Iteration 17432, loss = 1.31993104\n",
      "Iteration 17433, loss = 1.31992178\n",
      "Iteration 17434, loss = 1.31991246\n",
      "Iteration 17435, loss = 1.31990308\n",
      "Iteration 17436, loss = 1.31989380\n",
      "Iteration 17437, loss = 1.31988736\n",
      "Iteration 17438, loss = 1.31987594\n",
      "Iteration 17439, loss = 1.31986755\n",
      "Iteration 17440, loss = 1.31985916\n",
      "Iteration 17441, loss = 1.31985064\n",
      "Iteration 17442, loss = 1.31984192\n",
      "Iteration 17443, loss = 1.31983302\n",
      "Iteration 17444, loss = 1.31982396\n",
      "Iteration 17445, loss = 1.31981476\n",
      "Iteration 17446, loss = 1.31980564\n",
      "Iteration 17447, loss = 1.31979657\n",
      "Iteration 17448, loss = 1.31978749\n",
      "Iteration 17449, loss = 1.31977834\n",
      "Iteration 17450, loss = 1.31976914\n",
      "Iteration 17451, loss = 1.31975993\n",
      "Iteration 17452, loss = 1.31975078\n",
      "Iteration 17453, loss = 1.31974158\n",
      "Iteration 17454, loss = 1.31973234\n",
      "Iteration 17455, loss = 1.31972311\n",
      "Iteration 17456, loss = 1.31971390\n",
      "Iteration 17457, loss = 1.31970513\n",
      "Iteration 17458, loss = 1.31969610\n",
      "Iteration 17459, loss = 1.31968765\n",
      "Iteration 17460, loss = 1.31967928\n",
      "Iteration 17461, loss = 1.31967086\n",
      "Iteration 17462, loss = 1.31966224\n",
      "Iteration 17463, loss = 1.31965348\n",
      "Iteration 17464, loss = 1.31964454\n",
      "Iteration 17465, loss = 1.31963545\n",
      "Iteration 17466, loss = 1.31962628\n",
      "Iteration 17467, loss = 1.31961717\n",
      "Iteration 17468, loss = 1.31960807\n",
      "Iteration 17469, loss = 1.31959895\n",
      "Iteration 17470, loss = 1.31958978\n",
      "Iteration 17471, loss = 1.31958064\n",
      "Iteration 17472, loss = 1.31957150\n",
      "Iteration 17473, loss = 1.31956232\n",
      "Iteration 17474, loss = 1.31955308\n",
      "Iteration 17475, loss = 1.31954381\n",
      "Iteration 17476, loss = 1.31953459\n",
      "Iteration 17477, loss = 1.31952542\n",
      "Iteration 17478, loss = 1.31952004\n",
      "Iteration 17479, loss = 1.31950783\n",
      "Iteration 17480, loss = 1.31949945\n",
      "Iteration 17481, loss = 1.31949121\n",
      "Iteration 17482, loss = 1.31948282\n",
      "Iteration 17483, loss = 1.31947428\n",
      "Iteration 17484, loss = 1.31946556\n",
      "Iteration 17485, loss = 1.31945666\n",
      "Iteration 17486, loss = 1.31944760\n",
      "Iteration 17487, loss = 1.31943850\n",
      "Iteration 17488, loss = 1.31942941\n",
      "Iteration 17489, loss = 1.31942032\n",
      "Iteration 17490, loss = 1.31941126\n",
      "Iteration 17491, loss = 1.31940216\n",
      "Iteration 17492, loss = 1.31939303\n",
      "Iteration 17493, loss = 1.31938393\n",
      "Iteration 17494, loss = 1.31937475\n",
      "Iteration 17495, loss = 1.31936552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17496, loss = 1.31935635\n",
      "Iteration 17497, loss = 1.31934715\n",
      "Iteration 17498, loss = 1.31933800\n",
      "Iteration 17499, loss = 1.31933638\n",
      "Iteration 17500, loss = 1.31932128\n",
      "Iteration 17501, loss = 1.31931414\n",
      "Iteration 17502, loss = 1.31930712\n",
      "Iteration 17503, loss = 1.31929985\n",
      "Iteration 17504, loss = 1.31929230\n",
      "Iteration 17505, loss = 1.31928445\n",
      "Iteration 17506, loss = 1.31927631\n",
      "Iteration 17507, loss = 1.31926789\n",
      "Iteration 17508, loss = 1.31925922\n",
      "Iteration 17509, loss = 1.31925030\n",
      "Iteration 17510, loss = 1.31924116\n",
      "Iteration 17511, loss = 1.31923185\n",
      "Iteration 17512, loss = 1.31922265\n",
      "Iteration 17513, loss = 1.31921352\n",
      "Iteration 17514, loss = 1.31920435\n",
      "Iteration 17515, loss = 1.31919511\n",
      "Iteration 17516, loss = 1.31918581\n",
      "Iteration 17517, loss = 1.31917648\n",
      "Iteration 17518, loss = 1.31916711\n",
      "Iteration 17519, loss = 1.31915773\n",
      "Iteration 17520, loss = 1.31914843\n",
      "Iteration 17521, loss = 1.31913921\n",
      "Iteration 17522, loss = 1.31912996\n",
      "Iteration 17523, loss = 1.31912078\n",
      "Iteration 17524, loss = 1.31911161\n",
      "Iteration 17525, loss = 1.31910237\n",
      "Iteration 17526, loss = 1.31909311\n",
      "Iteration 17527, loss = 1.31908393\n",
      "Iteration 17528, loss = 1.31907477\n",
      "Iteration 17529, loss = 1.31906562\n",
      "Iteration 17530, loss = 1.31905655\n",
      "Iteration 17531, loss = 1.31904748\n",
      "Iteration 17532, loss = 1.31903845\n",
      "Iteration 17533, loss = 1.31902941\n",
      "Iteration 17534, loss = 1.31902029\n",
      "Iteration 17535, loss = 1.31901279\n",
      "Iteration 17536, loss = 1.31900280\n",
      "Iteration 17537, loss = 1.31899454\n",
      "Iteration 17538, loss = 1.31898634\n",
      "Iteration 17539, loss = 1.31897815\n",
      "Iteration 17540, loss = 1.31896979\n",
      "Iteration 17541, loss = 1.31896130\n",
      "Iteration 17542, loss = 1.31895266\n",
      "Iteration 17543, loss = 1.31894385\n",
      "Iteration 17544, loss = 1.31893490\n",
      "Iteration 17545, loss = 1.31892581\n",
      "Iteration 17546, loss = 1.31891676\n",
      "Iteration 17547, loss = 1.31890781\n",
      "Iteration 17548, loss = 1.31889885\n",
      "Iteration 17549, loss = 1.31888984\n",
      "Iteration 17550, loss = 1.31888080\n",
      "Iteration 17551, loss = 1.31887173\n",
      "Iteration 17552, loss = 1.31886269\n",
      "Iteration 17553, loss = 1.31885362\n",
      "Iteration 17554, loss = 1.31884768\n",
      "Iteration 17555, loss = 1.31883621\n",
      "Iteration 17556, loss = 1.31882788\n",
      "Iteration 17557, loss = 1.31881946\n",
      "Iteration 17558, loss = 1.31881100\n",
      "Iteration 17559, loss = 1.31880248\n",
      "Iteration 17560, loss = 1.31879392\n",
      "Iteration 17561, loss = 1.31878526\n",
      "Iteration 17562, loss = 1.31877649\n",
      "Iteration 17563, loss = 1.31876761\n",
      "Iteration 17564, loss = 1.31875865\n",
      "Iteration 17565, loss = 1.31874972\n",
      "Iteration 17566, loss = 1.31874077\n",
      "Iteration 17567, loss = 1.31873184\n",
      "Iteration 17568, loss = 1.31872293\n",
      "Iteration 17569, loss = 1.31871395\n",
      "Iteration 17570, loss = 1.31870496\n",
      "Iteration 17571, loss = 1.31869600\n",
      "Iteration 17572, loss = 1.31868710\n",
      "Iteration 17573, loss = 1.31867816\n",
      "Iteration 17574, loss = 1.31867089\n",
      "Iteration 17575, loss = 1.31866083\n",
      "Iteration 17576, loss = 1.31865279\n",
      "Iteration 17577, loss = 1.31864477\n",
      "Iteration 17578, loss = 1.31863660\n",
      "Iteration 17579, loss = 1.31862824\n",
      "Iteration 17580, loss = 1.31861969\n",
      "Iteration 17581, loss = 1.31861098\n",
      "Iteration 17582, loss = 1.31860215\n",
      "Iteration 17583, loss = 1.31859331\n",
      "Iteration 17584, loss = 1.31858454\n",
      "Iteration 17585, loss = 1.31857575\n",
      "Iteration 17586, loss = 1.31856689\n",
      "Iteration 17587, loss = 1.31855802\n",
      "Iteration 17588, loss = 1.31854912\n",
      "Iteration 17589, loss = 1.31854019\n",
      "Iteration 17590, loss = 1.31853122\n",
      "Iteration 17591, loss = 1.31852225\n",
      "Iteration 17592, loss = 1.31851334\n",
      "Iteration 17593, loss = 1.31850439\n",
      "Iteration 17594, loss = 1.31849543\n",
      "Iteration 17595, loss = 1.31849180\n",
      "Iteration 17596, loss = 1.31847825\n",
      "Iteration 17597, loss = 1.31847011\n",
      "Iteration 17598, loss = 1.31846192\n",
      "Iteration 17599, loss = 1.31845379\n",
      "Iteration 17600, loss = 1.31844551\n",
      "Iteration 17601, loss = 1.31843704\n",
      "Iteration 17602, loss = 1.31842840\n",
      "Iteration 17603, loss = 1.31841962\n",
      "Iteration 17604, loss = 1.31841079\n",
      "Iteration 17605, loss = 1.31840201\n",
      "Iteration 17606, loss = 1.31839326\n",
      "Iteration 17607, loss = 1.31838444\n",
      "Iteration 17608, loss = 1.31837560\n",
      "Iteration 17609, loss = 1.31836679\n",
      "Iteration 17610, loss = 1.31835793\n",
      "Iteration 17611, loss = 1.31834903\n",
      "Iteration 17612, loss = 1.31834016\n",
      "Iteration 17613, loss = 1.31833130\n",
      "Iteration 17614, loss = 1.31832244\n",
      "Iteration 17615, loss = 1.31831783\n",
      "Iteration 17616, loss = 1.31830540\n",
      "Iteration 17617, loss = 1.31829741\n",
      "Iteration 17618, loss = 1.31828946\n",
      "Iteration 17619, loss = 1.31828145\n",
      "Iteration 17620, loss = 1.31827324\n",
      "Iteration 17621, loss = 1.31826485\n",
      "Iteration 17622, loss = 1.31825628\n",
      "Iteration 17623, loss = 1.31824755\n",
      "Iteration 17624, loss = 1.31823870\n",
      "Iteration 17625, loss = 1.31822990\n",
      "Iteration 17626, loss = 1.31822116\n",
      "Iteration 17627, loss = 1.31821236\n",
      "Iteration 17628, loss = 1.31820354\n",
      "Iteration 17629, loss = 1.31819471\n",
      "Iteration 17630, loss = 1.31818592\n",
      "Iteration 17631, loss = 1.31817708\n",
      "Iteration 17632, loss = 1.31816815\n",
      "Iteration 17633, loss = 1.31815923\n",
      "Iteration 17634, loss = 1.31815032\n",
      "Iteration 17635, loss = 1.31814140\n",
      "Iteration 17636, loss = 1.31813969\n",
      "Iteration 17637, loss = 1.31812532\n",
      "Iteration 17638, loss = 1.31811861\n",
      "Iteration 17639, loss = 1.31811192\n",
      "Iteration 17640, loss = 1.31810498\n",
      "Iteration 17641, loss = 1.31809776\n",
      "Iteration 17642, loss = 1.31809025\n",
      "Iteration 17643, loss = 1.31808246\n",
      "Iteration 17644, loss = 1.31807439\n",
      "Iteration 17645, loss = 1.31806606\n",
      "Iteration 17646, loss = 1.31805749\n",
      "Iteration 17647, loss = 1.31804871\n",
      "Iteration 17648, loss = 1.31803974\n",
      "Iteration 17649, loss = 1.31803061\n",
      "Iteration 17650, loss = 1.31802160\n",
      "Iteration 17651, loss = 1.31801268\n",
      "Iteration 17652, loss = 1.31800375\n",
      "Iteration 17653, loss = 1.31799477\n",
      "Iteration 17654, loss = 1.31798575\n",
      "Iteration 17655, loss = 1.31797669\n",
      "Iteration 17656, loss = 1.31796762\n",
      "Iteration 17657, loss = 1.31795859\n",
      "Iteration 17658, loss = 1.31794965\n",
      "Iteration 17659, loss = 1.31794083\n",
      "Iteration 17660, loss = 1.31793210\n",
      "Iteration 17661, loss = 1.31792326\n",
      "Iteration 17662, loss = 1.31791430\n",
      "Iteration 17663, loss = 1.31790531\n",
      "Iteration 17664, loss = 1.31789631\n",
      "Iteration 17665, loss = 1.31788744\n",
      "Iteration 17666, loss = 1.31787870\n",
      "Iteration 17667, loss = 1.31786994\n",
      "Iteration 17668, loss = 1.31786127\n",
      "Iteration 17669, loss = 1.31785256\n",
      "Iteration 17670, loss = 1.31784380\n",
      "Iteration 17671, loss = 1.31783498\n",
      "Iteration 17672, loss = 1.31783004\n",
      "Iteration 17673, loss = 1.31781800\n",
      "Iteration 17674, loss = 1.31780996\n",
      "Iteration 17675, loss = 1.31780189\n",
      "Iteration 17676, loss = 1.31779386\n",
      "Iteration 17677, loss = 1.31778573\n",
      "Iteration 17678, loss = 1.31777752\n",
      "Iteration 17679, loss = 1.31776928\n",
      "Iteration 17680, loss = 1.31776088\n",
      "Iteration 17681, loss = 1.31775232\n",
      "Iteration 17682, loss = 1.31774364\n",
      "Iteration 17683, loss = 1.31773486\n",
      "Iteration 17684, loss = 1.31772610\n",
      "Iteration 17685, loss = 1.31771739\n",
      "Iteration 17686, loss = 1.31770868\n",
      "Iteration 17687, loss = 1.31769993\n",
      "Iteration 17688, loss = 1.31769116\n",
      "Iteration 17689, loss = 1.31768236\n",
      "Iteration 17690, loss = 1.31767359\n",
      "Iteration 17691, loss = 1.31767045\n",
      "Iteration 17692, loss = 1.31765677\n",
      "Iteration 17693, loss = 1.31764874\n",
      "Iteration 17694, loss = 1.31764061\n",
      "Iteration 17695, loss = 1.31763256\n",
      "Iteration 17696, loss = 1.31762444\n",
      "Iteration 17697, loss = 1.31761616\n",
      "Iteration 17698, loss = 1.31760772\n",
      "Iteration 17699, loss = 1.31759917\n",
      "Iteration 17700, loss = 1.31759053\n",
      "Iteration 17701, loss = 1.31758193\n",
      "Iteration 17702, loss = 1.31757332\n",
      "Iteration 17703, loss = 1.31756469\n",
      "Iteration 17704, loss = 1.31755607\n",
      "Iteration 17705, loss = 1.31754750\n",
      "Iteration 17706, loss = 1.31753887\n",
      "Iteration 17707, loss = 1.31753020\n",
      "Iteration 17708, loss = 1.31752157\n",
      "Iteration 17709, loss = 1.31751305\n",
      "Iteration 17710, loss = 1.31750457\n",
      "Iteration 17711, loss = 1.31749599\n",
      "Iteration 17712, loss = 1.31749050\n",
      "Iteration 17713, loss = 1.31747931\n",
      "Iteration 17714, loss = 1.31747174\n",
      "Iteration 17715, loss = 1.31746413\n",
      "Iteration 17716, loss = 1.31745632\n",
      "Iteration 17717, loss = 1.31744833\n",
      "Iteration 17718, loss = 1.31744014\n",
      "Iteration 17719, loss = 1.31743179\n",
      "Iteration 17720, loss = 1.31742328\n",
      "Iteration 17721, loss = 1.31741466\n",
      "Iteration 17722, loss = 1.31740613\n",
      "Iteration 17723, loss = 1.31739763\n",
      "Iteration 17724, loss = 1.31738906\n",
      "Iteration 17725, loss = 1.31738043\n",
      "Iteration 17726, loss = 1.31737176\n",
      "Iteration 17727, loss = 1.31736306\n",
      "Iteration 17728, loss = 1.31735432\n",
      "Iteration 17729, loss = 1.31734557\n",
      "Iteration 17730, loss = 1.31733683\n",
      "Iteration 17731, loss = 1.31732815\n",
      "Iteration 17732, loss = 1.31732016\n",
      "Iteration 17733, loss = 1.31731220\n",
      "Iteration 17734, loss = 1.31730417\n",
      "Iteration 17735, loss = 1.31729619\n",
      "Iteration 17736, loss = 1.31728814\n",
      "Iteration 17737, loss = 1.31727994\n",
      "Iteration 17738, loss = 1.31727162\n",
      "Iteration 17739, loss = 1.31726315\n",
      "Iteration 17740, loss = 1.31725455\n",
      "Iteration 17741, loss = 1.31724594\n",
      "Iteration 17742, loss = 1.31723743\n",
      "Iteration 17743, loss = 1.31722895\n",
      "Iteration 17744, loss = 1.31722044\n",
      "Iteration 17745, loss = 1.31721197\n",
      "Iteration 17746, loss = 1.31720353\n",
      "Iteration 17747, loss = 1.31719507\n",
      "Iteration 17748, loss = 1.31718652\n",
      "Iteration 17749, loss = 1.31717793\n",
      "Iteration 17750, loss = 1.31716940\n",
      "Iteration 17751, loss = 1.31716088\n",
      "Iteration 17752, loss = 1.31715575\n",
      "Iteration 17753, loss = 1.31714461\n",
      "Iteration 17754, loss = 1.31713704\n",
      "Iteration 17755, loss = 1.31712935\n",
      "Iteration 17756, loss = 1.31712161\n",
      "Iteration 17757, loss = 1.31711370\n",
      "Iteration 17758, loss = 1.31710560\n",
      "Iteration 17759, loss = 1.31709734\n",
      "Iteration 17760, loss = 1.31708891\n",
      "Iteration 17761, loss = 1.31708041\n",
      "Iteration 17762, loss = 1.31707185\n",
      "Iteration 17763, loss = 1.31706333\n",
      "Iteration 17764, loss = 1.31705483\n",
      "Iteration 17765, loss = 1.31704627\n",
      "Iteration 17766, loss = 1.31703767\n",
      "Iteration 17767, loss = 1.31702904\n",
      "Iteration 17768, loss = 1.31702040\n",
      "Iteration 17769, loss = 1.31701184\n",
      "Iteration 17770, loss = 1.31700330\n",
      "Iteration 17771, loss = 1.31699472\n",
      "Iteration 17772, loss = 1.31699220\n",
      "Iteration 17773, loss = 1.31697908\n",
      "Iteration 17774, loss = 1.31697242\n",
      "Iteration 17775, loss = 1.31696597\n",
      "Iteration 17776, loss = 1.31695934\n",
      "Iteration 17777, loss = 1.31695244\n",
      "Iteration 17778, loss = 1.31694524\n",
      "Iteration 17779, loss = 1.31693777\n",
      "Iteration 17780, loss = 1.31693002\n",
      "Iteration 17781, loss = 1.31692202\n",
      "Iteration 17782, loss = 1.31691378\n",
      "Iteration 17783, loss = 1.31690533\n",
      "Iteration 17784, loss = 1.31689669\n",
      "Iteration 17785, loss = 1.31688793\n",
      "Iteration 17786, loss = 1.31687936\n",
      "Iteration 17787, loss = 1.31687077\n",
      "Iteration 17788, loss = 1.31686212\n",
      "Iteration 17789, loss = 1.31685341\n",
      "Iteration 17790, loss = 1.31684465\n",
      "Iteration 17791, loss = 1.31683587\n",
      "Iteration 17792, loss = 1.31682707\n",
      "Iteration 17793, loss = 1.31681829\n",
      "Iteration 17794, loss = 1.31680956\n",
      "Iteration 17795, loss = 1.31680104\n",
      "Iteration 17796, loss = 1.31679252\n",
      "Iteration 17797, loss = 1.31678393\n",
      "Iteration 17798, loss = 1.31677538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17799, loss = 1.31676674\n",
      "Iteration 17800, loss = 1.31675807\n",
      "Iteration 17801, loss = 1.31674939\n",
      "Iteration 17802, loss = 1.31674080\n",
      "Iteration 17803, loss = 1.31673240\n",
      "Iteration 17804, loss = 1.31672398\n",
      "Iteration 17805, loss = 1.31671551\n",
      "Iteration 17806, loss = 1.31670702\n",
      "Iteration 17807, loss = 1.31669849\n",
      "Iteration 17808, loss = 1.31668994\n",
      "Iteration 17809, loss = 1.31668440\n",
      "Iteration 17810, loss = 1.31667371\n",
      "Iteration 17811, loss = 1.31666610\n",
      "Iteration 17812, loss = 1.31665844\n",
      "Iteration 17813, loss = 1.31665074\n",
      "Iteration 17814, loss = 1.31664295\n",
      "Iteration 17815, loss = 1.31663500\n",
      "Iteration 17816, loss = 1.31662695\n",
      "Iteration 17817, loss = 1.31661876\n",
      "Iteration 17818, loss = 1.31661043\n",
      "Iteration 17819, loss = 1.31660199\n",
      "Iteration 17820, loss = 1.31659353\n",
      "Iteration 17821, loss = 1.31658507\n",
      "Iteration 17822, loss = 1.31657663\n",
      "Iteration 17823, loss = 1.31656821\n",
      "Iteration 17824, loss = 1.31655975\n",
      "Iteration 17825, loss = 1.31655129\n",
      "Iteration 17826, loss = 1.31654285\n",
      "Iteration 17827, loss = 1.31653696\n",
      "Iteration 17828, loss = 1.31652667\n",
      "Iteration 17829, loss = 1.31651896\n",
      "Iteration 17830, loss = 1.31651116\n",
      "Iteration 17831, loss = 1.31650334\n",
      "Iteration 17832, loss = 1.31649544\n",
      "Iteration 17833, loss = 1.31648747\n",
      "Iteration 17834, loss = 1.31647937\n",
      "Iteration 17835, loss = 1.31647115\n",
      "Iteration 17836, loss = 1.31646284\n",
      "Iteration 17837, loss = 1.31645450\n",
      "Iteration 17838, loss = 1.31644617\n",
      "Iteration 17839, loss = 1.31643786\n",
      "Iteration 17840, loss = 1.31642957\n",
      "Iteration 17841, loss = 1.31642124\n",
      "Iteration 17842, loss = 1.31641283\n",
      "Iteration 17843, loss = 1.31640437\n",
      "Iteration 17844, loss = 1.31639593\n",
      "Iteration 17845, loss = 1.31638759\n",
      "Iteration 17846, loss = 1.31637929\n",
      "Iteration 17847, loss = 1.31637101\n",
      "Iteration 17848, loss = 1.31636381\n",
      "Iteration 17849, loss = 1.31635499\n",
      "Iteration 17850, loss = 1.31634756\n",
      "Iteration 17851, loss = 1.31634010\n",
      "Iteration 17852, loss = 1.31633251\n",
      "Iteration 17853, loss = 1.31632477\n",
      "Iteration 17854, loss = 1.31631685\n",
      "Iteration 17855, loss = 1.31630878\n",
      "Iteration 17856, loss = 1.31630056\n",
      "Iteration 17857, loss = 1.31629230\n",
      "Iteration 17858, loss = 1.31628399\n",
      "Iteration 17859, loss = 1.31627570\n",
      "Iteration 17860, loss = 1.31626742\n",
      "Iteration 17861, loss = 1.31625910\n",
      "Iteration 17862, loss = 1.31625073\n",
      "Iteration 17863, loss = 1.31624231\n",
      "Iteration 17864, loss = 1.31623392\n",
      "Iteration 17865, loss = 1.31622555\n",
      "Iteration 17866, loss = 1.31621726\n",
      "Iteration 17867, loss = 1.31620894\n",
      "Iteration 17868, loss = 1.31620116\n",
      "Iteration 17869, loss = 1.31619363\n",
      "Iteration 17870, loss = 1.31618605\n",
      "Iteration 17871, loss = 1.31617842\n",
      "Iteration 17872, loss = 1.31617074\n",
      "Iteration 17873, loss = 1.31616288\n",
      "Iteration 17874, loss = 1.31615485\n",
      "Iteration 17875, loss = 1.31614668\n",
      "Iteration 17876, loss = 1.31613842\n",
      "Iteration 17877, loss = 1.31613015\n",
      "Iteration 17878, loss = 1.31612192\n",
      "Iteration 17879, loss = 1.31611365\n",
      "Iteration 17880, loss = 1.31610544\n",
      "Iteration 17881, loss = 1.31609727\n",
      "Iteration 17882, loss = 1.31608900\n",
      "Iteration 17883, loss = 1.31608067\n",
      "Iteration 17884, loss = 1.31607231\n",
      "Iteration 17885, loss = 1.31606402\n",
      "Iteration 17886, loss = 1.31605571\n",
      "Iteration 17887, loss = 1.31604745\n",
      "Iteration 17888, loss = 1.31604069\n",
      "Iteration 17889, loss = 1.31603160\n",
      "Iteration 17890, loss = 1.31602412\n",
      "Iteration 17891, loss = 1.31601663\n",
      "Iteration 17892, loss = 1.31600916\n",
      "Iteration 17893, loss = 1.31600153\n",
      "Iteration 17894, loss = 1.31599372\n",
      "Iteration 17895, loss = 1.31598574\n",
      "Iteration 17896, loss = 1.31597761\n",
      "Iteration 17897, loss = 1.31596936\n",
      "Iteration 17898, loss = 1.31596108\n",
      "Iteration 17899, loss = 1.31595294\n",
      "Iteration 17900, loss = 1.31594478\n",
      "Iteration 17901, loss = 1.31593654\n",
      "Iteration 17902, loss = 1.31592825\n",
      "Iteration 17903, loss = 1.31591994\n",
      "Iteration 17904, loss = 1.31591163\n",
      "Iteration 17905, loss = 1.31590329\n",
      "Iteration 17906, loss = 1.31589498\n",
      "Iteration 17907, loss = 1.31588665\n",
      "Iteration 17908, loss = 1.31588103\n",
      "Iteration 17909, loss = 1.31587074\n",
      "Iteration 17910, loss = 1.31586319\n",
      "Iteration 17911, loss = 1.31585564\n",
      "Iteration 17912, loss = 1.31584807\n",
      "Iteration 17913, loss = 1.31584037\n",
      "Iteration 17914, loss = 1.31583261\n",
      "Iteration 17915, loss = 1.31582469\n",
      "Iteration 17916, loss = 1.31581662\n",
      "Iteration 17917, loss = 1.31580843\n",
      "Iteration 17918, loss = 1.31580017\n",
      "Iteration 17919, loss = 1.31579201\n",
      "Iteration 17920, loss = 1.31578394\n",
      "Iteration 17921, loss = 1.31577579\n",
      "Iteration 17922, loss = 1.31576762\n",
      "Iteration 17923, loss = 1.31575944\n",
      "Iteration 17924, loss = 1.31575129\n",
      "Iteration 17925, loss = 1.31574321\n",
      "Iteration 17926, loss = 1.31573508\n",
      "Iteration 17927, loss = 1.31572707\n",
      "Iteration 17928, loss = 1.31571942\n",
      "Iteration 17929, loss = 1.31571206\n",
      "Iteration 17930, loss = 1.31570471\n",
      "Iteration 17931, loss = 1.31569727\n",
      "Iteration 17932, loss = 1.31568976\n",
      "Iteration 17933, loss = 1.31568211\n",
      "Iteration 17934, loss = 1.31567429\n",
      "Iteration 17935, loss = 1.31566630\n",
      "Iteration 17936, loss = 1.31565816\n",
      "Iteration 17937, loss = 1.31564996\n",
      "Iteration 17938, loss = 1.31564180\n",
      "Iteration 17939, loss = 1.31563365\n",
      "Iteration 17940, loss = 1.31562546\n",
      "Iteration 17941, loss = 1.31561724\n",
      "Iteration 17942, loss = 1.31560897\n",
      "Iteration 17943, loss = 1.31560069\n",
      "Iteration 17944, loss = 1.31559236\n",
      "Iteration 17945, loss = 1.31558413\n",
      "Iteration 17946, loss = 1.31557595\n",
      "Iteration 17947, loss = 1.31556775\n",
      "Iteration 17948, loss = 1.31555960\n",
      "Iteration 17949, loss = 1.31556037\n",
      "Iteration 17950, loss = 1.31554465\n",
      "Iteration 17951, loss = 1.31553862\n",
      "Iteration 17952, loss = 1.31553254\n",
      "Iteration 17953, loss = 1.31552624\n",
      "Iteration 17954, loss = 1.31551967\n",
      "Iteration 17955, loss = 1.31551284\n",
      "Iteration 17956, loss = 1.31550574\n",
      "Iteration 17957, loss = 1.31549838\n",
      "Iteration 17958, loss = 1.31549078\n",
      "Iteration 17959, loss = 1.31548295\n",
      "Iteration 17960, loss = 1.31547491\n",
      "Iteration 17961, loss = 1.31546669\n",
      "Iteration 17962, loss = 1.31545832\n",
      "Iteration 17963, loss = 1.31544996\n",
      "Iteration 17964, loss = 1.31544161\n",
      "Iteration 17965, loss = 1.31543324\n",
      "Iteration 17966, loss = 1.31542485\n",
      "Iteration 17967, loss = 1.31541641\n",
      "Iteration 17968, loss = 1.31540795\n",
      "Iteration 17969, loss = 1.31539947\n",
      "Iteration 17970, loss = 1.31539102\n",
      "Iteration 17971, loss = 1.31538271\n",
      "Iteration 17972, loss = 1.31537441\n",
      "Iteration 17973, loss = 1.31536611\n",
      "Iteration 17974, loss = 1.31535785\n",
      "Iteration 17975, loss = 1.31534953\n",
      "Iteration 17976, loss = 1.31534121\n",
      "Iteration 17977, loss = 1.31533291\n",
      "Iteration 17978, loss = 1.31532464\n",
      "Iteration 17979, loss = 1.31531650\n",
      "Iteration 17980, loss = 1.31530837\n",
      "Iteration 17981, loss = 1.31530031\n",
      "Iteration 17982, loss = 1.31529223\n",
      "Iteration 17983, loss = 1.31528411\n",
      "Iteration 17984, loss = 1.31527593\n",
      "Iteration 17985, loss = 1.31527133\n",
      "Iteration 17986, loss = 1.31526019\n",
      "Iteration 17987, loss = 1.31525286\n",
      "Iteration 17988, loss = 1.31524546\n",
      "Iteration 17989, loss = 1.31523804\n",
      "Iteration 17990, loss = 1.31523060\n",
      "Iteration 17991, loss = 1.31522307\n",
      "Iteration 17992, loss = 1.31521539\n",
      "Iteration 17993, loss = 1.31520758\n",
      "Iteration 17994, loss = 1.31519967\n",
      "Iteration 17995, loss = 1.31519164\n",
      "Iteration 17996, loss = 1.31518355\n",
      "Iteration 17997, loss = 1.31517545\n",
      "Iteration 17998, loss = 1.31516736\n",
      "Iteration 17999, loss = 1.31515922\n",
      "Iteration 18000, loss = 1.31515107\n",
      "Iteration 18001, loss = 1.31514289\n",
      "Iteration 18002, loss = 1.31513473\n",
      "Iteration 18003, loss = 1.31512658\n",
      "Iteration 18004, loss = 1.31512106\n",
      "Iteration 18005, loss = 1.31511100\n",
      "Iteration 18006, loss = 1.31510363\n",
      "Iteration 18007, loss = 1.31509616\n",
      "Iteration 18008, loss = 1.31508860\n",
      "Iteration 18009, loss = 1.31508107\n",
      "Iteration 18010, loss = 1.31507346\n",
      "Iteration 18011, loss = 1.31506571\n",
      "Iteration 18012, loss = 1.31505782\n",
      "Iteration 18013, loss = 1.31504981\n",
      "Iteration 18014, loss = 1.31504178\n",
      "Iteration 18015, loss = 1.31503374\n",
      "Iteration 18016, loss = 1.31502569\n",
      "Iteration 18017, loss = 1.31501767\n",
      "Iteration 18018, loss = 1.31500968\n",
      "Iteration 18019, loss = 1.31500167\n",
      "Iteration 18020, loss = 1.31499367\n",
      "Iteration 18021, loss = 1.31498565\n",
      "Iteration 18022, loss = 1.31497763\n",
      "Iteration 18023, loss = 1.31496966\n",
      "Iteration 18024, loss = 1.31496467\n",
      "Iteration 18025, loss = 1.31495426\n",
      "Iteration 18026, loss = 1.31494713\n",
      "Iteration 18027, loss = 1.31494003\n",
      "Iteration 18028, loss = 1.31493283\n",
      "Iteration 18029, loss = 1.31492549\n",
      "Iteration 18030, loss = 1.31491798\n",
      "Iteration 18031, loss = 1.31491032\n",
      "Iteration 18032, loss = 1.31490251\n",
      "Iteration 18033, loss = 1.31489457\n",
      "Iteration 18034, loss = 1.31488661\n",
      "Iteration 18035, loss = 1.31487869\n",
      "Iteration 18036, loss = 1.31487071\n",
      "Iteration 18037, loss = 1.31486272\n",
      "Iteration 18038, loss = 1.31485467\n",
      "Iteration 18039, loss = 1.31484658\n",
      "Iteration 18040, loss = 1.31483849\n",
      "Iteration 18041, loss = 1.31483040\n",
      "Iteration 18042, loss = 1.31482229\n",
      "Iteration 18043, loss = 1.31481421\n",
      "Iteration 18044, loss = 1.31480624\n",
      "Iteration 18045, loss = 1.31479884\n",
      "Iteration 18046, loss = 1.31479166\n",
      "Iteration 18047, loss = 1.31478439\n",
      "Iteration 18048, loss = 1.31477703\n",
      "Iteration 18049, loss = 1.31476963\n",
      "Iteration 18050, loss = 1.31476210\n",
      "Iteration 18051, loss = 1.31475441\n",
      "Iteration 18052, loss = 1.31474659\n",
      "Iteration 18053, loss = 1.31473866\n",
      "Iteration 18054, loss = 1.31473068\n",
      "Iteration 18055, loss = 1.31472277\n",
      "Iteration 18056, loss = 1.31471491\n",
      "Iteration 18057, loss = 1.31470701\n",
      "Iteration 18058, loss = 1.31469911\n",
      "Iteration 18059, loss = 1.31469114\n",
      "Iteration 18060, loss = 1.31468317\n",
      "Iteration 18061, loss = 1.31467519\n",
      "Iteration 18062, loss = 1.31466721\n",
      "Iteration 18063, loss = 1.31465929\n",
      "Iteration 18064, loss = 1.31465273\n",
      "Iteration 18065, loss = 1.31464415\n",
      "Iteration 18066, loss = 1.31463708\n",
      "Iteration 18067, loss = 1.31462999\n",
      "Iteration 18068, loss = 1.31462283\n",
      "Iteration 18069, loss = 1.31461550\n",
      "Iteration 18070, loss = 1.31460801\n",
      "Iteration 18071, loss = 1.31460037\n",
      "Iteration 18072, loss = 1.31459259\n",
      "Iteration 18073, loss = 1.31458469\n",
      "Iteration 18074, loss = 1.31457681\n",
      "Iteration 18075, loss = 1.31456889\n",
      "Iteration 18076, loss = 1.31456095\n",
      "Iteration 18077, loss = 1.31455298\n",
      "Iteration 18078, loss = 1.31454502\n",
      "Iteration 18079, loss = 1.31453705\n",
      "Iteration 18080, loss = 1.31452911\n",
      "Iteration 18081, loss = 1.31452112\n",
      "Iteration 18082, loss = 1.31451309\n",
      "Iteration 18083, loss = 1.31450502\n",
      "Iteration 18084, loss = 1.31449705\n",
      "Iteration 18085, loss = 1.31449836\n",
      "Iteration 18086, loss = 1.31448283\n",
      "Iteration 18087, loss = 1.31447696\n",
      "Iteration 18088, loss = 1.31447110\n",
      "Iteration 18089, loss = 1.31446504\n",
      "Iteration 18090, loss = 1.31445872\n",
      "Iteration 18091, loss = 1.31445214\n",
      "Iteration 18092, loss = 1.31444530\n",
      "Iteration 18093, loss = 1.31443821\n",
      "Iteration 18094, loss = 1.31443087\n",
      "Iteration 18095, loss = 1.31442332\n",
      "Iteration 18096, loss = 1.31441555\n",
      "Iteration 18097, loss = 1.31440761\n",
      "Iteration 18098, loss = 1.31439951\n",
      "Iteration 18099, loss = 1.31439136\n",
      "Iteration 18100, loss = 1.31438324\n",
      "Iteration 18101, loss = 1.31437515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18102, loss = 1.31436702\n",
      "Iteration 18103, loss = 1.31435885\n",
      "Iteration 18104, loss = 1.31435064\n",
      "Iteration 18105, loss = 1.31434245\n",
      "Iteration 18106, loss = 1.31433432\n",
      "Iteration 18107, loss = 1.31432623\n",
      "Iteration 18108, loss = 1.31431822\n",
      "Iteration 18109, loss = 1.31431023\n",
      "Iteration 18110, loss = 1.31430232\n",
      "Iteration 18111, loss = 1.31429435\n",
      "Iteration 18112, loss = 1.31428634\n",
      "Iteration 18113, loss = 1.31427835\n",
      "Iteration 18114, loss = 1.31427037\n",
      "Iteration 18115, loss = 1.31426240\n",
      "Iteration 18116, loss = 1.31425445\n",
      "Iteration 18117, loss = 1.31424662\n",
      "Iteration 18118, loss = 1.31423877\n",
      "Iteration 18119, loss = 1.31423094\n",
      "Iteration 18120, loss = 1.31422308\n",
      "Iteration 18121, loss = 1.31421592\n",
      "Iteration 18122, loss = 1.31420791\n",
      "Iteration 18123, loss = 1.31420083\n",
      "Iteration 18124, loss = 1.31419373\n",
      "Iteration 18125, loss = 1.31418657\n",
      "Iteration 18126, loss = 1.31417930\n",
      "Iteration 18127, loss = 1.31417197\n",
      "Iteration 18128, loss = 1.31416459\n",
      "Iteration 18129, loss = 1.31415710\n",
      "Iteration 18130, loss = 1.31414950\n",
      "Iteration 18131, loss = 1.31414178\n",
      "Iteration 18132, loss = 1.31413395\n",
      "Iteration 18133, loss = 1.31412603\n",
      "Iteration 18134, loss = 1.31411810\n",
      "Iteration 18135, loss = 1.31411024\n",
      "Iteration 18136, loss = 1.31410235\n",
      "Iteration 18137, loss = 1.31409446\n",
      "Iteration 18138, loss = 1.31408654\n",
      "Iteration 18139, loss = 1.31407862\n",
      "Iteration 18140, loss = 1.31407482\n",
      "Iteration 18141, loss = 1.31406362\n",
      "Iteration 18142, loss = 1.31405652\n",
      "Iteration 18143, loss = 1.31404935\n",
      "Iteration 18144, loss = 1.31404212\n",
      "Iteration 18145, loss = 1.31403481\n",
      "Iteration 18146, loss = 1.31402740\n",
      "Iteration 18147, loss = 1.31401987\n",
      "Iteration 18148, loss = 1.31401221\n",
      "Iteration 18149, loss = 1.31400453\n",
      "Iteration 18150, loss = 1.31399681\n",
      "Iteration 18151, loss = 1.31398909\n",
      "Iteration 18152, loss = 1.31398135\n",
      "Iteration 18153, loss = 1.31397359\n",
      "Iteration 18154, loss = 1.31396588\n",
      "Iteration 18155, loss = 1.31395812\n",
      "Iteration 18156, loss = 1.31395045\n",
      "Iteration 18157, loss = 1.31394279\n",
      "Iteration 18158, loss = 1.31393507\n",
      "Iteration 18159, loss = 1.31392729\n",
      "Iteration 18160, loss = 1.31391956\n",
      "Iteration 18161, loss = 1.31391293\n",
      "Iteration 18162, loss = 1.31390481\n",
      "Iteration 18163, loss = 1.31389801\n",
      "Iteration 18164, loss = 1.31389119\n",
      "Iteration 18165, loss = 1.31388428\n",
      "Iteration 18166, loss = 1.31387720\n",
      "Iteration 18167, loss = 1.31386995\n",
      "Iteration 18168, loss = 1.31386255\n",
      "Iteration 18169, loss = 1.31385500\n",
      "Iteration 18170, loss = 1.31384735\n",
      "Iteration 18171, loss = 1.31383968\n",
      "Iteration 18172, loss = 1.31383197\n",
      "Iteration 18173, loss = 1.31382423\n",
      "Iteration 18174, loss = 1.31381646\n",
      "Iteration 18175, loss = 1.31380865\n",
      "Iteration 18176, loss = 1.31380083\n",
      "Iteration 18177, loss = 1.31379298\n",
      "Iteration 18178, loss = 1.31378510\n",
      "Iteration 18179, loss = 1.31377723\n",
      "Iteration 18180, loss = 1.31376941\n",
      "Iteration 18181, loss = 1.31376762\n",
      "Iteration 18182, loss = 1.31375442\n",
      "Iteration 18183, loss = 1.31374741\n",
      "Iteration 18184, loss = 1.31374034\n",
      "Iteration 18185, loss = 1.31373320\n",
      "Iteration 18186, loss = 1.31372607\n",
      "Iteration 18187, loss = 1.31371878\n",
      "Iteration 18188, loss = 1.31371135\n",
      "Iteration 18189, loss = 1.31370381\n",
      "Iteration 18190, loss = 1.31369618\n",
      "Iteration 18191, loss = 1.31368852\n",
      "Iteration 18192, loss = 1.31368086\n",
      "Iteration 18193, loss = 1.31367319\n",
      "Iteration 18194, loss = 1.31366557\n",
      "Iteration 18195, loss = 1.31365795\n",
      "Iteration 18196, loss = 1.31365029\n",
      "Iteration 18197, loss = 1.31364266\n",
      "Iteration 18198, loss = 1.31363500\n",
      "Iteration 18199, loss = 1.31362731\n",
      "Iteration 18200, loss = 1.31361965\n",
      "Iteration 18201, loss = 1.31361353\n",
      "Iteration 18202, loss = 1.31360503\n",
      "Iteration 18203, loss = 1.31359829\n",
      "Iteration 18204, loss = 1.31359154\n",
      "Iteration 18205, loss = 1.31358467\n",
      "Iteration 18206, loss = 1.31357763\n",
      "Iteration 18207, loss = 1.31357043\n",
      "Iteration 18208, loss = 1.31356308\n",
      "Iteration 18209, loss = 1.31355559\n",
      "Iteration 18210, loss = 1.31354796\n",
      "Iteration 18211, loss = 1.31354030\n",
      "Iteration 18212, loss = 1.31353265\n",
      "Iteration 18213, loss = 1.31352499\n",
      "Iteration 18214, loss = 1.31351730\n",
      "Iteration 18215, loss = 1.31350956\n",
      "Iteration 18216, loss = 1.31350180\n",
      "Iteration 18217, loss = 1.31349407\n",
      "Iteration 18218, loss = 1.31348633\n",
      "Iteration 18219, loss = 1.31347860\n",
      "Iteration 18220, loss = 1.31347092\n",
      "Iteration 18221, loss = 1.31346568\n",
      "Iteration 18222, loss = 1.31345607\n",
      "Iteration 18223, loss = 1.31344921\n",
      "Iteration 18224, loss = 1.31344225\n",
      "Iteration 18225, loss = 1.31343525\n",
      "Iteration 18226, loss = 1.31342816\n",
      "Iteration 18227, loss = 1.31342098\n",
      "Iteration 18228, loss = 1.31341365\n",
      "Iteration 18229, loss = 1.31340619\n",
      "Iteration 18230, loss = 1.31339862\n",
      "Iteration 18231, loss = 1.31339104\n",
      "Iteration 18232, loss = 1.31338347\n",
      "Iteration 18233, loss = 1.31337593\n",
      "Iteration 18234, loss = 1.31336837\n",
      "Iteration 18235, loss = 1.31336078\n",
      "Iteration 18236, loss = 1.31335323\n",
      "Iteration 18237, loss = 1.31334564\n",
      "Iteration 18238, loss = 1.31333802\n",
      "Iteration 18239, loss = 1.31333039\n",
      "Iteration 18240, loss = 1.31332270\n",
      "Iteration 18241, loss = 1.31331726\n",
      "Iteration 18242, loss = 1.31330824\n",
      "Iteration 18243, loss = 1.31330158\n",
      "Iteration 18244, loss = 1.31329494\n",
      "Iteration 18245, loss = 1.31328812\n",
      "Iteration 18246, loss = 1.31328114\n",
      "Iteration 18247, loss = 1.31327399\n",
      "Iteration 18248, loss = 1.31326669\n",
      "Iteration 18249, loss = 1.31325925\n",
      "Iteration 18250, loss = 1.31325171\n",
      "Iteration 18251, loss = 1.31324415\n",
      "Iteration 18252, loss = 1.31323659\n",
      "Iteration 18253, loss = 1.31322909\n",
      "Iteration 18254, loss = 1.31322152\n",
      "Iteration 18255, loss = 1.31321388\n",
      "Iteration 18256, loss = 1.31320618\n",
      "Iteration 18257, loss = 1.31319846\n",
      "Iteration 18258, loss = 1.31319075\n",
      "Iteration 18259, loss = 1.31318306\n",
      "Iteration 18260, loss = 1.31317543\n",
      "Iteration 18261, loss = 1.31316782\n",
      "Iteration 18262, loss = 1.31316195\n",
      "Iteration 18263, loss = 1.31315418\n",
      "Iteration 18264, loss = 1.31314859\n",
      "Iteration 18265, loss = 1.31314305\n",
      "Iteration 18266, loss = 1.31313730\n",
      "Iteration 18267, loss = 1.31313129\n",
      "Iteration 18268, loss = 1.31312502\n",
      "Iteration 18269, loss = 1.31311849\n",
      "Iteration 18270, loss = 1.31311172\n",
      "Iteration 18271, loss = 1.31310470\n",
      "Iteration 18272, loss = 1.31309748\n",
      "Iteration 18273, loss = 1.31309005\n",
      "Iteration 18274, loss = 1.31308244\n",
      "Iteration 18275, loss = 1.31307468\n",
      "Iteration 18276, loss = 1.31306684\n",
      "Iteration 18277, loss = 1.31305907\n",
      "Iteration 18278, loss = 1.31305127\n",
      "Iteration 18279, loss = 1.31304346\n",
      "Iteration 18280, loss = 1.31303561\n",
      "Iteration 18281, loss = 1.31302773\n",
      "Iteration 18282, loss = 1.31301983\n",
      "Iteration 18283, loss = 1.31301202\n",
      "Iteration 18284, loss = 1.31300430\n",
      "Iteration 18285, loss = 1.31299665\n",
      "Iteration 18286, loss = 1.31298900\n",
      "Iteration 18287, loss = 1.31298134\n",
      "Iteration 18288, loss = 1.31297366\n",
      "Iteration 18289, loss = 1.31296601\n",
      "Iteration 18290, loss = 1.31295829\n",
      "Iteration 18291, loss = 1.31295065\n",
      "Iteration 18292, loss = 1.31294315\n",
      "Iteration 18293, loss = 1.31293563\n",
      "Iteration 18294, loss = 1.31292809\n",
      "Iteration 18295, loss = 1.31292054\n",
      "Iteration 18296, loss = 1.31291297\n",
      "Iteration 18297, loss = 1.31290535\n",
      "Iteration 18298, loss = 1.31289774\n",
      "Iteration 18299, loss = 1.31289301\n",
      "Iteration 18300, loss = 1.31288347\n",
      "Iteration 18301, loss = 1.31287680\n",
      "Iteration 18302, loss = 1.31287007\n",
      "Iteration 18303, loss = 1.31286324\n",
      "Iteration 18304, loss = 1.31285630\n",
      "Iteration 18305, loss = 1.31284935\n",
      "Iteration 18306, loss = 1.31284229\n",
      "Iteration 18307, loss = 1.31283515\n",
      "Iteration 18308, loss = 1.31282788\n",
      "Iteration 18309, loss = 1.31282049\n",
      "Iteration 18310, loss = 1.31281300\n",
      "Iteration 18311, loss = 1.31280542\n",
      "Iteration 18312, loss = 1.31279782\n",
      "Iteration 18313, loss = 1.31279023\n",
      "Iteration 18314, loss = 1.31278268\n",
      "Iteration 18315, loss = 1.31277509\n",
      "Iteration 18316, loss = 1.31276749\n",
      "Iteration 18317, loss = 1.31275992\n",
      "Iteration 18318, loss = 1.31275595\n",
      "Iteration 18319, loss = 1.31274557\n",
      "Iteration 18320, loss = 1.31273881\n",
      "Iteration 18321, loss = 1.31273196\n",
      "Iteration 18322, loss = 1.31272505\n",
      "Iteration 18323, loss = 1.31271810\n",
      "Iteration 18324, loss = 1.31271103\n",
      "Iteration 18325, loss = 1.31270382\n",
      "Iteration 18326, loss = 1.31269652\n",
      "Iteration 18327, loss = 1.31268914\n",
      "Iteration 18328, loss = 1.31268171\n",
      "Iteration 18329, loss = 1.31267431\n",
      "Iteration 18330, loss = 1.31266690\n",
      "Iteration 18331, loss = 1.31265949\n",
      "Iteration 18332, loss = 1.31265205\n",
      "Iteration 18333, loss = 1.31264464\n",
      "Iteration 18334, loss = 1.31263726\n",
      "Iteration 18335, loss = 1.31262983\n",
      "Iteration 18336, loss = 1.31262236\n",
      "Iteration 18337, loss = 1.31261487\n",
      "Iteration 18338, loss = 1.31260745\n",
      "Iteration 18339, loss = 1.31260231\n",
      "Iteration 18340, loss = 1.31259340\n",
      "Iteration 18341, loss = 1.31258693\n",
      "Iteration 18342, loss = 1.31258046\n",
      "Iteration 18343, loss = 1.31257383\n",
      "Iteration 18344, loss = 1.31256703\n",
      "Iteration 18345, loss = 1.31256009\n",
      "Iteration 18346, loss = 1.31255299\n",
      "Iteration 18347, loss = 1.31254576\n",
      "Iteration 18348, loss = 1.31253843\n",
      "Iteration 18349, loss = 1.31253107\n",
      "Iteration 18350, loss = 1.31252369\n",
      "Iteration 18351, loss = 1.31251631\n",
      "Iteration 18352, loss = 1.31250888\n",
      "Iteration 18353, loss = 1.31250140\n",
      "Iteration 18354, loss = 1.31249392\n",
      "Iteration 18355, loss = 1.31248641\n",
      "Iteration 18356, loss = 1.31247889\n",
      "Iteration 18357, loss = 1.31247143\n",
      "Iteration 18358, loss = 1.31246401\n",
      "Iteration 18359, loss = 1.31246008\n",
      "Iteration 18360, loss = 1.31244973\n",
      "Iteration 18361, loss = 1.31244311\n",
      "Iteration 18362, loss = 1.31243643\n",
      "Iteration 18363, loss = 1.31242975\n",
      "Iteration 18364, loss = 1.31242296\n",
      "Iteration 18365, loss = 1.31241604\n",
      "Iteration 18366, loss = 1.31240897\n",
      "Iteration 18367, loss = 1.31240177\n",
      "Iteration 18368, loss = 1.31239445\n",
      "Iteration 18369, loss = 1.31238704\n",
      "Iteration 18370, loss = 1.31237963\n",
      "Iteration 18371, loss = 1.31237229\n",
      "Iteration 18372, loss = 1.31236500\n",
      "Iteration 18373, loss = 1.31235767\n",
      "Iteration 18374, loss = 1.31235030\n",
      "Iteration 18375, loss = 1.31234292\n",
      "Iteration 18376, loss = 1.31233550\n",
      "Iteration 18377, loss = 1.31232820\n",
      "Iteration 18378, loss = 1.31232091\n",
      "Iteration 18379, loss = 1.31231359\n",
      "Iteration 18380, loss = 1.31230998\n",
      "Iteration 18381, loss = 1.31229949\n",
      "Iteration 18382, loss = 1.31229302\n",
      "Iteration 18383, loss = 1.31228650\n",
      "Iteration 18384, loss = 1.31227990\n",
      "Iteration 18385, loss = 1.31227324\n",
      "Iteration 18386, loss = 1.31226642\n",
      "Iteration 18387, loss = 1.31225945\n",
      "Iteration 18388, loss = 1.31225233\n",
      "Iteration 18389, loss = 1.31224509\n",
      "Iteration 18390, loss = 1.31223775\n",
      "Iteration 18391, loss = 1.31223038\n",
      "Iteration 18392, loss = 1.31222305\n",
      "Iteration 18393, loss = 1.31221569\n",
      "Iteration 18394, loss = 1.31220829\n",
      "Iteration 18395, loss = 1.31220087\n",
      "Iteration 18396, loss = 1.31219343\n",
      "Iteration 18397, loss = 1.31218599\n",
      "Iteration 18398, loss = 1.31217853\n",
      "Iteration 18399, loss = 1.31217112\n",
      "Iteration 18400, loss = 1.31216512\n",
      "Iteration 18401, loss = 1.31215701\n",
      "Iteration 18402, loss = 1.31215043\n",
      "Iteration 18403, loss = 1.31214377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18404, loss = 1.31213712\n",
      "Iteration 18405, loss = 1.31213036\n",
      "Iteration 18406, loss = 1.31212348\n",
      "Iteration 18407, loss = 1.31211645\n",
      "Iteration 18408, loss = 1.31210930\n",
      "Iteration 18409, loss = 1.31210206\n",
      "Iteration 18410, loss = 1.31209481\n",
      "Iteration 18411, loss = 1.31208756\n",
      "Iteration 18412, loss = 1.31208024\n",
      "Iteration 18413, loss = 1.31207299\n",
      "Iteration 18414, loss = 1.31206575\n",
      "Iteration 18415, loss = 1.31205854\n",
      "Iteration 18416, loss = 1.31205127\n",
      "Iteration 18417, loss = 1.31204400\n",
      "Iteration 18418, loss = 1.31203673\n",
      "Iteration 18419, loss = 1.31202945\n",
      "Iteration 18420, loss = 1.31202280\n",
      "Iteration 18421, loss = 1.31201567\n",
      "Iteration 18422, loss = 1.31200928\n",
      "Iteration 18423, loss = 1.31200290\n",
      "Iteration 18424, loss = 1.31199645\n",
      "Iteration 18425, loss = 1.31198984\n",
      "Iteration 18426, loss = 1.31198307\n",
      "Iteration 18427, loss = 1.31197615\n",
      "Iteration 18428, loss = 1.31196908\n",
      "Iteration 18429, loss = 1.31196189\n",
      "Iteration 18430, loss = 1.31195461\n",
      "Iteration 18431, loss = 1.31194735\n",
      "Iteration 18432, loss = 1.31194005\n",
      "Iteration 18433, loss = 1.31193274\n",
      "Iteration 18434, loss = 1.31192539\n",
      "Iteration 18435, loss = 1.31191801\n",
      "Iteration 18436, loss = 1.31191060\n",
      "Iteration 18437, loss = 1.31190320\n",
      "Iteration 18438, loss = 1.31189587\n",
      "Iteration 18439, loss = 1.31188853\n",
      "Iteration 18440, loss = 1.31188121\n",
      "Iteration 18441, loss = 1.31187399\n",
      "Iteration 18442, loss = 1.31187671\n",
      "Iteration 18443, loss = 1.31186080\n",
      "Iteration 18444, loss = 1.31185563\n",
      "Iteration 18445, loss = 1.31185044\n",
      "Iteration 18446, loss = 1.31184502\n",
      "Iteration 18447, loss = 1.31183936\n",
      "Iteration 18448, loss = 1.31183345\n",
      "Iteration 18449, loss = 1.31182729\n",
      "Iteration 18450, loss = 1.31182088\n",
      "Iteration 18451, loss = 1.31181424\n",
      "Iteration 18452, loss = 1.31180738\n",
      "Iteration 18453, loss = 1.31180032\n",
      "Iteration 18454, loss = 1.31179309\n",
      "Iteration 18455, loss = 1.31178570\n",
      "Iteration 18456, loss = 1.31177818\n",
      "Iteration 18457, loss = 1.31177066\n",
      "Iteration 18458, loss = 1.31176317\n",
      "Iteration 18459, loss = 1.31175567\n",
      "Iteration 18460, loss = 1.31174815\n",
      "Iteration 18461, loss = 1.31174060\n",
      "Iteration 18462, loss = 1.31173303\n",
      "Iteration 18463, loss = 1.31172558\n",
      "Iteration 18464, loss = 1.31171814\n",
      "Iteration 18465, loss = 1.31171071\n",
      "Iteration 18466, loss = 1.31170333\n",
      "Iteration 18467, loss = 1.31169598\n",
      "Iteration 18468, loss = 1.31168865\n",
      "Iteration 18469, loss = 1.31168137\n",
      "Iteration 18470, loss = 1.31167404\n",
      "Iteration 18471, loss = 1.31166671\n",
      "Iteration 18472, loss = 1.31165943\n",
      "Iteration 18473, loss = 1.31165217\n",
      "Iteration 18474, loss = 1.31164487\n",
      "Iteration 18475, loss = 1.31163765\n",
      "Iteration 18476, loss = 1.31163043\n",
      "Iteration 18477, loss = 1.31162323\n",
      "Iteration 18478, loss = 1.31161599\n",
      "Iteration 18479, loss = 1.31161009\n",
      "Iteration 18480, loss = 1.31160219\n",
      "Iteration 18481, loss = 1.31159576\n",
      "Iteration 18482, loss = 1.31158934\n",
      "Iteration 18483, loss = 1.31158282\n",
      "Iteration 18484, loss = 1.31157620\n",
      "Iteration 18485, loss = 1.31156951\n",
      "Iteration 18486, loss = 1.31156272\n",
      "Iteration 18487, loss = 1.31155587\n",
      "Iteration 18488, loss = 1.31154893\n",
      "Iteration 18489, loss = 1.31154191\n",
      "Iteration 18490, loss = 1.31153480\n",
      "Iteration 18491, loss = 1.31152759\n",
      "Iteration 18492, loss = 1.31152031\n",
      "Iteration 18493, loss = 1.31151305\n",
      "Iteration 18494, loss = 1.31150580\n",
      "Iteration 18495, loss = 1.31149857\n",
      "Iteration 18496, loss = 1.31149132\n",
      "Iteration 18497, loss = 1.31148406\n",
      "Iteration 18498, loss = 1.31147678\n",
      "Iteration 18499, loss = 1.31147677\n",
      "Iteration 18500, loss = 1.31146302\n",
      "Iteration 18501, loss = 1.31145656\n",
      "Iteration 18502, loss = 1.31145005\n",
      "Iteration 18503, loss = 1.31144351\n",
      "Iteration 18504, loss = 1.31143688\n",
      "Iteration 18505, loss = 1.31143012\n",
      "Iteration 18506, loss = 1.31142323\n",
      "Iteration 18507, loss = 1.31141626\n",
      "Iteration 18508, loss = 1.31140920\n",
      "Iteration 18509, loss = 1.31140211\n",
      "Iteration 18510, loss = 1.31139505\n",
      "Iteration 18511, loss = 1.31138797\n",
      "Iteration 18512, loss = 1.31138088\n",
      "Iteration 18513, loss = 1.31137385\n",
      "Iteration 18514, loss = 1.31136683\n",
      "Iteration 18515, loss = 1.31135974\n",
      "Iteration 18516, loss = 1.31135263\n",
      "Iteration 18517, loss = 1.31134550\n",
      "Iteration 18518, loss = 1.31133840\n",
      "Iteration 18519, loss = 1.31133130\n",
      "Iteration 18520, loss = 1.31132424\n",
      "Iteration 18521, loss = 1.31131972\n",
      "Iteration 18522, loss = 1.31131067\n",
      "Iteration 18523, loss = 1.31130449\n",
      "Iteration 18524, loss = 1.31129830\n",
      "Iteration 18525, loss = 1.31129200\n",
      "Iteration 18526, loss = 1.31128554\n",
      "Iteration 18527, loss = 1.31127893\n",
      "Iteration 18528, loss = 1.31127218\n",
      "Iteration 18529, loss = 1.31126530\n",
      "Iteration 18530, loss = 1.31125830\n",
      "Iteration 18531, loss = 1.31125121\n",
      "Iteration 18532, loss = 1.31124413\n",
      "Iteration 18533, loss = 1.31123703\n",
      "Iteration 18534, loss = 1.31122988\n",
      "Iteration 18535, loss = 1.31122273\n",
      "Iteration 18536, loss = 1.31121554\n",
      "Iteration 18537, loss = 1.31120835\n",
      "Iteration 18538, loss = 1.31120116\n",
      "Iteration 18539, loss = 1.31119401\n",
      "Iteration 18540, loss = 1.31118688\n",
      "Iteration 18541, loss = 1.31118634\n",
      "Iteration 18542, loss = 1.31117327\n",
      "Iteration 18543, loss = 1.31116695\n",
      "Iteration 18544, loss = 1.31116056\n",
      "Iteration 18545, loss = 1.31115414\n",
      "Iteration 18546, loss = 1.31114764\n",
      "Iteration 18547, loss = 1.31114101\n",
      "Iteration 18548, loss = 1.31113426\n",
      "Iteration 18549, loss = 1.31112740\n",
      "Iteration 18550, loss = 1.31112043\n",
      "Iteration 18551, loss = 1.31111337\n",
      "Iteration 18552, loss = 1.31110634\n",
      "Iteration 18553, loss = 1.31109937\n",
      "Iteration 18554, loss = 1.31109241\n",
      "Iteration 18555, loss = 1.31108539\n",
      "Iteration 18556, loss = 1.31107833\n",
      "Iteration 18557, loss = 1.31107130\n",
      "Iteration 18558, loss = 1.31106425\n",
      "Iteration 18559, loss = 1.31105722\n",
      "Iteration 18560, loss = 1.31105018\n",
      "Iteration 18561, loss = 1.31104313\n",
      "Iteration 18562, loss = 1.31103608\n",
      "Iteration 18563, loss = 1.31103141\n",
      "Iteration 18564, loss = 1.31102279\n",
      "Iteration 18565, loss = 1.31101664\n",
      "Iteration 18566, loss = 1.31101045\n",
      "Iteration 18567, loss = 1.31100417\n",
      "Iteration 18568, loss = 1.31099779\n",
      "Iteration 18569, loss = 1.31099125\n",
      "Iteration 18570, loss = 1.31098457\n",
      "Iteration 18571, loss = 1.31097776\n",
      "Iteration 18572, loss = 1.31097082\n",
      "Iteration 18573, loss = 1.31096378\n",
      "Iteration 18574, loss = 1.31095670\n",
      "Iteration 18575, loss = 1.31094963\n",
      "Iteration 18576, loss = 1.31094258\n",
      "Iteration 18577, loss = 1.31093550\n",
      "Iteration 18578, loss = 1.31092839\n",
      "Iteration 18579, loss = 1.31092132\n",
      "Iteration 18580, loss = 1.31091424\n",
      "Iteration 18581, loss = 1.31090719\n",
      "Iteration 18582, loss = 1.31090015\n",
      "Iteration 18583, loss = 1.31089749\n",
      "Iteration 18584, loss = 1.31088663\n",
      "Iteration 18585, loss = 1.31088042\n",
      "Iteration 18586, loss = 1.31087412\n",
      "Iteration 18587, loss = 1.31086775\n",
      "Iteration 18588, loss = 1.31086133\n",
      "Iteration 18589, loss = 1.31085483\n",
      "Iteration 18590, loss = 1.31084819\n",
      "Iteration 18591, loss = 1.31084143\n",
      "Iteration 18592, loss = 1.31083457\n",
      "Iteration 18593, loss = 1.31082764\n",
      "Iteration 18594, loss = 1.31082066\n",
      "Iteration 18595, loss = 1.31081368\n",
      "Iteration 18596, loss = 1.31080678\n",
      "Iteration 18597, loss = 1.31079983\n",
      "Iteration 18598, loss = 1.31079282\n",
      "Iteration 18599, loss = 1.31078583\n",
      "Iteration 18600, loss = 1.31077880\n",
      "Iteration 18601, loss = 1.31077180\n",
      "Iteration 18602, loss = 1.31076480\n",
      "Iteration 18603, loss = 1.31075782\n",
      "Iteration 18604, loss = 1.31075177\n",
      "Iteration 18605, loss = 1.31074448\n",
      "Iteration 18606, loss = 1.31073841\n",
      "Iteration 18607, loss = 1.31073228\n",
      "Iteration 18608, loss = 1.31072607\n",
      "Iteration 18609, loss = 1.31071971\n",
      "Iteration 18610, loss = 1.31071320\n",
      "Iteration 18611, loss = 1.31070655\n",
      "Iteration 18612, loss = 1.31069978\n",
      "Iteration 18613, loss = 1.31069288\n",
      "Iteration 18614, loss = 1.31068595\n",
      "Iteration 18615, loss = 1.31067906\n",
      "Iteration 18616, loss = 1.31067212\n",
      "Iteration 18617, loss = 1.31066512\n",
      "Iteration 18618, loss = 1.31065813\n",
      "Iteration 18619, loss = 1.31065114\n",
      "Iteration 18620, loss = 1.31064412\n",
      "Iteration 18621, loss = 1.31063707\n",
      "Iteration 18622, loss = 1.31063002\n",
      "Iteration 18623, loss = 1.31062304\n",
      "Iteration 18624, loss = 1.31061605\n",
      "Iteration 18625, loss = 1.31061056\n",
      "Iteration 18626, loss = 1.31060277\n",
      "Iteration 18627, loss = 1.31059662\n",
      "Iteration 18628, loss = 1.31059041\n",
      "Iteration 18629, loss = 1.31058415\n",
      "Iteration 18630, loss = 1.31057782\n",
      "Iteration 18631, loss = 1.31057136\n",
      "Iteration 18632, loss = 1.31056477\n",
      "Iteration 18633, loss = 1.31055805\n",
      "Iteration 18634, loss = 1.31055122\n",
      "Iteration 18635, loss = 1.31054433\n",
      "Iteration 18636, loss = 1.31053743\n",
      "Iteration 18637, loss = 1.31053060\n",
      "Iteration 18638, loss = 1.31052375\n",
      "Iteration 18639, loss = 1.31051686\n",
      "Iteration 18640, loss = 1.31050992\n",
      "Iteration 18641, loss = 1.31050296\n",
      "Iteration 18642, loss = 1.31049595\n",
      "Iteration 18643, loss = 1.31048897\n",
      "Iteration 18644, loss = 1.31048207\n",
      "Iteration 18645, loss = 1.31047524\n",
      "Iteration 18646, loss = 1.31047356\n",
      "Iteration 18647, loss = 1.31046206\n",
      "Iteration 18648, loss = 1.31045602\n",
      "Iteration 18649, loss = 1.31044993\n",
      "Iteration 18650, loss = 1.31044378\n",
      "Iteration 18651, loss = 1.31043753\n",
      "Iteration 18652, loss = 1.31043114\n",
      "Iteration 18653, loss = 1.31042460\n",
      "Iteration 18654, loss = 1.31041792\n",
      "Iteration 18655, loss = 1.31041113\n",
      "Iteration 18656, loss = 1.31040427\n",
      "Iteration 18657, loss = 1.31039736\n",
      "Iteration 18658, loss = 1.31039044\n",
      "Iteration 18659, loss = 1.31038352\n",
      "Iteration 18660, loss = 1.31037658\n",
      "Iteration 18661, loss = 1.31036963\n",
      "Iteration 18662, loss = 1.31036271\n",
      "Iteration 18663, loss = 1.31035575\n",
      "Iteration 18664, loss = 1.31034880\n",
      "Iteration 18665, loss = 1.31034187\n",
      "Iteration 18666, loss = 1.31033495\n",
      "Iteration 18667, loss = 1.31032811\n",
      "Iteration 18668, loss = 1.31032953\n",
      "Iteration 18669, loss = 1.31031586\n",
      "Iteration 18670, loss = 1.31031101\n",
      "Iteration 18671, loss = 1.31030628\n",
      "Iteration 18672, loss = 1.31030133\n",
      "Iteration 18673, loss = 1.31029614\n",
      "Iteration 18674, loss = 1.31029070\n",
      "Iteration 18675, loss = 1.31028500\n",
      "Iteration 18676, loss = 1.31027904\n",
      "Iteration 18677, loss = 1.31027285\n",
      "Iteration 18678, loss = 1.31026643\n",
      "Iteration 18679, loss = 1.31025980\n",
      "Iteration 18680, loss = 1.31025299\n",
      "Iteration 18681, loss = 1.31024602\n",
      "Iteration 18682, loss = 1.31023890\n",
      "Iteration 18683, loss = 1.31023173\n",
      "Iteration 18684, loss = 1.31022461\n",
      "Iteration 18685, loss = 1.31021743\n",
      "Iteration 18686, loss = 1.31021021\n",
      "Iteration 18687, loss = 1.31020301\n",
      "Iteration 18688, loss = 1.31019579\n",
      "Iteration 18689, loss = 1.31018856\n",
      "Iteration 18690, loss = 1.31018135\n",
      "Iteration 18691, loss = 1.31017420\n",
      "Iteration 18692, loss = 1.31016711\n",
      "Iteration 18693, loss = 1.31016006\n",
      "Iteration 18694, loss = 1.31015306\n",
      "Iteration 18695, loss = 1.31014613\n",
      "Iteration 18696, loss = 1.31013925\n",
      "Iteration 18697, loss = 1.31013238\n",
      "Iteration 18698, loss = 1.31012552\n",
      "Iteration 18699, loss = 1.31011865\n",
      "Iteration 18700, loss = 1.31011177\n",
      "Iteration 18701, loss = 1.31010486\n",
      "Iteration 18702, loss = 1.31009798\n",
      "Iteration 18703, loss = 1.31009105\n",
      "Iteration 18704, loss = 1.31008417\n",
      "Iteration 18705, loss = 1.31007729\n",
      "Iteration 18706, loss = 1.31007197\n",
      "Iteration 18707, loss = 1.31006433\n",
      "Iteration 18708, loss = 1.31005837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18709, loss = 1.31005237\n",
      "Iteration 18710, loss = 1.31004626\n",
      "Iteration 18711, loss = 1.31004006\n",
      "Iteration 18712, loss = 1.31003376\n",
      "Iteration 18713, loss = 1.31002738\n",
      "Iteration 18714, loss = 1.31002096\n",
      "Iteration 18715, loss = 1.31001444\n",
      "Iteration 18716, loss = 1.31000783\n",
      "Iteration 18717, loss = 1.31000111\n",
      "Iteration 18718, loss = 1.30999429\n",
      "Iteration 18719, loss = 1.30998742\n",
      "Iteration 18720, loss = 1.30998052\n",
      "Iteration 18721, loss = 1.30997362\n",
      "Iteration 18722, loss = 1.30996675\n",
      "Iteration 18723, loss = 1.30995984\n",
      "Iteration 18724, loss = 1.30995292\n",
      "Iteration 18725, loss = 1.30994598\n",
      "Iteration 18726, loss = 1.30993959\n",
      "Iteration 18727, loss = 1.30993304\n",
      "Iteration 18728, loss = 1.30992694\n",
      "Iteration 18729, loss = 1.30992078\n",
      "Iteration 18730, loss = 1.30991462\n",
      "Iteration 18731, loss = 1.30990835\n",
      "Iteration 18732, loss = 1.30990195\n",
      "Iteration 18733, loss = 1.30989542\n",
      "Iteration 18734, loss = 1.30988879\n",
      "Iteration 18735, loss = 1.30988208\n",
      "Iteration 18736, loss = 1.30987538\n",
      "Iteration 18737, loss = 1.30986863\n",
      "Iteration 18738, loss = 1.30986190\n",
      "Iteration 18739, loss = 1.30985517\n",
      "Iteration 18740, loss = 1.30984841\n",
      "Iteration 18741, loss = 1.30984168\n",
      "Iteration 18742, loss = 1.30983502\n",
      "Iteration 18743, loss = 1.30982837\n",
      "Iteration 18744, loss = 1.30982172\n",
      "Iteration 18745, loss = 1.30981500\n",
      "Iteration 18746, loss = 1.30980823\n",
      "Iteration 18747, loss = 1.30980148\n",
      "Iteration 18748, loss = 1.30979476\n",
      "Iteration 18749, loss = 1.30978919\n",
      "Iteration 18750, loss = 1.30978203\n",
      "Iteration 18751, loss = 1.30977624\n",
      "Iteration 18752, loss = 1.30977041\n",
      "Iteration 18753, loss = 1.30976449\n",
      "Iteration 18754, loss = 1.30975844\n",
      "Iteration 18755, loss = 1.30975225\n",
      "Iteration 18756, loss = 1.30974592\n",
      "Iteration 18757, loss = 1.30973945\n",
      "Iteration 18758, loss = 1.30973287\n",
      "Iteration 18759, loss = 1.30972619\n",
      "Iteration 18760, loss = 1.30971949\n",
      "Iteration 18761, loss = 1.30971277\n",
      "Iteration 18762, loss = 1.30970599\n",
      "Iteration 18763, loss = 1.30969921\n",
      "Iteration 18764, loss = 1.30969240\n",
      "Iteration 18765, loss = 1.30968557\n",
      "Iteration 18766, loss = 1.30967874\n",
      "Iteration 18767, loss = 1.30967190\n",
      "Iteration 18768, loss = 1.30966506\n",
      "Iteration 18769, loss = 1.30965877\n",
      "Iteration 18770, loss = 1.30965223\n",
      "Iteration 18771, loss = 1.30964626\n",
      "Iteration 18772, loss = 1.30964022\n",
      "Iteration 18773, loss = 1.30963414\n",
      "Iteration 18774, loss = 1.30962800\n",
      "Iteration 18775, loss = 1.30962172\n",
      "Iteration 18776, loss = 1.30961532\n",
      "Iteration 18777, loss = 1.30960880\n",
      "Iteration 18778, loss = 1.30960218\n",
      "Iteration 18779, loss = 1.30959552\n",
      "Iteration 18780, loss = 1.30958885\n",
      "Iteration 18781, loss = 1.30958220\n",
      "Iteration 18782, loss = 1.30957552\n",
      "Iteration 18783, loss = 1.30956884\n",
      "Iteration 18784, loss = 1.30956221\n",
      "Iteration 18785, loss = 1.30955568\n",
      "Iteration 18786, loss = 1.30954911\n",
      "Iteration 18787, loss = 1.30954249\n",
      "Iteration 18788, loss = 1.30953581\n",
      "Iteration 18789, loss = 1.30952908\n",
      "Iteration 18790, loss = 1.30952236\n",
      "Iteration 18791, loss = 1.30951568\n",
      "Iteration 18792, loss = 1.30950898\n",
      "Iteration 18793, loss = 1.30950582\n",
      "Iteration 18794, loss = 1.30949656\n",
      "Iteration 18795, loss = 1.30949080\n",
      "Iteration 18796, loss = 1.30948500\n",
      "Iteration 18797, loss = 1.30947911\n",
      "Iteration 18798, loss = 1.30947313\n",
      "Iteration 18799, loss = 1.30946699\n",
      "Iteration 18800, loss = 1.30946072\n",
      "Iteration 18801, loss = 1.30945432\n",
      "Iteration 18802, loss = 1.30944780\n",
      "Iteration 18803, loss = 1.30944118\n",
      "Iteration 18804, loss = 1.30943448\n",
      "Iteration 18805, loss = 1.30942780\n",
      "Iteration 18806, loss = 1.30942109\n",
      "Iteration 18807, loss = 1.30941438\n",
      "Iteration 18808, loss = 1.30940764\n",
      "Iteration 18809, loss = 1.30940088\n",
      "Iteration 18810, loss = 1.30939415\n",
      "Iteration 18811, loss = 1.30938742\n",
      "Iteration 18812, loss = 1.30938068\n",
      "Iteration 18813, loss = 1.30937702\n",
      "Iteration 18814, loss = 1.30936793\n",
      "Iteration 18815, loss = 1.30936203\n",
      "Iteration 18816, loss = 1.30935604\n",
      "Iteration 18817, loss = 1.30934994\n",
      "Iteration 18818, loss = 1.30934383\n",
      "Iteration 18819, loss = 1.30933764\n",
      "Iteration 18820, loss = 1.30933135\n",
      "Iteration 18821, loss = 1.30932495\n",
      "Iteration 18822, loss = 1.30931845\n",
      "Iteration 18823, loss = 1.30931185\n",
      "Iteration 18824, loss = 1.30930524\n",
      "Iteration 18825, loss = 1.30929866\n",
      "Iteration 18826, loss = 1.30929211\n",
      "Iteration 18827, loss = 1.30928556\n",
      "Iteration 18828, loss = 1.30927898\n",
      "Iteration 18829, loss = 1.30927238\n",
      "Iteration 18830, loss = 1.30926578\n",
      "Iteration 18831, loss = 1.30925923\n",
      "Iteration 18832, loss = 1.30925265\n",
      "Iteration 18833, loss = 1.30924608\n",
      "Iteration 18834, loss = 1.30923949\n",
      "Iteration 18835, loss = 1.30923291\n",
      "Iteration 18836, loss = 1.30922951\n",
      "Iteration 18837, loss = 1.30922046\n",
      "Iteration 18838, loss = 1.30921477\n",
      "Iteration 18839, loss = 1.30920909\n",
      "Iteration 18840, loss = 1.30920331\n",
      "Iteration 18841, loss = 1.30919737\n",
      "Iteration 18842, loss = 1.30919128\n",
      "Iteration 18843, loss = 1.30918505\n",
      "Iteration 18844, loss = 1.30917869\n",
      "Iteration 18845, loss = 1.30917222\n",
      "Iteration 18846, loss = 1.30916564\n",
      "Iteration 18847, loss = 1.30915907\n",
      "Iteration 18848, loss = 1.30915246\n",
      "Iteration 18849, loss = 1.30914582\n",
      "Iteration 18850, loss = 1.30913916\n",
      "Iteration 18851, loss = 1.30913247\n",
      "Iteration 18852, loss = 1.30912575\n",
      "Iteration 18853, loss = 1.30911901\n",
      "Iteration 18854, loss = 1.30911235\n",
      "Iteration 18855, loss = 1.30910575\n",
      "Iteration 18856, loss = 1.30909919\n",
      "Iteration 18857, loss = 1.30909893\n",
      "Iteration 18858, loss = 1.30908645\n",
      "Iteration 18859, loss = 1.30908063\n",
      "Iteration 18860, loss = 1.30907480\n",
      "Iteration 18861, loss = 1.30906889\n",
      "Iteration 18862, loss = 1.30906289\n",
      "Iteration 18863, loss = 1.30905679\n",
      "Iteration 18864, loss = 1.30905060\n",
      "Iteration 18865, loss = 1.30904430\n",
      "Iteration 18866, loss = 1.30903788\n",
      "Iteration 18867, loss = 1.30903138\n",
      "Iteration 18868, loss = 1.30902486\n",
      "Iteration 18869, loss = 1.30901832\n",
      "Iteration 18870, loss = 1.30901181\n",
      "Iteration 18871, loss = 1.30900530\n",
      "Iteration 18872, loss = 1.30899876\n",
      "Iteration 18873, loss = 1.30899223\n",
      "Iteration 18874, loss = 1.30898568\n",
      "Iteration 18875, loss = 1.30897915\n",
      "Iteration 18876, loss = 1.30897260\n",
      "Iteration 18877, loss = 1.30896610\n",
      "Iteration 18878, loss = 1.30895958\n",
      "Iteration 18879, loss = 1.30895787\n",
      "Iteration 18880, loss = 1.30894712\n",
      "Iteration 18881, loss = 1.30894148\n",
      "Iteration 18882, loss = 1.30893580\n",
      "Iteration 18883, loss = 1.30893009\n",
      "Iteration 18884, loss = 1.30892424\n",
      "Iteration 18885, loss = 1.30891824\n",
      "Iteration 18886, loss = 1.30891209\n",
      "Iteration 18887, loss = 1.30890582\n",
      "Iteration 18888, loss = 1.30889943\n",
      "Iteration 18889, loss = 1.30889294\n",
      "Iteration 18890, loss = 1.30888642\n",
      "Iteration 18891, loss = 1.30887988\n",
      "Iteration 18892, loss = 1.30887332\n",
      "Iteration 18893, loss = 1.30886677\n",
      "Iteration 18894, loss = 1.30886019\n",
      "Iteration 18895, loss = 1.30885357\n",
      "Iteration 18896, loss = 1.30884694\n",
      "Iteration 18897, loss = 1.30884032\n",
      "Iteration 18898, loss = 1.30883370\n",
      "Iteration 18899, loss = 1.30882714\n",
      "Iteration 18900, loss = 1.30882060\n",
      "Iteration 18901, loss = 1.30881888\n",
      "Iteration 18902, loss = 1.30880812\n",
      "Iteration 18903, loss = 1.30880240\n",
      "Iteration 18904, loss = 1.30879665\n",
      "Iteration 18905, loss = 1.30879082\n",
      "Iteration 18906, loss = 1.30878490\n",
      "Iteration 18907, loss = 1.30877889\n",
      "Iteration 18908, loss = 1.30877275\n",
      "Iteration 18909, loss = 1.30876649\n",
      "Iteration 18910, loss = 1.30876012\n",
      "Iteration 18911, loss = 1.30875369\n",
      "Iteration 18912, loss = 1.30874727\n",
      "Iteration 18913, loss = 1.30874082\n",
      "Iteration 18914, loss = 1.30873443\n",
      "Iteration 18915, loss = 1.30872799\n",
      "Iteration 18916, loss = 1.30872151\n",
      "Iteration 18917, loss = 1.30871500\n",
      "Iteration 18918, loss = 1.30870845\n",
      "Iteration 18919, loss = 1.30870195\n",
      "Iteration 18920, loss = 1.30869543\n",
      "Iteration 18921, loss = 1.30868898\n",
      "Iteration 18922, loss = 1.30868395\n",
      "Iteration 18923, loss = 1.30867674\n",
      "Iteration 18924, loss = 1.30867114\n",
      "Iteration 18925, loss = 1.30866550\n",
      "Iteration 18926, loss = 1.30865980\n",
      "Iteration 18927, loss = 1.30865395\n",
      "Iteration 18928, loss = 1.30864795\n",
      "Iteration 18929, loss = 1.30864182\n",
      "Iteration 18930, loss = 1.30863557\n",
      "Iteration 18931, loss = 1.30862920\n",
      "Iteration 18932, loss = 1.30862278\n",
      "Iteration 18933, loss = 1.30861634\n",
      "Iteration 18934, loss = 1.30860986\n",
      "Iteration 18935, loss = 1.30860334\n",
      "Iteration 18936, loss = 1.30859683\n",
      "Iteration 18937, loss = 1.30859035\n",
      "Iteration 18938, loss = 1.30858388\n",
      "Iteration 18939, loss = 1.30857736\n",
      "Iteration 18940, loss = 1.30857084\n",
      "Iteration 18941, loss = 1.30856437\n",
      "Iteration 18942, loss = 1.30855791\n",
      "Iteration 18943, loss = 1.30855149\n",
      "Iteration 18944, loss = 1.30854510\n",
      "Iteration 18945, loss = 1.30854303\n",
      "Iteration 18946, loss = 1.30853371\n",
      "Iteration 18947, loss = 1.30852930\n",
      "Iteration 18948, loss = 1.30852499\n",
      "Iteration 18949, loss = 1.30852048\n",
      "Iteration 18950, loss = 1.30851575\n",
      "Iteration 18951, loss = 1.30851076\n",
      "Iteration 18952, loss = 1.30850552\n",
      "Iteration 18953, loss = 1.30850004\n",
      "Iteration 18954, loss = 1.30849431\n",
      "Iteration 18955, loss = 1.30848836\n",
      "Iteration 18956, loss = 1.30848221\n",
      "Iteration 18957, loss = 1.30847587\n",
      "Iteration 18958, loss = 1.30846936\n",
      "Iteration 18959, loss = 1.30846272\n",
      "Iteration 18960, loss = 1.30845596\n",
      "Iteration 18961, loss = 1.30844919\n",
      "Iteration 18962, loss = 1.30844245\n",
      "Iteration 18963, loss = 1.30843566\n",
      "Iteration 18964, loss = 1.30842885\n",
      "Iteration 18965, loss = 1.30842206\n",
      "Iteration 18966, loss = 1.30841527\n",
      "Iteration 18967, loss = 1.30840850\n",
      "Iteration 18968, loss = 1.30840175\n",
      "Iteration 18969, loss = 1.30839503\n",
      "Iteration 18970, loss = 1.30838832\n",
      "Iteration 18971, loss = 1.30838170\n",
      "Iteration 18972, loss = 1.30837514\n",
      "Iteration 18973, loss = 1.30836865\n",
      "Iteration 18974, loss = 1.30836222\n",
      "Iteration 18975, loss = 1.30835588\n",
      "Iteration 18976, loss = 1.30834953\n",
      "Iteration 18977, loss = 1.30834321\n",
      "Iteration 18978, loss = 1.30833681\n",
      "Iteration 18979, loss = 1.30833034\n",
      "Iteration 18980, loss = 1.30832381\n",
      "Iteration 18981, loss = 1.30831725\n",
      "Iteration 18982, loss = 1.30831075\n",
      "Iteration 18983, loss = 1.30830425\n",
      "Iteration 18984, loss = 1.30829785\n",
      "Iteration 18985, loss = 1.30829185\n",
      "Iteration 18986, loss = 1.30828598\n",
      "Iteration 18987, loss = 1.30828047\n",
      "Iteration 18988, loss = 1.30827488\n",
      "Iteration 18989, loss = 1.30826921\n",
      "Iteration 18990, loss = 1.30826343\n",
      "Iteration 18991, loss = 1.30825759\n",
      "Iteration 18992, loss = 1.30825167\n",
      "Iteration 18993, loss = 1.30824569\n",
      "Iteration 18994, loss = 1.30823962\n",
      "Iteration 18995, loss = 1.30823343\n",
      "Iteration 18996, loss = 1.30822714\n",
      "Iteration 18997, loss = 1.30822077\n",
      "Iteration 18998, loss = 1.30821434\n",
      "Iteration 18999, loss = 1.30820789\n",
      "Iteration 19000, loss = 1.30820143\n",
      "Iteration 19001, loss = 1.30819495\n",
      "Iteration 19002, loss = 1.30818847\n",
      "Iteration 19003, loss = 1.30818199\n",
      "Iteration 19004, loss = 1.30817550\n",
      "Iteration 19005, loss = 1.30816902\n",
      "Iteration 19006, loss = 1.30816714\n",
      "Iteration 19007, loss = 1.30815693\n",
      "Iteration 19008, loss = 1.30815127\n",
      "Iteration 19009, loss = 1.30814557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19010, loss = 1.30813981\n",
      "Iteration 19011, loss = 1.30813399\n",
      "Iteration 19012, loss = 1.30812806\n",
      "Iteration 19013, loss = 1.30812201\n",
      "Iteration 19014, loss = 1.30811585\n",
      "Iteration 19015, loss = 1.30810960\n",
      "Iteration 19016, loss = 1.30810328\n",
      "Iteration 19017, loss = 1.30809700\n",
      "Iteration 19018, loss = 1.30809070\n",
      "Iteration 19019, loss = 1.30808438\n",
      "Iteration 19020, loss = 1.30807806\n",
      "Iteration 19021, loss = 1.30807175\n",
      "Iteration 19022, loss = 1.30806550\n",
      "Iteration 19023, loss = 1.30805924\n",
      "Iteration 19024, loss = 1.30805301\n",
      "Iteration 19025, loss = 1.30804680\n",
      "Iteration 19026, loss = 1.30804054\n",
      "Iteration 19027, loss = 1.30803426\n",
      "Iteration 19028, loss = 1.30802794\n",
      "Iteration 19029, loss = 1.30802159\n",
      "Iteration 19030, loss = 1.30801530\n",
      "Iteration 19031, loss = 1.30801205\n",
      "Iteration 19032, loss = 1.30800361\n",
      "Iteration 19033, loss = 1.30799821\n",
      "Iteration 19034, loss = 1.30799273\n",
      "Iteration 19035, loss = 1.30798721\n",
      "Iteration 19036, loss = 1.30798159\n",
      "Iteration 19037, loss = 1.30797585\n",
      "Iteration 19038, loss = 1.30796997\n",
      "Iteration 19039, loss = 1.30796396\n",
      "Iteration 19040, loss = 1.30795784\n",
      "Iteration 19041, loss = 1.30795161\n",
      "Iteration 19042, loss = 1.30794531\n",
      "Iteration 19043, loss = 1.30793903\n",
      "Iteration 19044, loss = 1.30793274\n",
      "Iteration 19045, loss = 1.30792640\n",
      "Iteration 19046, loss = 1.30792003\n",
      "Iteration 19047, loss = 1.30791363\n",
      "Iteration 19048, loss = 1.30790723\n",
      "Iteration 19049, loss = 1.30790083\n",
      "Iteration 19050, loss = 1.30789446\n",
      "Iteration 19051, loss = 1.30788946\n",
      "Iteration 19052, loss = 1.30788247\n",
      "Iteration 19053, loss = 1.30787694\n",
      "Iteration 19054, loss = 1.30787133\n",
      "Iteration 19055, loss = 1.30786567\n",
      "Iteration 19056, loss = 1.30785995\n",
      "Iteration 19057, loss = 1.30785413\n",
      "Iteration 19058, loss = 1.30784818\n",
      "Iteration 19059, loss = 1.30784212\n",
      "Iteration 19060, loss = 1.30783595\n",
      "Iteration 19061, loss = 1.30782971\n",
      "Iteration 19062, loss = 1.30782346\n",
      "Iteration 19063, loss = 1.30781723\n",
      "Iteration 19064, loss = 1.30781098\n",
      "Iteration 19065, loss = 1.30780477\n",
      "Iteration 19066, loss = 1.30779857\n",
      "Iteration 19067, loss = 1.30779234\n",
      "Iteration 19068, loss = 1.30778614\n",
      "Iteration 19069, loss = 1.30777998\n",
      "Iteration 19070, loss = 1.30777379\n",
      "Iteration 19071, loss = 1.30776756\n",
      "Iteration 19072, loss = 1.30776128\n",
      "Iteration 19073, loss = 1.30775504\n",
      "Iteration 19074, loss = 1.30774880\n",
      "Iteration 19075, loss = 1.30774253\n",
      "Iteration 19076, loss = 1.30773701\n",
      "Iteration 19077, loss = 1.30773089\n",
      "Iteration 19078, loss = 1.30772558\n",
      "Iteration 19079, loss = 1.30772024\n",
      "Iteration 19080, loss = 1.30771482\n",
      "Iteration 19081, loss = 1.30770927\n",
      "Iteration 19082, loss = 1.30770357\n",
      "Iteration 19083, loss = 1.30769774\n",
      "Iteration 19084, loss = 1.30769178\n",
      "Iteration 19085, loss = 1.30768571\n",
      "Iteration 19086, loss = 1.30767953\n",
      "Iteration 19087, loss = 1.30767331\n",
      "Iteration 19088, loss = 1.30766708\n",
      "Iteration 19089, loss = 1.30766080\n",
      "Iteration 19090, loss = 1.30765449\n",
      "Iteration 19091, loss = 1.30764817\n",
      "Iteration 19092, loss = 1.30764183\n",
      "Iteration 19093, loss = 1.30763551\n",
      "Iteration 19094, loss = 1.30762918\n",
      "Iteration 19095, loss = 1.30762285\n",
      "Iteration 19096, loss = 1.30761653\n",
      "Iteration 19097, loss = 1.30761608\n",
      "Iteration 19098, loss = 1.30760462\n",
      "Iteration 19099, loss = 1.30759915\n",
      "Iteration 19100, loss = 1.30759361\n",
      "Iteration 19101, loss = 1.30758802\n",
      "Iteration 19102, loss = 1.30758236\n",
      "Iteration 19103, loss = 1.30757660\n",
      "Iteration 19104, loss = 1.30757073\n",
      "Iteration 19105, loss = 1.30756475\n",
      "Iteration 19106, loss = 1.30755867\n",
      "Iteration 19107, loss = 1.30755252\n",
      "Iteration 19108, loss = 1.30754633\n",
      "Iteration 19109, loss = 1.30754016\n",
      "Iteration 19110, loss = 1.30753402\n",
      "Iteration 19111, loss = 1.30752792\n",
      "Iteration 19112, loss = 1.30752183\n",
      "Iteration 19113, loss = 1.30751572\n",
      "Iteration 19114, loss = 1.30750958\n",
      "Iteration 19115, loss = 1.30750340\n",
      "Iteration 19116, loss = 1.30749722\n",
      "Iteration 19117, loss = 1.30749103\n",
      "Iteration 19118, loss = 1.30748485\n",
      "Iteration 19119, loss = 1.30747870\n",
      "Iteration 19120, loss = 1.30747254\n",
      "Iteration 19121, loss = 1.30746639\n",
      "Iteration 19122, loss = 1.30746100\n",
      "Iteration 19123, loss = 1.30745577\n",
      "Iteration 19124, loss = 1.30745050\n",
      "Iteration 19125, loss = 1.30744513\n",
      "Iteration 19126, loss = 1.30743962\n",
      "Iteration 19127, loss = 1.30743397\n",
      "Iteration 19128, loss = 1.30742818\n",
      "Iteration 19129, loss = 1.30742227\n",
      "Iteration 19130, loss = 1.30741625\n",
      "Iteration 19131, loss = 1.30741012\n",
      "Iteration 19132, loss = 1.30740397\n",
      "Iteration 19133, loss = 1.30739780\n",
      "Iteration 19134, loss = 1.30739159\n",
      "Iteration 19135, loss = 1.30738533\n",
      "Iteration 19136, loss = 1.30737903\n",
      "Iteration 19137, loss = 1.30737275\n",
      "Iteration 19138, loss = 1.30736647\n",
      "Iteration 19139, loss = 1.30736021\n",
      "Iteration 19140, loss = 1.30735398\n",
      "Iteration 19141, loss = 1.30734778\n",
      "Iteration 19142, loss = 1.30734525\n",
      "Iteration 19143, loss = 1.30733597\n",
      "Iteration 19144, loss = 1.30733058\n",
      "Iteration 19145, loss = 1.30732513\n",
      "Iteration 19146, loss = 1.30731961\n",
      "Iteration 19147, loss = 1.30731404\n",
      "Iteration 19148, loss = 1.30730838\n",
      "Iteration 19149, loss = 1.30730260\n",
      "Iteration 19150, loss = 1.30729671\n",
      "Iteration 19151, loss = 1.30729072\n",
      "Iteration 19152, loss = 1.30728464\n",
      "Iteration 19153, loss = 1.30727855\n",
      "Iteration 19154, loss = 1.30727251\n",
      "Iteration 19155, loss = 1.30726651\n",
      "Iteration 19156, loss = 1.30726047\n",
      "Iteration 19157, loss = 1.30725437\n",
      "Iteration 19158, loss = 1.30724824\n",
      "Iteration 19159, loss = 1.30724209\n",
      "Iteration 19160, loss = 1.30723598\n",
      "Iteration 19161, loss = 1.30722989\n",
      "Iteration 19162, loss = 1.30722383\n",
      "Iteration 19163, loss = 1.30721775\n",
      "Iteration 19164, loss = 1.30721167\n",
      "Iteration 19165, loss = 1.30720555\n",
      "Iteration 19166, loss = 1.30719941\n",
      "Iteration 19167, loss = 1.30719685\n",
      "Iteration 19168, loss = 1.30718799\n",
      "Iteration 19169, loss = 1.30718281\n",
      "Iteration 19170, loss = 1.30717760\n",
      "Iteration 19171, loss = 1.30717229\n",
      "Iteration 19172, loss = 1.30716685\n",
      "Iteration 19173, loss = 1.30716128\n",
      "Iteration 19174, loss = 1.30715557\n",
      "Iteration 19175, loss = 1.30714974\n",
      "Iteration 19176, loss = 1.30714379\n",
      "Iteration 19177, loss = 1.30713774\n",
      "Iteration 19178, loss = 1.30713163\n",
      "Iteration 19179, loss = 1.30712552\n",
      "Iteration 19180, loss = 1.30711937\n",
      "Iteration 19181, loss = 1.30711318\n",
      "Iteration 19182, loss = 1.30710695\n",
      "Iteration 19183, loss = 1.30710072\n",
      "Iteration 19184, loss = 1.30709454\n",
      "Iteration 19185, loss = 1.30708835\n",
      "Iteration 19186, loss = 1.30708219\n",
      "Iteration 19187, loss = 1.30707608\n",
      "Iteration 19188, loss = 1.30707155\n",
      "Iteration 19189, loss = 1.30706442\n",
      "Iteration 19190, loss = 1.30705913\n",
      "Iteration 19191, loss = 1.30705375\n",
      "Iteration 19192, loss = 1.30704832\n",
      "Iteration 19193, loss = 1.30704282\n",
      "Iteration 19194, loss = 1.30703723\n",
      "Iteration 19195, loss = 1.30703152\n",
      "Iteration 19196, loss = 1.30702569\n",
      "Iteration 19197, loss = 1.30701976\n",
      "Iteration 19198, loss = 1.30701374\n",
      "Iteration 19199, loss = 1.30700771\n",
      "Iteration 19200, loss = 1.30700167\n",
      "Iteration 19201, loss = 1.30699568\n",
      "Iteration 19202, loss = 1.30698967\n",
      "Iteration 19203, loss = 1.30698366\n",
      "Iteration 19204, loss = 1.30697763\n",
      "Iteration 19205, loss = 1.30697159\n",
      "Iteration 19206, loss = 1.30696557\n",
      "Iteration 19207, loss = 1.30695951\n",
      "Iteration 19208, loss = 1.30695347\n",
      "Iteration 19209, loss = 1.30694742\n",
      "Iteration 19210, loss = 1.30694136\n",
      "Iteration 19211, loss = 1.30693531\n",
      "Iteration 19212, loss = 1.30692979\n",
      "Iteration 19213, loss = 1.30692406\n",
      "Iteration 19214, loss = 1.30691894\n",
      "Iteration 19215, loss = 1.30691378\n",
      "Iteration 19216, loss = 1.30690856\n",
      "Iteration 19217, loss = 1.30690321\n",
      "Iteration 19218, loss = 1.30689772\n",
      "Iteration 19219, loss = 1.30689209\n",
      "Iteration 19220, loss = 1.30688633\n",
      "Iteration 19221, loss = 1.30688046\n",
      "Iteration 19222, loss = 1.30687448\n",
      "Iteration 19223, loss = 1.30686843\n",
      "Iteration 19224, loss = 1.30686238\n",
      "Iteration 19225, loss = 1.30685630\n",
      "Iteration 19226, loss = 1.30685016\n",
      "Iteration 19227, loss = 1.30684399\n",
      "Iteration 19228, loss = 1.30683785\n",
      "Iteration 19229, loss = 1.30683173\n",
      "Iteration 19230, loss = 1.30682559\n",
      "Iteration 19231, loss = 1.30681945\n",
      "Iteration 19232, loss = 1.30681334\n",
      "Iteration 19233, loss = 1.30680729\n",
      "Iteration 19234, loss = 1.30680155\n",
      "Iteration 19235, loss = 1.30679579\n",
      "Iteration 19236, loss = 1.30679057\n",
      "Iteration 19237, loss = 1.30678527\n",
      "Iteration 19238, loss = 1.30677989\n",
      "Iteration 19239, loss = 1.30677448\n",
      "Iteration 19240, loss = 1.30676897\n",
      "Iteration 19241, loss = 1.30676336\n",
      "Iteration 19242, loss = 1.30675763\n",
      "Iteration 19243, loss = 1.30675178\n",
      "Iteration 19244, loss = 1.30674585\n",
      "Iteration 19245, loss = 1.30673985\n",
      "Iteration 19246, loss = 1.30673384\n",
      "Iteration 19247, loss = 1.30672785\n",
      "Iteration 19248, loss = 1.30672192\n",
      "Iteration 19249, loss = 1.30671599\n",
      "Iteration 19250, loss = 1.30671002\n",
      "Iteration 19251, loss = 1.30670400\n",
      "Iteration 19252, loss = 1.30669802\n",
      "Iteration 19253, loss = 1.30669198\n",
      "Iteration 19254, loss = 1.30668601\n",
      "Iteration 19255, loss = 1.30668005\n",
      "Iteration 19256, loss = 1.30667407\n",
      "Iteration 19257, loss = 1.30666805\n",
      "Iteration 19258, loss = 1.30666453\n",
      "Iteration 19259, loss = 1.30665685\n",
      "Iteration 19260, loss = 1.30665177\n",
      "Iteration 19261, loss = 1.30664664\n",
      "Iteration 19262, loss = 1.30664145\n",
      "Iteration 19263, loss = 1.30663614\n",
      "Iteration 19264, loss = 1.30663069\n",
      "Iteration 19265, loss = 1.30662511\n",
      "Iteration 19266, loss = 1.30661941\n",
      "Iteration 19267, loss = 1.30661359\n",
      "Iteration 19268, loss = 1.30660768\n",
      "Iteration 19269, loss = 1.30660170\n",
      "Iteration 19270, loss = 1.30659573\n",
      "Iteration 19271, loss = 1.30658975\n",
      "Iteration 19272, loss = 1.30658372\n",
      "Iteration 19273, loss = 1.30657768\n",
      "Iteration 19274, loss = 1.30657162\n",
      "Iteration 19275, loss = 1.30656552\n",
      "Iteration 19276, loss = 1.30655943\n",
      "Iteration 19277, loss = 1.30655337\n",
      "Iteration 19278, loss = 1.30654730\n",
      "Iteration 19279, loss = 1.30654136\n",
      "Iteration 19280, loss = 1.30653547\n",
      "Iteration 19281, loss = 1.30653405\n",
      "Iteration 19282, loss = 1.30652403\n",
      "Iteration 19283, loss = 1.30651886\n",
      "Iteration 19284, loss = 1.30651364\n",
      "Iteration 19285, loss = 1.30650838\n",
      "Iteration 19286, loss = 1.30650308\n",
      "Iteration 19287, loss = 1.30649768\n",
      "Iteration 19288, loss = 1.30649216\n",
      "Iteration 19289, loss = 1.30648651\n",
      "Iteration 19290, loss = 1.30648076\n",
      "Iteration 19291, loss = 1.30647491\n",
      "Iteration 19292, loss = 1.30646900\n",
      "Iteration 19293, loss = 1.30646308\n",
      "Iteration 19294, loss = 1.30645717\n",
      "Iteration 19295, loss = 1.30645124\n",
      "Iteration 19296, loss = 1.30644528\n",
      "Iteration 19297, loss = 1.30643932\n",
      "Iteration 19298, loss = 1.30643333\n",
      "Iteration 19299, loss = 1.30642737\n",
      "Iteration 19300, loss = 1.30642145\n",
      "Iteration 19301, loss = 1.30641564\n",
      "Iteration 19302, loss = 1.30640981\n",
      "Iteration 19303, loss = 1.30640395\n",
      "Iteration 19304, loss = 1.30640207\n",
      "Iteration 19305, loss = 1.30639254\n",
      "Iteration 19306, loss = 1.30638752\n",
      "Iteration 19307, loss = 1.30638249\n",
      "Iteration 19308, loss = 1.30637742\n",
      "Iteration 19309, loss = 1.30637222\n",
      "Iteration 19310, loss = 1.30636688\n",
      "Iteration 19311, loss = 1.30636140\n",
      "Iteration 19312, loss = 1.30635580\n",
      "Iteration 19313, loss = 1.30635008\n",
      "Iteration 19314, loss = 1.30634425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19315, loss = 1.30633833\n",
      "Iteration 19316, loss = 1.30633241\n",
      "Iteration 19317, loss = 1.30632648\n",
      "Iteration 19318, loss = 1.30632054\n",
      "Iteration 19319, loss = 1.30631456\n",
      "Iteration 19320, loss = 1.30630854\n",
      "Iteration 19321, loss = 1.30630251\n",
      "Iteration 19322, loss = 1.30629646\n",
      "Iteration 19323, loss = 1.30629042\n",
      "Iteration 19324, loss = 1.30628452\n",
      "Iteration 19325, loss = 1.30627862\n",
      "Iteration 19326, loss = 1.30627278\n",
      "Iteration 19327, loss = 1.30626691\n",
      "Iteration 19328, loss = 1.30626541\n",
      "Iteration 19329, loss = 1.30625546\n",
      "Iteration 19330, loss = 1.30625039\n",
      "Iteration 19331, loss = 1.30624532\n",
      "Iteration 19332, loss = 1.30624015\n",
      "Iteration 19333, loss = 1.30623492\n",
      "Iteration 19334, loss = 1.30622962\n",
      "Iteration 19335, loss = 1.30622419\n",
      "Iteration 19336, loss = 1.30621864\n",
      "Iteration 19337, loss = 1.30621298\n",
      "Iteration 19338, loss = 1.30620722\n",
      "Iteration 19339, loss = 1.30620139\n",
      "Iteration 19340, loss = 1.30619553\n",
      "Iteration 19341, loss = 1.30618967\n",
      "Iteration 19342, loss = 1.30618379\n",
      "Iteration 19343, loss = 1.30617789\n",
      "Iteration 19344, loss = 1.30617200\n",
      "Iteration 19345, loss = 1.30616609\n",
      "Iteration 19346, loss = 1.30616017\n",
      "Iteration 19347, loss = 1.30615423\n",
      "Iteration 19348, loss = 1.30614840\n",
      "Iteration 19349, loss = 1.30614258\n",
      "Iteration 19350, loss = 1.30613676\n",
      "Iteration 19351, loss = 1.30613581\n",
      "Iteration 19352, loss = 1.30612555\n",
      "Iteration 19353, loss = 1.30612053\n",
      "Iteration 19354, loss = 1.30611544\n",
      "Iteration 19355, loss = 1.30611036\n",
      "Iteration 19356, loss = 1.30610519\n",
      "Iteration 19357, loss = 1.30609988\n",
      "Iteration 19358, loss = 1.30609444\n",
      "Iteration 19359, loss = 1.30608888\n",
      "Iteration 19360, loss = 1.30608320\n",
      "Iteration 19361, loss = 1.30607743\n",
      "Iteration 19362, loss = 1.30607160\n",
      "Iteration 19363, loss = 1.30606578\n",
      "Iteration 19364, loss = 1.30605995\n",
      "Iteration 19365, loss = 1.30605411\n",
      "Iteration 19366, loss = 1.30604824\n",
      "Iteration 19367, loss = 1.30604233\n",
      "Iteration 19368, loss = 1.30603639\n",
      "Iteration 19369, loss = 1.30603049\n",
      "Iteration 19370, loss = 1.30602462\n",
      "Iteration 19371, loss = 1.30601877\n",
      "Iteration 19372, loss = 1.30601291\n",
      "Iteration 19373, loss = 1.30600706\n",
      "Iteration 19374, loss = 1.30600127\n",
      "Iteration 19375, loss = 1.30599544\n",
      "Iteration 19376, loss = 1.30599416\n",
      "Iteration 19377, loss = 1.30598527\n",
      "Iteration 19378, loss = 1.30598150\n",
      "Iteration 19379, loss = 1.30597780\n",
      "Iteration 19380, loss = 1.30597392\n",
      "Iteration 19381, loss = 1.30596982\n",
      "Iteration 19382, loss = 1.30596549\n",
      "Iteration 19383, loss = 1.30596092\n",
      "Iteration 19384, loss = 1.30595610\n",
      "Iteration 19385, loss = 1.30595105\n",
      "Iteration 19386, loss = 1.30594578\n",
      "Iteration 19387, loss = 1.30594031\n",
      "Iteration 19388, loss = 1.30593465\n",
      "Iteration 19389, loss = 1.30592882\n",
      "Iteration 19390, loss = 1.30592286\n",
      "Iteration 19391, loss = 1.30591676\n",
      "Iteration 19392, loss = 1.30591057\n",
      "Iteration 19393, loss = 1.30590436\n",
      "Iteration 19394, loss = 1.30589818\n",
      "Iteration 19395, loss = 1.30589198\n",
      "Iteration 19396, loss = 1.30588576\n",
      "Iteration 19397, loss = 1.30587954\n",
      "Iteration 19398, loss = 1.30587337\n",
      "Iteration 19399, loss = 1.30586725\n",
      "Iteration 19400, loss = 1.30586116\n",
      "Iteration 19401, loss = 1.30585509\n",
      "Iteration 19402, loss = 1.30584905\n",
      "Iteration 19403, loss = 1.30584307\n",
      "Iteration 19404, loss = 1.30583722\n",
      "Iteration 19405, loss = 1.30583140\n",
      "Iteration 19406, loss = 1.30582554\n",
      "Iteration 19407, loss = 1.30581973\n",
      "Iteration 19408, loss = 1.30581395\n",
      "Iteration 19409, loss = 1.30580814\n",
      "Iteration 19410, loss = 1.30580232\n",
      "Iteration 19411, loss = 1.30579651\n",
      "Iteration 19412, loss = 1.30579066\n",
      "Iteration 19413, loss = 1.30578483\n",
      "Iteration 19414, loss = 1.30577902\n",
      "Iteration 19415, loss = 1.30577323\n",
      "Iteration 19416, loss = 1.30576742\n",
      "Iteration 19417, loss = 1.30576162\n",
      "Iteration 19418, loss = 1.30575584\n",
      "Iteration 19419, loss = 1.30575137\n",
      "Iteration 19420, loss = 1.30574518\n",
      "Iteration 19421, loss = 1.30574024\n",
      "Iteration 19422, loss = 1.30573524\n",
      "Iteration 19423, loss = 1.30573015\n",
      "Iteration 19424, loss = 1.30572499\n",
      "Iteration 19425, loss = 1.30571974\n",
      "Iteration 19426, loss = 1.30571440\n",
      "Iteration 19427, loss = 1.30570900\n",
      "Iteration 19428, loss = 1.30570355\n",
      "Iteration 19429, loss = 1.30569805\n",
      "Iteration 19430, loss = 1.30569245\n",
      "Iteration 19431, loss = 1.30568676\n",
      "Iteration 19432, loss = 1.30568100\n",
      "Iteration 19433, loss = 1.30567517\n",
      "Iteration 19434, loss = 1.30566931\n",
      "Iteration 19435, loss = 1.30566345\n",
      "Iteration 19436, loss = 1.30565759\n",
      "Iteration 19437, loss = 1.30565173\n",
      "Iteration 19438, loss = 1.30564588\n",
      "Iteration 19439, loss = 1.30564001\n",
      "Iteration 19440, loss = 1.30563414\n",
      "Iteration 19441, loss = 1.30563043\n",
      "Iteration 19442, loss = 1.30562331\n",
      "Iteration 19443, loss = 1.30561828\n",
      "Iteration 19444, loss = 1.30561323\n",
      "Iteration 19445, loss = 1.30560811\n",
      "Iteration 19446, loss = 1.30560291\n",
      "Iteration 19447, loss = 1.30559760\n",
      "Iteration 19448, loss = 1.30559218\n",
      "Iteration 19449, loss = 1.30558667\n",
      "Iteration 19450, loss = 1.30558106\n",
      "Iteration 19451, loss = 1.30557539\n",
      "Iteration 19452, loss = 1.30556970\n",
      "Iteration 19453, loss = 1.30556403\n",
      "Iteration 19454, loss = 1.30555838\n",
      "Iteration 19455, loss = 1.30555276\n",
      "Iteration 19456, loss = 1.30554712\n",
      "Iteration 19457, loss = 1.30554147\n",
      "Iteration 19458, loss = 1.30553580\n",
      "Iteration 19459, loss = 1.30553013\n",
      "Iteration 19460, loss = 1.30552450\n",
      "Iteration 19461, loss = 1.30551888\n",
      "Iteration 19462, loss = 1.30551324\n",
      "Iteration 19463, loss = 1.30550758\n",
      "Iteration 19464, loss = 1.30550190\n",
      "Iteration 19465, loss = 1.30549620\n",
      "Iteration 19466, loss = 1.30549048\n",
      "Iteration 19467, loss = 1.30548477\n",
      "Iteration 19468, loss = 1.30547906\n",
      "Iteration 19469, loss = 1.30547375\n",
      "Iteration 19470, loss = 1.30546858\n",
      "Iteration 19471, loss = 1.30546376\n",
      "Iteration 19472, loss = 1.30545890\n",
      "Iteration 19473, loss = 1.30545394\n",
      "Iteration 19474, loss = 1.30544888\n",
      "Iteration 19475, loss = 1.30544372\n",
      "Iteration 19476, loss = 1.30543844\n",
      "Iteration 19477, loss = 1.30543305\n",
      "Iteration 19478, loss = 1.30542755\n",
      "Iteration 19479, loss = 1.30542196\n",
      "Iteration 19480, loss = 1.30541629\n",
      "Iteration 19481, loss = 1.30541060\n",
      "Iteration 19482, loss = 1.30540493\n",
      "Iteration 19483, loss = 1.30539922\n",
      "Iteration 19484, loss = 1.30539348\n",
      "Iteration 19485, loss = 1.30538771\n",
      "Iteration 19486, loss = 1.30538194\n",
      "Iteration 19487, loss = 1.30537622\n",
      "Iteration 19488, loss = 1.30537055\n",
      "Iteration 19489, loss = 1.30536486\n",
      "Iteration 19490, loss = 1.30536032\n",
      "Iteration 19491, loss = 1.30535406\n",
      "Iteration 19492, loss = 1.30534924\n",
      "Iteration 19493, loss = 1.30534435\n",
      "Iteration 19494, loss = 1.30533939\n",
      "Iteration 19495, loss = 1.30533434\n",
      "Iteration 19496, loss = 1.30532919\n",
      "Iteration 19497, loss = 1.30532391\n",
      "Iteration 19498, loss = 1.30531852\n",
      "Iteration 19499, loss = 1.30531303\n",
      "Iteration 19500, loss = 1.30530744\n",
      "Iteration 19501, loss = 1.30530182\n",
      "Iteration 19502, loss = 1.30529619\n",
      "Iteration 19503, loss = 1.30529054\n",
      "Iteration 19504, loss = 1.30528492\n",
      "Iteration 19505, loss = 1.30527928\n",
      "Iteration 19506, loss = 1.30527365\n",
      "Iteration 19507, loss = 1.30526799\n",
      "Iteration 19508, loss = 1.30526232\n",
      "Iteration 19509, loss = 1.30525667\n",
      "Iteration 19510, loss = 1.30525108\n",
      "Iteration 19511, loss = 1.30524549\n",
      "Iteration 19512, loss = 1.30523993\n",
      "Iteration 19513, loss = 1.30523436\n",
      "Iteration 19514, loss = 1.30522874\n",
      "Iteration 19515, loss = 1.30522309\n",
      "Iteration 19516, loss = 1.30521740\n",
      "Iteration 19517, loss = 1.30521174\n",
      "Iteration 19518, loss = 1.30520629\n",
      "Iteration 19519, loss = 1.30520128\n",
      "Iteration 19520, loss = 1.30519656\n",
      "Iteration 19521, loss = 1.30519184\n",
      "Iteration 19522, loss = 1.30518706\n",
      "Iteration 19523, loss = 1.30518215\n",
      "Iteration 19524, loss = 1.30517712\n",
      "Iteration 19525, loss = 1.30517195\n",
      "Iteration 19526, loss = 1.30516666\n",
      "Iteration 19527, loss = 1.30516126\n",
      "Iteration 19528, loss = 1.30515576\n",
      "Iteration 19529, loss = 1.30515017\n",
      "Iteration 19530, loss = 1.30514452\n",
      "Iteration 19531, loss = 1.30513888\n",
      "Iteration 19532, loss = 1.30513321\n",
      "Iteration 19533, loss = 1.30512749\n",
      "Iteration 19534, loss = 1.30512175\n",
      "Iteration 19535, loss = 1.30511601\n",
      "Iteration 19536, loss = 1.30511027\n",
      "Iteration 19537, loss = 1.30510454\n",
      "Iteration 19538, loss = 1.30509884\n",
      "Iteration 19539, loss = 1.30509318\n",
      "Iteration 19540, loss = 1.30509216\n",
      "Iteration 19541, loss = 1.30508254\n",
      "Iteration 19542, loss = 1.30507770\n",
      "Iteration 19543, loss = 1.30507277\n",
      "Iteration 19544, loss = 1.30506779\n",
      "Iteration 19545, loss = 1.30506277\n",
      "Iteration 19546, loss = 1.30505765\n",
      "Iteration 19547, loss = 1.30505243\n",
      "Iteration 19548, loss = 1.30504711\n",
      "Iteration 19549, loss = 1.30504169\n",
      "Iteration 19550, loss = 1.30503618\n",
      "Iteration 19551, loss = 1.30503059\n",
      "Iteration 19552, loss = 1.30502499\n",
      "Iteration 19553, loss = 1.30501938\n",
      "Iteration 19554, loss = 1.30501382\n",
      "Iteration 19555, loss = 1.30500826\n",
      "Iteration 19556, loss = 1.30500274\n",
      "Iteration 19557, loss = 1.30499723\n",
      "Iteration 19558, loss = 1.30499177\n",
      "Iteration 19559, loss = 1.30498625\n",
      "Iteration 19560, loss = 1.30498072\n",
      "Iteration 19561, loss = 1.30497516\n",
      "Iteration 19562, loss = 1.30496959\n",
      "Iteration 19563, loss = 1.30496401\n",
      "Iteration 19564, loss = 1.30495843\n",
      "Iteration 19565, loss = 1.30495287\n",
      "Iteration 19566, loss = 1.30494733\n",
      "Iteration 19567, loss = 1.30494177\n",
      "Iteration 19568, loss = 1.30493942\n",
      "Iteration 19569, loss = 1.30493156\n",
      "Iteration 19570, loss = 1.30492688\n",
      "Iteration 19571, loss = 1.30492216\n",
      "Iteration 19572, loss = 1.30491738\n",
      "Iteration 19573, loss = 1.30491250\n",
      "Iteration 19574, loss = 1.30490749\n",
      "Iteration 19575, loss = 1.30490236\n",
      "Iteration 19576, loss = 1.30489711\n",
      "Iteration 19577, loss = 1.30489176\n",
      "Iteration 19578, loss = 1.30488630\n",
      "Iteration 19579, loss = 1.30488076\n",
      "Iteration 19580, loss = 1.30487519\n",
      "Iteration 19581, loss = 1.30486962\n",
      "Iteration 19582, loss = 1.30486401\n",
      "Iteration 19583, loss = 1.30485836\n",
      "Iteration 19584, loss = 1.30485268\n",
      "Iteration 19585, loss = 1.30484700\n",
      "Iteration 19586, loss = 1.30484137\n",
      "Iteration 19587, loss = 1.30483574\n",
      "Iteration 19588, loss = 1.30483012\n",
      "Iteration 19589, loss = 1.30482530\n",
      "Iteration 19590, loss = 1.30481964\n",
      "Iteration 19591, loss = 1.30481488\n",
      "Iteration 19592, loss = 1.30481003\n",
      "Iteration 19593, loss = 1.30480512\n",
      "Iteration 19594, loss = 1.30480017\n",
      "Iteration 19595, loss = 1.30479512\n",
      "Iteration 19596, loss = 1.30478995\n",
      "Iteration 19597, loss = 1.30478467\n",
      "Iteration 19598, loss = 1.30477930\n",
      "Iteration 19599, loss = 1.30477384\n",
      "Iteration 19600, loss = 1.30476831\n",
      "Iteration 19601, loss = 1.30476281\n",
      "Iteration 19602, loss = 1.30475736\n",
      "Iteration 19603, loss = 1.30475193\n",
      "Iteration 19604, loss = 1.30474648\n",
      "Iteration 19605, loss = 1.30474101\n",
      "Iteration 19606, loss = 1.30473551\n",
      "Iteration 19607, loss = 1.30473003\n",
      "Iteration 19608, loss = 1.30472453\n",
      "Iteration 19609, loss = 1.30471902\n",
      "Iteration 19610, loss = 1.30471353\n",
      "Iteration 19611, loss = 1.30470807\n",
      "Iteration 19612, loss = 1.30470262\n",
      "Iteration 19613, loss = 1.30469714\n",
      "Iteration 19614, loss = 1.30469162\n",
      "Iteration 19615, loss = 1.30468609\n",
      "Iteration 19616, loss = 1.30468057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19617, loss = 1.30467565\n",
      "Iteration 19618, loss = 1.30467044\n",
      "Iteration 19619, loss = 1.30466584\n",
      "Iteration 19620, loss = 1.30466119\n",
      "Iteration 19621, loss = 1.30465649\n",
      "Iteration 19622, loss = 1.30465166\n",
      "Iteration 19623, loss = 1.30464670\n",
      "Iteration 19624, loss = 1.30464162\n",
      "Iteration 19625, loss = 1.30463642\n",
      "Iteration 19626, loss = 1.30463112\n",
      "Iteration 19627, loss = 1.30462572\n",
      "Iteration 19628, loss = 1.30462023\n",
      "Iteration 19629, loss = 1.30461471\n",
      "Iteration 19630, loss = 1.30460919\n",
      "Iteration 19631, loss = 1.30460367\n",
      "Iteration 19632, loss = 1.30459811\n",
      "Iteration 19633, loss = 1.30459252\n",
      "Iteration 19634, loss = 1.30458691\n",
      "Iteration 19635, loss = 1.30458134\n",
      "Iteration 19636, loss = 1.30457580\n",
      "Iteration 19637, loss = 1.30457032\n",
      "Iteration 19638, loss = 1.30456486\n",
      "Iteration 19639, loss = 1.30456254\n",
      "Iteration 19640, loss = 1.30455435\n",
      "Iteration 19641, loss = 1.30454971\n",
      "Iteration 19642, loss = 1.30454502\n",
      "Iteration 19643, loss = 1.30454023\n",
      "Iteration 19644, loss = 1.30453537\n",
      "Iteration 19645, loss = 1.30453044\n",
      "Iteration 19646, loss = 1.30452539\n",
      "Iteration 19647, loss = 1.30452022\n",
      "Iteration 19648, loss = 1.30451495\n",
      "Iteration 19649, loss = 1.30450959\n",
      "Iteration 19650, loss = 1.30450416\n",
      "Iteration 19651, loss = 1.30449870\n",
      "Iteration 19652, loss = 1.30449321\n",
      "Iteration 19653, loss = 1.30448769\n",
      "Iteration 19654, loss = 1.30448221\n",
      "Iteration 19655, loss = 1.30447679\n",
      "Iteration 19656, loss = 1.30447138\n",
      "Iteration 19657, loss = 1.30446595\n",
      "Iteration 19658, loss = 1.30446052\n",
      "Iteration 19659, loss = 1.30445513\n",
      "Iteration 19660, loss = 1.30444972\n",
      "Iteration 19661, loss = 1.30444429\n",
      "Iteration 19662, loss = 1.30443883\n",
      "Iteration 19663, loss = 1.30443341\n",
      "Iteration 19664, loss = 1.30442799\n",
      "Iteration 19665, loss = 1.30442253\n",
      "Iteration 19666, loss = 1.30441708\n",
      "Iteration 19667, loss = 1.30441449\n",
      "Iteration 19668, loss = 1.30440702\n",
      "Iteration 19669, loss = 1.30440251\n",
      "Iteration 19670, loss = 1.30439796\n",
      "Iteration 19671, loss = 1.30439336\n",
      "Iteration 19672, loss = 1.30438865\n",
      "Iteration 19673, loss = 1.30438380\n",
      "Iteration 19674, loss = 1.30437882\n",
      "Iteration 19675, loss = 1.30437372\n",
      "Iteration 19676, loss = 1.30436851\n",
      "Iteration 19677, loss = 1.30436320\n",
      "Iteration 19678, loss = 1.30435781\n",
      "Iteration 19679, loss = 1.30435236\n",
      "Iteration 19680, loss = 1.30434691\n",
      "Iteration 19681, loss = 1.30434142\n",
      "Iteration 19682, loss = 1.30433590\n",
      "Iteration 19683, loss = 1.30433035\n",
      "Iteration 19684, loss = 1.30432477\n",
      "Iteration 19685, loss = 1.30431921\n",
      "Iteration 19686, loss = 1.30431366\n",
      "Iteration 19687, loss = 1.30430812\n",
      "Iteration 19688, loss = 1.30430264\n",
      "Iteration 19689, loss = 1.30429733\n",
      "Iteration 19690, loss = 1.30429241\n",
      "Iteration 19691, loss = 1.30428776\n",
      "Iteration 19692, loss = 1.30428303\n",
      "Iteration 19693, loss = 1.30427822\n",
      "Iteration 19694, loss = 1.30427337\n",
      "Iteration 19695, loss = 1.30426845\n",
      "Iteration 19696, loss = 1.30426342\n",
      "Iteration 19697, loss = 1.30425828\n",
      "Iteration 19698, loss = 1.30425304\n",
      "Iteration 19699, loss = 1.30424772\n",
      "Iteration 19700, loss = 1.30424234\n",
      "Iteration 19701, loss = 1.30423696\n",
      "Iteration 19702, loss = 1.30423159\n",
      "Iteration 19703, loss = 1.30422625\n",
      "Iteration 19704, loss = 1.30422091\n",
      "Iteration 19705, loss = 1.30421558\n",
      "Iteration 19706, loss = 1.30421024\n",
      "Iteration 19707, loss = 1.30420493\n",
      "Iteration 19708, loss = 1.30419962\n",
      "Iteration 19709, loss = 1.30419428\n",
      "Iteration 19710, loss = 1.30418891\n",
      "Iteration 19711, loss = 1.30418354\n",
      "Iteration 19712, loss = 1.30417815\n",
      "Iteration 19713, loss = 1.30417276\n",
      "Iteration 19714, loss = 1.30416737\n",
      "Iteration 19715, loss = 1.30416201\n",
      "Iteration 19716, loss = 1.30415664\n",
      "Iteration 19717, loss = 1.30415129\n",
      "Iteration 19718, loss = 1.30415016\n",
      "Iteration 19719, loss = 1.30414149\n",
      "Iteration 19720, loss = 1.30413702\n",
      "Iteration 19721, loss = 1.30413251\n",
      "Iteration 19722, loss = 1.30412792\n",
      "Iteration 19723, loss = 1.30412320\n",
      "Iteration 19724, loss = 1.30411836\n",
      "Iteration 19725, loss = 1.30411340\n",
      "Iteration 19726, loss = 1.30410832\n",
      "Iteration 19727, loss = 1.30410315\n",
      "Iteration 19728, loss = 1.30409787\n",
      "Iteration 19729, loss = 1.30409252\n",
      "Iteration 19730, loss = 1.30408709\n",
      "Iteration 19731, loss = 1.30408170\n",
      "Iteration 19732, loss = 1.30407628\n",
      "Iteration 19733, loss = 1.30407086\n",
      "Iteration 19734, loss = 1.30406540\n",
      "Iteration 19735, loss = 1.30405994\n",
      "Iteration 19736, loss = 1.30405447\n",
      "Iteration 19737, loss = 1.30404900\n",
      "Iteration 19738, loss = 1.30404360\n",
      "Iteration 19739, loss = 1.30403826\n",
      "Iteration 19740, loss = 1.30403615\n",
      "Iteration 19741, loss = 1.30402811\n",
      "Iteration 19742, loss = 1.30402356\n",
      "Iteration 19743, loss = 1.30401899\n",
      "Iteration 19744, loss = 1.30401433\n",
      "Iteration 19745, loss = 1.30400960\n",
      "Iteration 19746, loss = 1.30400478\n",
      "Iteration 19747, loss = 1.30399986\n",
      "Iteration 19748, loss = 1.30399483\n",
      "Iteration 19749, loss = 1.30398970\n",
      "Iteration 19750, loss = 1.30398448\n",
      "Iteration 19751, loss = 1.30397918\n",
      "Iteration 19752, loss = 1.30397384\n",
      "Iteration 19753, loss = 1.30396850\n",
      "Iteration 19754, loss = 1.30396317\n",
      "Iteration 19755, loss = 1.30395786\n",
      "Iteration 19756, loss = 1.30395257\n",
      "Iteration 19757, loss = 1.30394731\n",
      "Iteration 19758, loss = 1.30394201\n",
      "Iteration 19759, loss = 1.30393669\n",
      "Iteration 19760, loss = 1.30393137\n",
      "Iteration 19761, loss = 1.30392610\n",
      "Iteration 19762, loss = 1.30392080\n",
      "Iteration 19763, loss = 1.30391547\n",
      "Iteration 19764, loss = 1.30391020\n",
      "Iteration 19765, loss = 1.30390492\n",
      "Iteration 19766, loss = 1.30389959\n",
      "Iteration 19767, loss = 1.30389425\n",
      "Iteration 19768, loss = 1.30388956\n",
      "Iteration 19769, loss = 1.30388447\n",
      "Iteration 19770, loss = 1.30388006\n",
      "Iteration 19771, loss = 1.30387567\n",
      "Iteration 19772, loss = 1.30387119\n",
      "Iteration 19773, loss = 1.30386659\n",
      "Iteration 19774, loss = 1.30386185\n",
      "Iteration 19775, loss = 1.30385699\n",
      "Iteration 19776, loss = 1.30385201\n",
      "Iteration 19777, loss = 1.30384692\n",
      "Iteration 19778, loss = 1.30384173\n",
      "Iteration 19779, loss = 1.30383645\n",
      "Iteration 19780, loss = 1.30383112\n",
      "Iteration 19781, loss = 1.30382580\n",
      "Iteration 19782, loss = 1.30382044\n",
      "Iteration 19783, loss = 1.30381504\n",
      "Iteration 19784, loss = 1.30380961\n",
      "Iteration 19785, loss = 1.30380417\n",
      "Iteration 19786, loss = 1.30379878\n",
      "Iteration 19787, loss = 1.30379339\n",
      "Iteration 19788, loss = 1.30378804\n",
      "Iteration 19789, loss = 1.30378271\n",
      "Iteration 19790, loss = 1.30377739\n",
      "Iteration 19791, loss = 1.30377323\n",
      "Iteration 19792, loss = 1.30376729\n",
      "Iteration 19793, loss = 1.30376281\n",
      "Iteration 19794, loss = 1.30375827\n",
      "Iteration 19795, loss = 1.30375366\n",
      "Iteration 19796, loss = 1.30374897\n",
      "Iteration 19797, loss = 1.30374421\n",
      "Iteration 19798, loss = 1.30373936\n",
      "Iteration 19799, loss = 1.30373440\n",
      "Iteration 19800, loss = 1.30372933\n",
      "Iteration 19801, loss = 1.30372417\n",
      "Iteration 19802, loss = 1.30371894\n",
      "Iteration 19803, loss = 1.30371366\n",
      "Iteration 19804, loss = 1.30370838\n",
      "Iteration 19805, loss = 1.30370310\n",
      "Iteration 19806, loss = 1.30369783\n",
      "Iteration 19807, loss = 1.30369261\n",
      "Iteration 19808, loss = 1.30368738\n",
      "Iteration 19809, loss = 1.30368213\n",
      "Iteration 19810, loss = 1.30367691\n",
      "Iteration 19811, loss = 1.30367171\n",
      "Iteration 19812, loss = 1.30366646\n",
      "Iteration 19813, loss = 1.30366122\n",
      "Iteration 19814, loss = 1.30365596\n",
      "Iteration 19815, loss = 1.30365071\n",
      "Iteration 19816, loss = 1.30364545\n",
      "Iteration 19817, loss = 1.30364022\n",
      "Iteration 19818, loss = 1.30363498\n",
      "Iteration 19819, loss = 1.30363015\n",
      "Iteration 19820, loss = 1.30362533\n",
      "Iteration 19821, loss = 1.30362102\n",
      "Iteration 19822, loss = 1.30361671\n",
      "Iteration 19823, loss = 1.30361231\n",
      "Iteration 19824, loss = 1.30360777\n",
      "Iteration 19825, loss = 1.30360310\n",
      "Iteration 19826, loss = 1.30359831\n",
      "Iteration 19827, loss = 1.30359341\n",
      "Iteration 19828, loss = 1.30358839\n",
      "Iteration 19829, loss = 1.30358327\n",
      "Iteration 19830, loss = 1.30357806\n",
      "Iteration 19831, loss = 1.30357281\n",
      "Iteration 19832, loss = 1.30356754\n",
      "Iteration 19833, loss = 1.30356225\n",
      "Iteration 19834, loss = 1.30355692\n",
      "Iteration 19835, loss = 1.30355156\n",
      "Iteration 19836, loss = 1.30354617\n",
      "Iteration 19837, loss = 1.30354081\n",
      "Iteration 19838, loss = 1.30353548\n",
      "Iteration 19839, loss = 1.30353013\n",
      "Iteration 19840, loss = 1.30352478\n",
      "Iteration 19841, loss = 1.30351944\n",
      "Iteration 19842, loss = 1.30351417\n",
      "Iteration 19843, loss = 1.30351440\n",
      "Iteration 19844, loss = 1.30350427\n",
      "Iteration 19845, loss = 1.30349982\n",
      "Iteration 19846, loss = 1.30349532\n",
      "Iteration 19847, loss = 1.30349075\n",
      "Iteration 19848, loss = 1.30348613\n",
      "Iteration 19849, loss = 1.30348145\n",
      "Iteration 19850, loss = 1.30347665\n",
      "Iteration 19851, loss = 1.30347175\n",
      "Iteration 19852, loss = 1.30346675\n",
      "Iteration 19853, loss = 1.30346166\n",
      "Iteration 19854, loss = 1.30345649\n",
      "Iteration 19855, loss = 1.30345129\n",
      "Iteration 19856, loss = 1.30344609\n",
      "Iteration 19857, loss = 1.30344090\n",
      "Iteration 19858, loss = 1.30343577\n",
      "Iteration 19859, loss = 1.30343065\n",
      "Iteration 19860, loss = 1.30342555\n",
      "Iteration 19861, loss = 1.30342042\n",
      "Iteration 19862, loss = 1.30341528\n",
      "Iteration 19863, loss = 1.30341010\n",
      "Iteration 19864, loss = 1.30340489\n",
      "Iteration 19865, loss = 1.30339966\n",
      "Iteration 19866, loss = 1.30339446\n",
      "Iteration 19867, loss = 1.30338927\n",
      "Iteration 19868, loss = 1.30338410\n",
      "Iteration 19869, loss = 1.30337896\n",
      "Iteration 19870, loss = 1.30337379\n",
      "Iteration 19871, loss = 1.30336946\n",
      "Iteration 19872, loss = 1.30336426\n",
      "Iteration 19873, loss = 1.30335996\n",
      "Iteration 19874, loss = 1.30335563\n",
      "Iteration 19875, loss = 1.30335125\n",
      "Iteration 19876, loss = 1.30334676\n",
      "Iteration 19877, loss = 1.30334215\n",
      "Iteration 19878, loss = 1.30333741\n",
      "Iteration 19879, loss = 1.30333255\n",
      "Iteration 19880, loss = 1.30332759\n",
      "Iteration 19881, loss = 1.30332253\n",
      "Iteration 19882, loss = 1.30331739\n",
      "Iteration 19883, loss = 1.30331217\n",
      "Iteration 19884, loss = 1.30330696\n",
      "Iteration 19885, loss = 1.30330173\n",
      "Iteration 19886, loss = 1.30329647\n",
      "Iteration 19887, loss = 1.30329117\n",
      "Iteration 19888, loss = 1.30328586\n",
      "Iteration 19889, loss = 1.30328054\n",
      "Iteration 19890, loss = 1.30327527\n",
      "Iteration 19891, loss = 1.30327001\n",
      "Iteration 19892, loss = 1.30326486\n",
      "Iteration 19893, loss = 1.30325979\n",
      "Iteration 19894, loss = 1.30325470\n",
      "Iteration 19895, loss = 1.30325371\n",
      "Iteration 19896, loss = 1.30324478\n",
      "Iteration 19897, loss = 1.30324044\n",
      "Iteration 19898, loss = 1.30323609\n",
      "Iteration 19899, loss = 1.30323166\n",
      "Iteration 19900, loss = 1.30322715\n",
      "Iteration 19901, loss = 1.30322259\n",
      "Iteration 19902, loss = 1.30321793\n",
      "Iteration 19903, loss = 1.30321315\n",
      "Iteration 19904, loss = 1.30320826\n",
      "Iteration 19905, loss = 1.30320328\n",
      "Iteration 19906, loss = 1.30319821\n",
      "Iteration 19907, loss = 1.30319306\n",
      "Iteration 19908, loss = 1.30318792\n",
      "Iteration 19909, loss = 1.30318278\n",
      "Iteration 19910, loss = 1.30317764\n",
      "Iteration 19911, loss = 1.30317246\n",
      "Iteration 19912, loss = 1.30316730\n",
      "Iteration 19913, loss = 1.30316211\n",
      "Iteration 19914, loss = 1.30315689\n",
      "Iteration 19915, loss = 1.30315171\n",
      "Iteration 19916, loss = 1.30314658\n",
      "Iteration 19917, loss = 1.30314153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19918, loss = 1.30313648\n",
      "Iteration 19919, loss = 1.30313140\n",
      "Iteration 19920, loss = 1.30312629\n",
      "Iteration 19921, loss = 1.30312115\n",
      "Iteration 19922, loss = 1.30311600\n",
      "Iteration 19923, loss = 1.30311185\n",
      "Iteration 19924, loss = 1.30310647\n",
      "Iteration 19925, loss = 1.30310230\n",
      "Iteration 19926, loss = 1.30309808\n",
      "Iteration 19927, loss = 1.30309384\n",
      "Iteration 19928, loss = 1.30308946\n",
      "Iteration 19929, loss = 1.30308496\n",
      "Iteration 19930, loss = 1.30308033\n",
      "Iteration 19931, loss = 1.30307558\n",
      "Iteration 19932, loss = 1.30307072\n",
      "Iteration 19933, loss = 1.30306576\n",
      "Iteration 19934, loss = 1.30306070\n",
      "Iteration 19935, loss = 1.30305559\n",
      "Iteration 19936, loss = 1.30305047\n",
      "Iteration 19937, loss = 1.30304531\n",
      "Iteration 19938, loss = 1.30304011\n",
      "Iteration 19939, loss = 1.30303488\n",
      "Iteration 19940, loss = 1.30302962\n",
      "Iteration 19941, loss = 1.30302435\n",
      "Iteration 19942, loss = 1.30301911\n",
      "Iteration 19943, loss = 1.30301389\n",
      "Iteration 19944, loss = 1.30300872\n",
      "Iteration 19945, loss = 1.30300354\n",
      "Iteration 19946, loss = 1.30299836\n",
      "Iteration 19947, loss = 1.30299318\n",
      "Iteration 19948, loss = 1.30299230\n",
      "Iteration 19949, loss = 1.30298349\n",
      "Iteration 19950, loss = 1.30297916\n",
      "Iteration 19951, loss = 1.30297476\n",
      "Iteration 19952, loss = 1.30297031\n",
      "Iteration 19953, loss = 1.30296584\n",
      "Iteration 19954, loss = 1.30296129\n",
      "Iteration 19955, loss = 1.30295662\n",
      "Iteration 19956, loss = 1.30295184\n",
      "Iteration 19957, loss = 1.30294698\n",
      "Iteration 19958, loss = 1.30294202\n",
      "Iteration 19959, loss = 1.30293699\n",
      "Iteration 19960, loss = 1.30293191\n",
      "Iteration 19961, loss = 1.30292682\n",
      "Iteration 19962, loss = 1.30292177\n",
      "Iteration 19963, loss = 1.30291680\n",
      "Iteration 19964, loss = 1.30291185\n",
      "Iteration 19965, loss = 1.30290688\n",
      "Iteration 19966, loss = 1.30290185\n",
      "Iteration 19967, loss = 1.30289678\n",
      "Iteration 19968, loss = 1.30289168\n",
      "Iteration 19969, loss = 1.30288656\n",
      "Iteration 19970, loss = 1.30288144\n",
      "Iteration 19971, loss = 1.30287640\n",
      "Iteration 19972, loss = 1.30287141\n",
      "Iteration 19973, loss = 1.30286641\n",
      "Iteration 19974, loss = 1.30286139\n",
      "Iteration 19975, loss = 1.30285636\n",
      "Iteration 19976, loss = 1.30285130\n",
      "Iteration 19977, loss = 1.30285067\n",
      "Iteration 19978, loss = 1.30284193\n",
      "Iteration 19979, loss = 1.30283775\n",
      "Iteration 19980, loss = 1.30283353\n",
      "Iteration 19981, loss = 1.30282928\n",
      "Iteration 19982, loss = 1.30282491\n",
      "Iteration 19983, loss = 1.30282042\n",
      "Iteration 19984, loss = 1.30281581\n",
      "Iteration 19985, loss = 1.30281109\n",
      "Iteration 19986, loss = 1.30280627\n",
      "Iteration 19987, loss = 1.30280135\n",
      "Iteration 19988, loss = 1.30279634\n",
      "Iteration 19989, loss = 1.30279127\n",
      "Iteration 19990, loss = 1.30278620\n",
      "Iteration 19991, loss = 1.30278111\n",
      "Iteration 19992, loss = 1.30277598\n",
      "Iteration 19993, loss = 1.30277082\n",
      "Iteration 19994, loss = 1.30276564\n",
      "Iteration 19995, loss = 1.30276044\n",
      "Iteration 19996, loss = 1.30275525\n",
      "Iteration 19997, loss = 1.30275016\n",
      "Iteration 19998, loss = 1.30274514\n",
      "Iteration 19999, loss = 1.30274020\n",
      "Iteration 20000, loss = 1.30273523\n",
      "Iteration 20001, loss = 1.30273188\n",
      "Iteration 20002, loss = 1.30272553\n",
      "Iteration 20003, loss = 1.30272134\n",
      "Iteration 20004, loss = 1.30271709\n",
      "Iteration 20005, loss = 1.30271281\n",
      "Iteration 20006, loss = 1.30270847\n",
      "Iteration 20007, loss = 1.30270403\n",
      "Iteration 20008, loss = 1.30269948\n",
      "Iteration 20009, loss = 1.30269481\n",
      "Iteration 20010, loss = 1.30269004\n",
      "Iteration 20011, loss = 1.30268517\n",
      "Iteration 20012, loss = 1.30268022\n",
      "Iteration 20013, loss = 1.30267520\n",
      "Iteration 20014, loss = 1.30267019\n",
      "Iteration 20015, loss = 1.30266517\n",
      "Iteration 20016, loss = 1.30266017\n",
      "Iteration 20017, loss = 1.30265517\n",
      "Iteration 20018, loss = 1.30265013\n",
      "Iteration 20019, loss = 1.30264507\n",
      "Iteration 20020, loss = 1.30263999\n",
      "Iteration 20021, loss = 1.30263495\n",
      "Iteration 20022, loss = 1.30262995\n",
      "Iteration 20023, loss = 1.30262501\n",
      "Iteration 20024, loss = 1.30262007\n",
      "Iteration 20025, loss = 1.30261512\n",
      "Iteration 20026, loss = 1.30261016\n",
      "Iteration 20027, loss = 1.30260516\n",
      "Iteration 20028, loss = 1.30260011\n",
      "Iteration 20029, loss = 1.30259505\n",
      "Iteration 20030, loss = 1.30259094\n",
      "Iteration 20031, loss = 1.30258591\n",
      "Iteration 20032, loss = 1.30258185\n",
      "Iteration 20033, loss = 1.30257774\n",
      "Iteration 20034, loss = 1.30257360\n",
      "Iteration 20035, loss = 1.30256935\n",
      "Iteration 20036, loss = 1.30256497\n",
      "Iteration 20037, loss = 1.30256047\n",
      "Iteration 20038, loss = 1.30255585\n",
      "Iteration 20039, loss = 1.30255112\n",
      "Iteration 20040, loss = 1.30254629\n",
      "Iteration 20041, loss = 1.30254137\n",
      "Iteration 20042, loss = 1.30253636\n",
      "Iteration 20043, loss = 1.30253136\n",
      "Iteration 20044, loss = 1.30252633\n",
      "Iteration 20045, loss = 1.30252127\n",
      "Iteration 20046, loss = 1.30251617\n",
      "Iteration 20047, loss = 1.30251104\n",
      "Iteration 20048, loss = 1.30250590\n",
      "Iteration 20049, loss = 1.30250078\n",
      "Iteration 20050, loss = 1.30249567\n",
      "Iteration 20051, loss = 1.30249061\n",
      "Iteration 20052, loss = 1.30248556\n",
      "Iteration 20053, loss = 1.30248052\n",
      "Iteration 20054, loss = 1.30247551\n",
      "Iteration 20055, loss = 1.30247214\n",
      "Iteration 20056, loss = 1.30246608\n",
      "Iteration 20057, loss = 1.30246189\n",
      "Iteration 20058, loss = 1.30245764\n",
      "Iteration 20059, loss = 1.30245333\n",
      "Iteration 20060, loss = 1.30244897\n",
      "Iteration 20061, loss = 1.30244456\n",
      "Iteration 20062, loss = 1.30244005\n",
      "Iteration 20063, loss = 1.30243543\n",
      "Iteration 20064, loss = 1.30243071\n",
      "Iteration 20065, loss = 1.30242590\n",
      "Iteration 20066, loss = 1.30242102\n",
      "Iteration 20067, loss = 1.30241606\n",
      "Iteration 20068, loss = 1.30241109\n",
      "Iteration 20069, loss = 1.30240614\n",
      "Iteration 20070, loss = 1.30240121\n",
      "Iteration 20071, loss = 1.30239637\n",
      "Iteration 20072, loss = 1.30239149\n",
      "Iteration 20073, loss = 1.30238658\n",
      "Iteration 20074, loss = 1.30238163\n",
      "Iteration 20075, loss = 1.30237668\n",
      "Iteration 20076, loss = 1.30237172\n",
      "Iteration 20077, loss = 1.30236676\n",
      "Iteration 20078, loss = 1.30236183\n",
      "Iteration 20079, loss = 1.30235693\n",
      "Iteration 20080, loss = 1.30235206\n",
      "Iteration 20081, loss = 1.30234717\n",
      "Iteration 20082, loss = 1.30234226\n",
      "Iteration 20083, loss = 1.30233733\n",
      "Iteration 20084, loss = 1.30233275\n",
      "Iteration 20085, loss = 1.30232818\n",
      "Iteration 20086, loss = 1.30232414\n",
      "Iteration 20087, loss = 1.30232005\n",
      "Iteration 20088, loss = 1.30231592\n",
      "Iteration 20089, loss = 1.30231171\n",
      "Iteration 20090, loss = 1.30230737\n",
      "Iteration 20091, loss = 1.30230292\n",
      "Iteration 20092, loss = 1.30229835\n",
      "Iteration 20093, loss = 1.30229368\n",
      "Iteration 20094, loss = 1.30228890\n",
      "Iteration 20095, loss = 1.30228404\n",
      "Iteration 20096, loss = 1.30227910\n",
      "Iteration 20097, loss = 1.30227413\n",
      "Iteration 20098, loss = 1.30226915\n",
      "Iteration 20099, loss = 1.30226415\n",
      "Iteration 20100, loss = 1.30225912\n",
      "Iteration 20101, loss = 1.30225407\n",
      "Iteration 20102, loss = 1.30224901\n",
      "Iteration 20103, loss = 1.30224398\n",
      "Iteration 20104, loss = 1.30223898\n",
      "Iteration 20105, loss = 1.30223403\n",
      "Iteration 20106, loss = 1.30222912\n",
      "Iteration 20107, loss = 1.30222420\n",
      "Iteration 20108, loss = 1.30221933\n",
      "Iteration 20109, loss = 1.30221450\n",
      "Iteration 20110, loss = 1.30221573\n",
      "Iteration 20111, loss = 1.30220504\n",
      "Iteration 20112, loss = 1.30220100\n",
      "Iteration 20113, loss = 1.30219692\n",
      "Iteration 20114, loss = 1.30219278\n",
      "Iteration 20115, loss = 1.30218857\n",
      "Iteration 20116, loss = 1.30218429\n",
      "Iteration 20117, loss = 1.30217991\n",
      "Iteration 20118, loss = 1.30217541\n",
      "Iteration 20119, loss = 1.30217081\n",
      "Iteration 20120, loss = 1.30216611\n",
      "Iteration 20121, loss = 1.30216132\n",
      "Iteration 20122, loss = 1.30215645\n",
      "Iteration 20123, loss = 1.30215153\n",
      "Iteration 20124, loss = 1.30214662\n",
      "Iteration 20125, loss = 1.30214168\n",
      "Iteration 20126, loss = 1.30213673\n",
      "Iteration 20127, loss = 1.30213182\n",
      "Iteration 20128, loss = 1.30212690\n",
      "Iteration 20129, loss = 1.30212200\n",
      "Iteration 20130, loss = 1.30211707\n",
      "Iteration 20131, loss = 1.30211217\n",
      "Iteration 20132, loss = 1.30210726\n",
      "Iteration 20133, loss = 1.30210236\n",
      "Iteration 20134, loss = 1.30209751\n",
      "Iteration 20135, loss = 1.30209268\n",
      "Iteration 20136, loss = 1.30208788\n",
      "Iteration 20137, loss = 1.30208304\n",
      "Iteration 20138, loss = 1.30207818\n",
      "Iteration 20139, loss = 1.30207646\n",
      "Iteration 20140, loss = 1.30206899\n",
      "Iteration 20141, loss = 1.30206504\n",
      "Iteration 20142, loss = 1.30206104\n",
      "Iteration 20143, loss = 1.30205701\n",
      "Iteration 20144, loss = 1.30205288\n",
      "Iteration 20145, loss = 1.30204864\n",
      "Iteration 20146, loss = 1.30204427\n",
      "Iteration 20147, loss = 1.30203979\n",
      "Iteration 20148, loss = 1.30203520\n",
      "Iteration 20149, loss = 1.30203050\n",
      "Iteration 20150, loss = 1.30202572\n",
      "Iteration 20151, loss = 1.30202086\n",
      "Iteration 20152, loss = 1.30201600\n",
      "Iteration 20153, loss = 1.30201111\n",
      "Iteration 20154, loss = 1.30200620\n",
      "Iteration 20155, loss = 1.30200126\n",
      "Iteration 20156, loss = 1.30199628\n",
      "Iteration 20157, loss = 1.30199129\n",
      "Iteration 20158, loss = 1.30198632\n",
      "Iteration 20159, loss = 1.30198133\n",
      "Iteration 20160, loss = 1.30197634\n",
      "Iteration 20161, loss = 1.30197137\n",
      "Iteration 20162, loss = 1.30196647\n",
      "Iteration 20163, loss = 1.30196166\n",
      "Iteration 20164, loss = 1.30195686\n",
      "Iteration 20165, loss = 1.30195627\n",
      "Iteration 20166, loss = 1.30194764\n",
      "Iteration 20167, loss = 1.30194359\n",
      "Iteration 20168, loss = 1.30193949\n",
      "Iteration 20169, loss = 1.30193535\n",
      "Iteration 20170, loss = 1.30193117\n",
      "Iteration 20171, loss = 1.30192692\n",
      "Iteration 20172, loss = 1.30192256\n",
      "Iteration 20173, loss = 1.30191809\n",
      "Iteration 20174, loss = 1.30191352\n",
      "Iteration 20175, loss = 1.30190886\n",
      "Iteration 20176, loss = 1.30190412\n",
      "Iteration 20177, loss = 1.30189931\n",
      "Iteration 20178, loss = 1.30189448\n",
      "Iteration 20179, loss = 1.30188966\n",
      "Iteration 20180, loss = 1.30188482\n",
      "Iteration 20181, loss = 1.30187998\n",
      "Iteration 20182, loss = 1.30187516\n",
      "Iteration 20183, loss = 1.30187035\n",
      "Iteration 20184, loss = 1.30186552\n",
      "Iteration 20185, loss = 1.30186068\n",
      "Iteration 20186, loss = 1.30185585\n",
      "Iteration 20187, loss = 1.30185105\n",
      "Iteration 20188, loss = 1.30184632\n",
      "Iteration 20189, loss = 1.30184156\n",
      "Iteration 20190, loss = 1.30183679\n",
      "Iteration 20191, loss = 1.30183200\n",
      "Iteration 20192, loss = 1.30182719\n",
      "Iteration 20193, loss = 1.30182237\n",
      "Iteration 20194, loss = 1.30181752\n",
      "Iteration 20195, loss = 1.30181443\n",
      "Iteration 20196, loss = 1.30180877\n",
      "Iteration 20197, loss = 1.30180488\n",
      "Iteration 20198, loss = 1.30180092\n",
      "Iteration 20199, loss = 1.30179694\n",
      "Iteration 20200, loss = 1.30179289\n",
      "Iteration 20201, loss = 1.30178871\n",
      "Iteration 20202, loss = 1.30178442\n",
      "Iteration 20203, loss = 1.30178001\n",
      "Iteration 20204, loss = 1.30177549\n",
      "Iteration 20205, loss = 1.30177086\n",
      "Iteration 20206, loss = 1.30176615\n",
      "Iteration 20207, loss = 1.30176136\n",
      "Iteration 20208, loss = 1.30175651\n",
      "Iteration 20209, loss = 1.30175165\n",
      "Iteration 20210, loss = 1.30174678\n",
      "Iteration 20211, loss = 1.30174188\n",
      "Iteration 20212, loss = 1.30173696\n",
      "Iteration 20213, loss = 1.30173202\n",
      "Iteration 20214, loss = 1.30172711\n",
      "Iteration 20215, loss = 1.30172220\n",
      "Iteration 20216, loss = 1.30171732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20217, loss = 1.30171246\n",
      "Iteration 20218, loss = 1.30170764\n",
      "Iteration 20219, loss = 1.30170284\n",
      "Iteration 20220, loss = 1.30169802\n",
      "Iteration 20221, loss = 1.30169713\n",
      "Iteration 20222, loss = 1.30168892\n",
      "Iteration 20223, loss = 1.30168493\n",
      "Iteration 20224, loss = 1.30168091\n",
      "Iteration 20225, loss = 1.30167682\n",
      "Iteration 20226, loss = 1.30167268\n",
      "Iteration 20227, loss = 1.30166848\n",
      "Iteration 20228, loss = 1.30166417\n",
      "Iteration 20229, loss = 1.30165975\n",
      "Iteration 20230, loss = 1.30165524\n",
      "Iteration 20231, loss = 1.30165064\n",
      "Iteration 20232, loss = 1.30164596\n",
      "Iteration 20233, loss = 1.30164120\n",
      "Iteration 20234, loss = 1.30163644\n",
      "Iteration 20235, loss = 1.30163170\n",
      "Iteration 20236, loss = 1.30162698\n",
      "Iteration 20237, loss = 1.30162229\n",
      "Iteration 20238, loss = 1.30161758\n",
      "Iteration 20239, loss = 1.30161286\n",
      "Iteration 20240, loss = 1.30160809\n",
      "Iteration 20241, loss = 1.30160329\n",
      "Iteration 20242, loss = 1.30159849\n",
      "Iteration 20243, loss = 1.30159368\n",
      "Iteration 20244, loss = 1.30158896\n",
      "Iteration 20245, loss = 1.30158424\n",
      "Iteration 20246, loss = 1.30157954\n",
      "Iteration 20247, loss = 1.30157485\n",
      "Iteration 20248, loss = 1.30157015\n",
      "Iteration 20249, loss = 1.30156542\n",
      "Iteration 20250, loss = 1.30156066\n",
      "Iteration 20251, loss = 1.30155666\n",
      "Iteration 20252, loss = 1.30155187\n",
      "Iteration 20253, loss = 1.30154802\n",
      "Iteration 20254, loss = 1.30154412\n",
      "Iteration 20255, loss = 1.30154019\n",
      "Iteration 20256, loss = 1.30153618\n",
      "Iteration 20257, loss = 1.30153205\n",
      "Iteration 20258, loss = 1.30152780\n",
      "Iteration 20259, loss = 1.30152344\n",
      "Iteration 20260, loss = 1.30151897\n",
      "Iteration 20261, loss = 1.30151440\n",
      "Iteration 20262, loss = 1.30150975\n",
      "Iteration 20263, loss = 1.30150502\n",
      "Iteration 20264, loss = 1.30150025\n",
      "Iteration 20265, loss = 1.30149546\n",
      "Iteration 20266, loss = 1.30149066\n",
      "Iteration 20267, loss = 1.30148583\n",
      "Iteration 20268, loss = 1.30148097\n",
      "Iteration 20269, loss = 1.30147610\n",
      "Iteration 20270, loss = 1.30147125\n",
      "Iteration 20271, loss = 1.30146642\n",
      "Iteration 20272, loss = 1.30146163\n",
      "Iteration 20273, loss = 1.30145691\n",
      "Iteration 20274, loss = 1.30145221\n",
      "Iteration 20275, loss = 1.30144752\n",
      "Iteration 20276, loss = 1.30144281\n",
      "Iteration 20277, loss = 1.30143875\n",
      "Iteration 20278, loss = 1.30143378\n",
      "Iteration 20279, loss = 1.30142989\n",
      "Iteration 20280, loss = 1.30142599\n",
      "Iteration 20281, loss = 1.30142201\n",
      "Iteration 20282, loss = 1.30141797\n",
      "Iteration 20283, loss = 1.30141387\n",
      "Iteration 20284, loss = 1.30140965\n",
      "Iteration 20285, loss = 1.30140531\n",
      "Iteration 20286, loss = 1.30140088\n",
      "Iteration 20287, loss = 1.30139635\n",
      "Iteration 20288, loss = 1.30139174\n",
      "Iteration 20289, loss = 1.30138705\n",
      "Iteration 20290, loss = 1.30138236\n",
      "Iteration 20291, loss = 1.30137765\n",
      "Iteration 20292, loss = 1.30137295\n",
      "Iteration 20293, loss = 1.30136826\n",
      "Iteration 20294, loss = 1.30136355\n",
      "Iteration 20295, loss = 1.30135885\n",
      "Iteration 20296, loss = 1.30135412\n",
      "Iteration 20297, loss = 1.30134936\n",
      "Iteration 20298, loss = 1.30134461\n",
      "Iteration 20299, loss = 1.30133987\n",
      "Iteration 20300, loss = 1.30133517\n",
      "Iteration 20301, loss = 1.30133053\n",
      "Iteration 20302, loss = 1.30132590\n",
      "Iteration 20303, loss = 1.30132129\n",
      "Iteration 20304, loss = 1.30131667\n",
      "Iteration 20305, loss = 1.30131201\n",
      "Iteration 20306, loss = 1.30130733\n",
      "Iteration 20307, loss = 1.30130262\n",
      "Iteration 20308, loss = 1.30130040\n",
      "Iteration 20309, loss = 1.30129390\n",
      "Iteration 20310, loss = 1.30129013\n",
      "Iteration 20311, loss = 1.30128632\n",
      "Iteration 20312, loss = 1.30128251\n",
      "Iteration 20313, loss = 1.30127858\n",
      "Iteration 20314, loss = 1.30127453\n",
      "Iteration 20315, loss = 1.30127036\n",
      "Iteration 20316, loss = 1.30126608\n",
      "Iteration 20317, loss = 1.30126170\n",
      "Iteration 20318, loss = 1.30125721\n",
      "Iteration 20319, loss = 1.30125264\n",
      "Iteration 20320, loss = 1.30124798\n",
      "Iteration 20321, loss = 1.30124328\n",
      "Iteration 20322, loss = 1.30123857\n",
      "Iteration 20323, loss = 1.30123384\n",
      "Iteration 20324, loss = 1.30122907\n",
      "Iteration 20325, loss = 1.30122428\n",
      "Iteration 20326, loss = 1.30121947\n",
      "Iteration 20327, loss = 1.30121465\n",
      "Iteration 20328, loss = 1.30120987\n",
      "Iteration 20329, loss = 1.30120511\n",
      "Iteration 20330, loss = 1.30120035\n",
      "Iteration 20331, loss = 1.30119563\n",
      "Iteration 20332, loss = 1.30119096\n",
      "Iteration 20333, loss = 1.30118631\n",
      "Iteration 20334, loss = 1.30118169\n",
      "Iteration 20335, loss = 1.30117748\n",
      "Iteration 20336, loss = 1.30117362\n",
      "Iteration 20337, loss = 1.30116973\n",
      "Iteration 20338, loss = 1.30116577\n",
      "Iteration 20339, loss = 1.30116178\n",
      "Iteration 20340, loss = 1.30115770\n",
      "Iteration 20341, loss = 1.30115352\n",
      "Iteration 20342, loss = 1.30114924\n",
      "Iteration 20343, loss = 1.30114486\n",
      "Iteration 20344, loss = 1.30114039\n",
      "Iteration 20345, loss = 1.30113584\n",
      "Iteration 20346, loss = 1.30113122\n",
      "Iteration 20347, loss = 1.30112654\n",
      "Iteration 20348, loss = 1.30112187\n",
      "Iteration 20349, loss = 1.30111725\n",
      "Iteration 20350, loss = 1.30111265\n",
      "Iteration 20351, loss = 1.30110807\n",
      "Iteration 20352, loss = 1.30110347\n",
      "Iteration 20353, loss = 1.30109884\n",
      "Iteration 20354, loss = 1.30109421\n",
      "Iteration 20355, loss = 1.30108956\n",
      "Iteration 20356, loss = 1.30108493\n",
      "Iteration 20357, loss = 1.30108030\n",
      "Iteration 20358, loss = 1.30107571\n",
      "Iteration 20359, loss = 1.30107115\n",
      "Iteration 20360, loss = 1.30106656\n",
      "Iteration 20361, loss = 1.30106196\n",
      "Iteration 20362, loss = 1.30105736\n",
      "Iteration 20363, loss = 1.30105274\n",
      "Iteration 20364, loss = 1.30104812\n",
      "Iteration 20365, loss = 1.30104350\n",
      "Iteration 20366, loss = 1.30104063\n",
      "Iteration 20367, loss = 1.30103510\n",
      "Iteration 20368, loss = 1.30103137\n",
      "Iteration 20369, loss = 1.30102757\n",
      "Iteration 20370, loss = 1.30102375\n",
      "Iteration 20371, loss = 1.30101987\n",
      "Iteration 20372, loss = 1.30101588\n",
      "Iteration 20373, loss = 1.30101177\n",
      "Iteration 20374, loss = 1.30100755\n",
      "Iteration 20375, loss = 1.30100323\n",
      "Iteration 20376, loss = 1.30099881\n",
      "Iteration 20377, loss = 1.30099430\n",
      "Iteration 20378, loss = 1.30098971\n",
      "Iteration 20379, loss = 1.30098506\n",
      "Iteration 20380, loss = 1.30098041\n",
      "Iteration 20381, loss = 1.30097574\n",
      "Iteration 20382, loss = 1.30097103\n",
      "Iteration 20383, loss = 1.30096631\n",
      "Iteration 20384, loss = 1.30096156\n",
      "Iteration 20385, loss = 1.30095681\n",
      "Iteration 20386, loss = 1.30095206\n",
      "Iteration 20387, loss = 1.30094738\n",
      "Iteration 20388, loss = 1.30094276\n",
      "Iteration 20389, loss = 1.30093818\n",
      "Iteration 20390, loss = 1.30093359\n",
      "Iteration 20391, loss = 1.30092903\n",
      "Iteration 20392, loss = 1.30092540\n",
      "Iteration 20393, loss = 1.30092023\n",
      "Iteration 20394, loss = 1.30091643\n",
      "Iteration 20395, loss = 1.30091262\n",
      "Iteration 20396, loss = 1.30090879\n",
      "Iteration 20397, loss = 1.30090489\n",
      "Iteration 20398, loss = 1.30090090\n",
      "Iteration 20399, loss = 1.30089680\n",
      "Iteration 20400, loss = 1.30089259\n",
      "Iteration 20401, loss = 1.30088827\n",
      "Iteration 20402, loss = 1.30088387\n",
      "Iteration 20403, loss = 1.30087939\n",
      "Iteration 20404, loss = 1.30087483\n",
      "Iteration 20405, loss = 1.30087023\n",
      "Iteration 20406, loss = 1.30086565\n",
      "Iteration 20407, loss = 1.30086107\n",
      "Iteration 20408, loss = 1.30085650\n",
      "Iteration 20409, loss = 1.30085194\n",
      "Iteration 20410, loss = 1.30084739\n",
      "Iteration 20411, loss = 1.30084281\n",
      "Iteration 20412, loss = 1.30083824\n",
      "Iteration 20413, loss = 1.30083365\n",
      "Iteration 20414, loss = 1.30082904\n",
      "Iteration 20415, loss = 1.30082446\n",
      "Iteration 20416, loss = 1.30081988\n",
      "Iteration 20417, loss = 1.30081534\n",
      "Iteration 20418, loss = 1.30081084\n",
      "Iteration 20419, loss = 1.30080634\n",
      "Iteration 20420, loss = 1.30080183\n",
      "Iteration 20421, loss = 1.30079729\n",
      "Iteration 20422, loss = 1.30079273\n",
      "Iteration 20423, loss = 1.30078816\n",
      "Iteration 20424, loss = 1.30078432\n",
      "Iteration 20425, loss = 1.30077982\n",
      "Iteration 20426, loss = 1.30077617\n",
      "Iteration 20427, loss = 1.30077246\n",
      "Iteration 20428, loss = 1.30076872\n",
      "Iteration 20429, loss = 1.30076491\n",
      "Iteration 20430, loss = 1.30076100\n",
      "Iteration 20431, loss = 1.30075697\n",
      "Iteration 20432, loss = 1.30075283\n",
      "Iteration 20433, loss = 1.30074858\n",
      "Iteration 20434, loss = 1.30074423\n",
      "Iteration 20435, loss = 1.30073980\n",
      "Iteration 20436, loss = 1.30073528\n",
      "Iteration 20437, loss = 1.30073070\n",
      "Iteration 20438, loss = 1.30072612\n",
      "Iteration 20439, loss = 1.30072151\n",
      "Iteration 20440, loss = 1.30071688\n",
      "Iteration 20441, loss = 1.30071222\n",
      "Iteration 20442, loss = 1.30070754\n",
      "Iteration 20443, loss = 1.30070284\n",
      "Iteration 20444, loss = 1.30069814\n",
      "Iteration 20445, loss = 1.30069350\n",
      "Iteration 20446, loss = 1.30068890\n",
      "Iteration 20447, loss = 1.30068435\n",
      "Iteration 20448, loss = 1.30067978\n",
      "Iteration 20449, loss = 1.30067524\n",
      "Iteration 20450, loss = 1.30067074\n",
      "Iteration 20451, loss = 1.30067123\n",
      "Iteration 20452, loss = 1.30066212\n",
      "Iteration 20453, loss = 1.30065841\n",
      "Iteration 20454, loss = 1.30065465\n",
      "Iteration 20455, loss = 1.30065086\n",
      "Iteration 20456, loss = 1.30064702\n",
      "Iteration 20457, loss = 1.30064310\n",
      "Iteration 20458, loss = 1.30063907\n",
      "Iteration 20459, loss = 1.30063493\n",
      "Iteration 20460, loss = 1.30063070\n",
      "Iteration 20461, loss = 1.30062637\n",
      "Iteration 20462, loss = 1.30062197\n",
      "Iteration 20463, loss = 1.30061749\n",
      "Iteration 20464, loss = 1.30061297\n",
      "Iteration 20465, loss = 1.30060844\n",
      "Iteration 20466, loss = 1.30060389\n",
      "Iteration 20467, loss = 1.30059935\n",
      "Iteration 20468, loss = 1.30059483\n",
      "Iteration 20469, loss = 1.30059035\n",
      "Iteration 20470, loss = 1.30058588\n",
      "Iteration 20471, loss = 1.30058138\n",
      "Iteration 20472, loss = 1.30057687\n",
      "Iteration 20473, loss = 1.30057238\n",
      "Iteration 20474, loss = 1.30056787\n",
      "Iteration 20475, loss = 1.30056333\n",
      "Iteration 20476, loss = 1.30055882\n",
      "Iteration 20477, loss = 1.30055432\n",
      "Iteration 20478, loss = 1.30054987\n",
      "Iteration 20479, loss = 1.30054544\n",
      "Iteration 20480, loss = 1.30054098\n",
      "Iteration 20481, loss = 1.30053651\n",
      "Iteration 20482, loss = 1.30053202\n",
      "Iteration 20483, loss = 1.30052753\n",
      "Iteration 20484, loss = 1.30052485\n",
      "Iteration 20485, loss = 1.30051940\n",
      "Iteration 20486, loss = 1.30051580\n",
      "Iteration 20487, loss = 1.30051217\n",
      "Iteration 20488, loss = 1.30050854\n",
      "Iteration 20489, loss = 1.30050479\n",
      "Iteration 20490, loss = 1.30050093\n",
      "Iteration 20491, loss = 1.30049695\n",
      "Iteration 20492, loss = 1.30049287\n",
      "Iteration 20493, loss = 1.30048868\n",
      "Iteration 20494, loss = 1.30048439\n",
      "Iteration 20495, loss = 1.30048001\n",
      "Iteration 20496, loss = 1.30047555\n",
      "Iteration 20497, loss = 1.30047103\n",
      "Iteration 20498, loss = 1.30046651\n",
      "Iteration 20499, loss = 1.30046198\n",
      "Iteration 20500, loss = 1.30045740\n",
      "Iteration 20501, loss = 1.30045281\n",
      "Iteration 20502, loss = 1.30044819\n",
      "Iteration 20503, loss = 1.30044356\n",
      "Iteration 20504, loss = 1.30043893\n",
      "Iteration 20505, loss = 1.30043432\n",
      "Iteration 20506, loss = 1.30042973\n",
      "Iteration 20507, loss = 1.30042520\n",
      "Iteration 20508, loss = 1.30042073\n",
      "Iteration 20509, loss = 1.30041630\n",
      "Iteration 20510, loss = 1.30041316\n",
      "Iteration 20511, loss = 1.30040784\n",
      "Iteration 20512, loss = 1.30040415\n",
      "Iteration 20513, loss = 1.30040044\n",
      "Iteration 20514, loss = 1.30039668\n",
      "Iteration 20515, loss = 1.30039285\n",
      "Iteration 20516, loss = 1.30038897\n",
      "Iteration 20517, loss = 1.30038499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20518, loss = 1.30038090\n",
      "Iteration 20519, loss = 1.30037672\n",
      "Iteration 20520, loss = 1.30037245\n",
      "Iteration 20521, loss = 1.30036810\n",
      "Iteration 20522, loss = 1.30036368\n",
      "Iteration 20523, loss = 1.30035921\n",
      "Iteration 20524, loss = 1.30035472\n",
      "Iteration 20525, loss = 1.30035025\n",
      "Iteration 20526, loss = 1.30034575\n",
      "Iteration 20527, loss = 1.30034129\n",
      "Iteration 20528, loss = 1.30033690\n",
      "Iteration 20529, loss = 1.30033250\n",
      "Iteration 20530, loss = 1.30032808\n",
      "Iteration 20531, loss = 1.30032366\n",
      "Iteration 20532, loss = 1.30031926\n",
      "Iteration 20533, loss = 1.30031484\n",
      "Iteration 20534, loss = 1.30031042\n",
      "Iteration 20535, loss = 1.30030603\n",
      "Iteration 20536, loss = 1.30030163\n",
      "Iteration 20537, loss = 1.30029721\n",
      "Iteration 20538, loss = 1.30029278\n",
      "Iteration 20539, loss = 1.30028835\n",
      "Iteration 20540, loss = 1.30028392\n",
      "Iteration 20541, loss = 1.30027947\n",
      "Iteration 20542, loss = 1.30027507\n",
      "Iteration 20543, loss = 1.30027066\n",
      "Iteration 20544, loss = 1.30026661\n",
      "Iteration 20545, loss = 1.30026271\n",
      "Iteration 20546, loss = 1.30025914\n",
      "Iteration 20547, loss = 1.30025557\n",
      "Iteration 20548, loss = 1.30025196\n",
      "Iteration 20549, loss = 1.30024825\n",
      "Iteration 20550, loss = 1.30024442\n",
      "Iteration 20551, loss = 1.30024048\n",
      "Iteration 20552, loss = 1.30023644\n",
      "Iteration 20553, loss = 1.30023230\n",
      "Iteration 20554, loss = 1.30022806\n",
      "Iteration 20555, loss = 1.30022374\n",
      "Iteration 20556, loss = 1.30021934\n",
      "Iteration 20557, loss = 1.30021488\n",
      "Iteration 20558, loss = 1.30021039\n",
      "Iteration 20559, loss = 1.30020591\n",
      "Iteration 20560, loss = 1.30020141\n",
      "Iteration 20561, loss = 1.30019688\n",
      "Iteration 20562, loss = 1.30019233\n",
      "Iteration 20563, loss = 1.30018780\n",
      "Iteration 20564, loss = 1.30018327\n",
      "Iteration 20565, loss = 1.30017880\n",
      "Iteration 20566, loss = 1.30017438\n",
      "Iteration 20567, loss = 1.30016994\n",
      "Iteration 20568, loss = 1.30016556\n",
      "Iteration 20569, loss = 1.30016118\n",
      "Iteration 20570, loss = 1.30015791\n",
      "Iteration 20571, loss = 1.30015281\n",
      "Iteration 20572, loss = 1.30014924\n",
      "Iteration 20573, loss = 1.30014561\n",
      "Iteration 20574, loss = 1.30014194\n",
      "Iteration 20575, loss = 1.30013820\n",
      "Iteration 20576, loss = 1.30013441\n",
      "Iteration 20577, loss = 1.30013051\n",
      "Iteration 20578, loss = 1.30012650\n",
      "Iteration 20579, loss = 1.30012240\n",
      "Iteration 20580, loss = 1.30011820\n",
      "Iteration 20581, loss = 1.30011392\n",
      "Iteration 20582, loss = 1.30010957\n",
      "Iteration 20583, loss = 1.30010516\n",
      "Iteration 20584, loss = 1.30010076\n",
      "Iteration 20585, loss = 1.30009634\n",
      "Iteration 20586, loss = 1.30009191\n",
      "Iteration 20587, loss = 1.30008752\n",
      "Iteration 20588, loss = 1.30008314\n",
      "Iteration 20589, loss = 1.30007876\n",
      "Iteration 20590, loss = 1.30007437\n",
      "Iteration 20591, loss = 1.30006998\n",
      "Iteration 20592, loss = 1.30006561\n",
      "Iteration 20593, loss = 1.30006121\n",
      "Iteration 20594, loss = 1.30005684\n",
      "Iteration 20595, loss = 1.30005250\n",
      "Iteration 20596, loss = 1.30004816\n",
      "Iteration 20597, loss = 1.30004383\n",
      "Iteration 20598, loss = 1.30003950\n",
      "Iteration 20599, loss = 1.30003514\n",
      "Iteration 20600, loss = 1.30003078\n",
      "Iteration 20601, loss = 1.30002641\n",
      "Iteration 20602, loss = 1.30002204\n",
      "Iteration 20603, loss = 1.30001766\n",
      "Iteration 20604, loss = 1.30001330\n",
      "Iteration 20605, loss = 1.30001010\n",
      "Iteration 20606, loss = 1.30000548\n",
      "Iteration 20607, loss = 1.30000199\n",
      "Iteration 20608, loss = 1.29999855\n",
      "Iteration 20609, loss = 1.29999503\n",
      "Iteration 20610, loss = 1.29999139\n",
      "Iteration 20611, loss = 1.29998765\n",
      "Iteration 20612, loss = 1.29998379\n",
      "Iteration 20613, loss = 1.29997983\n",
      "Iteration 20614, loss = 1.29997577\n",
      "Iteration 20615, loss = 1.29997161\n",
      "Iteration 20616, loss = 1.29996736\n",
      "Iteration 20617, loss = 1.29996304\n",
      "Iteration 20618, loss = 1.29995865\n",
      "Iteration 20619, loss = 1.29995421\n",
      "Iteration 20620, loss = 1.29994977\n",
      "Iteration 20621, loss = 1.29994533\n",
      "Iteration 20622, loss = 1.29994087\n",
      "Iteration 20623, loss = 1.29993641\n",
      "Iteration 20624, loss = 1.29993194\n",
      "Iteration 20625, loss = 1.29992747\n",
      "Iteration 20626, loss = 1.29992300\n",
      "Iteration 20627, loss = 1.29991859\n",
      "Iteration 20628, loss = 1.29991421\n",
      "Iteration 20629, loss = 1.29990982\n",
      "Iteration 20630, loss = 1.29990546\n",
      "Iteration 20631, loss = 1.29990238\n",
      "Iteration 20632, loss = 1.29989729\n",
      "Iteration 20633, loss = 1.29989374\n",
      "Iteration 20634, loss = 1.29989015\n",
      "Iteration 20635, loss = 1.29988651\n",
      "Iteration 20636, loss = 1.29988278\n",
      "Iteration 20637, loss = 1.29987903\n",
      "Iteration 20638, loss = 1.29987517\n",
      "Iteration 20639, loss = 1.29987122\n",
      "Iteration 20640, loss = 1.29986717\n",
      "Iteration 20641, loss = 1.29986303\n",
      "Iteration 20642, loss = 1.29985881\n",
      "Iteration 20643, loss = 1.29985452\n",
      "Iteration 20644, loss = 1.29985017\n",
      "Iteration 20645, loss = 1.29984582\n",
      "Iteration 20646, loss = 1.29984146\n",
      "Iteration 20647, loss = 1.29983711\n",
      "Iteration 20648, loss = 1.29983277\n",
      "Iteration 20649, loss = 1.29982847\n",
      "Iteration 20650, loss = 1.29982420\n",
      "Iteration 20651, loss = 1.29981993\n",
      "Iteration 20652, loss = 1.29981564\n",
      "Iteration 20653, loss = 1.29981133\n",
      "Iteration 20654, loss = 1.29980702\n",
      "Iteration 20655, loss = 1.29980272\n",
      "Iteration 20656, loss = 1.29979844\n",
      "Iteration 20657, loss = 1.29979415\n",
      "Iteration 20658, loss = 1.29978987\n",
      "Iteration 20659, loss = 1.29978557\n",
      "Iteration 20660, loss = 1.29978128\n",
      "Iteration 20661, loss = 1.29977698\n",
      "Iteration 20662, loss = 1.29977268\n",
      "Iteration 20663, loss = 1.29976837\n",
      "Iteration 20664, loss = 1.29976408\n",
      "Iteration 20665, loss = 1.29975981\n",
      "Iteration 20666, loss = 1.29975554\n",
      "Iteration 20667, loss = 1.29975437\n",
      "Iteration 20668, loss = 1.29974780\n",
      "Iteration 20669, loss = 1.29974433\n",
      "Iteration 20670, loss = 1.29974088\n",
      "Iteration 20671, loss = 1.29973740\n",
      "Iteration 20672, loss = 1.29973381\n",
      "Iteration 20673, loss = 1.29973011\n",
      "Iteration 20674, loss = 1.29972631\n",
      "Iteration 20675, loss = 1.29972241\n",
      "Iteration 20676, loss = 1.29971840\n",
      "Iteration 20677, loss = 1.29971431\n",
      "Iteration 20678, loss = 1.29971013\n",
      "Iteration 20679, loss = 1.29970587\n",
      "Iteration 20680, loss = 1.29970155\n",
      "Iteration 20681, loss = 1.29969718\n",
      "Iteration 20682, loss = 1.29969282\n",
      "Iteration 20683, loss = 1.29968845\n",
      "Iteration 20684, loss = 1.29968406\n",
      "Iteration 20685, loss = 1.29967965\n",
      "Iteration 20686, loss = 1.29967523\n",
      "Iteration 20687, loss = 1.29967079\n",
      "Iteration 20688, loss = 1.29966638\n",
      "Iteration 20689, loss = 1.29966204\n",
      "Iteration 20690, loss = 1.29965776\n",
      "Iteration 20691, loss = 1.29965350\n",
      "Iteration 20692, loss = 1.29964926\n",
      "Iteration 20693, loss = 1.29965019\n",
      "Iteration 20694, loss = 1.29964118\n",
      "Iteration 20695, loss = 1.29963773\n",
      "Iteration 20696, loss = 1.29963423\n",
      "Iteration 20697, loss = 1.29963068\n",
      "Iteration 20698, loss = 1.29962707\n",
      "Iteration 20699, loss = 1.29962340\n",
      "Iteration 20700, loss = 1.29961964\n",
      "Iteration 20701, loss = 1.29961577\n",
      "Iteration 20702, loss = 1.29961180\n",
      "Iteration 20703, loss = 1.29960774\n",
      "Iteration 20704, loss = 1.29960360\n",
      "Iteration 20705, loss = 1.29959938\n",
      "Iteration 20706, loss = 1.29959511\n",
      "Iteration 20707, loss = 1.29959082\n",
      "Iteration 20708, loss = 1.29958652\n",
      "Iteration 20709, loss = 1.29958223\n",
      "Iteration 20710, loss = 1.29957798\n",
      "Iteration 20711, loss = 1.29957371\n",
      "Iteration 20712, loss = 1.29956946\n",
      "Iteration 20713, loss = 1.29956520\n",
      "Iteration 20714, loss = 1.29956094\n",
      "Iteration 20715, loss = 1.29955669\n",
      "Iteration 20716, loss = 1.29955245\n",
      "Iteration 20717, loss = 1.29954823\n",
      "Iteration 20718, loss = 1.29954402\n",
      "Iteration 20719, loss = 1.29953979\n",
      "Iteration 20720, loss = 1.29953558\n",
      "Iteration 20721, loss = 1.29953135\n",
      "Iteration 20722, loss = 1.29952709\n",
      "Iteration 20723, loss = 1.29952286\n",
      "Iteration 20724, loss = 1.29951861\n",
      "Iteration 20725, loss = 1.29951437\n",
      "Iteration 20726, loss = 1.29951015\n",
      "Iteration 20727, loss = 1.29950594\n",
      "Iteration 20728, loss = 1.29950174\n",
      "Iteration 20729, loss = 1.29949752\n",
      "Iteration 20730, loss = 1.29949997\n",
      "Iteration 20731, loss = 1.29949082\n",
      "Iteration 20732, loss = 1.29948865\n",
      "Iteration 20733, loss = 1.29948653\n",
      "Iteration 20734, loss = 1.29948430\n",
      "Iteration 20735, loss = 1.29948191\n",
      "Iteration 20736, loss = 1.29947934\n",
      "Iteration 20737, loss = 1.29947655\n",
      "Iteration 20738, loss = 1.29947354\n",
      "Iteration 20739, loss = 1.29947029\n",
      "Iteration 20740, loss = 1.29946682\n",
      "Iteration 20741, loss = 1.29946313\n",
      "Iteration 20742, loss = 1.29945925\n",
      "Iteration 20743, loss = 1.29945518\n",
      "Iteration 20744, loss = 1.29945095\n",
      "Iteration 20745, loss = 1.29944658\n",
      "Iteration 20746, loss = 1.29944209\n",
      "Iteration 20747, loss = 1.29943751\n",
      "Iteration 20748, loss = 1.29943285\n",
      "Iteration 20749, loss = 1.29942813\n",
      "Iteration 20750, loss = 1.29942337\n",
      "Iteration 20751, loss = 1.29941859\n",
      "Iteration 20752, loss = 1.29941380\n",
      "Iteration 20753, loss = 1.29940909\n",
      "Iteration 20754, loss = 1.29940444\n",
      "Iteration 20755, loss = 1.29939980\n",
      "Iteration 20756, loss = 1.29939519\n",
      "Iteration 20757, loss = 1.29939061\n",
      "Iteration 20758, loss = 1.29938606\n",
      "Iteration 20759, loss = 1.29938157\n",
      "Iteration 20760, loss = 1.29937711\n",
      "Iteration 20761, loss = 1.29937272\n",
      "Iteration 20762, loss = 1.29936839\n",
      "Iteration 20763, loss = 1.29936416\n",
      "Iteration 20764, loss = 1.29936008\n",
      "Iteration 20765, loss = 1.29935606\n",
      "Iteration 20766, loss = 1.29935202\n",
      "Iteration 20767, loss = 1.29934793\n",
      "Iteration 20768, loss = 1.29934381\n",
      "Iteration 20769, loss = 1.29933965\n",
      "Iteration 20770, loss = 1.29933545\n",
      "Iteration 20771, loss = 1.29933120\n",
      "Iteration 20772, loss = 1.29932692\n",
      "Iteration 20773, loss = 1.29932261\n",
      "Iteration 20774, loss = 1.29931828\n",
      "Iteration 20775, loss = 1.29931404\n",
      "Iteration 20776, loss = 1.29930987\n",
      "Iteration 20777, loss = 1.29930578\n",
      "Iteration 20778, loss = 1.29930167\n",
      "Iteration 20779, loss = 1.29929756\n",
      "Iteration 20780, loss = 1.29929346\n",
      "Iteration 20781, loss = 1.29928933\n",
      "Iteration 20782, loss = 1.29928520\n",
      "Iteration 20783, loss = 1.29928105\n",
      "Iteration 20784, loss = 1.29927690\n",
      "Iteration 20785, loss = 1.29927274\n",
      "Iteration 20786, loss = 1.29927032\n",
      "Iteration 20787, loss = 1.29926509\n",
      "Iteration 20788, loss = 1.29926160\n",
      "Iteration 20789, loss = 1.29925810\n",
      "Iteration 20790, loss = 1.29925455\n",
      "Iteration 20791, loss = 1.29925095\n",
      "Iteration 20792, loss = 1.29924729\n",
      "Iteration 20793, loss = 1.29924357\n",
      "Iteration 20794, loss = 1.29923981\n",
      "Iteration 20795, loss = 1.29923601\n",
      "Iteration 20796, loss = 1.29923212\n",
      "Iteration 20797, loss = 1.29922815\n",
      "Iteration 20798, loss = 1.29922411\n",
      "Iteration 20799, loss = 1.29922000\n",
      "Iteration 20800, loss = 1.29921584\n",
      "Iteration 20801, loss = 1.29921162\n",
      "Iteration 20802, loss = 1.29920741\n",
      "Iteration 20803, loss = 1.29920319\n",
      "Iteration 20804, loss = 1.29919895\n",
      "Iteration 20805, loss = 1.29919469\n",
      "Iteration 20806, loss = 1.29919042\n",
      "Iteration 20807, loss = 1.29918615\n",
      "Iteration 20808, loss = 1.29918187\n",
      "Iteration 20809, loss = 1.29917760\n",
      "Iteration 20810, loss = 1.29917336\n",
      "Iteration 20811, loss = 1.29916921\n",
      "Iteration 20812, loss = 1.29916519\n",
      "Iteration 20813, loss = 1.29916118\n",
      "Iteration 20814, loss = 1.29915717\n",
      "Iteration 20815, loss = 1.29915848\n",
      "Iteration 20816, loss = 1.29914910\n",
      "Iteration 20817, loss = 1.29914581\n",
      "Iteration 20818, loss = 1.29914251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20819, loss = 1.29913921\n",
      "Iteration 20820, loss = 1.29913579\n",
      "Iteration 20821, loss = 1.29913228\n",
      "Iteration 20822, loss = 1.29912866\n",
      "Iteration 20823, loss = 1.29912493\n",
      "Iteration 20824, loss = 1.29912112\n",
      "Iteration 20825, loss = 1.29911721\n",
      "Iteration 20826, loss = 1.29911322\n",
      "Iteration 20827, loss = 1.29910916\n",
      "Iteration 20828, loss = 1.29910503\n",
      "Iteration 20829, loss = 1.29910091\n",
      "Iteration 20830, loss = 1.29909676\n",
      "Iteration 20831, loss = 1.29909259\n",
      "Iteration 20832, loss = 1.29908839\n",
      "Iteration 20833, loss = 1.29908417\n",
      "Iteration 20834, loss = 1.29907993\n",
      "Iteration 20835, loss = 1.29907569\n",
      "Iteration 20836, loss = 1.29907149\n",
      "Iteration 20837, loss = 1.29906734\n",
      "Iteration 20838, loss = 1.29906322\n",
      "Iteration 20839, loss = 1.29905918\n",
      "Iteration 20840, loss = 1.29905513\n",
      "Iteration 20841, loss = 1.29905109\n",
      "Iteration 20842, loss = 1.29904704\n",
      "Iteration 20843, loss = 1.29904296\n",
      "Iteration 20844, loss = 1.29903884\n",
      "Iteration 20845, loss = 1.29903470\n",
      "Iteration 20846, loss = 1.29903053\n",
      "Iteration 20847, loss = 1.29902641\n",
      "Iteration 20848, loss = 1.29902231\n",
      "Iteration 20849, loss = 1.29901823\n",
      "Iteration 20850, loss = 1.29901416\n",
      "Iteration 20851, loss = 1.29901008\n",
      "Iteration 20852, loss = 1.29900601\n",
      "Iteration 20853, loss = 1.29900195\n",
      "Iteration 20854, loss = 1.29899899\n",
      "Iteration 20855, loss = 1.29899461\n",
      "Iteration 20856, loss = 1.29899132\n",
      "Iteration 20857, loss = 1.29898800\n",
      "Iteration 20858, loss = 1.29898466\n",
      "Iteration 20859, loss = 1.29898127\n",
      "Iteration 20860, loss = 1.29897778\n",
      "Iteration 20861, loss = 1.29897419\n",
      "Iteration 20862, loss = 1.29897050\n",
      "Iteration 20863, loss = 1.29896672\n",
      "Iteration 20864, loss = 1.29896285\n",
      "Iteration 20865, loss = 1.29895890\n",
      "Iteration 20866, loss = 1.29895488\n",
      "Iteration 20867, loss = 1.29895080\n",
      "Iteration 20868, loss = 1.29894667\n",
      "Iteration 20869, loss = 1.29894252\n",
      "Iteration 20870, loss = 1.29893839\n",
      "Iteration 20871, loss = 1.29893423\n",
      "Iteration 20872, loss = 1.29893005\n",
      "Iteration 20873, loss = 1.29892586\n",
      "Iteration 20874, loss = 1.29892166\n",
      "Iteration 20875, loss = 1.29891745\n",
      "Iteration 20876, loss = 1.29891326\n",
      "Iteration 20877, loss = 1.29890909\n",
      "Iteration 20878, loss = 1.29890499\n",
      "Iteration 20879, loss = 1.29890096\n",
      "Iteration 20880, loss = 1.29889694\n",
      "Iteration 20881, loss = 1.29889295\n",
      "Iteration 20882, loss = 1.29888894\n",
      "Iteration 20883, loss = 1.29888949\n",
      "Iteration 20884, loss = 1.29888107\n",
      "Iteration 20885, loss = 1.29887782\n",
      "Iteration 20886, loss = 1.29887454\n",
      "Iteration 20887, loss = 1.29887119\n",
      "Iteration 20888, loss = 1.29886782\n",
      "Iteration 20889, loss = 1.29886439\n",
      "Iteration 20890, loss = 1.29886085\n",
      "Iteration 20891, loss = 1.29885721\n",
      "Iteration 20892, loss = 1.29885347\n",
      "Iteration 20893, loss = 1.29884964\n",
      "Iteration 20894, loss = 1.29884572\n",
      "Iteration 20895, loss = 1.29884173\n",
      "Iteration 20896, loss = 1.29883767\n",
      "Iteration 20897, loss = 1.29883355\n",
      "Iteration 20898, loss = 1.29882942\n",
      "Iteration 20899, loss = 1.29882529\n",
      "Iteration 20900, loss = 1.29882115\n",
      "Iteration 20901, loss = 1.29881699\n",
      "Iteration 20902, loss = 1.29881282\n",
      "Iteration 20903, loss = 1.29880868\n",
      "Iteration 20904, loss = 1.29880452\n",
      "Iteration 20905, loss = 1.29880037\n",
      "Iteration 20906, loss = 1.29879626\n",
      "Iteration 20907, loss = 1.29879216\n",
      "Iteration 20908, loss = 1.29878806\n",
      "Iteration 20909, loss = 1.29878405\n",
      "Iteration 20910, loss = 1.29878008\n",
      "Iteration 20911, loss = 1.29877611\n",
      "Iteration 20912, loss = 1.29877212\n",
      "Iteration 20913, loss = 1.29876808\n",
      "Iteration 20914, loss = 1.29876403\n",
      "Iteration 20915, loss = 1.29875999\n",
      "Iteration 20916, loss = 1.29875592\n",
      "Iteration 20917, loss = 1.29875186\n",
      "Iteration 20918, loss = 1.29874777\n",
      "Iteration 20919, loss = 1.29874428\n",
      "Iteration 20920, loss = 1.29874054\n",
      "Iteration 20921, loss = 1.29873741\n",
      "Iteration 20922, loss = 1.29873423\n",
      "Iteration 20923, loss = 1.29873103\n",
      "Iteration 20924, loss = 1.29872778\n",
      "Iteration 20925, loss = 1.29872442\n",
      "Iteration 20926, loss = 1.29872096\n",
      "Iteration 20927, loss = 1.29871738\n",
      "Iteration 20928, loss = 1.29871370\n",
      "Iteration 20929, loss = 1.29870993\n",
      "Iteration 20930, loss = 1.29870606\n",
      "Iteration 20931, loss = 1.29870212\n",
      "Iteration 20932, loss = 1.29869810\n",
      "Iteration 20933, loss = 1.29869403\n",
      "Iteration 20934, loss = 1.29868995\n",
      "Iteration 20935, loss = 1.29868587\n",
      "Iteration 20936, loss = 1.29868175\n",
      "Iteration 20937, loss = 1.29867761\n",
      "Iteration 20938, loss = 1.29867346\n",
      "Iteration 20939, loss = 1.29866930\n",
      "Iteration 20940, loss = 1.29866513\n",
      "Iteration 20941, loss = 1.29866098\n",
      "Iteration 20942, loss = 1.29865684\n",
      "Iteration 20943, loss = 1.29865271\n",
      "Iteration 20944, loss = 1.29864857\n",
      "Iteration 20945, loss = 1.29864450\n",
      "Iteration 20946, loss = 1.29864046\n",
      "Iteration 20947, loss = 1.29863642\n",
      "Iteration 20948, loss = 1.29863253\n",
      "Iteration 20949, loss = 1.29862884\n",
      "Iteration 20950, loss = 1.29862554\n",
      "Iteration 20951, loss = 1.29862222\n",
      "Iteration 20952, loss = 1.29861885\n",
      "Iteration 20953, loss = 1.29861545\n",
      "Iteration 20954, loss = 1.29861199\n",
      "Iteration 20955, loss = 1.29860843\n",
      "Iteration 20956, loss = 1.29860478\n",
      "Iteration 20957, loss = 1.29860104\n",
      "Iteration 20958, loss = 1.29859722\n",
      "Iteration 20959, loss = 1.29859333\n",
      "Iteration 20960, loss = 1.29858937\n",
      "Iteration 20961, loss = 1.29858536\n",
      "Iteration 20962, loss = 1.29858130\n",
      "Iteration 20963, loss = 1.29857726\n",
      "Iteration 20964, loss = 1.29857320\n",
      "Iteration 20965, loss = 1.29856915\n",
      "Iteration 20966, loss = 1.29856515\n",
      "Iteration 20967, loss = 1.29856119\n",
      "Iteration 20968, loss = 1.29855724\n",
      "Iteration 20969, loss = 1.29855327\n",
      "Iteration 20970, loss = 1.29854927\n",
      "Iteration 20971, loss = 1.29854528\n",
      "Iteration 20972, loss = 1.29854128\n",
      "Iteration 20973, loss = 1.29853729\n",
      "Iteration 20974, loss = 1.29853328\n",
      "Iteration 20975, loss = 1.29852927\n",
      "Iteration 20976, loss = 1.29852527\n",
      "Iteration 20977, loss = 1.29852132\n",
      "Iteration 20978, loss = 1.29851739\n",
      "Iteration 20979, loss = 1.29851343\n",
      "Iteration 20980, loss = 1.29850945\n",
      "Iteration 20981, loss = 1.29850545\n",
      "Iteration 20982, loss = 1.29850148\n",
      "Iteration 20983, loss = 1.29849749\n",
      "Iteration 20984, loss = 1.29849352\n",
      "Iteration 20985, loss = 1.29849423\n",
      "Iteration 20986, loss = 1.29848640\n",
      "Iteration 20987, loss = 1.29848327\n",
      "Iteration 20988, loss = 1.29848008\n",
      "Iteration 20989, loss = 1.29847688\n",
      "Iteration 20990, loss = 1.29847364\n",
      "Iteration 20991, loss = 1.29847029\n",
      "Iteration 20992, loss = 1.29846683\n",
      "Iteration 20993, loss = 1.29846328\n",
      "Iteration 20994, loss = 1.29845963\n",
      "Iteration 20995, loss = 1.29845589\n",
      "Iteration 20996, loss = 1.29845207\n",
      "Iteration 20997, loss = 1.29844817\n",
      "Iteration 20998, loss = 1.29844421\n",
      "Iteration 20999, loss = 1.29844020\n",
      "Iteration 21000, loss = 1.29843617\n",
      "Iteration 21001, loss = 1.29843215\n",
      "Iteration 21002, loss = 1.29842810\n",
      "Iteration 21003, loss = 1.29842404\n",
      "Iteration 21004, loss = 1.29841996\n",
      "Iteration 21005, loss = 1.29841587\n",
      "Iteration 21006, loss = 1.29841181\n",
      "Iteration 21007, loss = 1.29840779\n",
      "Iteration 21008, loss = 1.29840384\n",
      "Iteration 21009, loss = 1.29839989\n",
      "Iteration 21010, loss = 1.29839593\n",
      "Iteration 21011, loss = 1.29839197\n",
      "Iteration 21012, loss = 1.29838806\n",
      "Iteration 21013, loss = 1.29838413\n",
      "Iteration 21014, loss = 1.29838019\n",
      "Iteration 21015, loss = 1.29837863\n",
      "Iteration 21016, loss = 1.29837270\n",
      "Iteration 21017, loss = 1.29836961\n",
      "Iteration 21018, loss = 1.29836648\n",
      "Iteration 21019, loss = 1.29836329\n",
      "Iteration 21020, loss = 1.29836008\n",
      "Iteration 21021, loss = 1.29835679\n",
      "Iteration 21022, loss = 1.29835340\n",
      "Iteration 21023, loss = 1.29834989\n",
      "Iteration 21024, loss = 1.29834629\n",
      "Iteration 21025, loss = 1.29834259\n",
      "Iteration 21026, loss = 1.29833880\n",
      "Iteration 21027, loss = 1.29833493\n",
      "Iteration 21028, loss = 1.29833099\n",
      "Iteration 21029, loss = 1.29832700\n",
      "Iteration 21030, loss = 1.29832297\n",
      "Iteration 21031, loss = 1.29831896\n",
      "Iteration 21032, loss = 1.29831493\n",
      "Iteration 21033, loss = 1.29831089\n",
      "Iteration 21034, loss = 1.29830684\n",
      "Iteration 21035, loss = 1.29830279\n",
      "Iteration 21036, loss = 1.29829874\n",
      "Iteration 21037, loss = 1.29829469\n",
      "Iteration 21038, loss = 1.29829067\n",
      "Iteration 21039, loss = 1.29828669\n",
      "Iteration 21040, loss = 1.29828273\n",
      "Iteration 21041, loss = 1.29827878\n",
      "Iteration 21042, loss = 1.29827482\n",
      "Iteration 21043, loss = 1.29827087\n",
      "Iteration 21044, loss = 1.29826692\n",
      "Iteration 21045, loss = 1.29826304\n",
      "Iteration 21046, loss = 1.29825916\n",
      "Iteration 21047, loss = 1.29825527\n",
      "Iteration 21048, loss = 1.29825137\n",
      "Iteration 21049, loss = 1.29824744\n",
      "Iteration 21050, loss = 1.29824785\n",
      "Iteration 21051, loss = 1.29824020\n",
      "Iteration 21052, loss = 1.29823718\n",
      "Iteration 21053, loss = 1.29823411\n",
      "Iteration 21054, loss = 1.29823104\n",
      "Iteration 21055, loss = 1.29822791\n",
      "Iteration 21056, loss = 1.29822467\n",
      "Iteration 21057, loss = 1.29822132\n",
      "Iteration 21058, loss = 1.29821787\n",
      "Iteration 21059, loss = 1.29821432\n",
      "Iteration 21060, loss = 1.29821068\n",
      "Iteration 21061, loss = 1.29820694\n",
      "Iteration 21062, loss = 1.29820313\n",
      "Iteration 21063, loss = 1.29819925\n",
      "Iteration 21064, loss = 1.29819531\n",
      "Iteration 21065, loss = 1.29819135\n",
      "Iteration 21066, loss = 1.29818739\n",
      "Iteration 21067, loss = 1.29818341\n",
      "Iteration 21068, loss = 1.29817940\n",
      "Iteration 21069, loss = 1.29817538\n",
      "Iteration 21070, loss = 1.29817134\n",
      "Iteration 21071, loss = 1.29816730\n",
      "Iteration 21072, loss = 1.29816326\n",
      "Iteration 21073, loss = 1.29815923\n",
      "Iteration 21074, loss = 1.29815523\n",
      "Iteration 21075, loss = 1.29815124\n",
      "Iteration 21076, loss = 1.29814727\n",
      "Iteration 21077, loss = 1.29814341\n",
      "Iteration 21078, loss = 1.29813958\n",
      "Iteration 21079, loss = 1.29813574\n",
      "Iteration 21080, loss = 1.29813188\n",
      "Iteration 21081, loss = 1.29813097\n",
      "Iteration 21082, loss = 1.29812439\n",
      "Iteration 21083, loss = 1.29812128\n",
      "Iteration 21084, loss = 1.29811815\n",
      "Iteration 21085, loss = 1.29811498\n",
      "Iteration 21086, loss = 1.29811180\n",
      "Iteration 21087, loss = 1.29810853\n",
      "Iteration 21088, loss = 1.29810515\n",
      "Iteration 21089, loss = 1.29810168\n",
      "Iteration 21090, loss = 1.29809812\n",
      "Iteration 21091, loss = 1.29809447\n",
      "Iteration 21092, loss = 1.29809074\n",
      "Iteration 21093, loss = 1.29808693\n",
      "Iteration 21094, loss = 1.29808306\n",
      "Iteration 21095, loss = 1.29807914\n",
      "Iteration 21096, loss = 1.29807521\n",
      "Iteration 21097, loss = 1.29807128\n",
      "Iteration 21098, loss = 1.29806731\n",
      "Iteration 21099, loss = 1.29806334\n",
      "Iteration 21100, loss = 1.29805942\n",
      "Iteration 21101, loss = 1.29805551\n",
      "Iteration 21102, loss = 1.29805159\n",
      "Iteration 21103, loss = 1.29804765\n",
      "Iteration 21104, loss = 1.29804370\n",
      "Iteration 21105, loss = 1.29803978\n",
      "Iteration 21106, loss = 1.29803588\n",
      "Iteration 21107, loss = 1.29803200\n",
      "Iteration 21108, loss = 1.29802817\n",
      "Iteration 21109, loss = 1.29802432\n",
      "Iteration 21110, loss = 1.29802046\n",
      "Iteration 21111, loss = 1.29801662\n",
      "Iteration 21112, loss = 1.29801280\n",
      "Iteration 21113, loss = 1.29800896\n",
      "Iteration 21114, loss = 1.29800508\n",
      "Iteration 21115, loss = 1.29800121\n",
      "Iteration 21116, loss = 1.29799734\n",
      "Iteration 21117, loss = 1.29799575\n",
      "Iteration 21118, loss = 1.29799046\n",
      "Iteration 21119, loss = 1.29798750\n",
      "Iteration 21120, loss = 1.29798450\n",
      "Iteration 21121, loss = 1.29798148\n",
      "Iteration 21122, loss = 1.29797838\n",
      "Iteration 21123, loss = 1.29797520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21124, loss = 1.29797190\n",
      "Iteration 21125, loss = 1.29796850\n",
      "Iteration 21126, loss = 1.29796500\n",
      "Iteration 21127, loss = 1.29796140\n",
      "Iteration 21128, loss = 1.29795772\n",
      "Iteration 21129, loss = 1.29795396\n",
      "Iteration 21130, loss = 1.29795013\n",
      "Iteration 21131, loss = 1.29794624\n",
      "Iteration 21132, loss = 1.29794234\n",
      "Iteration 21133, loss = 1.29793844\n",
      "Iteration 21134, loss = 1.29793451\n",
      "Iteration 21135, loss = 1.29793055\n",
      "Iteration 21136, loss = 1.29792659\n",
      "Iteration 21137, loss = 1.29792261\n",
      "Iteration 21138, loss = 1.29791863\n",
      "Iteration 21139, loss = 1.29791465\n",
      "Iteration 21140, loss = 1.29791070\n",
      "Iteration 21141, loss = 1.29790676\n",
      "Iteration 21142, loss = 1.29790289\n",
      "Iteration 21143, loss = 1.29789904\n",
      "Iteration 21144, loss = 1.29789520\n",
      "Iteration 21145, loss = 1.29789136\n",
      "Iteration 21146, loss = 1.29788754\n",
      "Iteration 21147, loss = 1.29788373\n",
      "Iteration 21148, loss = 1.29787998\n",
      "Iteration 21149, loss = 1.29788004\n",
      "Iteration 21150, loss = 1.29787262\n",
      "Iteration 21151, loss = 1.29786961\n",
      "Iteration 21152, loss = 1.29786656\n",
      "Iteration 21153, loss = 1.29786347\n",
      "Iteration 21154, loss = 1.29786037\n",
      "Iteration 21155, loss = 1.29785718\n",
      "Iteration 21156, loss = 1.29785390\n",
      "Iteration 21157, loss = 1.29785052\n",
      "Iteration 21158, loss = 1.29784704\n",
      "Iteration 21159, loss = 1.29784348\n",
      "Iteration 21160, loss = 1.29783982\n",
      "Iteration 21161, loss = 1.29783609\n",
      "Iteration 21162, loss = 1.29783230\n",
      "Iteration 21163, loss = 1.29782844\n",
      "Iteration 21164, loss = 1.29782456\n",
      "Iteration 21165, loss = 1.29782069\n",
      "Iteration 21166, loss = 1.29781678\n",
      "Iteration 21167, loss = 1.29781286\n",
      "Iteration 21168, loss = 1.29780892\n",
      "Iteration 21169, loss = 1.29780498\n",
      "Iteration 21170, loss = 1.29780106\n",
      "Iteration 21171, loss = 1.29779718\n",
      "Iteration 21172, loss = 1.29779334\n",
      "Iteration 21173, loss = 1.29778949\n",
      "Iteration 21174, loss = 1.29778567\n",
      "Iteration 21175, loss = 1.29778185\n",
      "Iteration 21176, loss = 1.29777803\n",
      "Iteration 21177, loss = 1.29777421\n",
      "Iteration 21178, loss = 1.29777043\n",
      "Iteration 21179, loss = 1.29776663\n",
      "Iteration 21180, loss = 1.29776284\n",
      "Iteration 21181, loss = 1.29775905\n",
      "Iteration 21182, loss = 1.29775528\n",
      "Iteration 21183, loss = 1.29775151\n",
      "Iteration 21184, loss = 1.29774990\n",
      "Iteration 21185, loss = 1.29774456\n",
      "Iteration 21186, loss = 1.29774167\n",
      "Iteration 21187, loss = 1.29773873\n",
      "Iteration 21188, loss = 1.29773580\n",
      "Iteration 21189, loss = 1.29773280\n",
      "Iteration 21190, loss = 1.29772969\n",
      "Iteration 21191, loss = 1.29772647\n",
      "Iteration 21192, loss = 1.29772314\n",
      "Iteration 21193, loss = 1.29771972\n",
      "Iteration 21194, loss = 1.29771619\n",
      "Iteration 21195, loss = 1.29771259\n",
      "Iteration 21196, loss = 1.29770890\n",
      "Iteration 21197, loss = 1.29770514\n",
      "Iteration 21198, loss = 1.29770131\n",
      "Iteration 21199, loss = 1.29769746\n",
      "Iteration 21200, loss = 1.29769361\n",
      "Iteration 21201, loss = 1.29768974\n",
      "Iteration 21202, loss = 1.29768585\n",
      "Iteration 21203, loss = 1.29768195\n",
      "Iteration 21204, loss = 1.29767803\n",
      "Iteration 21205, loss = 1.29767411\n",
      "Iteration 21206, loss = 1.29767019\n",
      "Iteration 21207, loss = 1.29766626\n",
      "Iteration 21208, loss = 1.29766237\n",
      "Iteration 21209, loss = 1.29765851\n",
      "Iteration 21210, loss = 1.29765470\n",
      "Iteration 21211, loss = 1.29765091\n",
      "Iteration 21212, loss = 1.29764711\n",
      "Iteration 21213, loss = 1.29764338\n",
      "Iteration 21214, loss = 1.29763968\n",
      "Iteration 21215, loss = 1.29763597\n",
      "Iteration 21216, loss = 1.29763225\n",
      "Iteration 21217, loss = 1.29762889\n",
      "Iteration 21218, loss = 1.29762503\n",
      "Iteration 21219, loss = 1.29762208\n",
      "Iteration 21220, loss = 1.29761910\n",
      "Iteration 21221, loss = 1.29761608\n",
      "Iteration 21222, loss = 1.29761302\n",
      "Iteration 21223, loss = 1.29760992\n",
      "Iteration 21224, loss = 1.29760672\n",
      "Iteration 21225, loss = 1.29760341\n",
      "Iteration 21226, loss = 1.29760001\n",
      "Iteration 21227, loss = 1.29759651\n",
      "Iteration 21228, loss = 1.29759293\n",
      "Iteration 21229, loss = 1.29758928\n",
      "Iteration 21230, loss = 1.29758555\n",
      "Iteration 21231, loss = 1.29758176\n",
      "Iteration 21232, loss = 1.29757794\n",
      "Iteration 21233, loss = 1.29757413\n",
      "Iteration 21234, loss = 1.29757029\n",
      "Iteration 21235, loss = 1.29756642\n",
      "Iteration 21236, loss = 1.29756254\n",
      "Iteration 21237, loss = 1.29755865\n",
      "Iteration 21238, loss = 1.29755479\n",
      "Iteration 21239, loss = 1.29755093\n",
      "Iteration 21240, loss = 1.29754707\n",
      "Iteration 21241, loss = 1.29754323\n",
      "Iteration 21242, loss = 1.29753946\n",
      "Iteration 21243, loss = 1.29753570\n",
      "Iteration 21244, loss = 1.29753199\n",
      "Iteration 21245, loss = 1.29752826\n",
      "Iteration 21246, loss = 1.29752452\n",
      "Iteration 21247, loss = 1.29752078\n",
      "Iteration 21248, loss = 1.29751707\n",
      "Iteration 21249, loss = 1.29751336\n",
      "Iteration 21250, loss = 1.29750964\n",
      "Iteration 21251, loss = 1.29750589\n",
      "Iteration 21252, loss = 1.29750579\n",
      "Iteration 21253, loss = 1.29749897\n",
      "Iteration 21254, loss = 1.29749615\n",
      "Iteration 21255, loss = 1.29749328\n",
      "Iteration 21256, loss = 1.29749039\n",
      "Iteration 21257, loss = 1.29748746\n",
      "Iteration 21258, loss = 1.29748443\n",
      "Iteration 21259, loss = 1.29748130\n",
      "Iteration 21260, loss = 1.29747805\n",
      "Iteration 21261, loss = 1.29747471\n",
      "Iteration 21262, loss = 1.29747127\n",
      "Iteration 21263, loss = 1.29746774\n",
      "Iteration 21264, loss = 1.29746413\n",
      "Iteration 21265, loss = 1.29746044\n",
      "Iteration 21266, loss = 1.29745669\n",
      "Iteration 21267, loss = 1.29745289\n",
      "Iteration 21268, loss = 1.29744911\n",
      "Iteration 21269, loss = 1.29744531\n",
      "Iteration 21270, loss = 1.29744148\n",
      "Iteration 21271, loss = 1.29743763\n",
      "Iteration 21272, loss = 1.29743378\n",
      "Iteration 21273, loss = 1.29742992\n",
      "Iteration 21274, loss = 1.29742605\n",
      "Iteration 21275, loss = 1.29742220\n",
      "Iteration 21276, loss = 1.29741836\n",
      "Iteration 21277, loss = 1.29741455\n",
      "Iteration 21278, loss = 1.29741079\n",
      "Iteration 21279, loss = 1.29740705\n",
      "Iteration 21280, loss = 1.29740331\n",
      "Iteration 21281, loss = 1.29739958\n",
      "Iteration 21282, loss = 1.29739585\n",
      "Iteration 21283, loss = 1.29739214\n",
      "Iteration 21284, loss = 1.29738849\n",
      "Iteration 21285, loss = 1.29738486\n",
      "Iteration 21286, loss = 1.29738120\n",
      "Iteration 21287, loss = 1.29737876\n",
      "Iteration 21288, loss = 1.29737417\n",
      "Iteration 21289, loss = 1.29737127\n",
      "Iteration 21290, loss = 1.29736835\n",
      "Iteration 21291, loss = 1.29736540\n",
      "Iteration 21292, loss = 1.29736243\n",
      "Iteration 21293, loss = 1.29735940\n",
      "Iteration 21294, loss = 1.29735626\n",
      "Iteration 21295, loss = 1.29735302\n",
      "Iteration 21296, loss = 1.29734969\n",
      "Iteration 21297, loss = 1.29734627\n",
      "Iteration 21298, loss = 1.29734276\n",
      "Iteration 21299, loss = 1.29733917\n",
      "Iteration 21300, loss = 1.29733551\n",
      "Iteration 21301, loss = 1.29733179\n",
      "Iteration 21302, loss = 1.29732802\n",
      "Iteration 21303, loss = 1.29732425\n",
      "Iteration 21304, loss = 1.29732048\n",
      "Iteration 21305, loss = 1.29731668\n",
      "Iteration 21306, loss = 1.29731286\n",
      "Iteration 21307, loss = 1.29730903\n",
      "Iteration 21308, loss = 1.29730520\n",
      "Iteration 21309, loss = 1.29730136\n",
      "Iteration 21310, loss = 1.29729753\n",
      "Iteration 21311, loss = 1.29729376\n",
      "Iteration 21312, loss = 1.29729002\n",
      "Iteration 21313, loss = 1.29728634\n",
      "Iteration 21314, loss = 1.29728267\n",
      "Iteration 21315, loss = 1.29727900\n",
      "Iteration 21316, loss = 1.29727534\n",
      "Iteration 21317, loss = 1.29727166\n",
      "Iteration 21318, loss = 1.29726799\n",
      "Iteration 21319, loss = 1.29726435\n",
      "Iteration 21320, loss = 1.29726069\n",
      "Iteration 21321, loss = 1.29726060\n",
      "Iteration 21322, loss = 1.29725379\n",
      "Iteration 21323, loss = 1.29725100\n",
      "Iteration 21324, loss = 1.29724818\n",
      "Iteration 21325, loss = 1.29724532\n",
      "Iteration 21326, loss = 1.29724242\n",
      "Iteration 21327, loss = 1.29723945\n",
      "Iteration 21328, loss = 1.29723636\n",
      "Iteration 21329, loss = 1.29723317\n",
      "Iteration 21330, loss = 1.29722989\n",
      "Iteration 21331, loss = 1.29722650\n",
      "Iteration 21332, loss = 1.29722303\n",
      "Iteration 21333, loss = 1.29721948\n",
      "Iteration 21334, loss = 1.29721586\n",
      "Iteration 21335, loss = 1.29721217\n",
      "Iteration 21336, loss = 1.29720847\n",
      "Iteration 21337, loss = 1.29720476\n",
      "Iteration 21338, loss = 1.29720102\n",
      "Iteration 21339, loss = 1.29719726\n",
      "Iteration 21340, loss = 1.29719348\n",
      "Iteration 21341, loss = 1.29718969\n",
      "Iteration 21342, loss = 1.29718589\n",
      "Iteration 21343, loss = 1.29718210\n",
      "Iteration 21344, loss = 1.29717833\n",
      "Iteration 21345, loss = 1.29717460\n",
      "Iteration 21346, loss = 1.29717091\n",
      "Iteration 21347, loss = 1.29716721\n",
      "Iteration 21348, loss = 1.29716351\n",
      "Iteration 21349, loss = 1.29715983\n",
      "Iteration 21350, loss = 1.29715618\n",
      "Iteration 21351, loss = 1.29715254\n",
      "Iteration 21352, loss = 1.29714889\n",
      "Iteration 21353, loss = 1.29714526\n",
      "Iteration 21354, loss = 1.29714166\n",
      "Iteration 21355, loss = 1.29713808\n",
      "Iteration 21356, loss = 1.29713447\n",
      "Iteration 21357, loss = 1.29713083\n",
      "Iteration 21358, loss = 1.29712719\n",
      "Iteration 21359, loss = 1.29712479\n",
      "Iteration 21360, loss = 1.29712048\n",
      "Iteration 21361, loss = 1.29711767\n",
      "Iteration 21362, loss = 1.29711484\n",
      "Iteration 21363, loss = 1.29711200\n",
      "Iteration 21364, loss = 1.29710914\n",
      "Iteration 21365, loss = 1.29710618\n",
      "Iteration 21366, loss = 1.29710313\n",
      "Iteration 21367, loss = 1.29709997\n",
      "Iteration 21368, loss = 1.29709671\n",
      "Iteration 21369, loss = 1.29709336\n",
      "Iteration 21370, loss = 1.29708992\n",
      "Iteration 21371, loss = 1.29708639\n",
      "Iteration 21372, loss = 1.29708279\n",
      "Iteration 21373, loss = 1.29707913\n",
      "Iteration 21374, loss = 1.29707542\n",
      "Iteration 21375, loss = 1.29707172\n",
      "Iteration 21376, loss = 1.29706800\n",
      "Iteration 21377, loss = 1.29706425\n",
      "Iteration 21378, loss = 1.29706048\n",
      "Iteration 21379, loss = 1.29705671\n",
      "Iteration 21380, loss = 1.29705292\n",
      "Iteration 21381, loss = 1.29704914\n",
      "Iteration 21382, loss = 1.29704535\n",
      "Iteration 21383, loss = 1.29704157\n",
      "Iteration 21384, loss = 1.29703779\n",
      "Iteration 21385, loss = 1.29703404\n",
      "Iteration 21386, loss = 1.29703034\n",
      "Iteration 21387, loss = 1.29702667\n",
      "Iteration 21388, loss = 1.29702305\n",
      "Iteration 21389, loss = 1.29701948\n",
      "Iteration 21390, loss = 1.29701595\n",
      "Iteration 21391, loss = 1.29701553\n",
      "Iteration 21392, loss = 1.29700905\n",
      "Iteration 21393, loss = 1.29700622\n",
      "Iteration 21394, loss = 1.29700336\n",
      "Iteration 21395, loss = 1.29700049\n",
      "Iteration 21396, loss = 1.29699758\n",
      "Iteration 21397, loss = 1.29699459\n",
      "Iteration 21398, loss = 1.29699150\n",
      "Iteration 21399, loss = 1.29698832\n",
      "Iteration 21400, loss = 1.29698505\n",
      "Iteration 21401, loss = 1.29698169\n",
      "Iteration 21402, loss = 1.29697826\n",
      "Iteration 21403, loss = 1.29697476\n",
      "Iteration 21404, loss = 1.29697120\n",
      "Iteration 21405, loss = 1.29696758\n",
      "Iteration 21406, loss = 1.29696392\n",
      "Iteration 21407, loss = 1.29696028\n",
      "Iteration 21408, loss = 1.29695662\n",
      "Iteration 21409, loss = 1.29695296\n",
      "Iteration 21410, loss = 1.29694931\n",
      "Iteration 21411, loss = 1.29694564\n",
      "Iteration 21412, loss = 1.29694199\n",
      "Iteration 21413, loss = 1.29693833\n",
      "Iteration 21414, loss = 1.29693472\n",
      "Iteration 21415, loss = 1.29693111\n",
      "Iteration 21416, loss = 1.29692753\n",
      "Iteration 21417, loss = 1.29692397\n",
      "Iteration 21418, loss = 1.29692040\n",
      "Iteration 21419, loss = 1.29691684\n",
      "Iteration 21420, loss = 1.29691329\n",
      "Iteration 21421, loss = 1.29690973\n",
      "Iteration 21422, loss = 1.29690615\n",
      "Iteration 21423, loss = 1.29690256\n",
      "Iteration 21424, loss = 1.29689896\n",
      "Iteration 21425, loss = 1.29689536\n",
      "Iteration 21426, loss = 1.29689179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21427, loss = 1.29688821\n",
      "Iteration 21428, loss = 1.29688463\n",
      "Iteration 21429, loss = 1.29688107\n",
      "Iteration 21430, loss = 1.29687753\n",
      "Iteration 21431, loss = 1.29687398\n",
      "Iteration 21432, loss = 1.29687043\n",
      "Iteration 21433, loss = 1.29686845\n",
      "Iteration 21434, loss = 1.29686403\n",
      "Iteration 21435, loss = 1.29686122\n",
      "Iteration 21436, loss = 1.29685840\n",
      "Iteration 21437, loss = 1.29685558\n",
      "Iteration 21438, loss = 1.29685271\n",
      "Iteration 21439, loss = 1.29684975\n",
      "Iteration 21440, loss = 1.29684670\n",
      "Iteration 21441, loss = 1.29684355\n",
      "Iteration 21442, loss = 1.29684030\n",
      "Iteration 21443, loss = 1.29683697\n",
      "Iteration 21444, loss = 1.29683356\n",
      "Iteration 21445, loss = 1.29683007\n",
      "Iteration 21446, loss = 1.29682652\n",
      "Iteration 21447, loss = 1.29682291\n",
      "Iteration 21448, loss = 1.29681925\n",
      "Iteration 21449, loss = 1.29681558\n",
      "Iteration 21450, loss = 1.29681192\n",
      "Iteration 21451, loss = 1.29680824\n",
      "Iteration 21452, loss = 1.29680455\n",
      "Iteration 21453, loss = 1.29680084\n",
      "Iteration 21454, loss = 1.29679714\n",
      "Iteration 21455, loss = 1.29679343\n",
      "Iteration 21456, loss = 1.29678973\n",
      "Iteration 21457, loss = 1.29678606\n",
      "Iteration 21458, loss = 1.29678246\n",
      "Iteration 21459, loss = 1.29677894\n",
      "Iteration 21460, loss = 1.29677544\n",
      "Iteration 21461, loss = 1.29677193\n",
      "Iteration 21462, loss = 1.29676847\n",
      "Iteration 21463, loss = 1.29676499\n",
      "Iteration 21464, loss = 1.29676621\n",
      "Iteration 21465, loss = 1.29675812\n",
      "Iteration 21466, loss = 1.29675544\n",
      "Iteration 21467, loss = 1.29675273\n",
      "Iteration 21468, loss = 1.29674995\n",
      "Iteration 21469, loss = 1.29674715\n",
      "Iteration 21470, loss = 1.29674428\n",
      "Iteration 21471, loss = 1.29674131\n",
      "Iteration 21472, loss = 1.29673823\n",
      "Iteration 21473, loss = 1.29673506\n",
      "Iteration 21474, loss = 1.29673179\n",
      "Iteration 21475, loss = 1.29672843\n",
      "Iteration 21476, loss = 1.29672499\n",
      "Iteration 21477, loss = 1.29672149\n",
      "Iteration 21478, loss = 1.29671792\n",
      "Iteration 21479, loss = 1.29671431\n",
      "Iteration 21480, loss = 1.29671070\n",
      "Iteration 21481, loss = 1.29670707\n",
      "Iteration 21482, loss = 1.29670343\n",
      "Iteration 21483, loss = 1.29669977\n",
      "Iteration 21484, loss = 1.29669610\n",
      "Iteration 21485, loss = 1.29669243\n",
      "Iteration 21486, loss = 1.29668879\n",
      "Iteration 21487, loss = 1.29668516\n",
      "Iteration 21488, loss = 1.29668156\n",
      "Iteration 21489, loss = 1.29667798\n",
      "Iteration 21490, loss = 1.29667443\n",
      "Iteration 21491, loss = 1.29667093\n",
      "Iteration 21492, loss = 1.29666747\n",
      "Iteration 21493, loss = 1.29666401\n",
      "Iteration 21494, loss = 1.29666054\n",
      "Iteration 21495, loss = 1.29665703\n",
      "Iteration 21496, loss = 1.29665351\n",
      "Iteration 21497, loss = 1.29664998\n",
      "Iteration 21498, loss = 1.29664644\n",
      "Iteration 21499, loss = 1.29664289\n",
      "Iteration 21500, loss = 1.29663934\n",
      "Iteration 21501, loss = 1.29663580\n",
      "Iteration 21502, loss = 1.29663231\n",
      "Iteration 21503, loss = 1.29662883\n",
      "Iteration 21504, loss = 1.29662535\n",
      "Iteration 21505, loss = 1.29662186\n",
      "Iteration 21506, loss = 1.29661838\n",
      "Iteration 21507, loss = 1.29661569\n",
      "Iteration 21508, loss = 1.29661212\n",
      "Iteration 21509, loss = 1.29660937\n",
      "Iteration 21510, loss = 1.29660661\n",
      "Iteration 21511, loss = 1.29660387\n",
      "Iteration 21512, loss = 1.29660106\n",
      "Iteration 21513, loss = 1.29659817\n",
      "Iteration 21514, loss = 1.29659518\n",
      "Iteration 21515, loss = 1.29659210\n",
      "Iteration 21516, loss = 1.29658893\n",
      "Iteration 21517, loss = 1.29658566\n",
      "Iteration 21518, loss = 1.29658232\n",
      "Iteration 21519, loss = 1.29657890\n",
      "Iteration 21520, loss = 1.29657541\n",
      "Iteration 21521, loss = 1.29657186\n",
      "Iteration 21522, loss = 1.29656827\n",
      "Iteration 21523, loss = 1.29656464\n",
      "Iteration 21524, loss = 1.29656103\n",
      "Iteration 21525, loss = 1.29655741\n",
      "Iteration 21526, loss = 1.29655378\n",
      "Iteration 21527, loss = 1.29655014\n",
      "Iteration 21528, loss = 1.29654649\n",
      "Iteration 21529, loss = 1.29654284\n",
      "Iteration 21530, loss = 1.29653920\n",
      "Iteration 21531, loss = 1.29653559\n",
      "Iteration 21532, loss = 1.29653200\n",
      "Iteration 21533, loss = 1.29652847\n",
      "Iteration 21534, loss = 1.29652498\n",
      "Iteration 21535, loss = 1.29652155\n",
      "Iteration 21536, loss = 1.29651813\n",
      "Iteration 21537, loss = 1.29651872\n",
      "Iteration 21538, loss = 1.29651150\n",
      "Iteration 21539, loss = 1.29650884\n",
      "Iteration 21540, loss = 1.29650615\n",
      "Iteration 21541, loss = 1.29650339\n",
      "Iteration 21542, loss = 1.29650058\n",
      "Iteration 21543, loss = 1.29649773\n",
      "Iteration 21544, loss = 1.29649479\n",
      "Iteration 21545, loss = 1.29649174\n",
      "Iteration 21546, loss = 1.29648859\n",
      "Iteration 21547, loss = 1.29648536\n",
      "Iteration 21548, loss = 1.29648205\n",
      "Iteration 21549, loss = 1.29647866\n",
      "Iteration 21550, loss = 1.29647521\n",
      "Iteration 21551, loss = 1.29647170\n",
      "Iteration 21552, loss = 1.29646817\n",
      "Iteration 21553, loss = 1.29646463\n",
      "Iteration 21554, loss = 1.29646107\n",
      "Iteration 21555, loss = 1.29645749\n",
      "Iteration 21556, loss = 1.29645391\n",
      "Iteration 21557, loss = 1.29645035\n",
      "Iteration 21558, loss = 1.29644682\n",
      "Iteration 21559, loss = 1.29644328\n",
      "Iteration 21560, loss = 1.29643977\n",
      "Iteration 21561, loss = 1.29643628\n",
      "Iteration 21562, loss = 1.29643281\n",
      "Iteration 21563, loss = 1.29642938\n",
      "Iteration 21564, loss = 1.29642595\n",
      "Iteration 21565, loss = 1.29642249\n",
      "Iteration 21566, loss = 1.29641905\n",
      "Iteration 21567, loss = 1.29641558\n",
      "Iteration 21568, loss = 1.29641213\n",
      "Iteration 21569, loss = 1.29640867\n",
      "Iteration 21570, loss = 1.29640519\n",
      "Iteration 21571, loss = 1.29640172\n",
      "Iteration 21572, loss = 1.29639825\n",
      "Iteration 21573, loss = 1.29639478\n",
      "Iteration 21574, loss = 1.29639134\n",
      "Iteration 21575, loss = 1.29638791\n",
      "Iteration 21576, loss = 1.29638446\n",
      "Iteration 21577, loss = 1.29638101\n",
      "Iteration 21578, loss = 1.29637758\n",
      "Iteration 21579, loss = 1.29637414\n",
      "Iteration 21580, loss = 1.29637071\n",
      "Iteration 21581, loss = 1.29636979\n",
      "Iteration 21582, loss = 1.29636458\n",
      "Iteration 21583, loss = 1.29636192\n",
      "Iteration 21584, loss = 1.29635927\n",
      "Iteration 21585, loss = 1.29635662\n",
      "Iteration 21586, loss = 1.29635389\n",
      "Iteration 21587, loss = 1.29635107\n",
      "Iteration 21588, loss = 1.29634815\n",
      "Iteration 21589, loss = 1.29634514\n",
      "Iteration 21590, loss = 1.29634204\n",
      "Iteration 21591, loss = 1.29633886\n",
      "Iteration 21592, loss = 1.29633559\n",
      "Iteration 21593, loss = 1.29633225\n",
      "Iteration 21594, loss = 1.29632884\n",
      "Iteration 21595, loss = 1.29632538\n",
      "Iteration 21596, loss = 1.29632187\n",
      "Iteration 21597, loss = 1.29631832\n",
      "Iteration 21598, loss = 1.29631480\n",
      "Iteration 21599, loss = 1.29631126\n",
      "Iteration 21600, loss = 1.29630772\n",
      "Iteration 21601, loss = 1.29630416\n",
      "Iteration 21602, loss = 1.29630059\n",
      "Iteration 21603, loss = 1.29629702\n",
      "Iteration 21604, loss = 1.29629346\n",
      "Iteration 21605, loss = 1.29628991\n",
      "Iteration 21606, loss = 1.29628642\n",
      "Iteration 21607, loss = 1.29628296\n",
      "Iteration 21608, loss = 1.29627957\n",
      "Iteration 21609, loss = 1.29627622\n",
      "Iteration 21610, loss = 1.29627286\n",
      "Iteration 21611, loss = 1.29626951\n",
      "Iteration 21612, loss = 1.29626614\n",
      "Iteration 21613, loss = 1.29626273\n",
      "Iteration 21614, loss = 1.29625930\n",
      "Iteration 21615, loss = 1.29625589\n",
      "Iteration 21616, loss = 1.29625277\n",
      "Iteration 21617, loss = 1.29625020\n",
      "Iteration 21618, loss = 1.29624761\n",
      "Iteration 21619, loss = 1.29624498\n",
      "Iteration 21620, loss = 1.29624232\n",
      "Iteration 21621, loss = 1.29623960\n",
      "Iteration 21622, loss = 1.29623677\n",
      "Iteration 21623, loss = 1.29623384\n",
      "Iteration 21624, loss = 1.29623081\n",
      "Iteration 21625, loss = 1.29622768\n",
      "Iteration 21626, loss = 1.29622445\n",
      "Iteration 21627, loss = 1.29622114\n",
      "Iteration 21628, loss = 1.29621776\n",
      "Iteration 21629, loss = 1.29621431\n",
      "Iteration 21630, loss = 1.29621080\n",
      "Iteration 21631, loss = 1.29620730\n",
      "Iteration 21632, loss = 1.29620378\n",
      "Iteration 21633, loss = 1.29620023\n",
      "Iteration 21634, loss = 1.29619667\n",
      "Iteration 21635, loss = 1.29619309\n",
      "Iteration 21636, loss = 1.29618951\n",
      "Iteration 21637, loss = 1.29618592\n",
      "Iteration 21638, loss = 1.29618234\n",
      "Iteration 21639, loss = 1.29617876\n",
      "Iteration 21640, loss = 1.29617522\n",
      "Iteration 21641, loss = 1.29617170\n",
      "Iteration 21642, loss = 1.29616820\n",
      "Iteration 21643, loss = 1.29616476\n",
      "Iteration 21644, loss = 1.29616137\n",
      "Iteration 21645, loss = 1.29615799\n",
      "Iteration 21646, loss = 1.29615466\n",
      "Iteration 21647, loss = 1.29615134\n",
      "Iteration 21648, loss = 1.29614800\n",
      "Iteration 21649, loss = 1.29614463\n",
      "Iteration 21650, loss = 1.29614122\n",
      "Iteration 21651, loss = 1.29613778\n",
      "Iteration 21652, loss = 1.29613434\n",
      "Iteration 21653, loss = 1.29613089\n",
      "Iteration 21654, loss = 1.29612746\n",
      "Iteration 21655, loss = 1.29612407\n",
      "Iteration 21656, loss = 1.29612418\n",
      "Iteration 21657, loss = 1.29611816\n",
      "Iteration 21658, loss = 1.29611564\n",
      "Iteration 21659, loss = 1.29611311\n",
      "Iteration 21660, loss = 1.29611059\n",
      "Iteration 21661, loss = 1.29610800\n",
      "Iteration 21662, loss = 1.29610532\n",
      "Iteration 21663, loss = 1.29610253\n",
      "Iteration 21664, loss = 1.29609965\n",
      "Iteration 21665, loss = 1.29609666\n",
      "Iteration 21666, loss = 1.29609358\n",
      "Iteration 21667, loss = 1.29609040\n",
      "Iteration 21668, loss = 1.29608715\n",
      "Iteration 21669, loss = 1.29608382\n",
      "Iteration 21670, loss = 1.29608042\n",
      "Iteration 21671, loss = 1.29607697\n",
      "Iteration 21672, loss = 1.29607348\n",
      "Iteration 21673, loss = 1.29607000\n",
      "Iteration 21674, loss = 1.29606650\n",
      "Iteration 21675, loss = 1.29606299\n",
      "Iteration 21676, loss = 1.29605947\n",
      "Iteration 21677, loss = 1.29605594\n",
      "Iteration 21678, loss = 1.29605241\n",
      "Iteration 21679, loss = 1.29604888\n",
      "Iteration 21680, loss = 1.29604535\n",
      "Iteration 21681, loss = 1.29604183\n",
      "Iteration 21682, loss = 1.29603831\n",
      "Iteration 21683, loss = 1.29603481\n",
      "Iteration 21684, loss = 1.29603131\n",
      "Iteration 21685, loss = 1.29602784\n",
      "Iteration 21686, loss = 1.29602441\n",
      "Iteration 21687, loss = 1.29602105\n",
      "Iteration 21688, loss = 1.29601778\n",
      "Iteration 21689, loss = 1.29601454\n",
      "Iteration 21690, loss = 1.29601128\n",
      "Iteration 21691, loss = 1.29601348\n",
      "Iteration 21692, loss = 1.29600469\n",
      "Iteration 21693, loss = 1.29600206\n",
      "Iteration 21694, loss = 1.29599942\n",
      "Iteration 21695, loss = 1.29599675\n",
      "Iteration 21696, loss = 1.29599408\n",
      "Iteration 21697, loss = 1.29599135\n",
      "Iteration 21698, loss = 1.29598853\n",
      "Iteration 21699, loss = 1.29598562\n",
      "Iteration 21700, loss = 1.29598262\n",
      "Iteration 21701, loss = 1.29597954\n",
      "Iteration 21702, loss = 1.29597638\n",
      "Iteration 21703, loss = 1.29597314\n",
      "Iteration 21704, loss = 1.29596984\n",
      "Iteration 21705, loss = 1.29596647\n",
      "Iteration 21706, loss = 1.29596306\n",
      "Iteration 21707, loss = 1.29595962\n",
      "Iteration 21708, loss = 1.29595619\n",
      "Iteration 21709, loss = 1.29595274\n",
      "Iteration 21710, loss = 1.29594927\n",
      "Iteration 21711, loss = 1.29594579\n",
      "Iteration 21712, loss = 1.29594230\n",
      "Iteration 21713, loss = 1.29593881\n",
      "Iteration 21714, loss = 1.29593534\n",
      "Iteration 21715, loss = 1.29593190\n",
      "Iteration 21716, loss = 1.29592848\n",
      "Iteration 21717, loss = 1.29592512\n",
      "Iteration 21718, loss = 1.29592181\n",
      "Iteration 21719, loss = 1.29591854\n",
      "Iteration 21720, loss = 1.29591528\n",
      "Iteration 21721, loss = 1.29591200\n",
      "Iteration 21722, loss = 1.29590869\n",
      "Iteration 21723, loss = 1.29590535\n",
      "Iteration 21724, loss = 1.29590198\n",
      "Iteration 21725, loss = 1.29589858\n",
      "Iteration 21726, loss = 1.29589519\n",
      "Iteration 21727, loss = 1.29589182\n",
      "Iteration 21728, loss = 1.29588847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21729, loss = 1.29588513\n",
      "Iteration 21730, loss = 1.29588185\n",
      "Iteration 21731, loss = 1.29587856\n",
      "Iteration 21732, loss = 1.29587527\n",
      "Iteration 21733, loss = 1.29587536\n",
      "Iteration 21734, loss = 1.29586948\n",
      "Iteration 21735, loss = 1.29586699\n",
      "Iteration 21736, loss = 1.29586446\n",
      "Iteration 21737, loss = 1.29586194\n",
      "Iteration 21738, loss = 1.29585937\n",
      "Iteration 21739, loss = 1.29585670\n",
      "Iteration 21740, loss = 1.29585394\n",
      "Iteration 21741, loss = 1.29585107\n",
      "Iteration 21742, loss = 1.29584811\n",
      "Iteration 21743, loss = 1.29584506\n",
      "Iteration 21744, loss = 1.29584192\n",
      "Iteration 21745, loss = 1.29583870\n",
      "Iteration 21746, loss = 1.29583541\n",
      "Iteration 21747, loss = 1.29583206\n",
      "Iteration 21748, loss = 1.29582867\n",
      "Iteration 21749, loss = 1.29582523\n",
      "Iteration 21750, loss = 1.29582178\n",
      "Iteration 21751, loss = 1.29581835\n",
      "Iteration 21752, loss = 1.29581490\n",
      "Iteration 21753, loss = 1.29581144\n",
      "Iteration 21754, loss = 1.29580798\n",
      "Iteration 21755, loss = 1.29580452\n",
      "Iteration 21756, loss = 1.29580105\n",
      "Iteration 21757, loss = 1.29579760\n",
      "Iteration 21758, loss = 1.29579415\n",
      "Iteration 21759, loss = 1.29579070\n",
      "Iteration 21760, loss = 1.29578730\n",
      "Iteration 21761, loss = 1.29578400\n",
      "Iteration 21762, loss = 1.29578074\n",
      "Iteration 21763, loss = 1.29577752\n",
      "Iteration 21764, loss = 1.29577432\n",
      "Iteration 21765, loss = 1.29577111\n",
      "Iteration 21766, loss = 1.29576785\n",
      "Iteration 21767, loss = 1.29576457\n",
      "Iteration 21768, loss = 1.29576125\n",
      "Iteration 21769, loss = 1.29575790\n",
      "Iteration 21770, loss = 1.29575559\n",
      "Iteration 21771, loss = 1.29575166\n",
      "Iteration 21772, loss = 1.29574920\n",
      "Iteration 21773, loss = 1.29574673\n",
      "Iteration 21774, loss = 1.29574424\n",
      "Iteration 21775, loss = 1.29574173\n",
      "Iteration 21776, loss = 1.29573915\n",
      "Iteration 21777, loss = 1.29573647\n",
      "Iteration 21778, loss = 1.29573368\n",
      "Iteration 21779, loss = 1.29573080\n",
      "Iteration 21780, loss = 1.29572781\n",
      "Iteration 21781, loss = 1.29572474\n",
      "Iteration 21782, loss = 1.29572157\n",
      "Iteration 21783, loss = 1.29571833\n",
      "Iteration 21784, loss = 1.29571501\n",
      "Iteration 21785, loss = 1.29571164\n",
      "Iteration 21786, loss = 1.29570824\n",
      "Iteration 21787, loss = 1.29570483\n",
      "Iteration 21788, loss = 1.29570140\n",
      "Iteration 21789, loss = 1.29569796\n",
      "Iteration 21790, loss = 1.29569450\n",
      "Iteration 21791, loss = 1.29569103\n",
      "Iteration 21792, loss = 1.29568756\n",
      "Iteration 21793, loss = 1.29568410\n",
      "Iteration 21794, loss = 1.29568063\n",
      "Iteration 21795, loss = 1.29567718\n",
      "Iteration 21796, loss = 1.29567373\n",
      "Iteration 21797, loss = 1.29567030\n",
      "Iteration 21798, loss = 1.29566690\n",
      "Iteration 21799, loss = 1.29566354\n",
      "Iteration 21800, loss = 1.29566021\n",
      "Iteration 21801, loss = 1.29565691\n",
      "Iteration 21802, loss = 1.29565371\n",
      "Iteration 21803, loss = 1.29565055\n",
      "Iteration 21804, loss = 1.29564737\n",
      "Iteration 21805, loss = 1.29564415\n",
      "Iteration 21806, loss = 1.29564090\n",
      "Iteration 21807, loss = 1.29564133\n",
      "Iteration 21808, loss = 1.29563452\n",
      "Iteration 21809, loss = 1.29563207\n",
      "Iteration 21810, loss = 1.29562959\n",
      "Iteration 21811, loss = 1.29562709\n",
      "Iteration 21812, loss = 1.29562459\n",
      "Iteration 21813, loss = 1.29562200\n",
      "Iteration 21814, loss = 1.29561933\n",
      "Iteration 21815, loss = 1.29561655\n",
      "Iteration 21816, loss = 1.29561368\n",
      "Iteration 21817, loss = 1.29561072\n",
      "Iteration 21818, loss = 1.29560768\n",
      "Iteration 21819, loss = 1.29560456\n",
      "Iteration 21820, loss = 1.29560136\n",
      "Iteration 21821, loss = 1.29559811\n",
      "Iteration 21822, loss = 1.29559479\n",
      "Iteration 21823, loss = 1.29559145\n",
      "Iteration 21824, loss = 1.29558811\n",
      "Iteration 21825, loss = 1.29558476\n",
      "Iteration 21826, loss = 1.29558138\n",
      "Iteration 21827, loss = 1.29557799\n",
      "Iteration 21828, loss = 1.29557460\n",
      "Iteration 21829, loss = 1.29557120\n",
      "Iteration 21830, loss = 1.29556779\n",
      "Iteration 21831, loss = 1.29556440\n",
      "Iteration 21832, loss = 1.29556100\n",
      "Iteration 21833, loss = 1.29555763\n",
      "Iteration 21834, loss = 1.29555430\n",
      "Iteration 21835, loss = 1.29555105\n",
      "Iteration 21836, loss = 1.29554785\n",
      "Iteration 21837, loss = 1.29554465\n",
      "Iteration 21838, loss = 1.29554144\n",
      "Iteration 21839, loss = 1.29553821\n",
      "Iteration 21840, loss = 1.29553500\n",
      "Iteration 21841, loss = 1.29553178\n",
      "Iteration 21842, loss = 1.29552852\n",
      "Iteration 21843, loss = 1.29552524\n",
      "Iteration 21844, loss = 1.29552195\n",
      "Iteration 21845, loss = 1.29551870\n",
      "Iteration 21846, loss = 1.29551545\n",
      "Iteration 21847, loss = 1.29551224\n",
      "Iteration 21848, loss = 1.29550902\n",
      "Iteration 21849, loss = 1.29550579\n",
      "Iteration 21850, loss = 1.29550257\n",
      "Iteration 21851, loss = 1.29549994\n",
      "Iteration 21852, loss = 1.29549689\n",
      "Iteration 21853, loss = 1.29549442\n",
      "Iteration 21854, loss = 1.29549195\n",
      "Iteration 21855, loss = 1.29548949\n",
      "Iteration 21856, loss = 1.29548700\n",
      "Iteration 21857, loss = 1.29548442\n",
      "Iteration 21858, loss = 1.29548174\n",
      "Iteration 21859, loss = 1.29547897\n",
      "Iteration 21860, loss = 1.29547610\n",
      "Iteration 21861, loss = 1.29547314\n",
      "Iteration 21862, loss = 1.29547009\n",
      "Iteration 21863, loss = 1.29546697\n",
      "Iteration 21864, loss = 1.29546377\n",
      "Iteration 21865, loss = 1.29546050\n",
      "Iteration 21866, loss = 1.29545719\n",
      "Iteration 21867, loss = 1.29545382\n",
      "Iteration 21868, loss = 1.29545045\n",
      "Iteration 21869, loss = 1.29544709\n",
      "Iteration 21870, loss = 1.29544371\n",
      "Iteration 21871, loss = 1.29544033\n",
      "Iteration 21872, loss = 1.29543693\n",
      "Iteration 21873, loss = 1.29543354\n",
      "Iteration 21874, loss = 1.29543014\n",
      "Iteration 21875, loss = 1.29542676\n",
      "Iteration 21876, loss = 1.29542337\n",
      "Iteration 21877, loss = 1.29542000\n",
      "Iteration 21878, loss = 1.29541665\n",
      "Iteration 21879, loss = 1.29541336\n",
      "Iteration 21880, loss = 1.29541012\n",
      "Iteration 21881, loss = 1.29540695\n",
      "Iteration 21882, loss = 1.29540380\n",
      "Iteration 21883, loss = 1.29540067\n",
      "Iteration 21884, loss = 1.29539753\n",
      "Iteration 21885, loss = 1.29539436\n",
      "Iteration 21886, loss = 1.29539467\n",
      "Iteration 21887, loss = 1.29538813\n",
      "Iteration 21888, loss = 1.29538574\n",
      "Iteration 21889, loss = 1.29538333\n",
      "Iteration 21890, loss = 1.29538087\n",
      "Iteration 21891, loss = 1.29537841\n",
      "Iteration 21892, loss = 1.29537588\n",
      "Iteration 21893, loss = 1.29537326\n",
      "Iteration 21894, loss = 1.29537054\n",
      "Iteration 21895, loss = 1.29536772\n",
      "Iteration 21896, loss = 1.29536481\n",
      "Iteration 21897, loss = 1.29536182\n",
      "Iteration 21898, loss = 1.29535875\n",
      "Iteration 21899, loss = 1.29535561\n",
      "Iteration 21900, loss = 1.29535240\n",
      "Iteration 21901, loss = 1.29534914\n",
      "Iteration 21902, loss = 1.29534585\n",
      "Iteration 21903, loss = 1.29534257\n",
      "Iteration 21904, loss = 1.29533927\n",
      "Iteration 21905, loss = 1.29533595\n",
      "Iteration 21906, loss = 1.29533261\n",
      "Iteration 21907, loss = 1.29532927\n",
      "Iteration 21908, loss = 1.29532592\n",
      "Iteration 21909, loss = 1.29532258\n",
      "Iteration 21910, loss = 1.29531924\n",
      "Iteration 21911, loss = 1.29531593\n",
      "Iteration 21912, loss = 1.29531267\n",
      "Iteration 21913, loss = 1.29530942\n",
      "Iteration 21914, loss = 1.29530620\n",
      "Iteration 21915, loss = 1.29530304\n",
      "Iteration 21916, loss = 1.29529991\n",
      "Iteration 21917, loss = 1.29529679\n",
      "Iteration 21918, loss = 1.29529367\n",
      "Iteration 21919, loss = 1.29529051\n",
      "Iteration 21920, loss = 1.29528732\n",
      "Iteration 21921, loss = 1.29528410\n",
      "Iteration 21922, loss = 1.29528086\n",
      "Iteration 21923, loss = 1.29527764\n",
      "Iteration 21924, loss = 1.29527443\n",
      "Iteration 21925, loss = 1.29527122\n",
      "Iteration 21926, loss = 1.29526802\n",
      "Iteration 21927, loss = 1.29526487\n",
      "Iteration 21928, loss = 1.29526174\n",
      "Iteration 21929, loss = 1.29525860\n",
      "Iteration 21930, loss = 1.29525545\n",
      "Iteration 21931, loss = 1.29525229\n",
      "Iteration 21932, loss = 1.29524912\n",
      "Iteration 21933, loss = 1.29524844\n",
      "Iteration 21934, loss = 1.29524349\n",
      "Iteration 21935, loss = 1.29524104\n",
      "Iteration 21936, loss = 1.29523862\n",
      "Iteration 21937, loss = 1.29523622\n",
      "Iteration 21938, loss = 1.29523375\n",
      "Iteration 21939, loss = 1.29523120\n",
      "Iteration 21940, loss = 1.29522856\n",
      "Iteration 21941, loss = 1.29522582\n",
      "Iteration 21942, loss = 1.29522300\n",
      "Iteration 21943, loss = 1.29522008\n",
      "Iteration 21944, loss = 1.29521709\n",
      "Iteration 21945, loss = 1.29521401\n",
      "Iteration 21946, loss = 1.29521087\n",
      "Iteration 21947, loss = 1.29520767\n",
      "Iteration 21948, loss = 1.29520442\n",
      "Iteration 21949, loss = 1.29520112\n",
      "Iteration 21950, loss = 1.29519779\n",
      "Iteration 21951, loss = 1.29519447\n",
      "Iteration 21952, loss = 1.29519116\n",
      "Iteration 21953, loss = 1.29518785\n",
      "Iteration 21954, loss = 1.29518452\n",
      "Iteration 21955, loss = 1.29518120\n",
      "Iteration 21956, loss = 1.29517787\n",
      "Iteration 21957, loss = 1.29517455\n",
      "Iteration 21958, loss = 1.29517124\n",
      "Iteration 21959, loss = 1.29516797\n",
      "Iteration 21960, loss = 1.29516475\n",
      "Iteration 21961, loss = 1.29516158\n",
      "Iteration 21962, loss = 1.29515848\n",
      "Iteration 21963, loss = 1.29515542\n",
      "Iteration 21964, loss = 1.29515235\n",
      "Iteration 21965, loss = 1.29514926\n",
      "Iteration 21966, loss = 1.29514613\n",
      "Iteration 21967, loss = 1.29514767\n",
      "Iteration 21968, loss = 1.29514001\n",
      "Iteration 21969, loss = 1.29513771\n",
      "Iteration 21970, loss = 1.29513539\n",
      "Iteration 21971, loss = 1.29513302\n",
      "Iteration 21972, loss = 1.29513062\n",
      "Iteration 21973, loss = 1.29512817\n",
      "Iteration 21974, loss = 1.29512562\n",
      "Iteration 21975, loss = 1.29512296\n",
      "Iteration 21976, loss = 1.29512021\n",
      "Iteration 21977, loss = 1.29511736\n",
      "Iteration 21978, loss = 1.29511442\n",
      "Iteration 21979, loss = 1.29511140\n",
      "Iteration 21980, loss = 1.29510831\n",
      "Iteration 21981, loss = 1.29510516\n",
      "Iteration 21982, loss = 1.29510195\n",
      "Iteration 21983, loss = 1.29509870\n",
      "Iteration 21984, loss = 1.29509546\n",
      "Iteration 21985, loss = 1.29509220\n",
      "Iteration 21986, loss = 1.29508893\n",
      "Iteration 21987, loss = 1.29508564\n",
      "Iteration 21988, loss = 1.29508234\n",
      "Iteration 21989, loss = 1.29507905\n",
      "Iteration 21990, loss = 1.29507575\n",
      "Iteration 21991, loss = 1.29507246\n",
      "Iteration 21992, loss = 1.29506917\n",
      "Iteration 21993, loss = 1.29506594\n",
      "Iteration 21994, loss = 1.29506276\n",
      "Iteration 21995, loss = 1.29505963\n",
      "Iteration 21996, loss = 1.29505652\n",
      "Iteration 21997, loss = 1.29505345\n",
      "Iteration 21998, loss = 1.29505039\n",
      "Iteration 21999, loss = 1.29504732\n",
      "Iteration 22000, loss = 1.29504422\n",
      "Iteration 22001, loss = 1.29504110\n",
      "Iteration 22002, loss = 1.29503794\n",
      "Iteration 22003, loss = 1.29503477\n",
      "Iteration 22004, loss = 1.29503158\n",
      "Iteration 22005, loss = 1.29502839\n",
      "Iteration 22006, loss = 1.29502524\n",
      "Iteration 22007, loss = 1.29502212\n",
      "Iteration 22008, loss = 1.29501902\n",
      "Iteration 22009, loss = 1.29501594\n",
      "Iteration 22010, loss = 1.29501286\n",
      "Iteration 22011, loss = 1.29500977\n",
      "Iteration 22012, loss = 1.29500667\n",
      "Iteration 22013, loss = 1.29500357\n",
      "Iteration 22014, loss = 1.29500045\n",
      "Iteration 22015, loss = 1.29499733\n",
      "Iteration 22016, loss = 1.29500120\n",
      "Iteration 22017, loss = 1.29499260\n",
      "Iteration 22018, loss = 1.29499136\n",
      "Iteration 22019, loss = 1.29499027\n",
      "Iteration 22020, loss = 1.29498916\n",
      "Iteration 22021, loss = 1.29498796\n",
      "Iteration 22022, loss = 1.29498661\n",
      "Iteration 22023, loss = 1.29498507\n",
      "Iteration 22024, loss = 1.29498331\n",
      "Iteration 22025, loss = 1.29498133\n",
      "Iteration 22026, loss = 1.29497911\n",
      "Iteration 22027, loss = 1.29497666\n",
      "Iteration 22028, loss = 1.29497399\n",
      "Iteration 22029, loss = 1.29497113\n",
      "Iteration 22030, loss = 1.29496808\n",
      "Iteration 22031, loss = 1.29496487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22032, loss = 1.29496153\n",
      "Iteration 22033, loss = 1.29495808\n",
      "Iteration 22034, loss = 1.29495454\n",
      "Iteration 22035, loss = 1.29495092\n",
      "Iteration 22036, loss = 1.29494726\n",
      "Iteration 22037, loss = 1.29494357\n",
      "Iteration 22038, loss = 1.29493986\n",
      "Iteration 22039, loss = 1.29493615\n",
      "Iteration 22040, loss = 1.29493245\n",
      "Iteration 22041, loss = 1.29492877\n",
      "Iteration 22042, loss = 1.29492511\n",
      "Iteration 22043, loss = 1.29492148\n",
      "Iteration 22044, loss = 1.29491795\n",
      "Iteration 22045, loss = 1.29491448\n",
      "Iteration 22046, loss = 1.29491105\n",
      "Iteration 22047, loss = 1.29490765\n",
      "Iteration 22048, loss = 1.29490429\n",
      "Iteration 22049, loss = 1.29490096\n",
      "Iteration 22050, loss = 1.29489766\n",
      "Iteration 22051, loss = 1.29489441\n",
      "Iteration 22052, loss = 1.29489126\n",
      "Iteration 22053, loss = 1.29488825\n",
      "Iteration 22054, loss = 1.29488529\n",
      "Iteration 22055, loss = 1.29488233\n",
      "Iteration 22056, loss = 1.29487935\n",
      "Iteration 22057, loss = 1.29487634\n",
      "Iteration 22058, loss = 1.29487329\n",
      "Iteration 22059, loss = 1.29487020\n",
      "Iteration 22060, loss = 1.29486709\n",
      "Iteration 22061, loss = 1.29486395\n",
      "Iteration 22062, loss = 1.29486078\n",
      "Iteration 22063, loss = 1.29485759\n",
      "Iteration 22064, loss = 1.29485440\n",
      "Iteration 22065, loss = 1.29485123\n",
      "Iteration 22066, loss = 1.29484815\n",
      "Iteration 22067, loss = 1.29484514\n",
      "Iteration 22068, loss = 1.29484212\n",
      "Iteration 22069, loss = 1.29483910\n",
      "Iteration 22070, loss = 1.29483606\n",
      "Iteration 22071, loss = 1.29483301\n",
      "Iteration 22072, loss = 1.29482996\n",
      "Iteration 22073, loss = 1.29482690\n",
      "Iteration 22074, loss = 1.29482383\n",
      "Iteration 22075, loss = 1.29482076\n",
      "Iteration 22076, loss = 1.29481769\n",
      "Iteration 22077, loss = 1.29481462\n",
      "Iteration 22078, loss = 1.29481154\n",
      "Iteration 22079, loss = 1.29480846\n",
      "Iteration 22080, loss = 1.29480538\n",
      "Iteration 22081, loss = 1.29480233\n",
      "Iteration 22082, loss = 1.29479927\n",
      "Iteration 22083, loss = 1.29479624\n",
      "Iteration 22084, loss = 1.29479320\n",
      "Iteration 22085, loss = 1.29479017\n",
      "Iteration 22086, loss = 1.29478711\n",
      "Iteration 22087, loss = 1.29478405\n",
      "Iteration 22088, loss = 1.29478101\n",
      "Iteration 22089, loss = 1.29477798\n",
      "Iteration 22090, loss = 1.29477494\n",
      "Iteration 22091, loss = 1.29477191\n",
      "Iteration 22092, loss = 1.29477005\n",
      "Iteration 22093, loss = 1.29476649\n",
      "Iteration 22094, loss = 1.29476414\n",
      "Iteration 22095, loss = 1.29476179\n",
      "Iteration 22096, loss = 1.29475942\n",
      "Iteration 22097, loss = 1.29475704\n",
      "Iteration 22098, loss = 1.29475463\n",
      "Iteration 22099, loss = 1.29475215\n",
      "Iteration 22100, loss = 1.29474958\n",
      "Iteration 22101, loss = 1.29474692\n",
      "Iteration 22102, loss = 1.29474418\n",
      "Iteration 22103, loss = 1.29474136\n",
      "Iteration 22104, loss = 1.29473846\n",
      "Iteration 22105, loss = 1.29473550\n",
      "Iteration 22106, loss = 1.29473248\n",
      "Iteration 22107, loss = 1.29472940\n",
      "Iteration 22108, loss = 1.29472628\n",
      "Iteration 22109, loss = 1.29472312\n",
      "Iteration 22110, loss = 1.29471995\n",
      "Iteration 22111, loss = 1.29471680\n",
      "Iteration 22112, loss = 1.29471363\n",
      "Iteration 22113, loss = 1.29471046\n",
      "Iteration 22114, loss = 1.29470728\n",
      "Iteration 22115, loss = 1.29470410\n",
      "Iteration 22116, loss = 1.29470093\n",
      "Iteration 22117, loss = 1.29469775\n",
      "Iteration 22118, loss = 1.29469459\n",
      "Iteration 22119, loss = 1.29469144\n",
      "Iteration 22120, loss = 1.29468835\n",
      "Iteration 22121, loss = 1.29468531\n",
      "Iteration 22122, loss = 1.29468234\n",
      "Iteration 22123, loss = 1.29467940\n",
      "Iteration 22124, loss = 1.29467644\n",
      "Iteration 22125, loss = 1.29467347\n",
      "Iteration 22126, loss = 1.29467047\n",
      "Iteration 22127, loss = 1.29466745\n",
      "Iteration 22128, loss = 1.29466439\n",
      "Iteration 22129, loss = 1.29466132\n",
      "Iteration 22130, loss = 1.29465824\n",
      "Iteration 22131, loss = 1.29465517\n",
      "Iteration 22132, loss = 1.29465211\n",
      "Iteration 22133, loss = 1.29464908\n",
      "Iteration 22134, loss = 1.29464609\n",
      "Iteration 22135, loss = 1.29464311\n",
      "Iteration 22136, loss = 1.29464053\n",
      "Iteration 22137, loss = 1.29463784\n",
      "Iteration 22138, loss = 1.29463558\n",
      "Iteration 22139, loss = 1.29463332\n",
      "Iteration 22140, loss = 1.29463108\n",
      "Iteration 22141, loss = 1.29462880\n",
      "Iteration 22142, loss = 1.29462643\n",
      "Iteration 22143, loss = 1.29462397\n",
      "Iteration 22144, loss = 1.29462141\n",
      "Iteration 22145, loss = 1.29461877\n",
      "Iteration 22146, loss = 1.29461602\n",
      "Iteration 22147, loss = 1.29461320\n",
      "Iteration 22148, loss = 1.29461029\n",
      "Iteration 22149, loss = 1.29460731\n",
      "Iteration 22150, loss = 1.29460427\n",
      "Iteration 22151, loss = 1.29460117\n",
      "Iteration 22152, loss = 1.29459803\n",
      "Iteration 22153, loss = 1.29459485\n",
      "Iteration 22154, loss = 1.29459168\n",
      "Iteration 22155, loss = 1.29458851\n",
      "Iteration 22156, loss = 1.29458533\n",
      "Iteration 22157, loss = 1.29458214\n",
      "Iteration 22158, loss = 1.29457895\n",
      "Iteration 22159, loss = 1.29457576\n",
      "Iteration 22160, loss = 1.29457258\n",
      "Iteration 22161, loss = 1.29456940\n",
      "Iteration 22162, loss = 1.29456623\n",
      "Iteration 22163, loss = 1.29456307\n",
      "Iteration 22164, loss = 1.29455992\n",
      "Iteration 22165, loss = 1.29455679\n",
      "Iteration 22166, loss = 1.29455373\n",
      "Iteration 22167, loss = 1.29455071\n",
      "Iteration 22168, loss = 1.29454778\n",
      "Iteration 22169, loss = 1.29454490\n",
      "Iteration 22170, loss = 1.29454197\n",
      "Iteration 22171, loss = 1.29453902\n",
      "Iteration 22172, loss = 1.29453604\n",
      "Iteration 22173, loss = 1.29453303\n",
      "Iteration 22174, loss = 1.29452999\n",
      "Iteration 22175, loss = 1.29452693\n",
      "Iteration 22176, loss = 1.29452385\n",
      "Iteration 22177, loss = 1.29452079\n",
      "Iteration 22178, loss = 1.29451777\n",
      "Iteration 22179, loss = 1.29451481\n",
      "Iteration 22180, loss = 1.29451257\n",
      "Iteration 22181, loss = 1.29450965\n",
      "Iteration 22182, loss = 1.29450746\n",
      "Iteration 22183, loss = 1.29450525\n",
      "Iteration 22184, loss = 1.29450306\n",
      "Iteration 22185, loss = 1.29450083\n",
      "Iteration 22186, loss = 1.29449852\n",
      "Iteration 22187, loss = 1.29449612\n",
      "Iteration 22188, loss = 1.29449362\n",
      "Iteration 22189, loss = 1.29449102\n",
      "Iteration 22190, loss = 1.29448834\n",
      "Iteration 22191, loss = 1.29448556\n",
      "Iteration 22192, loss = 1.29448271\n",
      "Iteration 22193, loss = 1.29447979\n",
      "Iteration 22194, loss = 1.29447680\n",
      "Iteration 22195, loss = 1.29447376\n",
      "Iteration 22196, loss = 1.29447067\n",
      "Iteration 22197, loss = 1.29446754\n",
      "Iteration 22198, loss = 1.29446441\n",
      "Iteration 22199, loss = 1.29446129\n",
      "Iteration 22200, loss = 1.29445816\n",
      "Iteration 22201, loss = 1.29445502\n",
      "Iteration 22202, loss = 1.29445187\n",
      "Iteration 22203, loss = 1.29444873\n",
      "Iteration 22204, loss = 1.29444559\n",
      "Iteration 22205, loss = 1.29444245\n",
      "Iteration 22206, loss = 1.29443932\n",
      "Iteration 22207, loss = 1.29443620\n",
      "Iteration 22208, loss = 1.29443309\n",
      "Iteration 22209, loss = 1.29442999\n",
      "Iteration 22210, loss = 1.29442690\n",
      "Iteration 22211, loss = 1.29442387\n",
      "Iteration 22212, loss = 1.29442093\n",
      "Iteration 22213, loss = 1.29441805\n",
      "Iteration 22214, loss = 1.29441520\n",
      "Iteration 22215, loss = 1.29441232\n",
      "Iteration 22216, loss = 1.29440940\n",
      "Iteration 22217, loss = 1.29440645\n",
      "Iteration 22218, loss = 1.29440346\n",
      "Iteration 22219, loss = 1.29440046\n",
      "Iteration 22220, loss = 1.29439742\n",
      "Iteration 22221, loss = 1.29439623\n",
      "Iteration 22222, loss = 1.29439188\n",
      "Iteration 22223, loss = 1.29438970\n",
      "Iteration 22224, loss = 1.29438751\n",
      "Iteration 22225, loss = 1.29438532\n",
      "Iteration 22226, loss = 1.29438315\n",
      "Iteration 22227, loss = 1.29438089\n",
      "Iteration 22228, loss = 1.29437854\n",
      "Iteration 22229, loss = 1.29437610\n",
      "Iteration 22230, loss = 1.29437356\n",
      "Iteration 22231, loss = 1.29437092\n",
      "Iteration 22232, loss = 1.29436819\n",
      "Iteration 22233, loss = 1.29436537\n",
      "Iteration 22234, loss = 1.29436248\n",
      "Iteration 22235, loss = 1.29435952\n",
      "Iteration 22236, loss = 1.29435649\n",
      "Iteration 22237, loss = 1.29435342\n",
      "Iteration 22238, loss = 1.29435031\n",
      "Iteration 22239, loss = 1.29434721\n",
      "Iteration 22240, loss = 1.29434410\n",
      "Iteration 22241, loss = 1.29434097\n",
      "Iteration 22242, loss = 1.29433783\n",
      "Iteration 22243, loss = 1.29433468\n",
      "Iteration 22244, loss = 1.29433154\n",
      "Iteration 22245, loss = 1.29432840\n",
      "Iteration 22246, loss = 1.29432526\n",
      "Iteration 22247, loss = 1.29432213\n",
      "Iteration 22248, loss = 1.29431902\n",
      "Iteration 22249, loss = 1.29431591\n",
      "Iteration 22250, loss = 1.29431282\n",
      "Iteration 22251, loss = 1.29430973\n",
      "Iteration 22252, loss = 1.29430667\n",
      "Iteration 22253, loss = 1.29430365\n",
      "Iteration 22254, loss = 1.29430072\n",
      "Iteration 22255, loss = 1.29429786\n",
      "Iteration 22256, loss = 1.29429503\n",
      "Iteration 22257, loss = 1.29429216\n",
      "Iteration 22258, loss = 1.29428927\n",
      "Iteration 22259, loss = 1.29428634\n",
      "Iteration 22260, loss = 1.29428338\n",
      "Iteration 22261, loss = 1.29428040\n",
      "Iteration 22262, loss = 1.29427739\n",
      "Iteration 22263, loss = 1.29427436\n",
      "Iteration 22264, loss = 1.29427314\n",
      "Iteration 22265, loss = 1.29426909\n",
      "Iteration 22266, loss = 1.29426696\n",
      "Iteration 22267, loss = 1.29426483\n",
      "Iteration 22268, loss = 1.29426271\n",
      "Iteration 22269, loss = 1.29426057\n",
      "Iteration 22270, loss = 1.29425835\n",
      "Iteration 22271, loss = 1.29425604\n",
      "Iteration 22272, loss = 1.29425363\n",
      "Iteration 22273, loss = 1.29425113\n",
      "Iteration 22274, loss = 1.29424853\n",
      "Iteration 22275, loss = 1.29424584\n",
      "Iteration 22276, loss = 1.29424307\n",
      "Iteration 22277, loss = 1.29424022\n",
      "Iteration 22278, loss = 1.29423730\n",
      "Iteration 22279, loss = 1.29423433\n",
      "Iteration 22280, loss = 1.29423130\n",
      "Iteration 22281, loss = 1.29422823\n",
      "Iteration 22282, loss = 1.29422516\n",
      "Iteration 22283, loss = 1.29422209\n",
      "Iteration 22284, loss = 1.29421901\n",
      "Iteration 22285, loss = 1.29421592\n",
      "Iteration 22286, loss = 1.29421282\n",
      "Iteration 22287, loss = 1.29420972\n",
      "Iteration 22288, loss = 1.29420663\n",
      "Iteration 22289, loss = 1.29420354\n",
      "Iteration 22290, loss = 1.29420045\n",
      "Iteration 22291, loss = 1.29419738\n",
      "Iteration 22292, loss = 1.29419431\n",
      "Iteration 22293, loss = 1.29419126\n",
      "Iteration 22294, loss = 1.29418821\n",
      "Iteration 22295, loss = 1.29418518\n",
      "Iteration 22296, loss = 1.29418216\n",
      "Iteration 22297, loss = 1.29417920\n",
      "Iteration 22298, loss = 1.29417630\n",
      "Iteration 22299, loss = 1.29417347\n",
      "Iteration 22300, loss = 1.29417066\n",
      "Iteration 22301, loss = 1.29416783\n",
      "Iteration 22302, loss = 1.29416495\n",
      "Iteration 22303, loss = 1.29416204\n",
      "Iteration 22304, loss = 1.29415911\n",
      "Iteration 22305, loss = 1.29415615\n",
      "Iteration 22306, loss = 1.29415317\n",
      "Iteration 22307, loss = 1.29415019\n",
      "Iteration 22308, loss = 1.29414724\n",
      "Iteration 22309, loss = 1.29414434\n",
      "Iteration 22310, loss = 1.29414379\n",
      "Iteration 22311, loss = 1.29413926\n",
      "Iteration 22312, loss = 1.29413709\n",
      "Iteration 22313, loss = 1.29413493\n",
      "Iteration 22314, loss = 1.29413279\n",
      "Iteration 22315, loss = 1.29413064\n",
      "Iteration 22316, loss = 1.29412842\n",
      "Iteration 22317, loss = 1.29412610\n",
      "Iteration 22318, loss = 1.29412370\n",
      "Iteration 22319, loss = 1.29412120\n",
      "Iteration 22320, loss = 1.29411860\n",
      "Iteration 22321, loss = 1.29411592\n",
      "Iteration 22322, loss = 1.29411315\n",
      "Iteration 22323, loss = 1.29411031\n",
      "Iteration 22324, loss = 1.29410740\n",
      "Iteration 22325, loss = 1.29410443\n",
      "Iteration 22326, loss = 1.29410141\n",
      "Iteration 22327, loss = 1.29409835\n",
      "Iteration 22328, loss = 1.29409528\n",
      "Iteration 22329, loss = 1.29409223\n",
      "Iteration 22330, loss = 1.29408915\n",
      "Iteration 22331, loss = 1.29408607\n",
      "Iteration 22332, loss = 1.29408299\n",
      "Iteration 22333, loss = 1.29407991\n",
      "Iteration 22334, loss = 1.29407683\n",
      "Iteration 22335, loss = 1.29407375\n",
      "Iteration 22336, loss = 1.29407068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22337, loss = 1.29406763\n",
      "Iteration 22338, loss = 1.29406458\n",
      "Iteration 22339, loss = 1.29406155\n",
      "Iteration 22340, loss = 1.29405853\n",
      "Iteration 22341, loss = 1.29405552\n",
      "Iteration 22342, loss = 1.29405260\n",
      "Iteration 22343, loss = 1.29404974\n",
      "Iteration 22344, loss = 1.29404694\n",
      "Iteration 22345, loss = 1.29404413\n",
      "Iteration 22346, loss = 1.29404132\n",
      "Iteration 22347, loss = 1.29403847\n",
      "Iteration 22348, loss = 1.29403560\n",
      "Iteration 22349, loss = 1.29403269\n",
      "Iteration 22350, loss = 1.29403163\n",
      "Iteration 22351, loss = 1.29402715\n",
      "Iteration 22352, loss = 1.29402505\n",
      "Iteration 22353, loss = 1.29402294\n",
      "Iteration 22354, loss = 1.29402080\n",
      "Iteration 22355, loss = 1.29401867\n",
      "Iteration 22356, loss = 1.29401648\n",
      "Iteration 22357, loss = 1.29401420\n",
      "Iteration 22358, loss = 1.29401183\n",
      "Iteration 22359, loss = 1.29400937\n",
      "Iteration 22360, loss = 1.29400681\n",
      "Iteration 22361, loss = 1.29400417\n",
      "Iteration 22362, loss = 1.29400145\n",
      "Iteration 22363, loss = 1.29399866\n",
      "Iteration 22364, loss = 1.29399580\n",
      "Iteration 22365, loss = 1.29399289\n",
      "Iteration 22366, loss = 1.29398992\n",
      "Iteration 22367, loss = 1.29398693\n",
      "Iteration 22368, loss = 1.29398393\n",
      "Iteration 22369, loss = 1.29398092\n",
      "Iteration 22370, loss = 1.29397791\n",
      "Iteration 22371, loss = 1.29397488\n",
      "Iteration 22372, loss = 1.29397185\n",
      "Iteration 22373, loss = 1.29396881\n",
      "Iteration 22374, loss = 1.29396578\n",
      "Iteration 22375, loss = 1.29396275\n",
      "Iteration 22376, loss = 1.29395973\n",
      "Iteration 22377, loss = 1.29395672\n",
      "Iteration 22378, loss = 1.29395371\n",
      "Iteration 22379, loss = 1.29395072\n",
      "Iteration 22380, loss = 1.29394778\n",
      "Iteration 22381, loss = 1.29394490\n",
      "Iteration 22382, loss = 1.29394205\n",
      "Iteration 22383, loss = 1.29393927\n",
      "Iteration 22384, loss = 1.29393648\n",
      "Iteration 22385, loss = 1.29393368\n",
      "Iteration 22386, loss = 1.29393085\n",
      "Iteration 22387, loss = 1.29392800\n",
      "Iteration 22388, loss = 1.29392512\n",
      "Iteration 22389, loss = 1.29392221\n",
      "Iteration 22390, loss = 1.29391928\n",
      "Iteration 22391, loss = 1.29391634\n",
      "Iteration 22392, loss = 1.29391344\n",
      "Iteration 22393, loss = 1.29391057\n",
      "Iteration 22394, loss = 1.29390773\n",
      "Iteration 22395, loss = 1.29390492\n",
      "Iteration 22396, loss = 1.29390210\n",
      "Iteration 22397, loss = 1.29389927\n",
      "Iteration 22398, loss = 1.29389643\n",
      "Iteration 22399, loss = 1.29389358\n",
      "Iteration 22400, loss = 1.29389073\n",
      "Iteration 22401, loss = 1.29388791\n",
      "Iteration 22402, loss = 1.29388568\n",
      "Iteration 22403, loss = 1.29388352\n",
      "Iteration 22404, loss = 1.29388140\n",
      "Iteration 22405, loss = 1.29387930\n",
      "Iteration 22406, loss = 1.29387715\n",
      "Iteration 22407, loss = 1.29387491\n",
      "Iteration 22408, loss = 1.29387259\n",
      "Iteration 22409, loss = 1.29387019\n",
      "Iteration 22410, loss = 1.29386770\n",
      "Iteration 22411, loss = 1.29386512\n",
      "Iteration 22412, loss = 1.29386246\n",
      "Iteration 22413, loss = 1.29385972\n",
      "Iteration 22414, loss = 1.29385691\n",
      "Iteration 22415, loss = 1.29385404\n",
      "Iteration 22416, loss = 1.29385112\n",
      "Iteration 22417, loss = 1.29384816\n",
      "Iteration 22418, loss = 1.29384515\n",
      "Iteration 22419, loss = 1.29384212\n",
      "Iteration 22420, loss = 1.29383911\n",
      "Iteration 22421, loss = 1.29383611\n",
      "Iteration 22422, loss = 1.29383310\n",
      "Iteration 22423, loss = 1.29383008\n",
      "Iteration 22424, loss = 1.29382707\n",
      "Iteration 22425, loss = 1.29382406\n",
      "Iteration 22426, loss = 1.29382105\n",
      "Iteration 22427, loss = 1.29381806\n",
      "Iteration 22428, loss = 1.29381507\n",
      "Iteration 22429, loss = 1.29381212\n",
      "Iteration 22430, loss = 1.29380923\n",
      "Iteration 22431, loss = 1.29380642\n",
      "Iteration 22432, loss = 1.29380370\n",
      "Iteration 22433, loss = 1.29380095\n",
      "Iteration 22434, loss = 1.29379818\n",
      "Iteration 22435, loss = 1.29379539\n",
      "Iteration 22436, loss = 1.29379256\n",
      "Iteration 22437, loss = 1.29378971\n",
      "Iteration 22438, loss = 1.29378684\n",
      "Iteration 22439, loss = 1.29378395\n",
      "Iteration 22440, loss = 1.29378192\n",
      "Iteration 22441, loss = 1.29377869\n",
      "Iteration 22442, loss = 1.29377671\n",
      "Iteration 22443, loss = 1.29377472\n",
      "Iteration 22444, loss = 1.29377269\n",
      "Iteration 22445, loss = 1.29377064\n",
      "Iteration 22446, loss = 1.29376854\n",
      "Iteration 22447, loss = 1.29376634\n",
      "Iteration 22448, loss = 1.29376404\n",
      "Iteration 22449, loss = 1.29376164\n",
      "Iteration 22450, loss = 1.29375915\n",
      "Iteration 22451, loss = 1.29375656\n",
      "Iteration 22452, loss = 1.29375388\n",
      "Iteration 22453, loss = 1.29375113\n",
      "Iteration 22454, loss = 1.29374830\n",
      "Iteration 22455, loss = 1.29374542\n",
      "Iteration 22456, loss = 1.29374249\n",
      "Iteration 22457, loss = 1.29373952\n",
      "Iteration 22458, loss = 1.29373657\n",
      "Iteration 22459, loss = 1.29373359\n",
      "Iteration 22460, loss = 1.29373061\n",
      "Iteration 22461, loss = 1.29372761\n",
      "Iteration 22462, loss = 1.29372461\n",
      "Iteration 22463, loss = 1.29372161\n",
      "Iteration 22464, loss = 1.29371861\n",
      "Iteration 22465, loss = 1.29371562\n",
      "Iteration 22466, loss = 1.29371263\n",
      "Iteration 22467, loss = 1.29370966\n",
      "Iteration 22468, loss = 1.29370670\n",
      "Iteration 22469, loss = 1.29370375\n",
      "Iteration 22470, loss = 1.29370081\n",
      "Iteration 22471, loss = 1.29369791\n",
      "Iteration 22472, loss = 1.29369506\n",
      "Iteration 22473, loss = 1.29369229\n",
      "Iteration 22474, loss = 1.29368954\n",
      "Iteration 22475, loss = 1.29368681\n",
      "Iteration 22476, loss = 1.29368407\n",
      "Iteration 22477, loss = 1.29368130\n",
      "Iteration 22478, loss = 1.29367850\n",
      "Iteration 22479, loss = 1.29367567\n",
      "Iteration 22480, loss = 1.29367282\n",
      "Iteration 22481, loss = 1.29366994\n",
      "Iteration 22482, loss = 1.29366706\n",
      "Iteration 22483, loss = 1.29366423\n",
      "Iteration 22484, loss = 1.29366139\n",
      "Iteration 22485, loss = 1.29365861\n",
      "Iteration 22486, loss = 1.29365584\n",
      "Iteration 22487, loss = 1.29365307\n",
      "Iteration 22488, loss = 1.29365029\n",
      "Iteration 22489, loss = 1.29364751\n",
      "Iteration 22490, loss = 1.29364472\n",
      "Iteration 22491, loss = 1.29364192\n",
      "Iteration 22492, loss = 1.29363912\n",
      "Iteration 22493, loss = 1.29363783\n",
      "Iteration 22494, loss = 1.29363415\n",
      "Iteration 22495, loss = 1.29363202\n",
      "Iteration 22496, loss = 1.29362996\n",
      "Iteration 22497, loss = 1.29362791\n",
      "Iteration 22498, loss = 1.29362580\n",
      "Iteration 22499, loss = 1.29362362\n",
      "Iteration 22500, loss = 1.29362136\n",
      "Iteration 22501, loss = 1.29361902\n",
      "Iteration 22502, loss = 1.29361658\n",
      "Iteration 22503, loss = 1.29361406\n",
      "Iteration 22504, loss = 1.29361146\n",
      "Iteration 22505, loss = 1.29360879\n",
      "Iteration 22506, loss = 1.29360604\n",
      "Iteration 22507, loss = 1.29360323\n",
      "Iteration 22508, loss = 1.29360037\n",
      "Iteration 22509, loss = 1.29359747\n",
      "Iteration 22510, loss = 1.29359452\n",
      "Iteration 22511, loss = 1.29359155\n",
      "Iteration 22512, loss = 1.29358857\n",
      "Iteration 22513, loss = 1.29358561\n",
      "Iteration 22514, loss = 1.29358266\n",
      "Iteration 22515, loss = 1.29357970\n",
      "Iteration 22516, loss = 1.29357674\n",
      "Iteration 22517, loss = 1.29357378\n",
      "Iteration 22518, loss = 1.29357083\n",
      "Iteration 22519, loss = 1.29356789\n",
      "Iteration 22520, loss = 1.29356496\n",
      "Iteration 22521, loss = 1.29356207\n",
      "Iteration 22522, loss = 1.29355924\n",
      "Iteration 22523, loss = 1.29355649\n",
      "Iteration 22524, loss = 1.29355380\n",
      "Iteration 22525, loss = 1.29355110\n",
      "Iteration 22526, loss = 1.29354838\n",
      "Iteration 22527, loss = 1.29354562\n",
      "Iteration 22528, loss = 1.29354285\n",
      "Iteration 22529, loss = 1.29354005\n",
      "Iteration 22530, loss = 1.29353753\n",
      "Iteration 22531, loss = 1.29353474\n",
      "Iteration 22532, loss = 1.29353280\n",
      "Iteration 22533, loss = 1.29353084\n",
      "Iteration 22534, loss = 1.29352884\n",
      "Iteration 22535, loss = 1.29352680\n",
      "Iteration 22536, loss = 1.29352472\n",
      "Iteration 22537, loss = 1.29352256\n",
      "Iteration 22538, loss = 1.29352030\n",
      "Iteration 22539, loss = 1.29351794\n",
      "Iteration 22540, loss = 1.29351548\n",
      "Iteration 22541, loss = 1.29351293\n",
      "Iteration 22542, loss = 1.29351030\n",
      "Iteration 22543, loss = 1.29350760\n",
      "Iteration 22544, loss = 1.29350483\n",
      "Iteration 22545, loss = 1.29350200\n",
      "Iteration 22546, loss = 1.29349912\n",
      "Iteration 22547, loss = 1.29349622\n",
      "Iteration 22548, loss = 1.29349331\n",
      "Iteration 22549, loss = 1.29349039\n",
      "Iteration 22550, loss = 1.29348746\n",
      "Iteration 22551, loss = 1.29348452\n",
      "Iteration 22552, loss = 1.29348158\n",
      "Iteration 22553, loss = 1.29347864\n",
      "Iteration 22554, loss = 1.29347570\n",
      "Iteration 22555, loss = 1.29347276\n",
      "Iteration 22556, loss = 1.29346984\n",
      "Iteration 22557, loss = 1.29346692\n",
      "Iteration 22558, loss = 1.29346402\n",
      "Iteration 22559, loss = 1.29346114\n",
      "Iteration 22560, loss = 1.29345834\n",
      "Iteration 22561, loss = 1.29345559\n",
      "Iteration 22562, loss = 1.29345287\n",
      "Iteration 22563, loss = 1.29345019\n",
      "Iteration 22564, loss = 1.29344750\n",
      "Iteration 22565, loss = 1.29344478\n",
      "Iteration 22566, loss = 1.29344204\n",
      "Iteration 22567, loss = 1.29343928\n",
      "Iteration 22568, loss = 1.29343649\n",
      "Iteration 22569, loss = 1.29343368\n",
      "Iteration 22570, loss = 1.29343085\n",
      "Iteration 22571, loss = 1.29342803\n",
      "Iteration 22572, loss = 1.29342525\n",
      "Iteration 22573, loss = 1.29342248\n",
      "Iteration 22574, loss = 1.29341973\n",
      "Iteration 22575, loss = 1.29341701\n",
      "Iteration 22576, loss = 1.29341430\n",
      "Iteration 22577, loss = 1.29341157\n",
      "Iteration 22578, loss = 1.29340883\n",
      "Iteration 22579, loss = 1.29340609\n",
      "Iteration 22580, loss = 1.29340334\n",
      "Iteration 22581, loss = 1.29340059\n",
      "Iteration 22582, loss = 1.29339783\n",
      "Iteration 22583, loss = 1.29339507\n",
      "Iteration 22584, loss = 1.29339231\n",
      "Iteration 22585, loss = 1.29339139\n",
      "Iteration 22586, loss = 1.29338746\n",
      "Iteration 22587, loss = 1.29338542\n",
      "Iteration 22588, loss = 1.29338343\n",
      "Iteration 22589, loss = 1.29338144\n",
      "Iteration 22590, loss = 1.29337939\n",
      "Iteration 22591, loss = 1.29337727\n",
      "Iteration 22592, loss = 1.29337507\n",
      "Iteration 22593, loss = 1.29337279\n",
      "Iteration 22594, loss = 1.29337042\n",
      "Iteration 22595, loss = 1.29336796\n",
      "Iteration 22596, loss = 1.29336543\n",
      "Iteration 22597, loss = 1.29336282\n",
      "Iteration 22598, loss = 1.29336015\n",
      "Iteration 22599, loss = 1.29335741\n",
      "Iteration 22600, loss = 1.29335463\n",
      "Iteration 22601, loss = 1.29335180\n",
      "Iteration 22602, loss = 1.29334893\n",
      "Iteration 22603, loss = 1.29334604\n",
      "Iteration 22604, loss = 1.29334313\n",
      "Iteration 22605, loss = 1.29334024\n",
      "Iteration 22606, loss = 1.29333736\n",
      "Iteration 22607, loss = 1.29333448\n",
      "Iteration 22608, loss = 1.29333159\n",
      "Iteration 22609, loss = 1.29332871\n",
      "Iteration 22610, loss = 1.29332583\n",
      "Iteration 22611, loss = 1.29332296\n",
      "Iteration 22612, loss = 1.29332012\n",
      "Iteration 22613, loss = 1.29331733\n",
      "Iteration 22614, loss = 1.29331460\n",
      "Iteration 22615, loss = 1.29331191\n",
      "Iteration 22616, loss = 1.29330928\n",
      "Iteration 22617, loss = 1.29330662\n",
      "Iteration 22618, loss = 1.29330394\n",
      "Iteration 22619, loss = 1.29330123\n",
      "Iteration 22620, loss = 1.29329850\n",
      "Iteration 22621, loss = 1.29329575\n",
      "Iteration 22622, loss = 1.29329297\n",
      "Iteration 22623, loss = 1.29329018\n",
      "Iteration 22624, loss = 1.29328738\n",
      "Iteration 22625, loss = 1.29328461\n",
      "Iteration 22626, loss = 1.29328186\n",
      "Iteration 22627, loss = 1.29327917\n",
      "Iteration 22628, loss = 1.29327648\n",
      "Iteration 22629, loss = 1.29327380\n",
      "Iteration 22630, loss = 1.29327110\n",
      "Iteration 22631, loss = 1.29326839\n",
      "Iteration 22632, loss = 1.29326855\n",
      "Iteration 22633, loss = 1.29326367\n",
      "Iteration 22634, loss = 1.29326169\n",
      "Iteration 22635, loss = 1.29325971\n",
      "Iteration 22636, loss = 1.29325773\n",
      "Iteration 22637, loss = 1.29325575\n",
      "Iteration 22638, loss = 1.29325369\n",
      "Iteration 22639, loss = 1.29325154\n",
      "Iteration 22640, loss = 1.29324929\n",
      "Iteration 22641, loss = 1.29324696\n",
      "Iteration 22642, loss = 1.29324452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22643, loss = 1.29324200\n",
      "Iteration 22644, loss = 1.29323940\n",
      "Iteration 22645, loss = 1.29323673\n",
      "Iteration 22646, loss = 1.29323399\n",
      "Iteration 22647, loss = 1.29323119\n",
      "Iteration 22648, loss = 1.29322834\n",
      "Iteration 22649, loss = 1.29322546\n",
      "Iteration 22650, loss = 1.29322255\n",
      "Iteration 22651, loss = 1.29321965\n",
      "Iteration 22652, loss = 1.29321675\n",
      "Iteration 22653, loss = 1.29321385\n",
      "Iteration 22654, loss = 1.29321095\n",
      "Iteration 22655, loss = 1.29320805\n",
      "Iteration 22656, loss = 1.29320515\n",
      "Iteration 22657, loss = 1.29320227\n",
      "Iteration 22658, loss = 1.29319939\n",
      "Iteration 22659, loss = 1.29319652\n",
      "Iteration 22660, loss = 1.29319366\n",
      "Iteration 22661, loss = 1.29319083\n",
      "Iteration 22662, loss = 1.29318804\n",
      "Iteration 22663, loss = 1.29318537\n",
      "Iteration 22664, loss = 1.29318274\n",
      "Iteration 22665, loss = 1.29318012\n",
      "Iteration 22666, loss = 1.29317749\n",
      "Iteration 22667, loss = 1.29317483\n",
      "Iteration 22668, loss = 1.29317214\n",
      "Iteration 22669, loss = 1.29316943\n",
      "Iteration 22670, loss = 1.29316670\n",
      "Iteration 22671, loss = 1.29316394\n",
      "Iteration 22672, loss = 1.29316117\n",
      "Iteration 22673, loss = 1.29315840\n",
      "Iteration 22674, loss = 1.29315564\n",
      "Iteration 22675, loss = 1.29315291\n",
      "Iteration 22676, loss = 1.29315025\n",
      "Iteration 22677, loss = 1.29314759\n",
      "Iteration 22678, loss = 1.29314493\n",
      "Iteration 22679, loss = 1.29314225\n",
      "Iteration 22680, loss = 1.29313969\n",
      "Iteration 22681, loss = 1.29313763\n",
      "Iteration 22682, loss = 1.29313571\n",
      "Iteration 22683, loss = 1.29313378\n",
      "Iteration 22684, loss = 1.29313188\n",
      "Iteration 22685, loss = 1.29312995\n",
      "Iteration 22686, loss = 1.29312793\n",
      "Iteration 22687, loss = 1.29312583\n",
      "Iteration 22688, loss = 1.29312363\n",
      "Iteration 22689, loss = 1.29312133\n",
      "Iteration 22690, loss = 1.29311895\n",
      "Iteration 22691, loss = 1.29311647\n",
      "Iteration 22692, loss = 1.29311392\n",
      "Iteration 22693, loss = 1.29311129\n",
      "Iteration 22694, loss = 1.29310860\n",
      "Iteration 22695, loss = 1.29310585\n",
      "Iteration 22696, loss = 1.29310306\n",
      "Iteration 22697, loss = 1.29310022\n",
      "Iteration 22698, loss = 1.29309736\n",
      "Iteration 22699, loss = 1.29309448\n",
      "Iteration 22700, loss = 1.29309163\n",
      "Iteration 22701, loss = 1.29308878\n",
      "Iteration 22702, loss = 1.29308592\n",
      "Iteration 22703, loss = 1.29308306\n",
      "Iteration 22704, loss = 1.29308021\n",
      "Iteration 22705, loss = 1.29307736\n",
      "Iteration 22706, loss = 1.29307453\n",
      "Iteration 22707, loss = 1.29307170\n",
      "Iteration 22708, loss = 1.29306888\n",
      "Iteration 22709, loss = 1.29306607\n",
      "Iteration 22710, loss = 1.29306327\n",
      "Iteration 22711, loss = 1.29306051\n",
      "Iteration 22712, loss = 1.29305785\n",
      "Iteration 22713, loss = 1.29305527\n",
      "Iteration 22714, loss = 1.29305268\n",
      "Iteration 22715, loss = 1.29305008\n",
      "Iteration 22716, loss = 1.29304744\n",
      "Iteration 22717, loss = 1.29304478\n",
      "Iteration 22718, loss = 1.29304210\n",
      "Iteration 22719, loss = 1.29303939\n",
      "Iteration 22720, loss = 1.29303666\n",
      "Iteration 22721, loss = 1.29303391\n",
      "Iteration 22722, loss = 1.29303115\n",
      "Iteration 22723, loss = 1.29302846\n",
      "Iteration 22724, loss = 1.29302582\n",
      "Iteration 22725, loss = 1.29302319\n",
      "Iteration 22726, loss = 1.29302055\n",
      "Iteration 22727, loss = 1.29301791\n",
      "Iteration 22728, loss = 1.29301647\n",
      "Iteration 22729, loss = 1.29301327\n",
      "Iteration 22730, loss = 1.29301134\n",
      "Iteration 22731, loss = 1.29300941\n",
      "Iteration 22732, loss = 1.29300751\n",
      "Iteration 22733, loss = 1.29300560\n",
      "Iteration 22734, loss = 1.29300360\n",
      "Iteration 22735, loss = 1.29300152\n",
      "Iteration 22736, loss = 1.29299935\n",
      "Iteration 22737, loss = 1.29299708\n",
      "Iteration 22738, loss = 1.29299472\n",
      "Iteration 22739, loss = 1.29299227\n",
      "Iteration 22740, loss = 1.29298973\n",
      "Iteration 22741, loss = 1.29298712\n",
      "Iteration 22742, loss = 1.29298444\n",
      "Iteration 22743, loss = 1.29298170\n",
      "Iteration 22744, loss = 1.29297891\n",
      "Iteration 22745, loss = 1.29297608\n",
      "Iteration 22746, loss = 1.29297322\n",
      "Iteration 22747, loss = 1.29297035\n",
      "Iteration 22748, loss = 1.29296751\n",
      "Iteration 22749, loss = 1.29296465\n",
      "Iteration 22750, loss = 1.29296180\n",
      "Iteration 22751, loss = 1.29295894\n",
      "Iteration 22752, loss = 1.29295609\n",
      "Iteration 22753, loss = 1.29295325\n",
      "Iteration 22754, loss = 1.29295042\n",
      "Iteration 22755, loss = 1.29294759\n",
      "Iteration 22756, loss = 1.29294478\n",
      "Iteration 22757, loss = 1.29294198\n",
      "Iteration 22758, loss = 1.29293921\n",
      "Iteration 22759, loss = 1.29293646\n",
      "Iteration 22760, loss = 1.29293379\n",
      "Iteration 22761, loss = 1.29293121\n",
      "Iteration 22762, loss = 1.29292866\n",
      "Iteration 22763, loss = 1.29292608\n",
      "Iteration 22764, loss = 1.29292348\n",
      "Iteration 22765, loss = 1.29292084\n",
      "Iteration 22766, loss = 1.29291819\n",
      "Iteration 22767, loss = 1.29291551\n",
      "Iteration 22768, loss = 1.29291281\n",
      "Iteration 22769, loss = 1.29291009\n",
      "Iteration 22770, loss = 1.29290736\n",
      "Iteration 22771, loss = 1.29290465\n",
      "Iteration 22772, loss = 1.29290201\n",
      "Iteration 22773, loss = 1.29289938\n",
      "Iteration 22774, loss = 1.29289677\n",
      "Iteration 22775, loss = 1.29289674\n",
      "Iteration 22776, loss = 1.29289227\n",
      "Iteration 22777, loss = 1.29289041\n",
      "Iteration 22778, loss = 1.29288856\n",
      "Iteration 22779, loss = 1.29288671\n",
      "Iteration 22780, loss = 1.29288485\n",
      "Iteration 22781, loss = 1.29288291\n",
      "Iteration 22782, loss = 1.29288088\n",
      "Iteration 22783, loss = 1.29287876\n",
      "Iteration 22784, loss = 1.29287653\n",
      "Iteration 22785, loss = 1.29287422\n",
      "Iteration 22786, loss = 1.29287181\n",
      "Iteration 22787, loss = 1.29286932\n",
      "Iteration 22788, loss = 1.29286675\n",
      "Iteration 22789, loss = 1.29286412\n",
      "Iteration 22790, loss = 1.29286142\n",
      "Iteration 22791, loss = 1.29285868\n",
      "Iteration 22792, loss = 1.29285589\n",
      "Iteration 22793, loss = 1.29285307\n",
      "Iteration 22794, loss = 1.29285024\n",
      "Iteration 22795, loss = 1.29284743\n",
      "Iteration 22796, loss = 1.29284462\n",
      "Iteration 22797, loss = 1.29284180\n",
      "Iteration 22798, loss = 1.29283898\n",
      "Iteration 22799, loss = 1.29283617\n",
      "Iteration 22800, loss = 1.29283337\n",
      "Iteration 22801, loss = 1.29283057\n",
      "Iteration 22802, loss = 1.29282778\n",
      "Iteration 22803, loss = 1.29282500\n",
      "Iteration 22804, loss = 1.29282224\n",
      "Iteration 22805, loss = 1.29281949\n",
      "Iteration 22806, loss = 1.29281674\n",
      "Iteration 22807, loss = 1.29281402\n",
      "Iteration 22808, loss = 1.29281141\n",
      "Iteration 22809, loss = 1.29280885\n",
      "Iteration 22810, loss = 1.29280630\n",
      "Iteration 22811, loss = 1.29280374\n",
      "Iteration 22812, loss = 1.29280117\n",
      "Iteration 22813, loss = 1.29279857\n",
      "Iteration 22814, loss = 1.29279594\n",
      "Iteration 22815, loss = 1.29279328\n",
      "Iteration 22816, loss = 1.29279061\n",
      "Iteration 22817, loss = 1.29278791\n",
      "Iteration 22818, loss = 1.29278523\n",
      "Iteration 22819, loss = 1.29278256\n",
      "Iteration 22820, loss = 1.29277995\n",
      "Iteration 22821, loss = 1.29277737\n",
      "Iteration 22822, loss = 1.29277478\n",
      "Iteration 22823, loss = 1.29277219\n",
      "Iteration 22824, loss = 1.29276959\n",
      "Iteration 22825, loss = 1.29276698\n",
      "Iteration 22826, loss = 1.29276437\n",
      "Iteration 22827, loss = 1.29276359\n",
      "Iteration 22828, loss = 1.29275978\n",
      "Iteration 22829, loss = 1.29275786\n",
      "Iteration 22830, loss = 1.29275596\n",
      "Iteration 22831, loss = 1.29275411\n",
      "Iteration 22832, loss = 1.29275222\n",
      "Iteration 22833, loss = 1.29275026\n",
      "Iteration 22834, loss = 1.29274822\n",
      "Iteration 22835, loss = 1.29274608\n",
      "Iteration 22836, loss = 1.29274386\n",
      "Iteration 22837, loss = 1.29274154\n",
      "Iteration 22838, loss = 1.29273914\n",
      "Iteration 22839, loss = 1.29273665\n",
      "Iteration 22840, loss = 1.29273409\n",
      "Iteration 22841, loss = 1.29273147\n",
      "Iteration 22842, loss = 1.29272878\n",
      "Iteration 22843, loss = 1.29272605\n",
      "Iteration 22844, loss = 1.29272328\n",
      "Iteration 22845, loss = 1.29272048\n",
      "Iteration 22846, loss = 1.29271765\n",
      "Iteration 22847, loss = 1.29271485\n",
      "Iteration 22848, loss = 1.29271206\n",
      "Iteration 22849, loss = 1.29270926\n",
      "Iteration 22850, loss = 1.29270646\n",
      "Iteration 22851, loss = 1.29270367\n",
      "Iteration 22852, loss = 1.29270088\n",
      "Iteration 22853, loss = 1.29269810\n",
      "Iteration 22854, loss = 1.29269533\n",
      "Iteration 22855, loss = 1.29269258\n",
      "Iteration 22856, loss = 1.29268984\n",
      "Iteration 22857, loss = 1.29268711\n",
      "Iteration 22858, loss = 1.29268450\n",
      "Iteration 22859, loss = 1.29268195\n",
      "Iteration 22860, loss = 1.29267944\n",
      "Iteration 22861, loss = 1.29267692\n",
      "Iteration 22862, loss = 1.29267437\n",
      "Iteration 22863, loss = 1.29267180\n",
      "Iteration 22864, loss = 1.29266920\n",
      "Iteration 22865, loss = 1.29266658\n",
      "Iteration 22866, loss = 1.29266394\n",
      "Iteration 22867, loss = 1.29266128\n",
      "Iteration 22868, loss = 1.29265860\n",
      "Iteration 22869, loss = 1.29265594\n",
      "Iteration 22870, loss = 1.29265329\n",
      "Iteration 22871, loss = 1.29265069\n",
      "Iteration 22872, loss = 1.29264942\n",
      "Iteration 22873, loss = 1.29264633\n",
      "Iteration 22874, loss = 1.29264456\n",
      "Iteration 22875, loss = 1.29264280\n",
      "Iteration 22876, loss = 1.29264102\n",
      "Iteration 22877, loss = 1.29263924\n",
      "Iteration 22878, loss = 1.29263738\n",
      "Iteration 22879, loss = 1.29263543\n",
      "Iteration 22880, loss = 1.29263338\n",
      "Iteration 22881, loss = 1.29263122\n",
      "Iteration 22882, loss = 1.29262897\n",
      "Iteration 22883, loss = 1.29262662\n",
      "Iteration 22884, loss = 1.29262418\n",
      "Iteration 22885, loss = 1.29262166\n",
      "Iteration 22886, loss = 1.29261907\n",
      "Iteration 22887, loss = 1.29261642\n",
      "Iteration 22888, loss = 1.29261371\n",
      "Iteration 22889, loss = 1.29261096\n",
      "Iteration 22890, loss = 1.29260818\n",
      "Iteration 22891, loss = 1.29260540\n",
      "Iteration 22892, loss = 1.29260263\n",
      "Iteration 22893, loss = 1.29259986\n",
      "Iteration 22894, loss = 1.29259707\n",
      "Iteration 22895, loss = 1.29259429\n",
      "Iteration 22896, loss = 1.29259152\n",
      "Iteration 22897, loss = 1.29258875\n",
      "Iteration 22898, loss = 1.29258599\n",
      "Iteration 22899, loss = 1.29258324\n",
      "Iteration 22900, loss = 1.29258050\n",
      "Iteration 22901, loss = 1.29257777\n",
      "Iteration 22902, loss = 1.29257506\n",
      "Iteration 22903, loss = 1.29257236\n",
      "Iteration 22904, loss = 1.29256967\n",
      "Iteration 22905, loss = 1.29256700\n",
      "Iteration 22906, loss = 1.29256439\n",
      "Iteration 22907, loss = 1.29256184\n",
      "Iteration 22908, loss = 1.29255935\n",
      "Iteration 22909, loss = 1.29255686\n",
      "Iteration 22910, loss = 1.29255435\n",
      "Iteration 22911, loss = 1.29255180\n",
      "Iteration 22912, loss = 1.29254923\n",
      "Iteration 22913, loss = 1.29254663\n",
      "Iteration 22914, loss = 1.29254401\n",
      "Iteration 22915, loss = 1.29254137\n",
      "Iteration 22916, loss = 1.29253872\n",
      "Iteration 22917, loss = 1.29253608\n",
      "Iteration 22918, loss = 1.29253350\n",
      "Iteration 22919, loss = 1.29253097\n",
      "Iteration 22920, loss = 1.29252843\n",
      "Iteration 22921, loss = 1.29252589\n",
      "Iteration 22922, loss = 1.29252334\n",
      "Iteration 22923, loss = 1.29252078\n",
      "Iteration 22924, loss = 1.29251822\n",
      "Iteration 22925, loss = 1.29251565\n",
      "Iteration 22926, loss = 1.29251308\n",
      "Iteration 22927, loss = 1.29251050\n",
      "Iteration 22928, loss = 1.29251026\n",
      "Iteration 22929, loss = 1.29250596\n",
      "Iteration 22930, loss = 1.29250406\n",
      "Iteration 22931, loss = 1.29250220\n",
      "Iteration 22932, loss = 1.29250039\n",
      "Iteration 22933, loss = 1.29249854\n",
      "Iteration 22934, loss = 1.29249661\n",
      "Iteration 22935, loss = 1.29249461\n",
      "Iteration 22936, loss = 1.29249252\n",
      "Iteration 22937, loss = 1.29249034\n",
      "Iteration 22938, loss = 1.29248808\n",
      "Iteration 22939, loss = 1.29248572\n",
      "Iteration 22940, loss = 1.29248329\n",
      "Iteration 22941, loss = 1.29248079\n",
      "Iteration 22942, loss = 1.29247822\n",
      "Iteration 22943, loss = 1.29247559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22944, loss = 1.29247292\n",
      "Iteration 22945, loss = 1.29247020\n",
      "Iteration 22946, loss = 1.29246745\n",
      "Iteration 22947, loss = 1.29246468\n",
      "Iteration 22948, loss = 1.29246191\n",
      "Iteration 22949, loss = 1.29245917\n",
      "Iteration 22950, loss = 1.29245642\n",
      "Iteration 22951, loss = 1.29245368\n",
      "Iteration 22952, loss = 1.29245094\n",
      "Iteration 22953, loss = 1.29244821\n",
      "Iteration 22954, loss = 1.29244548\n",
      "Iteration 22955, loss = 1.29244277\n",
      "Iteration 22956, loss = 1.29244006\n",
      "Iteration 22957, loss = 1.29243737\n",
      "Iteration 22958, loss = 1.29243477\n",
      "Iteration 22959, loss = 1.29243228\n",
      "Iteration 22960, loss = 1.29242981\n",
      "Iteration 22961, loss = 1.29242733\n",
      "Iteration 22962, loss = 1.29242482\n",
      "Iteration 22963, loss = 1.29242230\n",
      "Iteration 22964, loss = 1.29241975\n",
      "Iteration 22965, loss = 1.29241718\n",
      "Iteration 22966, loss = 1.29241459\n",
      "Iteration 22967, loss = 1.29241199\n",
      "Iteration 22968, loss = 1.29240936\n",
      "Iteration 22969, loss = 1.29240673\n",
      "Iteration 22970, loss = 1.29240413\n",
      "Iteration 22971, loss = 1.29240225\n",
      "Iteration 22972, loss = 1.29239983\n",
      "Iteration 22973, loss = 1.29239813\n",
      "Iteration 22974, loss = 1.29239643\n",
      "Iteration 22975, loss = 1.29239471\n",
      "Iteration 22976, loss = 1.29239298\n",
      "Iteration 22977, loss = 1.29239119\n",
      "Iteration 22978, loss = 1.29238930\n",
      "Iteration 22979, loss = 1.29238731\n",
      "Iteration 22980, loss = 1.29238521\n",
      "Iteration 22981, loss = 1.29238301\n",
      "Iteration 22982, loss = 1.29238071\n",
      "Iteration 22983, loss = 1.29237832\n",
      "Iteration 22984, loss = 1.29237585\n",
      "Iteration 22985, loss = 1.29237331\n",
      "Iteration 22986, loss = 1.29237070\n",
      "Iteration 22987, loss = 1.29236804\n",
      "Iteration 22988, loss = 1.29236534\n",
      "Iteration 22989, loss = 1.29236260\n",
      "Iteration 22990, loss = 1.29235986\n",
      "Iteration 22991, loss = 1.29235713\n",
      "Iteration 22992, loss = 1.29235439\n",
      "Iteration 22993, loss = 1.29235165\n",
      "Iteration 22994, loss = 1.29234891\n",
      "Iteration 22995, loss = 1.29234617\n",
      "Iteration 22996, loss = 1.29234344\n",
      "Iteration 22997, loss = 1.29234072\n",
      "Iteration 22998, loss = 1.29233801\n",
      "Iteration 22999, loss = 1.29233532\n",
      "Iteration 23000, loss = 1.29233263\n",
      "Iteration 23001, loss = 1.29232996\n",
      "Iteration 23002, loss = 1.29232731\n",
      "Iteration 23003, loss = 1.29232466\n",
      "Iteration 23004, loss = 1.29232203\n",
      "Iteration 23005, loss = 1.29231943\n",
      "Iteration 23006, loss = 1.29231692\n",
      "Iteration 23007, loss = 1.29231447\n",
      "Iteration 23008, loss = 1.29231203\n",
      "Iteration 23009, loss = 1.29230957\n",
      "Iteration 23010, loss = 1.29230708\n",
      "Iteration 23011, loss = 1.29230457\n",
      "Iteration 23012, loss = 1.29230202\n",
      "Iteration 23013, loss = 1.29229946\n",
      "Iteration 23014, loss = 1.29229687\n",
      "Iteration 23015, loss = 1.29229427\n",
      "Iteration 23016, loss = 1.29229167\n",
      "Iteration 23017, loss = 1.29228910\n",
      "Iteration 23018, loss = 1.29228660\n",
      "Iteration 23019, loss = 1.29228411\n",
      "Iteration 23020, loss = 1.29228162\n",
      "Iteration 23021, loss = 1.29227912\n",
      "Iteration 23022, loss = 1.29227661\n",
      "Iteration 23023, loss = 1.29227409\n",
      "Iteration 23024, loss = 1.29227157\n",
      "Iteration 23025, loss = 1.29226905\n",
      "Iteration 23026, loss = 1.29226652\n",
      "Iteration 23027, loss = 1.29226399\n",
      "Iteration 23028, loss = 1.29226146\n",
      "Iteration 23029, loss = 1.29225892\n",
      "Iteration 23030, loss = 1.29225878\n",
      "Iteration 23031, loss = 1.29225450\n",
      "Iteration 23032, loss = 1.29225266\n",
      "Iteration 23033, loss = 1.29225091\n",
      "Iteration 23034, loss = 1.29224915\n",
      "Iteration 23035, loss = 1.29224735\n",
      "Iteration 23036, loss = 1.29224547\n",
      "Iteration 23037, loss = 1.29224352\n",
      "Iteration 23038, loss = 1.29224149\n",
      "Iteration 23039, loss = 1.29223936\n",
      "Iteration 23040, loss = 1.29223716\n",
      "Iteration 23041, loss = 1.29223487\n",
      "Iteration 23042, loss = 1.29223250\n",
      "Iteration 23043, loss = 1.29223007\n",
      "Iteration 23044, loss = 1.29222757\n",
      "Iteration 23045, loss = 1.29222502\n",
      "Iteration 23046, loss = 1.29222242\n",
      "Iteration 23047, loss = 1.29221978\n",
      "Iteration 23048, loss = 1.29221711\n",
      "Iteration 23049, loss = 1.29221441\n",
      "Iteration 23050, loss = 1.29221170\n",
      "Iteration 23051, loss = 1.29220899\n",
      "Iteration 23052, loss = 1.29220632\n",
      "Iteration 23053, loss = 1.29220365\n",
      "Iteration 23054, loss = 1.29220098\n",
      "Iteration 23055, loss = 1.29219831\n",
      "Iteration 23056, loss = 1.29219565\n",
      "Iteration 23057, loss = 1.29219301\n",
      "Iteration 23058, loss = 1.29219037\n",
      "Iteration 23059, loss = 1.29218775\n",
      "Iteration 23060, loss = 1.29218526\n",
      "Iteration 23061, loss = 1.29218284\n",
      "Iteration 23062, loss = 1.29218042\n",
      "Iteration 23063, loss = 1.29217798\n",
      "Iteration 23064, loss = 1.29217552\n",
      "Iteration 23065, loss = 1.29217303\n",
      "Iteration 23066, loss = 1.29217053\n",
      "Iteration 23067, loss = 1.29216800\n",
      "Iteration 23068, loss = 1.29216546\n",
      "Iteration 23069, loss = 1.29216290\n",
      "Iteration 23070, loss = 1.29216032\n",
      "Iteration 23071, loss = 1.29215774\n",
      "Iteration 23072, loss = 1.29215518\n",
      "Iteration 23073, loss = 1.29215271\n",
      "Iteration 23074, loss = 1.29215026\n",
      "Iteration 23075, loss = 1.29214779\n",
      "Iteration 23076, loss = 1.29214532\n",
      "Iteration 23077, loss = 1.29214283\n",
      "Iteration 23078, loss = 1.29214034\n",
      "Iteration 23079, loss = 1.29213785\n",
      "Iteration 23080, loss = 1.29213534\n",
      "Iteration 23081, loss = 1.29213420\n",
      "Iteration 23082, loss = 1.29213100\n",
      "Iteration 23083, loss = 1.29212922\n",
      "Iteration 23084, loss = 1.29212745\n",
      "Iteration 23085, loss = 1.29212569\n",
      "Iteration 23086, loss = 1.29212393\n",
      "Iteration 23087, loss = 1.29212210\n",
      "Iteration 23088, loss = 1.29212019\n",
      "Iteration 23089, loss = 1.29211818\n",
      "Iteration 23090, loss = 1.29211608\n",
      "Iteration 23091, loss = 1.29211388\n",
      "Iteration 23092, loss = 1.29211160\n",
      "Iteration 23093, loss = 1.29210923\n",
      "Iteration 23094, loss = 1.29210679\n",
      "Iteration 23095, loss = 1.29210428\n",
      "Iteration 23096, loss = 1.29210171\n",
      "Iteration 23097, loss = 1.29209909\n",
      "Iteration 23098, loss = 1.29209644\n",
      "Iteration 23099, loss = 1.29209375\n",
      "Iteration 23100, loss = 1.29209103\n",
      "Iteration 23101, loss = 1.29208833\n",
      "Iteration 23102, loss = 1.29208564\n",
      "Iteration 23103, loss = 1.29208295\n",
      "Iteration 23104, loss = 1.29208027\n",
      "Iteration 23105, loss = 1.29207758\n",
      "Iteration 23106, loss = 1.29207491\n",
      "Iteration 23107, loss = 1.29207225\n",
      "Iteration 23108, loss = 1.29206959\n",
      "Iteration 23109, loss = 1.29206695\n",
      "Iteration 23110, loss = 1.29206432\n",
      "Iteration 23111, loss = 1.29206173\n",
      "Iteration 23112, loss = 1.29205925\n",
      "Iteration 23113, loss = 1.29205685\n",
      "Iteration 23114, loss = 1.29205445\n",
      "Iteration 23115, loss = 1.29205202\n",
      "Iteration 23116, loss = 1.29204958\n",
      "Iteration 23117, loss = 1.29204711\n",
      "Iteration 23118, loss = 1.29204462\n",
      "Iteration 23119, loss = 1.29204212\n",
      "Iteration 23120, loss = 1.29203959\n",
      "Iteration 23121, loss = 1.29203705\n",
      "Iteration 23122, loss = 1.29203449\n",
      "Iteration 23123, loss = 1.29203193\n",
      "Iteration 23124, loss = 1.29202942\n",
      "Iteration 23125, loss = 1.29202695\n",
      "Iteration 23126, loss = 1.29202451\n",
      "Iteration 23127, loss = 1.29202206\n",
      "Iteration 23128, loss = 1.29201961\n",
      "Iteration 23129, loss = 1.29201715\n",
      "Iteration 23130, loss = 1.29201468\n",
      "Iteration 23131, loss = 1.29201220\n",
      "Iteration 23132, loss = 1.29200972\n",
      "Iteration 23133, loss = 1.29200724\n",
      "Iteration 23134, loss = 1.29200475\n",
      "Iteration 23135, loss = 1.29200591\n",
      "Iteration 23136, loss = 1.29200047\n",
      "Iteration 23137, loss = 1.29199872\n",
      "Iteration 23138, loss = 1.29199698\n",
      "Iteration 23139, loss = 1.29199528\n",
      "Iteration 23140, loss = 1.29199355\n",
      "Iteration 23141, loss = 1.29199174\n",
      "Iteration 23142, loss = 1.29198985\n",
      "Iteration 23143, loss = 1.29198787\n",
      "Iteration 23144, loss = 1.29198580\n",
      "Iteration 23145, loss = 1.29198364\n",
      "Iteration 23146, loss = 1.29198139\n",
      "Iteration 23147, loss = 1.29197907\n",
      "Iteration 23148, loss = 1.29197667\n",
      "Iteration 23149, loss = 1.29197421\n",
      "Iteration 23150, loss = 1.29197169\n",
      "Iteration 23151, loss = 1.29196913\n",
      "Iteration 23152, loss = 1.29196652\n",
      "Iteration 23153, loss = 1.29196389\n",
      "Iteration 23154, loss = 1.29196123\n",
      "Iteration 23155, loss = 1.29195855\n",
      "Iteration 23156, loss = 1.29195591\n",
      "Iteration 23157, loss = 1.29195327\n",
      "Iteration 23158, loss = 1.29195063\n",
      "Iteration 23159, loss = 1.29194800\n",
      "Iteration 23160, loss = 1.29194537\n",
      "Iteration 23161, loss = 1.29194275\n",
      "Iteration 23162, loss = 1.29194014\n",
      "Iteration 23163, loss = 1.29193755\n",
      "Iteration 23164, loss = 1.29193497\n",
      "Iteration 23165, loss = 1.29193246\n",
      "Iteration 23166, loss = 1.29193002\n",
      "Iteration 23167, loss = 1.29192763\n",
      "Iteration 23168, loss = 1.29192525\n",
      "Iteration 23169, loss = 1.29192284\n",
      "Iteration 23170, loss = 1.29192042\n",
      "Iteration 23171, loss = 1.29191797\n",
      "Iteration 23172, loss = 1.29191550\n",
      "Iteration 23173, loss = 1.29191302\n",
      "Iteration 23174, loss = 1.29191051\n",
      "Iteration 23175, loss = 1.29190799\n",
      "Iteration 23176, loss = 1.29190547\n",
      "Iteration 23177, loss = 1.29190297\n",
      "Iteration 23178, loss = 1.29190050\n",
      "Iteration 23179, loss = 1.29189805\n",
      "Iteration 23180, loss = 1.29189562\n",
      "Iteration 23181, loss = 1.29189320\n",
      "Iteration 23182, loss = 1.29189076\n",
      "Iteration 23183, loss = 1.29188832\n",
      "Iteration 23184, loss = 1.29188587\n",
      "Iteration 23185, loss = 1.29188342\n",
      "Iteration 23186, loss = 1.29188096\n",
      "Iteration 23187, loss = 1.29187850\n",
      "Iteration 23188, loss = 1.29187603\n",
      "Iteration 23189, loss = 1.29187760\n",
      "Iteration 23190, loss = 1.29187174\n",
      "Iteration 23191, loss = 1.29186998\n",
      "Iteration 23192, loss = 1.29186823\n",
      "Iteration 23193, loss = 1.29186653\n",
      "Iteration 23194, loss = 1.29186481\n",
      "Iteration 23195, loss = 1.29186302\n",
      "Iteration 23196, loss = 1.29186115\n",
      "Iteration 23197, loss = 1.29185919\n",
      "Iteration 23198, loss = 1.29185714\n",
      "Iteration 23199, loss = 1.29185499\n",
      "Iteration 23200, loss = 1.29185276\n",
      "Iteration 23201, loss = 1.29185045\n",
      "Iteration 23202, loss = 1.29184806\n",
      "Iteration 23203, loss = 1.29184560\n",
      "Iteration 23204, loss = 1.29184309\n",
      "Iteration 23205, loss = 1.29184053\n",
      "Iteration 23206, loss = 1.29183793\n",
      "Iteration 23207, loss = 1.29183529\n",
      "Iteration 23208, loss = 1.29183263\n",
      "Iteration 23209, loss = 1.29182995\n",
      "Iteration 23210, loss = 1.29182730\n",
      "Iteration 23211, loss = 1.29182466\n",
      "Iteration 23212, loss = 1.29182203\n",
      "Iteration 23213, loss = 1.29181940\n",
      "Iteration 23214, loss = 1.29181677\n",
      "Iteration 23215, loss = 1.29181416\n",
      "Iteration 23216, loss = 1.29181155\n",
      "Iteration 23217, loss = 1.29180896\n",
      "Iteration 23218, loss = 1.29180642\n",
      "Iteration 23219, loss = 1.29180396\n",
      "Iteration 23220, loss = 1.29180157\n",
      "Iteration 23221, loss = 1.29179922\n",
      "Iteration 23222, loss = 1.29179685\n",
      "Iteration 23223, loss = 1.29179446\n",
      "Iteration 23224, loss = 1.29179204\n",
      "Iteration 23225, loss = 1.29178961\n",
      "Iteration 23226, loss = 1.29178716\n",
      "Iteration 23227, loss = 1.29178469\n",
      "Iteration 23228, loss = 1.29178221\n",
      "Iteration 23229, loss = 1.29177971\n",
      "Iteration 23230, loss = 1.29177719\n",
      "Iteration 23231, loss = 1.29177467\n",
      "Iteration 23232, loss = 1.29177220\n",
      "Iteration 23233, loss = 1.29176978\n",
      "Iteration 23234, loss = 1.29176738\n",
      "Iteration 23235, loss = 1.29176499\n",
      "Iteration 23236, loss = 1.29176258\n",
      "Iteration 23237, loss = 1.29176016\n",
      "Iteration 23238, loss = 1.29175774\n",
      "Iteration 23239, loss = 1.29175532\n",
      "Iteration 23240, loss = 1.29175288\n",
      "Iteration 23241, loss = 1.29175207\n",
      "Iteration 23242, loss = 1.29174874\n",
      "Iteration 23243, loss = 1.29174707\n",
      "Iteration 23244, loss = 1.29174541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23245, loss = 1.29174377\n",
      "Iteration 23246, loss = 1.29174211\n",
      "Iteration 23247, loss = 1.29174037\n",
      "Iteration 23248, loss = 1.29173854\n",
      "Iteration 23249, loss = 1.29173662\n",
      "Iteration 23250, loss = 1.29173460\n",
      "Iteration 23251, loss = 1.29173249\n",
      "Iteration 23252, loss = 1.29173029\n",
      "Iteration 23253, loss = 1.29172801\n",
      "Iteration 23254, loss = 1.29172565\n",
      "Iteration 23255, loss = 1.29172322\n",
      "Iteration 23256, loss = 1.29172074\n",
      "Iteration 23257, loss = 1.29171821\n",
      "Iteration 23258, loss = 1.29171564\n",
      "Iteration 23259, loss = 1.29171303\n",
      "Iteration 23260, loss = 1.29171040\n",
      "Iteration 23261, loss = 1.29170777\n",
      "Iteration 23262, loss = 1.29170516\n",
      "Iteration 23263, loss = 1.29170256\n",
      "Iteration 23264, loss = 1.29169995\n",
      "Iteration 23265, loss = 1.29169735\n",
      "Iteration 23266, loss = 1.29169476\n",
      "Iteration 23267, loss = 1.29169217\n",
      "Iteration 23268, loss = 1.29168960\n",
      "Iteration 23269, loss = 1.29168703\n",
      "Iteration 23270, loss = 1.29168448\n",
      "Iteration 23271, loss = 1.29168196\n",
      "Iteration 23272, loss = 1.29167945\n",
      "Iteration 23273, loss = 1.29167700\n",
      "Iteration 23274, loss = 1.29167466\n",
      "Iteration 23275, loss = 1.29167234\n",
      "Iteration 23276, loss = 1.29166999\n",
      "Iteration 23277, loss = 1.29166762\n",
      "Iteration 23278, loss = 1.29166523\n",
      "Iteration 23279, loss = 1.29166281\n",
      "Iteration 23280, loss = 1.29166038\n",
      "Iteration 23281, loss = 1.29165792\n",
      "Iteration 23282, loss = 1.29165545\n",
      "Iteration 23283, loss = 1.29165296\n",
      "Iteration 23284, loss = 1.29165047\n",
      "Iteration 23285, loss = 1.29164803\n",
      "Iteration 23286, loss = 1.29164566\n",
      "Iteration 23287, loss = 1.29164329\n",
      "Iteration 23288, loss = 1.29164090\n",
      "Iteration 23289, loss = 1.29163851\n",
      "Iteration 23290, loss = 1.29163612\n",
      "Iteration 23291, loss = 1.29163372\n",
      "Iteration 23292, loss = 1.29163131\n",
      "Iteration 23293, loss = 1.29162890\n",
      "Iteration 23294, loss = 1.29162648\n",
      "Iteration 23295, loss = 1.29162406\n",
      "Iteration 23296, loss = 1.29162164\n",
      "Iteration 23297, loss = 1.29161922\n",
      "Iteration 23298, loss = 1.29161692\n",
      "Iteration 23299, loss = 1.29161499\n",
      "Iteration 23300, loss = 1.29161325\n",
      "Iteration 23301, loss = 1.29161156\n",
      "Iteration 23302, loss = 1.29160993\n",
      "Iteration 23303, loss = 1.29160825\n",
      "Iteration 23304, loss = 1.29160650\n",
      "Iteration 23305, loss = 1.29160467\n",
      "Iteration 23306, loss = 1.29160276\n",
      "Iteration 23307, loss = 1.29160076\n",
      "Iteration 23308, loss = 1.29159866\n",
      "Iteration 23309, loss = 1.29159648\n",
      "Iteration 23310, loss = 1.29159422\n",
      "Iteration 23311, loss = 1.29159188\n",
      "Iteration 23312, loss = 1.29158948\n",
      "Iteration 23313, loss = 1.29158702\n",
      "Iteration 23314, loss = 1.29158450\n",
      "Iteration 23315, loss = 1.29158195\n",
      "Iteration 23316, loss = 1.29157936\n",
      "Iteration 23317, loss = 1.29157675\n",
      "Iteration 23318, loss = 1.29157412\n",
      "Iteration 23319, loss = 1.29157150\n",
      "Iteration 23320, loss = 1.29156890\n",
      "Iteration 23321, loss = 1.29156631\n",
      "Iteration 23322, loss = 1.29156372\n",
      "Iteration 23323, loss = 1.29156114\n",
      "Iteration 23324, loss = 1.29155857\n",
      "Iteration 23325, loss = 1.29155601\n",
      "Iteration 23326, loss = 1.29155346\n",
      "Iteration 23327, loss = 1.29155094\n",
      "Iteration 23328, loss = 1.29154851\n",
      "Iteration 23329, loss = 1.29154618\n",
      "Iteration 23330, loss = 1.29154386\n",
      "Iteration 23331, loss = 1.29154153\n",
      "Iteration 23332, loss = 1.29153918\n",
      "Iteration 23333, loss = 1.29153682\n",
      "Iteration 23334, loss = 1.29153443\n",
      "Iteration 23335, loss = 1.29153202\n",
      "Iteration 23336, loss = 1.29152960\n",
      "Iteration 23337, loss = 1.29152716\n",
      "Iteration 23338, loss = 1.29152471\n",
      "Iteration 23339, loss = 1.29152224\n",
      "Iteration 23340, loss = 1.29151976\n",
      "Iteration 23341, loss = 1.29151732\n",
      "Iteration 23342, loss = 1.29151496\n",
      "Iteration 23343, loss = 1.29151262\n",
      "Iteration 23344, loss = 1.29151026\n",
      "Iteration 23345, loss = 1.29150789\n",
      "Iteration 23346, loss = 1.29150552\n",
      "Iteration 23347, loss = 1.29150314\n",
      "Iteration 23348, loss = 1.29150075\n",
      "Iteration 23349, loss = 1.29150267\n",
      "Iteration 23350, loss = 1.29149671\n",
      "Iteration 23351, loss = 1.29149509\n",
      "Iteration 23352, loss = 1.29149349\n",
      "Iteration 23353, loss = 1.29149188\n",
      "Iteration 23354, loss = 1.29149027\n",
      "Iteration 23355, loss = 1.29148859\n",
      "Iteration 23356, loss = 1.29148682\n",
      "Iteration 23357, loss = 1.29148495\n",
      "Iteration 23358, loss = 1.29148298\n",
      "Iteration 23359, loss = 1.29148092\n",
      "Iteration 23360, loss = 1.29147877\n",
      "Iteration 23361, loss = 1.29147654\n",
      "Iteration 23362, loss = 1.29147423\n",
      "Iteration 23363, loss = 1.29147185\n",
      "Iteration 23364, loss = 1.29146941\n",
      "Iteration 23365, loss = 1.29146692\n",
      "Iteration 23366, loss = 1.29146439\n",
      "Iteration 23367, loss = 1.29146182\n",
      "Iteration 23368, loss = 1.29145924\n",
      "Iteration 23369, loss = 1.29145664\n",
      "Iteration 23370, loss = 1.29145407\n",
      "Iteration 23371, loss = 1.29145150\n",
      "Iteration 23372, loss = 1.29144894\n",
      "Iteration 23373, loss = 1.29144637\n",
      "Iteration 23374, loss = 1.29144382\n",
      "Iteration 23375, loss = 1.29144127\n",
      "Iteration 23376, loss = 1.29143874\n",
      "Iteration 23377, loss = 1.29143621\n",
      "Iteration 23378, loss = 1.29143370\n",
      "Iteration 23379, loss = 1.29143120\n",
      "Iteration 23380, loss = 1.29142872\n",
      "Iteration 23381, loss = 1.29142629\n",
      "Iteration 23382, loss = 1.29142397\n",
      "Iteration 23383, loss = 1.29142168\n",
      "Iteration 23384, loss = 1.29141938\n",
      "Iteration 23385, loss = 1.29141707\n",
      "Iteration 23386, loss = 1.29141473\n",
      "Iteration 23387, loss = 1.29141237\n",
      "Iteration 23388, loss = 1.29140998\n",
      "Iteration 23389, loss = 1.29140758\n",
      "Iteration 23390, loss = 1.29140516\n",
      "Iteration 23391, loss = 1.29140273\n",
      "Iteration 23392, loss = 1.29140028\n",
      "Iteration 23393, loss = 1.29139786\n",
      "Iteration 23394, loss = 1.29139549\n",
      "Iteration 23395, loss = 1.29139316\n",
      "Iteration 23396, loss = 1.29139083\n",
      "Iteration 23397, loss = 1.29138849\n",
      "Iteration 23398, loss = 1.29138614\n",
      "Iteration 23399, loss = 1.29138378\n",
      "Iteration 23400, loss = 1.29138142\n",
      "Iteration 23401, loss = 1.29137906\n",
      "Iteration 23402, loss = 1.29137669\n",
      "Iteration 23403, loss = 1.29137431\n",
      "Iteration 23404, loss = 1.29137194\n",
      "Iteration 23405, loss = 1.29136956\n",
      "Iteration 23406, loss = 1.29136718\n",
      "Iteration 23407, loss = 1.29136481\n",
      "Iteration 23408, loss = 1.29136244\n",
      "Iteration 23409, loss = 1.29136009\n",
      "Iteration 23410, loss = 1.29135774\n",
      "Iteration 23411, loss = 1.29135610\n",
      "Iteration 23412, loss = 1.29135356\n",
      "Iteration 23413, loss = 1.29135189\n",
      "Iteration 23414, loss = 1.29135029\n",
      "Iteration 23415, loss = 1.29134874\n",
      "Iteration 23416, loss = 1.29134714\n",
      "Iteration 23417, loss = 1.29134548\n",
      "Iteration 23418, loss = 1.29134374\n",
      "Iteration 23419, loss = 1.29134190\n",
      "Iteration 23420, loss = 1.29133998\n",
      "Iteration 23421, loss = 1.29133796\n",
      "Iteration 23422, loss = 1.29133585\n",
      "Iteration 23423, loss = 1.29133365\n",
      "Iteration 23424, loss = 1.29133136\n",
      "Iteration 23425, loss = 1.29132901\n",
      "Iteration 23426, loss = 1.29132660\n",
      "Iteration 23427, loss = 1.29132413\n",
      "Iteration 23428, loss = 1.29132161\n",
      "Iteration 23429, loss = 1.29131906\n",
      "Iteration 23430, loss = 1.29131648\n",
      "Iteration 23431, loss = 1.29131389\n",
      "Iteration 23432, loss = 1.29131128\n",
      "Iteration 23433, loss = 1.29130871\n",
      "Iteration 23434, loss = 1.29130615\n",
      "Iteration 23435, loss = 1.29130359\n",
      "Iteration 23436, loss = 1.29130104\n",
      "Iteration 23437, loss = 1.29129850\n",
      "Iteration 23438, loss = 1.29129597\n",
      "Iteration 23439, loss = 1.29129346\n",
      "Iteration 23440, loss = 1.29129098\n",
      "Iteration 23441, loss = 1.29128854\n",
      "Iteration 23442, loss = 1.29128612\n",
      "Iteration 23443, loss = 1.29128372\n",
      "Iteration 23444, loss = 1.29128138\n",
      "Iteration 23445, loss = 1.29127907\n",
      "Iteration 23446, loss = 1.29127679\n",
      "Iteration 23447, loss = 1.29127449\n",
      "Iteration 23448, loss = 1.29127217\n",
      "Iteration 23449, loss = 1.29126983\n",
      "Iteration 23450, loss = 1.29126747\n",
      "Iteration 23451, loss = 1.29126510\n",
      "Iteration 23452, loss = 1.29126270\n",
      "Iteration 23453, loss = 1.29126029\n",
      "Iteration 23454, loss = 1.29125789\n",
      "Iteration 23455, loss = 1.29125550\n",
      "Iteration 23456, loss = 1.29125311\n",
      "Iteration 23457, loss = 1.29125077\n",
      "Iteration 23458, loss = 1.29125094\n",
      "Iteration 23459, loss = 1.29124688\n",
      "Iteration 23460, loss = 1.29124536\n",
      "Iteration 23461, loss = 1.29124385\n",
      "Iteration 23462, loss = 1.29124233\n",
      "Iteration 23463, loss = 1.29124082\n",
      "Iteration 23464, loss = 1.29123924\n",
      "Iteration 23465, loss = 1.29123757\n",
      "Iteration 23466, loss = 1.29123580\n",
      "Iteration 23467, loss = 1.29123393\n",
      "Iteration 23468, loss = 1.29123195\n",
      "Iteration 23469, loss = 1.29122987\n",
      "Iteration 23470, loss = 1.29122770\n",
      "Iteration 23471, loss = 1.29122545\n",
      "Iteration 23472, loss = 1.29122312\n",
      "Iteration 23473, loss = 1.29122072\n",
      "Iteration 23474, loss = 1.29121827\n",
      "Iteration 23475, loss = 1.29121577\n",
      "Iteration 23476, loss = 1.29121324\n",
      "Iteration 23477, loss = 1.29121068\n",
      "Iteration 23478, loss = 1.29120810\n",
      "Iteration 23479, loss = 1.29120555\n",
      "Iteration 23480, loss = 1.29120300\n",
      "Iteration 23481, loss = 1.29120046\n",
      "Iteration 23482, loss = 1.29119791\n",
      "Iteration 23483, loss = 1.29119538\n",
      "Iteration 23484, loss = 1.29119285\n",
      "Iteration 23485, loss = 1.29119034\n",
      "Iteration 23486, loss = 1.29118784\n",
      "Iteration 23487, loss = 1.29118536\n",
      "Iteration 23488, loss = 1.29118289\n",
      "Iteration 23489, loss = 1.29118043\n",
      "Iteration 23490, loss = 1.29117798\n",
      "Iteration 23491, loss = 1.29117555\n",
      "Iteration 23492, loss = 1.29117314\n",
      "Iteration 23493, loss = 1.29117078\n",
      "Iteration 23494, loss = 1.29116846\n",
      "Iteration 23495, loss = 1.29116617\n",
      "Iteration 23496, loss = 1.29116391\n",
      "Iteration 23497, loss = 1.29116165\n",
      "Iteration 23498, loss = 1.29115936\n",
      "Iteration 23499, loss = 1.29115704\n",
      "Iteration 23500, loss = 1.29115470\n",
      "Iteration 23501, loss = 1.29115234\n",
      "Iteration 23502, loss = 1.29114996\n",
      "Iteration 23503, loss = 1.29114757\n",
      "Iteration 23504, loss = 1.29114517\n",
      "Iteration 23505, loss = 1.29114283\n",
      "Iteration 23506, loss = 1.29114052\n",
      "Iteration 23507, loss = 1.29113823\n",
      "Iteration 23508, loss = 1.29113593\n",
      "Iteration 23509, loss = 1.29113363\n",
      "Iteration 23510, loss = 1.29113132\n",
      "Iteration 23511, loss = 1.29112900\n",
      "Iteration 23512, loss = 1.29112668\n",
      "Iteration 23513, loss = 1.29112436\n",
      "Iteration 23514, loss = 1.29112203\n",
      "Iteration 23515, loss = 1.29111970\n",
      "Iteration 23516, loss = 1.29111737\n",
      "Iteration 23517, loss = 1.29111504\n",
      "Iteration 23518, loss = 1.29111270\n",
      "Iteration 23519, loss = 1.29111037\n",
      "Iteration 23520, loss = 1.29110805\n",
      "Iteration 23521, loss = 1.29110574\n",
      "Iteration 23522, loss = 1.29110342\n",
      "Iteration 23523, loss = 1.29110446\n",
      "Iteration 23524, loss = 1.29109939\n",
      "Iteration 23525, loss = 1.29109779\n",
      "Iteration 23526, loss = 1.29109628\n",
      "Iteration 23527, loss = 1.29109479\n",
      "Iteration 23528, loss = 1.29109326\n",
      "Iteration 23529, loss = 1.29109166\n",
      "Iteration 23530, loss = 1.29108998\n",
      "Iteration 23531, loss = 1.29108821\n",
      "Iteration 23532, loss = 1.29108635\n",
      "Iteration 23533, loss = 1.29108439\n",
      "Iteration 23534, loss = 1.29108235\n",
      "Iteration 23535, loss = 1.29108021\n",
      "Iteration 23536, loss = 1.29107800\n",
      "Iteration 23537, loss = 1.29107571\n",
      "Iteration 23538, loss = 1.29107336\n",
      "Iteration 23539, loss = 1.29107096\n",
      "Iteration 23540, loss = 1.29106851\n",
      "Iteration 23541, loss = 1.29106603\n",
      "Iteration 23542, loss = 1.29106352\n",
      "Iteration 23543, loss = 1.29106099\n",
      "Iteration 23544, loss = 1.29105844\n",
      "Iteration 23545, loss = 1.29105590\n",
      "Iteration 23546, loss = 1.29105339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23547, loss = 1.29105089\n",
      "Iteration 23548, loss = 1.29104840\n",
      "Iteration 23549, loss = 1.29104592\n",
      "Iteration 23550, loss = 1.29104344\n",
      "Iteration 23551, loss = 1.29104098\n",
      "Iteration 23552, loss = 1.29103854\n",
      "Iteration 23553, loss = 1.29103611\n",
      "Iteration 23554, loss = 1.29103372\n",
      "Iteration 23555, loss = 1.29103136\n",
      "Iteration 23556, loss = 1.29102903\n",
      "Iteration 23557, loss = 1.29102672\n",
      "Iteration 23558, loss = 1.29102443\n",
      "Iteration 23559, loss = 1.29102218\n",
      "Iteration 23560, loss = 1.29101993\n",
      "Iteration 23561, loss = 1.29101765\n",
      "Iteration 23562, loss = 1.29101536\n",
      "Iteration 23563, loss = 1.29101304\n",
      "Iteration 23564, loss = 1.29101071\n",
      "Iteration 23565, loss = 1.29100836\n",
      "Iteration 23566, loss = 1.29100600\n",
      "Iteration 23567, loss = 1.29100365\n",
      "Iteration 23568, loss = 1.29100131\n",
      "Iteration 23569, loss = 1.29099901\n",
      "Iteration 23570, loss = 1.29099674\n",
      "Iteration 23571, loss = 1.29099447\n",
      "Iteration 23572, loss = 1.29099219\n",
      "Iteration 23573, loss = 1.29098991\n",
      "Iteration 23574, loss = 1.29098762\n",
      "Iteration 23575, loss = 1.29098532\n",
      "Iteration 23576, loss = 1.29098302\n",
      "Iteration 23577, loss = 1.29098072\n",
      "Iteration 23578, loss = 1.29097841\n",
      "Iteration 23579, loss = 1.29097610\n",
      "Iteration 23580, loss = 1.29097379\n",
      "Iteration 23581, loss = 1.29097226\n",
      "Iteration 23582, loss = 1.29096977\n",
      "Iteration 23583, loss = 1.29096814\n",
      "Iteration 23584, loss = 1.29096655\n",
      "Iteration 23585, loss = 1.29096502\n",
      "Iteration 23586, loss = 1.29096346\n",
      "Iteration 23587, loss = 1.29096184\n",
      "Iteration 23588, loss = 1.29096014\n",
      "Iteration 23589, loss = 1.29095835\n",
      "Iteration 23590, loss = 1.29095647\n",
      "Iteration 23591, loss = 1.29095450\n",
      "Iteration 23592, loss = 1.29095244\n",
      "Iteration 23593, loss = 1.29095030\n",
      "Iteration 23594, loss = 1.29094808\n",
      "Iteration 23595, loss = 1.29094578\n",
      "Iteration 23596, loss = 1.29094343\n",
      "Iteration 23597, loss = 1.29094103\n",
      "Iteration 23598, loss = 1.29093858\n",
      "Iteration 23599, loss = 1.29093610\n",
      "Iteration 23600, loss = 1.29093359\n",
      "Iteration 23601, loss = 1.29093106\n",
      "Iteration 23602, loss = 1.29092853\n",
      "Iteration 23603, loss = 1.29092602\n",
      "Iteration 23604, loss = 1.29092352\n",
      "Iteration 23605, loss = 1.29092103\n",
      "Iteration 23606, loss = 1.29091855\n",
      "Iteration 23607, loss = 1.29091608\n",
      "Iteration 23608, loss = 1.29091362\n",
      "Iteration 23609, loss = 1.29091117\n",
      "Iteration 23610, loss = 1.29090874\n",
      "Iteration 23611, loss = 1.29090633\n",
      "Iteration 23612, loss = 1.29090401\n",
      "Iteration 23613, loss = 1.29090180\n",
      "Iteration 23614, loss = 1.29089959\n",
      "Iteration 23615, loss = 1.29089737\n",
      "Iteration 23616, loss = 1.29089512\n",
      "Iteration 23617, loss = 1.29089286\n",
      "Iteration 23618, loss = 1.29089058\n",
      "Iteration 23619, loss = 1.29088828\n",
      "Iteration 23620, loss = 1.29088597\n",
      "Iteration 23621, loss = 1.29088364\n",
      "Iteration 23622, loss = 1.29088129\n",
      "Iteration 23623, loss = 1.29087894\n",
      "Iteration 23624, loss = 1.29087657\n",
      "Iteration 23625, loss = 1.29087423\n",
      "Iteration 23626, loss = 1.29087197\n",
      "Iteration 23627, loss = 1.29086972\n",
      "Iteration 23628, loss = 1.29086747\n",
      "Iteration 23629, loss = 1.29086521\n",
      "Iteration 23630, loss = 1.29086295\n",
      "Iteration 23631, loss = 1.29086068\n",
      "Iteration 23632, loss = 1.29085840\n",
      "Iteration 23633, loss = 1.29085612\n",
      "Iteration 23634, loss = 1.29085383\n",
      "Iteration 23635, loss = 1.29085154\n",
      "Iteration 23636, loss = 1.29084925\n",
      "Iteration 23637, loss = 1.29084695\n",
      "Iteration 23638, loss = 1.29084466\n",
      "Iteration 23639, loss = 1.29084237\n",
      "Iteration 23640, loss = 1.29084548\n",
      "Iteration 23641, loss = 1.29083844\n",
      "Iteration 23642, loss = 1.29083688\n",
      "Iteration 23643, loss = 1.29083536\n",
      "Iteration 23644, loss = 1.29083389\n",
      "Iteration 23645, loss = 1.29083237\n",
      "Iteration 23646, loss = 1.29083079\n",
      "Iteration 23647, loss = 1.29082912\n",
      "Iteration 23648, loss = 1.29082736\n",
      "Iteration 23649, loss = 1.29082552\n",
      "Iteration 23650, loss = 1.29082358\n",
      "Iteration 23651, loss = 1.29082155\n",
      "Iteration 23652, loss = 1.29081944\n",
      "Iteration 23653, loss = 1.29081726\n",
      "Iteration 23654, loss = 1.29081500\n",
      "Iteration 23655, loss = 1.29081269\n",
      "Iteration 23656, loss = 1.29081033\n",
      "Iteration 23657, loss = 1.29080792\n",
      "Iteration 23658, loss = 1.29080548\n",
      "Iteration 23659, loss = 1.29080301\n",
      "Iteration 23660, loss = 1.29080053\n",
      "Iteration 23661, loss = 1.29079803\n",
      "Iteration 23662, loss = 1.29079556\n",
      "Iteration 23663, loss = 1.29079310\n",
      "Iteration 23664, loss = 1.29079065\n",
      "Iteration 23665, loss = 1.29078821\n",
      "Iteration 23666, loss = 1.29078577\n",
      "Iteration 23667, loss = 1.29078335\n",
      "Iteration 23668, loss = 1.29078094\n",
      "Iteration 23669, loss = 1.29077854\n",
      "Iteration 23670, loss = 1.29077615\n",
      "Iteration 23671, loss = 1.29077383\n",
      "Iteration 23672, loss = 1.29077162\n",
      "Iteration 23673, loss = 1.29076941\n",
      "Iteration 23674, loss = 1.29076721\n",
      "Iteration 23675, loss = 1.29076500\n",
      "Iteration 23676, loss = 1.29076276\n",
      "Iteration 23677, loss = 1.29076051\n",
      "Iteration 23678, loss = 1.29075825\n",
      "Iteration 23679, loss = 1.29075596\n",
      "Iteration 23680, loss = 1.29075366\n",
      "Iteration 23681, loss = 1.29075135\n",
      "Iteration 23682, loss = 1.29074902\n",
      "Iteration 23683, loss = 1.29074668\n",
      "Iteration 23684, loss = 1.29074437\n",
      "Iteration 23685, loss = 1.29074212\n",
      "Iteration 23686, loss = 1.29073989\n",
      "Iteration 23687, loss = 1.29073766\n",
      "Iteration 23688, loss = 1.29073542\n",
      "Iteration 23689, loss = 1.29073318\n",
      "Iteration 23690, loss = 1.29073093\n",
      "Iteration 23691, loss = 1.29072867\n",
      "Iteration 23692, loss = 1.29072641\n",
      "Iteration 23693, loss = 1.29072415\n",
      "Iteration 23694, loss = 1.29072188\n",
      "Iteration 23695, loss = 1.29071961\n",
      "Iteration 23696, loss = 1.29071734\n",
      "Iteration 23697, loss = 1.29071508\n",
      "Iteration 23698, loss = 1.29071281\n",
      "Iteration 23699, loss = 1.29071054\n",
      "Iteration 23700, loss = 1.29070828\n",
      "Iteration 23701, loss = 1.29070603\n",
      "Iteration 23702, loss = 1.29070406\n",
      "Iteration 23703, loss = 1.29070209\n",
      "Iteration 23704, loss = 1.29070053\n",
      "Iteration 23705, loss = 1.29069902\n",
      "Iteration 23706, loss = 1.29069758\n",
      "Iteration 23707, loss = 1.29069611\n",
      "Iteration 23708, loss = 1.29069457\n",
      "Iteration 23709, loss = 1.29069295\n",
      "Iteration 23710, loss = 1.29069124\n",
      "Iteration 23711, loss = 1.29068944\n",
      "Iteration 23712, loss = 1.29068753\n",
      "Iteration 23713, loss = 1.29068554\n",
      "Iteration 23714, loss = 1.29068345\n",
      "Iteration 23715, loss = 1.29068128\n",
      "Iteration 23716, loss = 1.29067903\n",
      "Iteration 23717, loss = 1.29067672\n",
      "Iteration 23718, loss = 1.29067436\n",
      "Iteration 23719, loss = 1.29067194\n",
      "Iteration 23720, loss = 1.29066949\n",
      "Iteration 23721, loss = 1.29066702\n",
      "Iteration 23722, loss = 1.29066452\n",
      "Iteration 23723, loss = 1.29066201\n",
      "Iteration 23724, loss = 1.29065951\n",
      "Iteration 23725, loss = 1.29065705\n",
      "Iteration 23726, loss = 1.29065459\n",
      "Iteration 23727, loss = 1.29065213\n",
      "Iteration 23728, loss = 1.29064969\n",
      "Iteration 23729, loss = 1.29064726\n",
      "Iteration 23730, loss = 1.29064485\n",
      "Iteration 23731, loss = 1.29064244\n",
      "Iteration 23732, loss = 1.29064006\n",
      "Iteration 23733, loss = 1.29063772\n",
      "Iteration 23734, loss = 1.29063542\n",
      "Iteration 23735, loss = 1.29063316\n",
      "Iteration 23736, loss = 1.29063098\n",
      "Iteration 23737, loss = 1.29062880\n",
      "Iteration 23738, loss = 1.29062661\n",
      "Iteration 23739, loss = 1.29062441\n",
      "Iteration 23740, loss = 1.29062218\n",
      "Iteration 23741, loss = 1.29061994\n",
      "Iteration 23742, loss = 1.29061768\n",
      "Iteration 23743, loss = 1.29061540\n",
      "Iteration 23744, loss = 1.29061311\n",
      "Iteration 23745, loss = 1.29061081\n",
      "Iteration 23746, loss = 1.29060849\n",
      "Iteration 23747, loss = 1.29060620\n",
      "Iteration 23748, loss = 1.29060394\n",
      "Iteration 23749, loss = 1.29060170\n",
      "Iteration 23750, loss = 1.29059950\n",
      "Iteration 23751, loss = 1.29059729\n",
      "Iteration 23752, loss = 1.29059507\n",
      "Iteration 23753, loss = 1.29059284\n",
      "Iteration 23754, loss = 1.29059061\n",
      "Iteration 23755, loss = 1.29058837\n",
      "Iteration 23756, loss = 1.29058613\n",
      "Iteration 23757, loss = 1.29058389\n",
      "Iteration 23758, loss = 1.29058434\n",
      "Iteration 23759, loss = 1.29058009\n",
      "Iteration 23760, loss = 1.29057860\n",
      "Iteration 23761, loss = 1.29057712\n",
      "Iteration 23762, loss = 1.29057569\n",
      "Iteration 23763, loss = 1.29057423\n",
      "Iteration 23764, loss = 1.29057271\n",
      "Iteration 23765, loss = 1.29057110\n",
      "Iteration 23766, loss = 1.29056940\n",
      "Iteration 23767, loss = 1.29056761\n",
      "Iteration 23768, loss = 1.29056572\n",
      "Iteration 23769, loss = 1.29056374\n",
      "Iteration 23770, loss = 1.29056167\n",
      "Iteration 23771, loss = 1.29055952\n",
      "Iteration 23772, loss = 1.29055730\n",
      "Iteration 23773, loss = 1.29055502\n",
      "Iteration 23774, loss = 1.29055269\n",
      "Iteration 23775, loss = 1.29055031\n",
      "Iteration 23776, loss = 1.29054790\n",
      "Iteration 23777, loss = 1.29054547\n",
      "Iteration 23778, loss = 1.29054301\n",
      "Iteration 23779, loss = 1.29054055\n",
      "Iteration 23780, loss = 1.29053810\n",
      "Iteration 23781, loss = 1.29053567\n",
      "Iteration 23782, loss = 1.29053325\n",
      "Iteration 23783, loss = 1.29053084\n",
      "Iteration 23784, loss = 1.29052844\n",
      "Iteration 23785, loss = 1.29052605\n",
      "Iteration 23786, loss = 1.29052367\n",
      "Iteration 23787, loss = 1.29052130\n",
      "Iteration 23788, loss = 1.29051895\n",
      "Iteration 23789, loss = 1.29051660\n",
      "Iteration 23790, loss = 1.29051429\n",
      "Iteration 23791, loss = 1.29051204\n",
      "Iteration 23792, loss = 1.29050987\n",
      "Iteration 23793, loss = 1.29050773\n",
      "Iteration 23794, loss = 1.29050557\n",
      "Iteration 23795, loss = 1.29050339\n",
      "Iteration 23796, loss = 1.29050119\n",
      "Iteration 23797, loss = 1.29049897\n",
      "Iteration 23798, loss = 1.29049673\n",
      "Iteration 23799, loss = 1.29049447\n",
      "Iteration 23800, loss = 1.29049220\n",
      "Iteration 23801, loss = 1.29048992\n",
      "Iteration 23802, loss = 1.29048763\n",
      "Iteration 23803, loss = 1.29048535\n",
      "Iteration 23804, loss = 1.29048314\n",
      "Iteration 23805, loss = 1.29048096\n",
      "Iteration 23806, loss = 1.29047877\n",
      "Iteration 23807, loss = 1.29047657\n",
      "Iteration 23808, loss = 1.29047437\n",
      "Iteration 23809, loss = 1.29047216\n",
      "Iteration 23810, loss = 1.29046994\n",
      "Iteration 23811, loss = 1.29046772\n",
      "Iteration 23812, loss = 1.29046550\n",
      "Iteration 23813, loss = 1.29046328\n",
      "Iteration 23814, loss = 1.29046105\n",
      "Iteration 23815, loss = 1.29045882\n",
      "Iteration 23816, loss = 1.29045659\n",
      "Iteration 23817, loss = 1.29045436\n",
      "Iteration 23818, loss = 1.29045213\n",
      "Iteration 23819, loss = 1.29044991\n",
      "Iteration 23820, loss = 1.29044772\n",
      "Iteration 23821, loss = 1.29044552\n",
      "Iteration 23822, loss = 1.29044331\n",
      "Iteration 23823, loss = 1.29044109\n",
      "Iteration 23824, loss = 1.29043887\n",
      "Iteration 23825, loss = 1.29043665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23826, loss = 1.29043445\n",
      "Iteration 23827, loss = 1.29043224\n",
      "Iteration 23828, loss = 1.29043128\n",
      "Iteration 23829, loss = 1.29042844\n",
      "Iteration 23830, loss = 1.29042694\n",
      "Iteration 23831, loss = 1.29042555\n",
      "Iteration 23832, loss = 1.29042417\n",
      "Iteration 23833, loss = 1.29042275\n",
      "Iteration 23834, loss = 1.29042127\n",
      "Iteration 23835, loss = 1.29041971\n",
      "Iteration 23836, loss = 1.29041805\n",
      "Iteration 23837, loss = 1.29041629\n",
      "Iteration 23838, loss = 1.29041444\n",
      "Iteration 23839, loss = 1.29041248\n",
      "Iteration 23840, loss = 1.29041044\n",
      "Iteration 23841, loss = 1.29040831\n",
      "Iteration 23842, loss = 1.29040610\n",
      "Iteration 23843, loss = 1.29040382\n",
      "Iteration 23844, loss = 1.29040149\n",
      "Iteration 23845, loss = 1.29039911\n",
      "Iteration 23846, loss = 1.29039670\n",
      "Iteration 23847, loss = 1.29039425\n",
      "Iteration 23848, loss = 1.29039179\n",
      "Iteration 23849, loss = 1.29038931\n",
      "Iteration 23850, loss = 1.29038683\n",
      "Iteration 23851, loss = 1.29038436\n",
      "Iteration 23852, loss = 1.29038193\n",
      "Iteration 23853, loss = 1.29037951\n",
      "Iteration 23854, loss = 1.29037710\n",
      "Iteration 23855, loss = 1.29037471\n",
      "Iteration 23856, loss = 1.29037233\n",
      "Iteration 23857, loss = 1.29036997\n",
      "Iteration 23858, loss = 1.29036762\n",
      "Iteration 23859, loss = 1.29036529\n",
      "Iteration 23860, loss = 1.29036298\n",
      "Iteration 23861, loss = 1.29036071\n",
      "Iteration 23862, loss = 1.29035855\n",
      "Iteration 23863, loss = 1.29035644\n",
      "Iteration 23864, loss = 1.29035431\n",
      "Iteration 23865, loss = 1.29035216\n",
      "Iteration 23866, loss = 1.29034999\n",
      "Iteration 23867, loss = 1.29034780\n",
      "Iteration 23868, loss = 1.29034559\n",
      "Iteration 23869, loss = 1.29034336\n",
      "Iteration 23870, loss = 1.29034112\n",
      "Iteration 23871, loss = 1.29033887\n",
      "Iteration 23872, loss = 1.29033660\n",
      "Iteration 23873, loss = 1.29033433\n",
      "Iteration 23874, loss = 1.29033208\n",
      "Iteration 23875, loss = 1.29032990\n",
      "Iteration 23876, loss = 1.29032774\n",
      "Iteration 23877, loss = 1.29032557\n",
      "Iteration 23878, loss = 1.29032340\n",
      "Iteration 23879, loss = 1.29032298\n",
      "Iteration 23880, loss = 1.29031974\n",
      "Iteration 23881, loss = 1.29031834\n",
      "Iteration 23882, loss = 1.29031696\n",
      "Iteration 23883, loss = 1.29031560\n",
      "Iteration 23884, loss = 1.29031424\n",
      "Iteration 23885, loss = 1.29031280\n",
      "Iteration 23886, loss = 1.29031128\n",
      "Iteration 23887, loss = 1.29030965\n",
      "Iteration 23888, loss = 1.29030793\n",
      "Iteration 23889, loss = 1.29030610\n",
      "Iteration 23890, loss = 1.29030418\n",
      "Iteration 23891, loss = 1.29030216\n",
      "Iteration 23892, loss = 1.29030006\n",
      "Iteration 23893, loss = 1.29029788\n",
      "Iteration 23894, loss = 1.29029563\n",
      "Iteration 23895, loss = 1.29029333\n",
      "Iteration 23896, loss = 1.29029098\n",
      "Iteration 23897, loss = 1.29028859\n",
      "Iteration 23898, loss = 1.29028618\n",
      "Iteration 23899, loss = 1.29028374\n",
      "Iteration 23900, loss = 1.29028130\n",
      "Iteration 23901, loss = 1.29027888\n",
      "Iteration 23902, loss = 1.29027647\n",
      "Iteration 23903, loss = 1.29027407\n",
      "Iteration 23904, loss = 1.29027168\n",
      "Iteration 23905, loss = 1.29026930\n",
      "Iteration 23906, loss = 1.29026693\n",
      "Iteration 23907, loss = 1.29026458\n",
      "Iteration 23908, loss = 1.29026224\n",
      "Iteration 23909, loss = 1.29025991\n",
      "Iteration 23910, loss = 1.29025760\n",
      "Iteration 23911, loss = 1.29025530\n",
      "Iteration 23912, loss = 1.29025302\n",
      "Iteration 23913, loss = 1.29025074\n",
      "Iteration 23914, loss = 1.29024849\n",
      "Iteration 23915, loss = 1.29024630\n",
      "Iteration 23916, loss = 1.29024417\n",
      "Iteration 23917, loss = 1.29024207\n",
      "Iteration 23918, loss = 1.29023996\n",
      "Iteration 23919, loss = 1.29023782\n",
      "Iteration 23920, loss = 1.29023565\n",
      "Iteration 23921, loss = 1.29023347\n",
      "Iteration 23922, loss = 1.29023127\n",
      "Iteration 23923, loss = 1.29022904\n",
      "Iteration 23924, loss = 1.29022681\n",
      "Iteration 23925, loss = 1.29022456\n",
      "Iteration 23926, loss = 1.29022231\n",
      "Iteration 23927, loss = 1.29022012\n",
      "Iteration 23928, loss = 1.29021797\n",
      "Iteration 23929, loss = 1.29021582\n",
      "Iteration 23930, loss = 1.29021366\n",
      "Iteration 23931, loss = 1.29021150\n",
      "Iteration 23932, loss = 1.29020933\n",
      "Iteration 23933, loss = 1.29020716\n",
      "Iteration 23934, loss = 1.29020498\n",
      "Iteration 23935, loss = 1.29020280\n",
      "Iteration 23936, loss = 1.29020062\n",
      "Iteration 23937, loss = 1.29019844\n",
      "Iteration 23938, loss = 1.29019625\n",
      "Iteration 23939, loss = 1.29019406\n",
      "Iteration 23940, loss = 1.29019187\n",
      "Iteration 23941, loss = 1.29018969\n",
      "Iteration 23942, loss = 1.29018750\n",
      "Iteration 23943, loss = 1.29018534\n",
      "Iteration 23944, loss = 1.29018318\n",
      "Iteration 23945, loss = 1.29018101\n",
      "Iteration 23946, loss = 1.29017884\n",
      "Iteration 23947, loss = 1.29017666\n",
      "Iteration 23948, loss = 1.29017448\n",
      "Iteration 23949, loss = 1.29017232\n",
      "Iteration 23950, loss = 1.29017015\n",
      "Iteration 23951, loss = 1.29016838\n",
      "Iteration 23952, loss = 1.29016647\n",
      "Iteration 23953, loss = 1.29016504\n",
      "Iteration 23954, loss = 1.29016373\n",
      "Iteration 23955, loss = 1.29016241\n",
      "Iteration 23956, loss = 1.29016106\n",
      "Iteration 23957, loss = 1.29015963\n",
      "Iteration 23958, loss = 1.29015812\n",
      "Iteration 23959, loss = 1.29015652\n",
      "Iteration 23960, loss = 1.29015482\n",
      "Iteration 23961, loss = 1.29015303\n",
      "Iteration 23962, loss = 1.29015113\n",
      "Iteration 23963, loss = 1.29014915\n",
      "Iteration 23964, loss = 1.29014708\n",
      "Iteration 23965, loss = 1.29014493\n",
      "Iteration 23966, loss = 1.29014272\n",
      "Iteration 23967, loss = 1.29014045\n",
      "Iteration 23968, loss = 1.29013814\n",
      "Iteration 23969, loss = 1.29013578\n",
      "Iteration 23970, loss = 1.29013340\n",
      "Iteration 23971, loss = 1.29013100\n",
      "Iteration 23972, loss = 1.29012859\n",
      "Iteration 23973, loss = 1.29012617\n",
      "Iteration 23974, loss = 1.29012375\n",
      "Iteration 23975, loss = 1.29012135\n",
      "Iteration 23976, loss = 1.29011899\n",
      "Iteration 23977, loss = 1.29011664\n",
      "Iteration 23978, loss = 1.29011430\n",
      "Iteration 23979, loss = 1.29011197\n",
      "Iteration 23980, loss = 1.29010966\n",
      "Iteration 23981, loss = 1.29010736\n",
      "Iteration 23982, loss = 1.29010508\n",
      "Iteration 23983, loss = 1.29010281\n",
      "Iteration 23984, loss = 1.29010058\n",
      "Iteration 23985, loss = 1.29009838\n",
      "Iteration 23986, loss = 1.29009620\n",
      "Iteration 23987, loss = 1.29009411\n",
      "Iteration 23988, loss = 1.29009202\n",
      "Iteration 23989, loss = 1.29008992\n",
      "Iteration 23990, loss = 1.29008780\n",
      "Iteration 23991, loss = 1.29008565\n",
      "Iteration 23992, loss = 1.29008349\n",
      "Iteration 23993, loss = 1.29008131\n",
      "Iteration 23994, loss = 1.29007911\n",
      "Iteration 23995, loss = 1.29007690\n",
      "Iteration 23996, loss = 1.29007468\n",
      "Iteration 23997, loss = 1.29007245\n",
      "Iteration 23998, loss = 1.29007029\n",
      "Iteration 23999, loss = 1.29006815\n",
      "Iteration 24000, loss = 1.29006603\n",
      "Iteration 24001, loss = 1.29006389\n",
      "Iteration 24002, loss = 1.29006176\n",
      "Iteration 24003, loss = 1.29005961\n",
      "Iteration 24004, loss = 1.29005746\n",
      "Iteration 24005, loss = 1.29005531\n",
      "Iteration 24006, loss = 1.29005316\n",
      "Iteration 24007, loss = 1.29005100\n",
      "Iteration 24008, loss = 1.29004884\n",
      "Iteration 24009, loss = 1.29004667\n",
      "Iteration 24010, loss = 1.29004451\n",
      "Iteration 24011, loss = 1.29004235\n",
      "Iteration 24012, loss = 1.29004019\n",
      "Iteration 24013, loss = 1.29003803\n",
      "Iteration 24014, loss = 1.29003958\n",
      "Iteration 24015, loss = 1.29003427\n",
      "Iteration 24016, loss = 1.29003280\n",
      "Iteration 24017, loss = 1.29003138\n",
      "Iteration 24018, loss = 1.29003005\n",
      "Iteration 24019, loss = 1.29002867\n",
      "Iteration 24020, loss = 1.29002724\n",
      "Iteration 24021, loss = 1.29002572\n",
      "Iteration 24022, loss = 1.29002412\n",
      "Iteration 24023, loss = 1.29002243\n",
      "Iteration 24024, loss = 1.29002063\n",
      "Iteration 24025, loss = 1.29001874\n",
      "Iteration 24026, loss = 1.29001676\n",
      "Iteration 24027, loss = 1.29001470\n",
      "Iteration 24028, loss = 1.29001256\n",
      "Iteration 24029, loss = 1.29001035\n",
      "Iteration 24030, loss = 1.29000809\n",
      "Iteration 24031, loss = 1.29000578\n",
      "Iteration 24032, loss = 1.29000344\n",
      "Iteration 24033, loss = 1.29000107\n",
      "Iteration 24034, loss = 1.28999867\n",
      "Iteration 24035, loss = 1.28999626\n",
      "Iteration 24036, loss = 1.28999385\n",
      "Iteration 24037, loss = 1.28999146\n",
      "Iteration 24038, loss = 1.28998909\n",
      "Iteration 24039, loss = 1.28998674\n",
      "Iteration 24040, loss = 1.28998439\n",
      "Iteration 24041, loss = 1.28998206\n",
      "Iteration 24042, loss = 1.28997975\n",
      "Iteration 24043, loss = 1.28997744\n",
      "Iteration 24044, loss = 1.28997516\n",
      "Iteration 24045, loss = 1.28997293\n",
      "Iteration 24046, loss = 1.28997079\n",
      "Iteration 24047, loss = 1.28996866\n",
      "Iteration 24048, loss = 1.28996653\n",
      "Iteration 24049, loss = 1.28996441\n",
      "Iteration 24050, loss = 1.28996228\n",
      "Iteration 24051, loss = 1.28996018\n",
      "Iteration 24052, loss = 1.28995805\n",
      "Iteration 24053, loss = 1.28995591\n",
      "Iteration 24054, loss = 1.28995376\n",
      "Iteration 24055, loss = 1.28995158\n",
      "Iteration 24056, loss = 1.28994940\n",
      "Iteration 24057, loss = 1.28994720\n",
      "Iteration 24058, loss = 1.28994500\n",
      "Iteration 24059, loss = 1.28994280\n",
      "Iteration 24060, loss = 1.28994065\n",
      "Iteration 24061, loss = 1.28993851\n",
      "Iteration 24062, loss = 1.28993641\n",
      "Iteration 24063, loss = 1.28993429\n",
      "Iteration 24064, loss = 1.28993217\n",
      "Iteration 24065, loss = 1.28993005\n",
      "Iteration 24066, loss = 1.28992792\n",
      "Iteration 24067, loss = 1.28992578\n",
      "Iteration 24068, loss = 1.28992365\n",
      "Iteration 24069, loss = 1.28992150\n",
      "Iteration 24070, loss = 1.28991936\n",
      "Iteration 24071, loss = 1.28991721\n",
      "Iteration 24072, loss = 1.28991506\n",
      "Iteration 24073, loss = 1.28991291\n",
      "Iteration 24074, loss = 1.28991077\n",
      "Iteration 24075, loss = 1.28990864\n",
      "Iteration 24076, loss = 1.28990651\n",
      "Iteration 24077, loss = 1.28990527\n",
      "Iteration 24078, loss = 1.28990284\n",
      "Iteration 24079, loss = 1.28990143\n",
      "Iteration 24080, loss = 1.28990007\n",
      "Iteration 24081, loss = 1.28989877\n",
      "Iteration 24082, loss = 1.28989744\n",
      "Iteration 24083, loss = 1.28989605\n",
      "Iteration 24084, loss = 1.28989458\n",
      "Iteration 24085, loss = 1.28989301\n",
      "Iteration 24086, loss = 1.28989135\n",
      "Iteration 24087, loss = 1.28988959\n",
      "Iteration 24088, loss = 1.28988774\n",
      "Iteration 24089, loss = 1.28988580\n",
      "Iteration 24090, loss = 1.28988377\n",
      "Iteration 24091, loss = 1.28988167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24092, loss = 1.28987951\n",
      "Iteration 24093, loss = 1.28987728\n",
      "Iteration 24094, loss = 1.28987502\n",
      "Iteration 24095, loss = 1.28987271\n",
      "Iteration 24096, loss = 1.28987038\n",
      "Iteration 24097, loss = 1.28986803\n",
      "Iteration 24098, loss = 1.28986566\n",
      "Iteration 24099, loss = 1.28986329\n",
      "Iteration 24100, loss = 1.28986093\n",
      "Iteration 24101, loss = 1.28985860\n",
      "Iteration 24102, loss = 1.28985628\n",
      "Iteration 24103, loss = 1.28985397\n",
      "Iteration 24104, loss = 1.28985167\n",
      "Iteration 24105, loss = 1.28984939\n",
      "Iteration 24106, loss = 1.28984712\n",
      "Iteration 24107, loss = 1.28984486\n",
      "Iteration 24108, loss = 1.28984262\n",
      "Iteration 24109, loss = 1.28984043\n",
      "Iteration 24110, loss = 1.28983827\n",
      "Iteration 24111, loss = 1.28983614\n",
      "Iteration 24112, loss = 1.28983404\n",
      "Iteration 24113, loss = 1.28983195\n",
      "Iteration 24114, loss = 1.28982986\n",
      "Iteration 24115, loss = 1.28982778\n",
      "Iteration 24116, loss = 1.28982569\n",
      "Iteration 24117, loss = 1.28982357\n",
      "Iteration 24118, loss = 1.28982144\n",
      "Iteration 24119, loss = 1.28981929\n",
      "Iteration 24120, loss = 1.28981713\n",
      "Iteration 24121, loss = 1.28981495\n",
      "Iteration 24122, loss = 1.28981280\n",
      "Iteration 24123, loss = 1.28981064\n",
      "Iteration 24124, loss = 1.28980851\n",
      "Iteration 24125, loss = 1.28980639\n",
      "Iteration 24126, loss = 1.28980430\n",
      "Iteration 24127, loss = 1.28980220\n",
      "Iteration 24128, loss = 1.28980010\n",
      "Iteration 24129, loss = 1.28979799\n",
      "Iteration 24130, loss = 1.28979588\n",
      "Iteration 24131, loss = 1.28979377\n",
      "Iteration 24132, loss = 1.28979165\n",
      "Iteration 24133, loss = 1.28978952\n",
      "Iteration 24134, loss = 1.28978740\n",
      "Iteration 24135, loss = 1.28978527\n",
      "Iteration 24136, loss = 1.28978315\n",
      "Iteration 24137, loss = 1.28978104\n",
      "Iteration 24138, loss = 1.28977892\n",
      "Iteration 24139, loss = 1.28977681\n",
      "Iteration 24140, loss = 1.28977470\n",
      "Iteration 24141, loss = 1.28977259\n",
      "Iteration 24142, loss = 1.28977048\n",
      "Iteration 24143, loss = 1.28976837\n",
      "Iteration 24144, loss = 1.28976627\n",
      "Iteration 24145, loss = 1.28976416\n",
      "Iteration 24146, loss = 1.28976205\n",
      "Iteration 24147, loss = 1.28975994\n",
      "Iteration 24148, loss = 1.28975783\n",
      "Iteration 24149, loss = 1.28975852\n",
      "Iteration 24150, loss = 1.28975422\n",
      "Iteration 24151, loss = 1.28975281\n",
      "Iteration 24152, loss = 1.28975153\n",
      "Iteration 24153, loss = 1.28975026\n",
      "Iteration 24154, loss = 1.28974896\n",
      "Iteration 24155, loss = 1.28974759\n",
      "Iteration 24156, loss = 1.28974615\n",
      "Iteration 24157, loss = 1.28974461\n",
      "Iteration 24158, loss = 1.28974297\n",
      "Iteration 24159, loss = 1.28974122\n",
      "Iteration 24160, loss = 1.28973938\n",
      "Iteration 24161, loss = 1.28973745\n",
      "Iteration 24162, loss = 1.28973542\n",
      "Iteration 24163, loss = 1.28973332\n",
      "Iteration 24164, loss = 1.28973115\n",
      "Iteration 24165, loss = 1.28972892\n",
      "Iteration 24166, loss = 1.28972664\n",
      "Iteration 24167, loss = 1.28972432\n",
      "Iteration 24168, loss = 1.28972197\n",
      "Iteration 24169, loss = 1.28971961\n",
      "Iteration 24170, loss = 1.28971723\n",
      "Iteration 24171, loss = 1.28971484\n",
      "Iteration 24172, loss = 1.28971246\n",
      "Iteration 24173, loss = 1.28971009\n",
      "Iteration 24174, loss = 1.28970776\n",
      "Iteration 24175, loss = 1.28970545\n",
      "Iteration 24176, loss = 1.28970315\n",
      "Iteration 24177, loss = 1.28970087\n",
      "Iteration 24178, loss = 1.28969860\n",
      "Iteration 24179, loss = 1.28969635\n",
      "Iteration 24180, loss = 1.28969412\n",
      "Iteration 24181, loss = 1.28969190\n",
      "Iteration 24182, loss = 1.28968976\n",
      "Iteration 24183, loss = 1.28968766\n",
      "Iteration 24184, loss = 1.28968561\n",
      "Iteration 24185, loss = 1.28968357\n",
      "Iteration 24186, loss = 1.28968151\n",
      "Iteration 24187, loss = 1.28967945\n",
      "Iteration 24188, loss = 1.28967736\n",
      "Iteration 24189, loss = 1.28967526\n",
      "Iteration 24190, loss = 1.28967315\n",
      "Iteration 24191, loss = 1.28967102\n",
      "Iteration 24192, loss = 1.28966887\n",
      "Iteration 24193, loss = 1.28966672\n",
      "Iteration 24194, loss = 1.28966455\n",
      "Iteration 24195, loss = 1.28966242\n",
      "Iteration 24196, loss = 1.28966031\n",
      "Iteration 24197, loss = 1.28965822\n",
      "Iteration 24198, loss = 1.28965616\n",
      "Iteration 24199, loss = 1.28965408\n",
      "Iteration 24200, loss = 1.28965200\n",
      "Iteration 24201, loss = 1.28964992\n",
      "Iteration 24202, loss = 1.28964783\n",
      "Iteration 24203, loss = 1.28964573\n",
      "Iteration 24204, loss = 1.28964364\n",
      "Iteration 24205, loss = 1.28964154\n",
      "Iteration 24206, loss = 1.28963943\n",
      "Iteration 24207, loss = 1.28963733\n",
      "Iteration 24208, loss = 1.28963753\n",
      "Iteration 24209, loss = 1.28963378\n",
      "Iteration 24210, loss = 1.28963242\n",
      "Iteration 24211, loss = 1.28963109\n",
      "Iteration 24212, loss = 1.28962981\n",
      "Iteration 24213, loss = 1.28962851\n",
      "Iteration 24214, loss = 1.28962715\n",
      "Iteration 24215, loss = 1.28962570\n",
      "Iteration 24216, loss = 1.28962417\n",
      "Iteration 24217, loss = 1.28962254\n",
      "Iteration 24218, loss = 1.28962081\n",
      "Iteration 24219, loss = 1.28961899\n",
      "Iteration 24220, loss = 1.28961708\n",
      "Iteration 24221, loss = 1.28961509\n",
      "Iteration 24222, loss = 1.28961303\n",
      "Iteration 24223, loss = 1.28961090\n",
      "Iteration 24224, loss = 1.28960871\n",
      "Iteration 24225, loss = 1.28960648\n",
      "Iteration 24226, loss = 1.28960422\n",
      "Iteration 24227, loss = 1.28960192\n",
      "Iteration 24228, loss = 1.28959961\n",
      "Iteration 24229, loss = 1.28959729\n",
      "Iteration 24230, loss = 1.28959495\n",
      "Iteration 24231, loss = 1.28959263\n",
      "Iteration 24232, loss = 1.28959033\n",
      "Iteration 24233, loss = 1.28958805\n",
      "Iteration 24234, loss = 1.28958578\n",
      "Iteration 24235, loss = 1.28958353\n",
      "Iteration 24236, loss = 1.28958128\n",
      "Iteration 24237, loss = 1.28957905\n",
      "Iteration 24238, loss = 1.28957684\n",
      "Iteration 24239, loss = 1.28957464\n",
      "Iteration 24240, loss = 1.28957245\n",
      "Iteration 24241, loss = 1.28957030\n",
      "Iteration 24242, loss = 1.28956823\n",
      "Iteration 24243, loss = 1.28956622\n",
      "Iteration 24244, loss = 1.28956421\n",
      "Iteration 24245, loss = 1.28956218\n",
      "Iteration 24246, loss = 1.28956013\n",
      "Iteration 24247, loss = 1.28955807\n",
      "Iteration 24248, loss = 1.28955598\n",
      "Iteration 24249, loss = 1.28955389\n",
      "Iteration 24250, loss = 1.28955178\n",
      "Iteration 24251, loss = 1.28954965\n",
      "Iteration 24252, loss = 1.28954752\n",
      "Iteration 24253, loss = 1.28954537\n",
      "Iteration 24254, loss = 1.28954324\n",
      "Iteration 24255, loss = 1.28954116\n",
      "Iteration 24256, loss = 1.28953911\n",
      "Iteration 24257, loss = 1.28953706\n",
      "Iteration 24258, loss = 1.28953501\n",
      "Iteration 24259, loss = 1.28953294\n",
      "Iteration 24260, loss = 1.28953088\n",
      "Iteration 24261, loss = 1.28952881\n",
      "Iteration 24262, loss = 1.28952673\n",
      "Iteration 24263, loss = 1.28952465\n",
      "Iteration 24264, loss = 1.28952257\n",
      "Iteration 24265, loss = 1.28952049\n",
      "Iteration 24266, loss = 1.28951840\n",
      "Iteration 24267, loss = 1.28951632\n",
      "Iteration 24268, loss = 1.28951423\n",
      "Iteration 24269, loss = 1.28951215\n",
      "Iteration 24270, loss = 1.28951007\n",
      "Iteration 24271, loss = 1.28950799\n",
      "Iteration 24272, loss = 1.28950593\n",
      "Iteration 24273, loss = 1.28950387\n",
      "Iteration 24274, loss = 1.28950180\n",
      "Iteration 24275, loss = 1.28949973\n",
      "Iteration 24276, loss = 1.28949765\n",
      "Iteration 24277, loss = 1.28949557\n",
      "Iteration 24278, loss = 1.28949351\n",
      "Iteration 24279, loss = 1.28949145\n",
      "Iteration 24280, loss = 1.28948939\n",
      "Iteration 24281, loss = 1.28948732\n",
      "Iteration 24282, loss = 1.28948525\n",
      "Iteration 24283, loss = 1.28948318\n",
      "Iteration 24284, loss = 1.28948111\n",
      "Iteration 24285, loss = 1.28947905\n",
      "Iteration 24286, loss = 1.28947699\n",
      "Iteration 24287, loss = 1.28947622\n",
      "Iteration 24288, loss = 1.28947343\n",
      "Iteration 24289, loss = 1.28947209\n",
      "Iteration 24290, loss = 1.28947088\n",
      "Iteration 24291, loss = 1.28946967\n",
      "Iteration 24292, loss = 1.28946843\n",
      "Iteration 24293, loss = 1.28946712\n",
      "Iteration 24294, loss = 1.28946573\n",
      "Iteration 24295, loss = 1.28946425\n",
      "Iteration 24296, loss = 1.28946267\n",
      "Iteration 24297, loss = 1.28946098\n",
      "Iteration 24298, loss = 1.28945919\n",
      "Iteration 24299, loss = 1.28945730\n",
      "Iteration 24300, loss = 1.28945532\n",
      "Iteration 24301, loss = 1.28945326\n",
      "Iteration 24302, loss = 1.28945113\n",
      "Iteration 24303, loss = 1.28944894\n",
      "Iteration 24304, loss = 1.28944670\n",
      "Iteration 24305, loss = 1.28944442\n",
      "Iteration 24306, loss = 1.28944211\n",
      "Iteration 24307, loss = 1.28943978\n",
      "Iteration 24308, loss = 1.28943743\n",
      "Iteration 24309, loss = 1.28943508\n",
      "Iteration 24310, loss = 1.28943273\n",
      "Iteration 24311, loss = 1.28943039\n",
      "Iteration 24312, loss = 1.28942808\n",
      "Iteration 24313, loss = 1.28942580\n",
      "Iteration 24314, loss = 1.28942354\n",
      "Iteration 24315, loss = 1.28942129\n",
      "Iteration 24316, loss = 1.28941906\n",
      "Iteration 24317, loss = 1.28941685\n",
      "Iteration 24318, loss = 1.28941465\n",
      "Iteration 24319, loss = 1.28941247\n",
      "Iteration 24320, loss = 1.28941032\n",
      "Iteration 24321, loss = 1.28940822\n",
      "Iteration 24322, loss = 1.28940619\n",
      "Iteration 24323, loss = 1.28940420\n",
      "Iteration 24324, loss = 1.28940221\n",
      "Iteration 24325, loss = 1.28940020\n",
      "Iteration 24326, loss = 1.28939817\n",
      "Iteration 24327, loss = 1.28939613\n",
      "Iteration 24328, loss = 1.28939407\n",
      "Iteration 24329, loss = 1.28939199\n",
      "Iteration 24330, loss = 1.28938990\n",
      "Iteration 24331, loss = 1.28938779\n",
      "Iteration 24332, loss = 1.28938568\n",
      "Iteration 24333, loss = 1.28938356\n",
      "Iteration 24334, loss = 1.28938146\n",
      "Iteration 24335, loss = 1.28937938\n",
      "Iteration 24336, loss = 1.28937735\n",
      "Iteration 24337, loss = 1.28937533\n",
      "Iteration 24338, loss = 1.28937329\n",
      "Iteration 24339, loss = 1.28937125\n",
      "Iteration 24340, loss = 1.28936921\n",
      "Iteration 24341, loss = 1.28936716\n",
      "Iteration 24342, loss = 1.28936670\n",
      "Iteration 24343, loss = 1.28936375\n",
      "Iteration 24344, loss = 1.28936247\n",
      "Iteration 24345, loss = 1.28936122\n",
      "Iteration 24346, loss = 1.28936001\n",
      "Iteration 24347, loss = 1.28935879\n",
      "Iteration 24348, loss = 1.28935751\n",
      "Iteration 24349, loss = 1.28935614\n",
      "Iteration 24350, loss = 1.28935467\n",
      "Iteration 24351, loss = 1.28935311\n",
      "Iteration 24352, loss = 1.28935144\n",
      "Iteration 24353, loss = 1.28934967\n",
      "Iteration 24354, loss = 1.28934780\n",
      "Iteration 24355, loss = 1.28934585\n",
      "Iteration 24356, loss = 1.28934382\n",
      "Iteration 24357, loss = 1.28934172\n",
      "Iteration 24358, loss = 1.28933957\n",
      "Iteration 24359, loss = 1.28933736\n",
      "Iteration 24360, loss = 1.28933512\n",
      "Iteration 24361, loss = 1.28933285\n",
      "Iteration 24362, loss = 1.28933056\n",
      "Iteration 24363, loss = 1.28932826\n",
      "Iteration 24364, loss = 1.28932595\n",
      "Iteration 24365, loss = 1.28932366\n",
      "Iteration 24366, loss = 1.28932139\n",
      "Iteration 24367, loss = 1.28931914\n",
      "Iteration 24368, loss = 1.28931689\n",
      "Iteration 24369, loss = 1.28931466\n",
      "Iteration 24370, loss = 1.28931245\n",
      "Iteration 24371, loss = 1.28931025\n",
      "Iteration 24372, loss = 1.28930806\n",
      "Iteration 24373, loss = 1.28930589\n",
      "Iteration 24374, loss = 1.28930373\n",
      "Iteration 24375, loss = 1.28930159\n",
      "Iteration 24376, loss = 1.28929945\n",
      "Iteration 24377, loss = 1.28929734\n",
      "Iteration 24378, loss = 1.28929528\n",
      "Iteration 24379, loss = 1.28929328\n",
      "Iteration 24380, loss = 1.28929130\n",
      "Iteration 24381, loss = 1.28928932\n",
      "Iteration 24382, loss = 1.28928733\n",
      "Iteration 24383, loss = 1.28928531\n",
      "Iteration 24384, loss = 1.28928328\n",
      "Iteration 24385, loss = 1.28928122\n",
      "Iteration 24386, loss = 1.28927916\n",
      "Iteration 24387, loss = 1.28927707\n",
      "Iteration 24388, loss = 1.28927498\n",
      "Iteration 24389, loss = 1.28927288\n",
      "Iteration 24390, loss = 1.28927082\n",
      "Iteration 24391, loss = 1.28926878\n",
      "Iteration 24392, loss = 1.28926677\n",
      "Iteration 24393, loss = 1.28926475\n",
      "Iteration 24394, loss = 1.28926273\n",
      "Iteration 24395, loss = 1.28926071\n",
      "Iteration 24396, loss = 1.28925868\n",
      "Iteration 24397, loss = 1.28925665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24398, loss = 1.28925462\n",
      "Iteration 24399, loss = 1.28925258\n",
      "Iteration 24400, loss = 1.28925054\n",
      "Iteration 24401, loss = 1.28924850\n",
      "Iteration 24402, loss = 1.28924645\n",
      "Iteration 24403, loss = 1.28924441\n",
      "Iteration 24404, loss = 1.28924237\n",
      "Iteration 24405, loss = 1.28924033\n",
      "Iteration 24406, loss = 1.28923829\n",
      "Iteration 24407, loss = 1.28923626\n",
      "Iteration 24408, loss = 1.28923424\n",
      "Iteration 24409, loss = 1.28923221\n",
      "Iteration 24410, loss = 1.28923018\n",
      "Iteration 24411, loss = 1.28922815\n",
      "Iteration 24412, loss = 1.28922612\n",
      "Iteration 24413, loss = 1.28922409\n",
      "Iteration 24414, loss = 1.28922206\n",
      "Iteration 24415, loss = 1.28922004\n",
      "Iteration 24416, loss = 1.28921802\n",
      "Iteration 24417, loss = 1.28921599\n",
      "Iteration 24418, loss = 1.28921396\n",
      "Iteration 24419, loss = 1.28921193\n",
      "Iteration 24420, loss = 1.28920991\n",
      "Iteration 24421, loss = 1.28920788\n",
      "Iteration 24422, loss = 1.28920586\n",
      "Iteration 24423, loss = 1.28920626\n",
      "Iteration 24424, loss = 1.28920244\n",
      "Iteration 24425, loss = 1.28920119\n",
      "Iteration 24426, loss = 1.28920005\n",
      "Iteration 24427, loss = 1.28919889\n",
      "Iteration 24428, loss = 1.28919770\n",
      "Iteration 24429, loss = 1.28919644\n",
      "Iteration 24430, loss = 1.28919510\n",
      "Iteration 24431, loss = 1.28919366\n",
      "Iteration 24432, loss = 1.28919213\n",
      "Iteration 24433, loss = 1.28919049\n",
      "Iteration 24434, loss = 1.28918875\n",
      "Iteration 24435, loss = 1.28918692\n",
      "Iteration 24436, loss = 1.28918500\n",
      "Iteration 24437, loss = 1.28918300\n",
      "Iteration 24438, loss = 1.28918093\n",
      "Iteration 24439, loss = 1.28917881\n",
      "Iteration 24440, loss = 1.28917663\n",
      "Iteration 24441, loss = 1.28917442\n",
      "Iteration 24442, loss = 1.28917217\n",
      "Iteration 24443, loss = 1.28916991\n",
      "Iteration 24444, loss = 1.28916763\n",
      "Iteration 24445, loss = 1.28916535\n",
      "Iteration 24446, loss = 1.28916306\n",
      "Iteration 24447, loss = 1.28916078\n",
      "Iteration 24448, loss = 1.28915851\n",
      "Iteration 24449, loss = 1.28915627\n",
      "Iteration 24450, loss = 1.28915407\n",
      "Iteration 24451, loss = 1.28915188\n",
      "Iteration 24452, loss = 1.28914970\n",
      "Iteration 24453, loss = 1.28914754\n",
      "Iteration 24454, loss = 1.28914540\n",
      "Iteration 24455, loss = 1.28914327\n",
      "Iteration 24456, loss = 1.28914115\n",
      "Iteration 24457, loss = 1.28913907\n",
      "Iteration 24458, loss = 1.28913703\n",
      "Iteration 24459, loss = 1.28913505\n",
      "Iteration 24460, loss = 1.28913309\n",
      "Iteration 24461, loss = 1.28913112\n",
      "Iteration 24462, loss = 1.28912915\n",
      "Iteration 24463, loss = 1.28912717\n",
      "Iteration 24464, loss = 1.28912517\n",
      "Iteration 24465, loss = 1.28912315\n",
      "Iteration 24466, loss = 1.28912111\n",
      "Iteration 24467, loss = 1.28911906\n",
      "Iteration 24468, loss = 1.28911700\n",
      "Iteration 24469, loss = 1.28911629\n",
      "Iteration 24470, loss = 1.28911363\n",
      "Iteration 24471, loss = 1.28911295\n",
      "Iteration 24472, loss = 1.28911260\n",
      "Iteration 24473, loss = 1.28911239\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03286518\n",
      "Iteration 2, loss = 2.03276069\n",
      "Iteration 3, loss = 2.03261169\n",
      "Iteration 4, loss = 2.03242267\n",
      "Iteration 5, loss = 2.03219765\n",
      "Iteration 6, loss = 2.03194030\n",
      "Iteration 7, loss = 2.03165389\n",
      "Iteration 8, loss = 2.03134138\n",
      "Iteration 9, loss = 2.03100544\n",
      "Iteration 10, loss = 2.03064846\n",
      "Iteration 11, loss = 2.03027260\n",
      "Iteration 12, loss = 2.02987983\n",
      "Iteration 13, loss = 2.02947188\n",
      "Iteration 14, loss = 2.02905033\n",
      "Iteration 15, loss = 2.02861662\n",
      "Iteration 16, loss = 2.02817202\n",
      "Iteration 17, loss = 2.02771763\n",
      "Iteration 18, loss = 2.02725447\n",
      "Iteration 19, loss = 2.02678353\n",
      "Iteration 20, loss = 2.02630566\n",
      "Iteration 21, loss = 2.02582160\n",
      "Iteration 22, loss = 2.02533208\n",
      "Iteration 23, loss = 2.02483768\n",
      "Iteration 24, loss = 2.02433895\n",
      "Iteration 25, loss = 2.02383638\n",
      "Iteration 26, loss = 2.02333042\n",
      "Iteration 27, loss = 2.02282147\n",
      "Iteration 28, loss = 2.02230990\n",
      "Iteration 29, loss = 2.02179603\n",
      "Iteration 30, loss = 2.02128016\n",
      "Iteration 31, loss = 2.02076254\n",
      "Iteration 32, loss = 2.02024341\n",
      "Iteration 33, loss = 2.01972299\n",
      "Iteration 34, loss = 2.01920146\n",
      "Iteration 35, loss = 2.01867907\n",
      "Iteration 36, loss = 2.01815595\n",
      "Iteration 37, loss = 2.01763221\n",
      "Iteration 38, loss = 2.01710796\n",
      "Iteration 39, loss = 2.01658332\n",
      "Iteration 40, loss = 2.01605839\n",
      "Iteration 41, loss = 2.01553328\n",
      "Iteration 42, loss = 2.01500806\n",
      "Iteration 43, loss = 2.01448279\n",
      "Iteration 44, loss = 2.01395754\n",
      "Iteration 45, loss = 2.01343238\n",
      "Iteration 46, loss = 2.01290734\n",
      "Iteration 47, loss = 2.01238260\n",
      "Iteration 48, loss = 2.01185810\n",
      "Iteration 49, loss = 2.01133387\n",
      "Iteration 50, loss = 2.01080994\n",
      "Iteration 51, loss = 2.01028635\n",
      "Iteration 52, loss = 2.00976312\n",
      "Iteration 53, loss = 2.00924026\n",
      "Iteration 54, loss = 2.00871782\n",
      "Iteration 55, loss = 2.00819579\n",
      "Iteration 56, loss = 2.00767421\n",
      "Iteration 57, loss = 2.00715310\n",
      "Iteration 58, loss = 2.00663252\n",
      "Iteration 59, loss = 2.00611248\n",
      "Iteration 60, loss = 2.00559293\n",
      "Iteration 61, loss = 2.00507388\n",
      "Iteration 62, loss = 2.00455535\n",
      "Iteration 63, loss = 2.00403735\n",
      "Iteration 64, loss = 2.00351987\n",
      "Iteration 65, loss = 2.00300292\n",
      "Iteration 66, loss = 2.00248651\n",
      "Iteration 67, loss = 2.00197065\n",
      "Iteration 68, loss = 2.00145534\n",
      "Iteration 69, loss = 2.00094057\n",
      "Iteration 70, loss = 2.00042635\n",
      "Iteration 71, loss = 1.99991269\n",
      "Iteration 72, loss = 1.99939958\n",
      "Iteration 73, loss = 1.99888703\n",
      "Iteration 74, loss = 1.99837509\n",
      "Iteration 75, loss = 1.99786376\n",
      "Iteration 76, loss = 1.99735299\n",
      "Iteration 77, loss = 1.99684278\n",
      "Iteration 78, loss = 1.99633315\n",
      "Iteration 79, loss = 1.99582407\n",
      "Iteration 80, loss = 1.99531557\n",
      "Iteration 81, loss = 1.99480763\n",
      "Iteration 82, loss = 1.99430024\n",
      "Iteration 83, loss = 1.99379340\n",
      "Iteration 84, loss = 1.99328711\n",
      "Iteration 85, loss = 1.99278139\n",
      "Iteration 86, loss = 1.99227623\n",
      "Iteration 87, loss = 1.99177162\n",
      "Iteration 88, loss = 1.99126758\n",
      "Iteration 89, loss = 1.99076409\n",
      "Iteration 90, loss = 1.99026118\n",
      "Iteration 91, loss = 1.98975891\n",
      "Iteration 92, loss = 1.98925721\n",
      "Iteration 93, loss = 1.98875606\n",
      "Iteration 94, loss = 1.98825547\n",
      "Iteration 95, loss = 1.98775544\n",
      "Iteration 96, loss = 1.98725597\n",
      "Iteration 97, loss = 1.98675705\n",
      "Iteration 98, loss = 1.98625868\n",
      "Iteration 99, loss = 1.98576086\n",
      "Iteration 100, loss = 1.98526360\n",
      "Iteration 101, loss = 1.98476689\n",
      "Iteration 102, loss = 1.98427073\n",
      "Iteration 103, loss = 1.98377512\n",
      "Iteration 104, loss = 1.98328005\n",
      "Iteration 105, loss = 1.98278552\n",
      "Iteration 106, loss = 1.98229154\n",
      "Iteration 107, loss = 1.98179810\n",
      "Iteration 108, loss = 1.98130520\n",
      "Iteration 109, loss = 1.98081284\n",
      "Iteration 110, loss = 1.98032101\n",
      "Iteration 111, loss = 1.97982972\n",
      "Iteration 112, loss = 1.97933897\n",
      "Iteration 113, loss = 1.97884875\n",
      "Iteration 114, loss = 1.97835917\n",
      "Iteration 115, loss = 1.97787015\n",
      "Iteration 116, loss = 1.97738166\n",
      "Iteration 117, loss = 1.97689371\n",
      "Iteration 118, loss = 1.97640629\n",
      "Iteration 119, loss = 1.97591940\n",
      "Iteration 120, loss = 1.97543305\n",
      "Iteration 121, loss = 1.97494722\n",
      "Iteration 122, loss = 1.97446193\n",
      "Iteration 123, loss = 1.97397715\n",
      "Iteration 124, loss = 1.97349291\n",
      "Iteration 125, loss = 1.97300918\n",
      "Iteration 126, loss = 1.97252598\n",
      "Iteration 127, loss = 1.97204329\n",
      "Iteration 128, loss = 1.97156113\n",
      "Iteration 129, loss = 1.97107948\n",
      "Iteration 130, loss = 1.97059835\n",
      "Iteration 131, loss = 1.97011773\n",
      "Iteration 132, loss = 1.96963763\n",
      "Iteration 133, loss = 1.96915804\n",
      "Iteration 134, loss = 1.96867895\n",
      "Iteration 135, loss = 1.96820038\n",
      "Iteration 136, loss = 1.96772232\n",
      "Iteration 137, loss = 1.96724477\n",
      "Iteration 138, loss = 1.96676772\n",
      "Iteration 139, loss = 1.96629118\n",
      "Iteration 140, loss = 1.96581512\n",
      "Iteration 141, loss = 1.96533954\n",
      "Iteration 142, loss = 1.96486384\n",
      "Iteration 143, loss = 1.96438778\n",
      "Iteration 144, loss = 1.96391212\n",
      "Iteration 145, loss = 1.96343685\n",
      "Iteration 146, loss = 1.96296200\n",
      "Iteration 147, loss = 1.96248757\n",
      "Iteration 148, loss = 1.96201356\n",
      "Iteration 149, loss = 1.96153997\n",
      "Iteration 150, loss = 1.96106682\n",
      "Iteration 151, loss = 1.96060129\n",
      "Iteration 152, loss = 1.96013786\n",
      "Iteration 153, loss = 1.95967538\n",
      "Iteration 154, loss = 1.95921244\n",
      "Iteration 155, loss = 1.95875806\n",
      "Iteration 156, loss = 1.95830686\n",
      "Iteration 157, loss = 1.95785830\n",
      "Iteration 158, loss = 1.95742082\n",
      "Iteration 159, loss = 1.95699255\n",
      "Iteration 160, loss = 1.95656564\n",
      "Iteration 161, loss = 1.95613551\n",
      "Iteration 162, loss = 1.95571093\n",
      "Iteration 163, loss = 1.95530103\n",
      "Iteration 164, loss = 1.95489238\n",
      "Iteration 165, loss = 1.95448575\n",
      "Iteration 166, loss = 1.95408459\n",
      "Iteration 167, loss = 1.95369721\n",
      "Iteration 168, loss = 1.95331209\n",
      "Iteration 169, loss = 1.95292970\n",
      "Iteration 170, loss = 1.95255528\n",
      "Iteration 171, loss = 1.95218308\n",
      "Iteration 172, loss = 1.95181769\n",
      "Iteration 173, loss = 1.95146230\n",
      "Iteration 174, loss = 1.95112110\n",
      "Iteration 175, loss = 1.95078301\n",
      "Iteration 176, loss = 1.95044917\n",
      "Iteration 177, loss = 1.95012609\n",
      "Iteration 178, loss = 1.94981141\n",
      "Iteration 179, loss = 1.94950954\n",
      "Iteration 180, loss = 1.94921676\n",
      "Iteration 181, loss = 1.94893438\n",
      "Iteration 182, loss = 1.94866050\n",
      "Iteration 183, loss = 1.94839725\n",
      "Iteration 184, loss = 1.94815863\n",
      "Iteration 185, loss = 1.94792590\n",
      "Iteration 186, loss = 1.94769960\n",
      "Iteration 187, loss = 1.94747601\n",
      "Iteration 188, loss = 1.94725450\n",
      "Iteration 189, loss = 1.94703472\n",
      "Iteration 190, loss = 1.94681692\n",
      "Iteration 191, loss = 1.94660145\n",
      "Iteration 192, loss = 1.94639232\n",
      "Iteration 193, loss = 1.94618495\n",
      "Iteration 194, loss = 1.94597895\n",
      "Iteration 195, loss = 1.94577362\n",
      "Iteration 196, loss = 1.94556851\n",
      "Iteration 197, loss = 1.94536418\n",
      "Iteration 198, loss = 1.94516078\n",
      "Iteration 199, loss = 1.94495821\n",
      "Iteration 200, loss = 1.94475602\n",
      "Iteration 201, loss = 1.94455428\n",
      "Iteration 202, loss = 1.94435315\n",
      "Iteration 203, loss = 1.94415257\n",
      "Iteration 204, loss = 1.94395250\n",
      "Iteration 205, loss = 1.94375290\n",
      "Iteration 206, loss = 1.94355372\n",
      "Iteration 207, loss = 1.94335693\n",
      "Iteration 208, loss = 1.94316295\n",
      "Iteration 209, loss = 1.94296956\n",
      "Iteration 210, loss = 1.94277669\n",
      "Iteration 211, loss = 1.94258273\n",
      "Iteration 212, loss = 1.94238847\n",
      "Iteration 213, loss = 1.94219439\n",
      "Iteration 214, loss = 1.94200055\n",
      "Iteration 215, loss = 1.94180693\n",
      "Iteration 216, loss = 1.94161353\n",
      "Iteration 217, loss = 1.94142032\n",
      "Iteration 218, loss = 1.94122730\n",
      "Iteration 219, loss = 1.94103445\n",
      "Iteration 220, loss = 1.94084177\n",
      "Iteration 221, loss = 1.94064925\n",
      "Iteration 222, loss = 1.94045688\n",
      "Iteration 223, loss = 1.94026465\n",
      "Iteration 224, loss = 1.94007371\n",
      "Iteration 225, loss = 1.93988535\n",
      "Iteration 226, loss = 1.93969734\n",
      "Iteration 227, loss = 1.93950992\n",
      "Iteration 228, loss = 1.93932847\n",
      "Iteration 229, loss = 1.93914790\n",
      "Iteration 230, loss = 1.93896790\n",
      "Iteration 231, loss = 1.93878841\n",
      "Iteration 232, loss = 1.93860938\n",
      "Iteration 233, loss = 1.93843080\n",
      "Iteration 234, loss = 1.93825298\n",
      "Iteration 235, loss = 1.93807553\n",
      "Iteration 236, loss = 1.93789844\n",
      "Iteration 237, loss = 1.93772166\n",
      "Iteration 238, loss = 1.93754536\n",
      "Iteration 239, loss = 1.93736950\n",
      "Iteration 240, loss = 1.93719518\n",
      "Iteration 241, loss = 1.93702230\n",
      "Iteration 242, loss = 1.93684978\n",
      "Iteration 243, loss = 1.93667945\n",
      "Iteration 244, loss = 1.93650993\n",
      "Iteration 245, loss = 1.93634080\n",
      "Iteration 246, loss = 1.93617203\n",
      "Iteration 247, loss = 1.93600360\n",
      "Iteration 248, loss = 1.93583547\n",
      "Iteration 249, loss = 1.93566761\n",
      "Iteration 250, loss = 1.93550002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 251, loss = 1.93533266\n",
      "Iteration 252, loss = 1.93516553\n",
      "Iteration 253, loss = 1.93499860\n",
      "Iteration 254, loss = 1.93483186\n",
      "Iteration 255, loss = 1.93466530\n",
      "Iteration 256, loss = 1.93449890\n",
      "Iteration 257, loss = 1.93433266\n",
      "Iteration 258, loss = 1.93416657\n",
      "Iteration 259, loss = 1.93400122\n",
      "Iteration 260, loss = 1.93383741\n",
      "Iteration 261, loss = 1.93367383\n",
      "Iteration 262, loss = 1.93351047\n",
      "Iteration 263, loss = 1.93334731\n",
      "Iteration 264, loss = 1.93318435\n",
      "Iteration 265, loss = 1.93302155\n",
      "Iteration 266, loss = 1.93285892\n",
      "Iteration 267, loss = 1.93269645\n",
      "Iteration 268, loss = 1.93253411\n",
      "Iteration 269, loss = 1.93237192\n",
      "Iteration 270, loss = 1.93220985\n",
      "Iteration 271, loss = 1.93204790\n",
      "Iteration 272, loss = 1.93188607\n",
      "Iteration 273, loss = 1.93172434\n",
      "Iteration 274, loss = 1.93156272\n",
      "Iteration 275, loss = 1.93140120\n",
      "Iteration 276, loss = 1.93123978\n",
      "Iteration 277, loss = 1.93107827\n",
      "Iteration 278, loss = 1.93091670\n",
      "Iteration 279, loss = 1.93075519\n",
      "Iteration 280, loss = 1.93059375\n",
      "Iteration 281, loss = 1.93043238\n",
      "Iteration 282, loss = 1.93027107\n",
      "Iteration 283, loss = 1.93010982\n",
      "Iteration 284, loss = 1.92994863\n",
      "Iteration 285, loss = 1.92978751\n",
      "Iteration 286, loss = 1.92962690\n",
      "Iteration 287, loss = 1.92946793\n",
      "Iteration 288, loss = 1.92930891\n",
      "Iteration 289, loss = 1.92914998\n",
      "Iteration 290, loss = 1.92899118\n",
      "Iteration 291, loss = 1.92883252\n",
      "Iteration 292, loss = 1.92867398\n",
      "Iteration 293, loss = 1.92851555\n",
      "Iteration 294, loss = 1.92835724\n",
      "Iteration 295, loss = 1.92819903\n",
      "Iteration 296, loss = 1.92804126\n",
      "Iteration 297, loss = 1.92788485\n",
      "Iteration 298, loss = 1.92772863\n",
      "Iteration 299, loss = 1.92757258\n",
      "Iteration 300, loss = 1.92741786\n",
      "Iteration 301, loss = 1.92726382\n",
      "Iteration 302, loss = 1.92710999\n",
      "Iteration 303, loss = 1.92695635\n",
      "Iteration 304, loss = 1.92680290\n",
      "Iteration 305, loss = 1.92664962\n",
      "Iteration 306, loss = 1.92649650\n",
      "Iteration 307, loss = 1.92634352\n",
      "Iteration 308, loss = 1.92619068\n",
      "Iteration 309, loss = 1.92603797\n",
      "Iteration 310, loss = 1.92588538\n",
      "Iteration 311, loss = 1.92573353\n",
      "Iteration 312, loss = 1.92558234\n",
      "Iteration 313, loss = 1.92543130\n",
      "Iteration 314, loss = 1.92528041\n",
      "Iteration 315, loss = 1.92512966\n",
      "Iteration 316, loss = 1.92497903\n",
      "Iteration 317, loss = 1.92482853\n",
      "Iteration 318, loss = 1.92467813\n",
      "Iteration 319, loss = 1.92452785\n",
      "Iteration 320, loss = 1.92437766\n",
      "Iteration 321, loss = 1.92422757\n",
      "Iteration 322, loss = 1.92407758\n",
      "Iteration 323, loss = 1.92392767\n",
      "Iteration 324, loss = 1.92377785\n",
      "Iteration 325, loss = 1.92362811\n",
      "Iteration 326, loss = 1.92347844\n",
      "Iteration 327, loss = 1.92332886\n",
      "Iteration 328, loss = 1.92317934\n",
      "Iteration 329, loss = 1.92302990\n",
      "Iteration 330, loss = 1.92288053\n",
      "Iteration 331, loss = 1.92273122\n",
      "Iteration 332, loss = 1.92258198\n",
      "Iteration 333, loss = 1.92243281\n",
      "Iteration 334, loss = 1.92228369\n",
      "Iteration 335, loss = 1.92213464\n",
      "Iteration 336, loss = 1.92198566\n",
      "Iteration 337, loss = 1.92183673\n",
      "Iteration 338, loss = 1.92168854\n",
      "Iteration 339, loss = 1.92154061\n",
      "Iteration 340, loss = 1.92139278\n",
      "Iteration 341, loss = 1.92124506\n",
      "Iteration 342, loss = 1.92109742\n",
      "Iteration 343, loss = 1.92094988\n",
      "Iteration 344, loss = 1.92080242\n",
      "Iteration 345, loss = 1.92065504\n",
      "Iteration 346, loss = 1.92050775\n",
      "Iteration 347, loss = 1.92036052\n",
      "Iteration 348, loss = 1.92021338\n",
      "Iteration 349, loss = 1.92006630\n",
      "Iteration 350, loss = 1.91991929\n",
      "Iteration 351, loss = 1.91977235\n",
      "Iteration 352, loss = 1.91962548\n",
      "Iteration 353, loss = 1.91947867\n",
      "Iteration 354, loss = 1.91933192\n",
      "Iteration 355, loss = 1.91918595\n",
      "Iteration 356, loss = 1.91904015\n",
      "Iteration 357, loss = 1.91889445\n",
      "Iteration 358, loss = 1.91874884\n",
      "Iteration 359, loss = 1.91860331\n",
      "Iteration 360, loss = 1.91845786\n",
      "Iteration 361, loss = 1.91831250\n",
      "Iteration 362, loss = 1.91816721\n",
      "Iteration 363, loss = 1.91802199\n",
      "Iteration 364, loss = 1.91787685\n",
      "Iteration 365, loss = 1.91773177\n",
      "Iteration 366, loss = 1.91758676\n",
      "Iteration 367, loss = 1.91744182\n",
      "Iteration 368, loss = 1.91729694\n",
      "Iteration 369, loss = 1.91715212\n",
      "Iteration 370, loss = 1.91700737\n",
      "Iteration 371, loss = 1.91686267\n",
      "Iteration 372, loss = 1.91671804\n",
      "Iteration 373, loss = 1.91657346\n",
      "Iteration 374, loss = 1.91642894\n",
      "Iteration 375, loss = 1.91628447\n",
      "Iteration 376, loss = 1.91614007\n",
      "Iteration 377, loss = 1.91599571\n",
      "Iteration 378, loss = 1.91585141\n",
      "Iteration 379, loss = 1.91570717\n",
      "Iteration 380, loss = 1.91556298\n",
      "Iteration 381, loss = 1.91541885\n",
      "Iteration 382, loss = 1.91527476\n",
      "Iteration 383, loss = 1.91513073\n",
      "Iteration 384, loss = 1.91498675\n",
      "Iteration 385, loss = 1.91484283\n",
      "Iteration 386, loss = 1.91469895\n",
      "Iteration 387, loss = 1.91455513\n",
      "Iteration 388, loss = 1.91441136\n",
      "Iteration 389, loss = 1.91426764\n",
      "Iteration 390, loss = 1.91412397\n",
      "Iteration 391, loss = 1.91398036\n",
      "Iteration 392, loss = 1.91383679\n",
      "Iteration 393, loss = 1.91369327\n",
      "Iteration 394, loss = 1.91354981\n",
      "Iteration 395, loss = 1.91340639\n",
      "Iteration 396, loss = 1.91326303\n",
      "Iteration 397, loss = 1.91311972\n",
      "Iteration 398, loss = 1.91297645\n",
      "Iteration 399, loss = 1.91283324\n",
      "Iteration 400, loss = 1.91269008\n",
      "Iteration 401, loss = 1.91254696\n",
      "Iteration 402, loss = 1.91240390\n",
      "Iteration 403, loss = 1.91226088\n",
      "Iteration 404, loss = 1.91211792\n",
      "Iteration 405, loss = 1.91197501\n",
      "Iteration 406, loss = 1.91183214\n",
      "Iteration 407, loss = 1.91168933\n",
      "Iteration 408, loss = 1.91154656\n",
      "Iteration 409, loss = 1.91140385\n",
      "Iteration 410, loss = 1.91126118\n",
      "Iteration 411, loss = 1.91111857\n",
      "Iteration 412, loss = 1.91097600\n",
      "Iteration 413, loss = 1.91083348\n",
      "Iteration 414, loss = 1.91069102\n",
      "Iteration 415, loss = 1.91054860\n",
      "Iteration 416, loss = 1.91040623\n",
      "Iteration 417, loss = 1.91026391\n",
      "Iteration 418, loss = 1.91012164\n",
      "Iteration 419, loss = 1.90997942\n",
      "Iteration 420, loss = 1.90983725\n",
      "Iteration 421, loss = 1.90969513\n",
      "Iteration 422, loss = 1.90955306\n",
      "Iteration 423, loss = 1.90941104\n",
      "Iteration 424, loss = 1.90926906\n",
      "Iteration 425, loss = 1.90912714\n",
      "Iteration 426, loss = 1.90898526\n",
      "Iteration 427, loss = 1.90884344\n",
      "Iteration 428, loss = 1.90870166\n",
      "Iteration 429, loss = 1.90855993\n",
      "Iteration 430, loss = 1.90841825\n",
      "Iteration 431, loss = 1.90827662\n",
      "Iteration 432, loss = 1.90813504\n",
      "Iteration 433, loss = 1.90799351\n",
      "Iteration 434, loss = 1.90785203\n",
      "Iteration 435, loss = 1.90771060\n",
      "Iteration 436, loss = 1.90756921\n",
      "Iteration 437, loss = 1.90742788\n",
      "Iteration 438, loss = 1.90728659\n",
      "Iteration 439, loss = 1.90714535\n",
      "Iteration 440, loss = 1.90700417\n",
      "Iteration 441, loss = 1.90686303\n",
      "Iteration 442, loss = 1.90672194\n",
      "Iteration 443, loss = 1.90658089\n",
      "Iteration 444, loss = 1.90643990\n",
      "Iteration 445, loss = 1.90629896\n",
      "Iteration 446, loss = 1.90615806\n",
      "Iteration 447, loss = 1.90601722\n",
      "Iteration 448, loss = 1.90587642\n",
      "Iteration 449, loss = 1.90573567\n",
      "Iteration 450, loss = 1.90559497\n",
      "Iteration 451, loss = 1.90545432\n",
      "Iteration 452, loss = 1.90531372\n",
      "Iteration 453, loss = 1.90517317\n",
      "Iteration 454, loss = 1.90503266\n",
      "Iteration 455, loss = 1.90489221\n",
      "Iteration 456, loss = 1.90475180\n",
      "Iteration 457, loss = 1.90461144\n",
      "Iteration 458, loss = 1.90447113\n",
      "Iteration 459, loss = 1.90433087\n",
      "Iteration 460, loss = 1.90419066\n",
      "Iteration 461, loss = 1.90405049\n",
      "Iteration 462, loss = 1.90391038\n",
      "Iteration 463, loss = 1.90377031\n",
      "Iteration 464, loss = 1.90363029\n",
      "Iteration 465, loss = 1.90349033\n",
      "Iteration 466, loss = 1.90335040\n",
      "Iteration 467, loss = 1.90321053\n",
      "Iteration 468, loss = 1.90307071\n",
      "Iteration 469, loss = 1.90293093\n",
      "Iteration 470, loss = 1.90279121\n",
      "Iteration 471, loss = 1.90265153\n",
      "Iteration 472, loss = 1.90251190\n",
      "Iteration 473, loss = 1.90237232\n",
      "Iteration 474, loss = 1.90223278\n",
      "Iteration 475, loss = 1.90209330\n",
      "Iteration 476, loss = 1.90195386\n",
      "Iteration 477, loss = 1.90181447\n",
      "Iteration 478, loss = 1.90167513\n",
      "Iteration 479, loss = 1.90153584\n",
      "Iteration 480, loss = 1.90139660\n",
      "Iteration 481, loss = 1.90125740\n",
      "Iteration 482, loss = 1.90111826\n",
      "Iteration 483, loss = 1.90097916\n",
      "Iteration 484, loss = 1.90084011\n",
      "Iteration 485, loss = 1.90070111\n",
      "Iteration 486, loss = 1.90056215\n",
      "Iteration 487, loss = 1.90042325\n",
      "Iteration 488, loss = 1.90028439\n",
      "Iteration 489, loss = 1.90014558\n",
      "Iteration 490, loss = 1.90000682\n",
      "Iteration 491, loss = 1.89986811\n",
      "Iteration 492, loss = 1.89972944\n",
      "Iteration 493, loss = 1.89959082\n",
      "Iteration 494, loss = 1.89945226\n",
      "Iteration 495, loss = 1.89931373\n",
      "Iteration 496, loss = 1.89917526\n",
      "Iteration 497, loss = 1.89903684\n",
      "Iteration 498, loss = 1.89889846\n",
      "Iteration 499, loss = 1.89876013\n",
      "Iteration 500, loss = 1.89862185\n",
      "Iteration 501, loss = 1.89848362\n",
      "Iteration 502, loss = 1.89834543\n",
      "Iteration 503, loss = 1.89820730\n",
      "Iteration 504, loss = 1.89806921\n",
      "Iteration 505, loss = 1.89793117\n",
      "Iteration 506, loss = 1.89779318\n",
      "Iteration 507, loss = 1.89765523\n",
      "Iteration 508, loss = 1.89751733\n",
      "Iteration 509, loss = 1.89737948\n",
      "Iteration 510, loss = 1.89724168\n",
      "Iteration 511, loss = 1.89710393\n",
      "Iteration 512, loss = 1.89696622\n",
      "Iteration 513, loss = 1.89682856\n",
      "Iteration 514, loss = 1.89669095\n",
      "Iteration 515, loss = 1.89655339\n",
      "Iteration 516, loss = 1.89641588\n",
      "Iteration 517, loss = 1.89627841\n",
      "Iteration 518, loss = 1.89614099\n",
      "Iteration 519, loss = 1.89600362\n",
      "Iteration 520, loss = 1.89586629\n",
      "Iteration 521, loss = 1.89572902\n",
      "Iteration 522, loss = 1.89559179\n",
      "Iteration 523, loss = 1.89545461\n",
      "Iteration 524, loss = 1.89531747\n",
      "Iteration 525, loss = 1.89518039\n",
      "Iteration 526, loss = 1.89504335\n",
      "Iteration 527, loss = 1.89490636\n",
      "Iteration 528, loss = 1.89476941\n",
      "Iteration 529, loss = 1.89463252\n",
      "Iteration 530, loss = 1.89449567\n",
      "Iteration 531, loss = 1.89435887\n",
      "Iteration 532, loss = 1.89422211\n",
      "Iteration 533, loss = 1.89408541\n",
      "Iteration 534, loss = 1.89394875\n",
      "Iteration 535, loss = 1.89381214\n",
      "Iteration 536, loss = 1.89367571\n",
      "Iteration 537, loss = 1.89353962\n",
      "Iteration 538, loss = 1.89340359\n",
      "Iteration 539, loss = 1.89326763\n",
      "Iteration 540, loss = 1.89313173\n",
      "Iteration 541, loss = 1.89299589\n",
      "Iteration 542, loss = 1.89286011\n",
      "Iteration 543, loss = 1.89272438\n",
      "Iteration 544, loss = 1.89258871\n",
      "Iteration 545, loss = 1.89245310\n",
      "Iteration 546, loss = 1.89231754\n",
      "Iteration 547, loss = 1.89218204\n",
      "Iteration 548, loss = 1.89204658\n",
      "Iteration 549, loss = 1.89191118\n",
      "Iteration 550, loss = 1.89177584\n",
      "Iteration 551, loss = 1.89164054\n",
      "Iteration 552, loss = 1.89150529\n",
      "Iteration 553, loss = 1.89137010\n",
      "Iteration 554, loss = 1.89123495\n",
      "Iteration 555, loss = 1.89109986\n",
      "Iteration 556, loss = 1.89096481\n",
      "Iteration 557, loss = 1.89082981\n",
      "Iteration 558, loss = 1.89069487\n",
      "Iteration 559, loss = 1.89055997\n",
      "Iteration 560, loss = 1.89042511\n",
      "Iteration 561, loss = 1.89029031\n",
      "Iteration 562, loss = 1.89015556\n",
      "Iteration 563, loss = 1.89002085\n",
      "Iteration 564, loss = 1.88988619\n",
      "Iteration 565, loss = 1.88975158\n",
      "Iteration 566, loss = 1.88961701\n",
      "Iteration 567, loss = 1.88948250\n",
      "Iteration 568, loss = 1.88934803\n",
      "Iteration 569, loss = 1.88921361\n",
      "Iteration 570, loss = 1.88907923\n",
      "Iteration 571, loss = 1.88894490\n",
      "Iteration 572, loss = 1.88881062\n",
      "Iteration 573, loss = 1.88867639\n",
      "Iteration 574, loss = 1.88854220\n",
      "Iteration 575, loss = 1.88840806\n",
      "Iteration 576, loss = 1.88827397\n",
      "Iteration 577, loss = 1.88813992\n",
      "Iteration 578, loss = 1.88800592\n",
      "Iteration 579, loss = 1.88787197\n",
      "Iteration 580, loss = 1.88773806\n",
      "Iteration 581, loss = 1.88760420\n",
      "Iteration 582, loss = 1.88747039\n",
      "Iteration 583, loss = 1.88733662\n",
      "Iteration 584, loss = 1.88720290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 585, loss = 1.88706922\n",
      "Iteration 586, loss = 1.88693559\n",
      "Iteration 587, loss = 1.88680201\n",
      "Iteration 588, loss = 1.88666848\n",
      "Iteration 589, loss = 1.88653499\n",
      "Iteration 590, loss = 1.88640154\n",
      "Iteration 591, loss = 1.88626815\n",
      "Iteration 592, loss = 1.88613480\n",
      "Iteration 593, loss = 1.88600149\n",
      "Iteration 594, loss = 1.88586824\n",
      "Iteration 595, loss = 1.88573502\n",
      "Iteration 596, loss = 1.88560186\n",
      "Iteration 597, loss = 1.88546874\n",
      "Iteration 598, loss = 1.88533567\n",
      "Iteration 599, loss = 1.88520264\n",
      "Iteration 600, loss = 1.88506966\n",
      "Iteration 601, loss = 1.88493672\n",
      "Iteration 602, loss = 1.88480383\n",
      "Iteration 603, loss = 1.88467099\n",
      "Iteration 604, loss = 1.88453819\n",
      "Iteration 605, loss = 1.88440544\n",
      "Iteration 606, loss = 1.88427274\n",
      "Iteration 607, loss = 1.88414008\n",
      "Iteration 608, loss = 1.88400747\n",
      "Iteration 609, loss = 1.88387490\n",
      "Iteration 610, loss = 1.88374238\n",
      "Iteration 611, loss = 1.88360990\n",
      "Iteration 612, loss = 1.88347747\n",
      "Iteration 613, loss = 1.88334509\n",
      "Iteration 614, loss = 1.88321275\n",
      "Iteration 615, loss = 1.88308046\n",
      "Iteration 616, loss = 1.88294822\n",
      "Iteration 617, loss = 1.88281602\n",
      "Iteration 618, loss = 1.88268386\n",
      "Iteration 619, loss = 1.88255175\n",
      "Iteration 620, loss = 1.88241969\n",
      "Iteration 621, loss = 1.88228767\n",
      "Iteration 622, loss = 1.88215570\n",
      "Iteration 623, loss = 1.88202378\n",
      "Iteration 624, loss = 1.88189190\n",
      "Iteration 625, loss = 1.88176006\n",
      "Iteration 626, loss = 1.88162827\n",
      "Iteration 627, loss = 1.88149653\n",
      "Iteration 628, loss = 1.88136485\n",
      "Iteration 629, loss = 1.88123322\n",
      "Iteration 630, loss = 1.88110164\n",
      "Iteration 631, loss = 1.88097011\n",
      "Iteration 632, loss = 1.88083862\n",
      "Iteration 633, loss = 1.88070719\n",
      "Iteration 634, loss = 1.88057579\n",
      "Iteration 635, loss = 1.88044445\n",
      "Iteration 636, loss = 1.88031315\n",
      "Iteration 637, loss = 1.88018190\n",
      "Iteration 638, loss = 1.88005069\n",
      "Iteration 639, loss = 1.87991953\n",
      "Iteration 640, loss = 1.87978841\n",
      "Iteration 641, loss = 1.87965734\n",
      "Iteration 642, loss = 1.87952632\n",
      "Iteration 643, loss = 1.87939534\n",
      "Iteration 644, loss = 1.87926441\n",
      "Iteration 645, loss = 1.87913352\n",
      "Iteration 646, loss = 1.87900268\n",
      "Iteration 647, loss = 1.87887189\n",
      "Iteration 648, loss = 1.87874114\n",
      "Iteration 649, loss = 1.87861043\n",
      "Iteration 650, loss = 1.87847978\n",
      "Iteration 651, loss = 1.87834916\n",
      "Iteration 652, loss = 1.87821859\n",
      "Iteration 653, loss = 1.87808807\n",
      "Iteration 654, loss = 1.87795759\n",
      "Iteration 655, loss = 1.87782716\n",
      "Iteration 656, loss = 1.87769677\n",
      "Iteration 657, loss = 1.87756643\n",
      "Iteration 658, loss = 1.87743613\n",
      "Iteration 659, loss = 1.87730588\n",
      "Iteration 660, loss = 1.87717567\n",
      "Iteration 661, loss = 1.87704551\n",
      "Iteration 662, loss = 1.87691540\n",
      "Iteration 663, loss = 1.87678533\n",
      "Iteration 664, loss = 1.87665530\n",
      "Iteration 665, loss = 1.87652532\n",
      "Iteration 666, loss = 1.87639538\n",
      "Iteration 667, loss = 1.87626549\n",
      "Iteration 668, loss = 1.87613564\n",
      "Iteration 669, loss = 1.87600584\n",
      "Iteration 670, loss = 1.87587608\n",
      "Iteration 671, loss = 1.87574637\n",
      "Iteration 672, loss = 1.87561671\n",
      "Iteration 673, loss = 1.87548708\n",
      "Iteration 674, loss = 1.87535751\n",
      "Iteration 675, loss = 1.87522797\n",
      "Iteration 676, loss = 1.87509849\n",
      "Iteration 677, loss = 1.87496904\n",
      "Iteration 678, loss = 1.87483964\n",
      "Iteration 679, loss = 1.87471029\n",
      "Iteration 680, loss = 1.87458098\n",
      "Iteration 681, loss = 1.87445172\n",
      "Iteration 682, loss = 1.87432250\n",
      "Iteration 683, loss = 1.87419332\n",
      "Iteration 684, loss = 1.87406420\n",
      "Iteration 685, loss = 1.87393511\n",
      "Iteration 686, loss = 1.87380607\n",
      "Iteration 687, loss = 1.87367707\n",
      "Iteration 688, loss = 1.87354812\n",
      "Iteration 689, loss = 1.87341922\n",
      "Iteration 690, loss = 1.87329035\n",
      "Iteration 691, loss = 1.87316154\n",
      "Iteration 692, loss = 1.87303276\n",
      "Iteration 693, loss = 1.87290403\n",
      "Iteration 694, loss = 1.87277535\n",
      "Iteration 695, loss = 1.87264671\n",
      "Iteration 696, loss = 1.87251812\n",
      "Iteration 697, loss = 1.87238957\n",
      "Iteration 698, loss = 1.87226106\n",
      "Iteration 699, loss = 1.87213260\n",
      "Iteration 700, loss = 1.87200418\n",
      "Iteration 701, loss = 1.87187581\n",
      "Iteration 702, loss = 1.87174748\n",
      "Iteration 703, loss = 1.87161920\n",
      "Iteration 704, loss = 1.87149096\n",
      "Iteration 705, loss = 1.87136276\n",
      "Iteration 706, loss = 1.87123461\n",
      "Iteration 707, loss = 1.87110650\n",
      "Iteration 708, loss = 1.87097844\n",
      "Iteration 709, loss = 1.87085042\n",
      "Iteration 710, loss = 1.87072245\n",
      "Iteration 711, loss = 1.87059452\n",
      "Iteration 712, loss = 1.87046664\n",
      "Iteration 713, loss = 1.87033880\n",
      "Iteration 714, loss = 1.87021100\n",
      "Iteration 715, loss = 1.87008325\n",
      "Iteration 716, loss = 1.86995554\n",
      "Iteration 717, loss = 1.86982787\n",
      "Iteration 718, loss = 1.86970025\n",
      "Iteration 719, loss = 1.86957268\n",
      "Iteration 720, loss = 1.86944515\n",
      "Iteration 721, loss = 1.86931766\n",
      "Iteration 722, loss = 1.86919022\n",
      "Iteration 723, loss = 1.86906282\n",
      "Iteration 724, loss = 1.86893546\n",
      "Iteration 725, loss = 1.86880815\n",
      "Iteration 726, loss = 1.86868088\n",
      "Iteration 727, loss = 1.86855366\n",
      "Iteration 728, loss = 1.86842648\n",
      "Iteration 729, loss = 1.86829934\n",
      "Iteration 730, loss = 1.86817225\n",
      "Iteration 731, loss = 1.86804521\n",
      "Iteration 732, loss = 1.86791820\n",
      "Iteration 733, loss = 1.86779124\n",
      "Iteration 734, loss = 1.86766433\n",
      "Iteration 735, loss = 1.86753746\n",
      "Iteration 736, loss = 1.86741063\n",
      "Iteration 737, loss = 1.86728385\n",
      "Iteration 738, loss = 1.86715711\n",
      "Iteration 739, loss = 1.86703041\n",
      "Iteration 740, loss = 1.86690376\n",
      "Iteration 741, loss = 1.86677715\n",
      "Iteration 742, loss = 1.86665058\n",
      "Iteration 743, loss = 1.86652406\n",
      "Iteration 744, loss = 1.86639759\n",
      "Iteration 745, loss = 1.86627115\n",
      "Iteration 746, loss = 1.86614476\n",
      "Iteration 747, loss = 1.86601842\n",
      "Iteration 748, loss = 1.86589211\n",
      "Iteration 749, loss = 1.86576586\n",
      "Iteration 750, loss = 1.86563964\n",
      "Iteration 751, loss = 1.86551347\n",
      "Iteration 752, loss = 1.86538734\n",
      "Iteration 753, loss = 1.86526126\n",
      "Iteration 754, loss = 1.86513522\n",
      "Iteration 755, loss = 1.86500922\n",
      "Iteration 756, loss = 1.86488327\n",
      "Iteration 757, loss = 1.86475736\n",
      "Iteration 758, loss = 1.86463149\n",
      "Iteration 759, loss = 1.86450567\n",
      "Iteration 760, loss = 1.86437989\n",
      "Iteration 761, loss = 1.86425416\n",
      "Iteration 762, loss = 1.86412847\n",
      "Iteration 763, loss = 1.86400282\n",
      "Iteration 764, loss = 1.86387721\n",
      "Iteration 765, loss = 1.86375165\n",
      "Iteration 766, loss = 1.86362613\n",
      "Iteration 767, loss = 1.86350066\n",
      "Iteration 768, loss = 1.86337523\n",
      "Iteration 769, loss = 1.86324984\n",
      "Iteration 770, loss = 1.86312449\n",
      "Iteration 771, loss = 1.86299919\n",
      "Iteration 772, loss = 1.86287394\n",
      "Iteration 773, loss = 1.86274872\n",
      "Iteration 774, loss = 1.86262355\n",
      "Iteration 775, loss = 1.86249842\n",
      "Iteration 776, loss = 1.86237334\n",
      "Iteration 777, loss = 1.86224830\n",
      "Iteration 778, loss = 1.86212330\n",
      "Iteration 779, loss = 1.86199834\n",
      "Iteration 780, loss = 1.86187343\n",
      "Iteration 781, loss = 1.86174856\n",
      "Iteration 782, loss = 1.86162374\n",
      "Iteration 783, loss = 1.86149896\n",
      "Iteration 784, loss = 1.86137422\n",
      "Iteration 785, loss = 1.86124952\n",
      "Iteration 786, loss = 1.86112487\n",
      "Iteration 787, loss = 1.86100026\n",
      "Iteration 788, loss = 1.86087570\n",
      "Iteration 789, loss = 1.86075117\n",
      "Iteration 790, loss = 1.86062669\n",
      "Iteration 791, loss = 1.86050226\n",
      "Iteration 792, loss = 1.86037786\n",
      "Iteration 793, loss = 1.86025351\n",
      "Iteration 794, loss = 1.86012920\n",
      "Iteration 795, loss = 1.86000494\n",
      "Iteration 796, loss = 1.85988072\n",
      "Iteration 797, loss = 1.85975654\n",
      "Iteration 798, loss = 1.85963240\n",
      "Iteration 799, loss = 1.85950831\n",
      "Iteration 800, loss = 1.85938426\n",
      "Iteration 801, loss = 1.85926025\n",
      "Iteration 802, loss = 1.85913629\n",
      "Iteration 803, loss = 1.85901237\n",
      "Iteration 804, loss = 1.85888849\n",
      "Iteration 805, loss = 1.85876465\n",
      "Iteration 806, loss = 1.85864086\n",
      "Iteration 807, loss = 1.85851711\n",
      "Iteration 808, loss = 1.85839341\n",
      "Iteration 809, loss = 1.85826974\n",
      "Iteration 810, loss = 1.85814612\n",
      "Iteration 811, loss = 1.85802254\n",
      "Iteration 812, loss = 1.85789901\n",
      "Iteration 813, loss = 1.85777551\n",
      "Iteration 814, loss = 1.85765206\n",
      "Iteration 815, loss = 1.85752865\n",
      "Iteration 816, loss = 1.85740529\n",
      "Iteration 817, loss = 1.85728197\n",
      "Iteration 818, loss = 1.85715869\n",
      "Iteration 819, loss = 1.85703545\n",
      "Iteration 820, loss = 1.85691226\n",
      "Iteration 821, loss = 1.85678911\n",
      "Iteration 822, loss = 1.85666600\n",
      "Iteration 823, loss = 1.85654293\n",
      "Iteration 824, loss = 1.85641991\n",
      "Iteration 825, loss = 1.85629692\n",
      "Iteration 826, loss = 1.85617399\n",
      "Iteration 827, loss = 1.85605109\n",
      "Iteration 828, loss = 1.85592824\n",
      "Iteration 829, loss = 1.85580543\n",
      "Iteration 830, loss = 1.85568266\n",
      "Iteration 831, loss = 1.85555993\n",
      "Iteration 832, loss = 1.85543725\n",
      "Iteration 833, loss = 1.85531461\n",
      "Iteration 834, loss = 1.85519201\n",
      "Iteration 835, loss = 1.85506945\n",
      "Iteration 836, loss = 1.85494694\n",
      "Iteration 837, loss = 1.85482446\n",
      "Iteration 838, loss = 1.85470204\n",
      "Iteration 839, loss = 1.85457965\n",
      "Iteration 840, loss = 1.85445730\n",
      "Iteration 841, loss = 1.85433500\n",
      "Iteration 842, loss = 1.85421274\n",
      "Iteration 843, loss = 1.85409052\n",
      "Iteration 844, loss = 1.85396835\n",
      "Iteration 845, loss = 1.85384622\n",
      "Iteration 846, loss = 1.85372413\n",
      "Iteration 847, loss = 1.85360208\n",
      "Iteration 848, loss = 1.85348007\n",
      "Iteration 849, loss = 1.85335811\n",
      "Iteration 850, loss = 1.85323619\n",
      "Iteration 851, loss = 1.85311431\n",
      "Iteration 852, loss = 1.85299247\n",
      "Iteration 853, loss = 1.85287067\n",
      "Iteration 854, loss = 1.85274892\n",
      "Iteration 855, loss = 1.85262721\n",
      "Iteration 856, loss = 1.85250554\n",
      "Iteration 857, loss = 1.85238391\n",
      "Iteration 858, loss = 1.85226233\n",
      "Iteration 859, loss = 1.85214079\n",
      "Iteration 860, loss = 1.85201931\n",
      "Iteration 861, loss = 1.85189807\n",
      "Iteration 862, loss = 1.85177688\n",
      "Iteration 863, loss = 1.85165573\n",
      "Iteration 864, loss = 1.85153462\n",
      "Iteration 865, loss = 1.85141355\n",
      "Iteration 866, loss = 1.85129252\n",
      "Iteration 867, loss = 1.85117154\n",
      "Iteration 868, loss = 1.85105060\n",
      "Iteration 869, loss = 1.85092969\n",
      "Iteration 870, loss = 1.85080884\n",
      "Iteration 871, loss = 1.85068802\n",
      "Iteration 872, loss = 1.85056724\n",
      "Iteration 873, loss = 1.85044651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 874, loss = 1.85032582\n",
      "Iteration 875, loss = 1.85020517\n",
      "Iteration 876, loss = 1.85008456\n",
      "Iteration 877, loss = 1.84996399\n",
      "Iteration 878, loss = 1.84984347\n",
      "Iteration 879, loss = 1.84972299\n",
      "Iteration 880, loss = 1.84960254\n",
      "Iteration 881, loss = 1.84948214\n",
      "Iteration 882, loss = 1.84936179\n",
      "Iteration 883, loss = 1.84924147\n",
      "Iteration 884, loss = 1.84912120\n",
      "Iteration 885, loss = 1.84900096\n",
      "Iteration 886, loss = 1.84888077\n",
      "Iteration 887, loss = 1.84876062\n",
      "Iteration 888, loss = 1.84864051\n",
      "Iteration 889, loss = 1.84852045\n",
      "Iteration 890, loss = 1.84840042\n",
      "Iteration 891, loss = 1.84828044\n",
      "Iteration 892, loss = 1.84816050\n",
      "Iteration 893, loss = 1.84804059\n",
      "Iteration 894, loss = 1.84792074\n",
      "Iteration 895, loss = 1.84780092\n",
      "Iteration 896, loss = 1.84768114\n",
      "Iteration 897, loss = 1.84756141\n",
      "Iteration 898, loss = 1.84744171\n",
      "Iteration 899, loss = 1.84732206\n",
      "Iteration 900, loss = 1.84720245\n",
      "Iteration 901, loss = 1.84708288\n",
      "Iteration 902, loss = 1.84696335\n",
      "Iteration 903, loss = 1.84684387\n",
      "Iteration 904, loss = 1.84672442\n",
      "Iteration 905, loss = 1.84660502\n",
      "Iteration 906, loss = 1.84648566\n",
      "Iteration 907, loss = 1.84636633\n",
      "Iteration 908, loss = 1.84624705\n",
      "Iteration 909, loss = 1.84612782\n",
      "Iteration 910, loss = 1.84600862\n",
      "Iteration 911, loss = 1.84588946\n",
      "Iteration 912, loss = 1.84577035\n",
      "Iteration 913, loss = 1.84565127\n",
      "Iteration 914, loss = 1.84553224\n",
      "Iteration 915, loss = 1.84541325\n",
      "Iteration 916, loss = 1.84529430\n",
      "Iteration 917, loss = 1.84517539\n",
      "Iteration 918, loss = 1.84505652\n",
      "Iteration 919, loss = 1.84493770\n",
      "Iteration 920, loss = 1.84481891\n",
      "Iteration 921, loss = 1.84470017\n",
      "Iteration 922, loss = 1.84458146\n",
      "Iteration 923, loss = 1.84446280\n",
      "Iteration 924, loss = 1.84434418\n",
      "Iteration 925, loss = 1.84422560\n",
      "Iteration 926, loss = 1.84410706\n",
      "Iteration 927, loss = 1.84398856\n",
      "Iteration 928, loss = 1.84387011\n",
      "Iteration 929, loss = 1.84375169\n",
      "Iteration 930, loss = 1.84363332\n",
      "Iteration 931, loss = 1.84351498\n",
      "Iteration 932, loss = 1.84339669\n",
      "Iteration 933, loss = 1.84327844\n",
      "Iteration 934, loss = 1.84316022\n",
      "Iteration 935, loss = 1.84304205\n",
      "Iteration 936, loss = 1.84292392\n",
      "Iteration 937, loss = 1.84280584\n",
      "Iteration 938, loss = 1.84268779\n",
      "Iteration 939, loss = 1.84256978\n",
      "Iteration 940, loss = 1.84245182\n",
      "Iteration 941, loss = 1.84233389\n",
      "Iteration 942, loss = 1.84221601\n",
      "Iteration 943, loss = 1.84209816\n",
      "Iteration 944, loss = 1.84198036\n",
      "Iteration 945, loss = 1.84186260\n",
      "Iteration 946, loss = 1.84174488\n",
      "Iteration 947, loss = 1.84162719\n",
      "Iteration 948, loss = 1.84150955\n",
      "Iteration 949, loss = 1.84139196\n",
      "Iteration 950, loss = 1.84127440\n",
      "Iteration 951, loss = 1.84115688\n",
      "Iteration 952, loss = 1.84103940\n",
      "Iteration 953, loss = 1.84092197\n",
      "Iteration 954, loss = 1.84080457\n",
      "Iteration 955, loss = 1.84068721\n",
      "Iteration 956, loss = 1.84056990\n",
      "Iteration 957, loss = 1.84045263\n",
      "Iteration 958, loss = 1.84033539\n",
      "Iteration 959, loss = 1.84021820\n",
      "Iteration 960, loss = 1.84010105\n",
      "Iteration 961, loss = 1.83998393\n",
      "Iteration 962, loss = 1.83986686\n",
      "Iteration 963, loss = 1.83974983\n",
      "Iteration 964, loss = 1.83963284\n",
      "Iteration 965, loss = 1.83951589\n",
      "Iteration 966, loss = 1.83939898\n",
      "Iteration 967, loss = 1.83928211\n",
      "Iteration 968, loss = 1.83916529\n",
      "Iteration 969, loss = 1.83904850\n",
      "Iteration 970, loss = 1.83893175\n",
      "Iteration 971, loss = 1.83881504\n",
      "Iteration 972, loss = 1.83869838\n",
      "Iteration 973, loss = 1.83858175\n",
      "Iteration 974, loss = 1.83846516\n",
      "Iteration 975, loss = 1.83834862\n",
      "Iteration 976, loss = 1.83823211\n",
      "Iteration 977, loss = 1.83811565\n",
      "Iteration 978, loss = 1.83799922\n",
      "Iteration 979, loss = 1.83788284\n",
      "Iteration 980, loss = 1.83776649\n",
      "Iteration 981, loss = 1.83765019\n",
      "Iteration 982, loss = 1.83753393\n",
      "Iteration 983, loss = 1.83741770\n",
      "Iteration 984, loss = 1.83730152\n",
      "Iteration 985, loss = 1.83718538\n",
      "Iteration 986, loss = 1.83706927\n",
      "Iteration 987, loss = 1.83695321\n",
      "Iteration 988, loss = 1.83683719\n",
      "Iteration 989, loss = 1.83672120\n",
      "Iteration 990, loss = 1.83660526\n",
      "Iteration 991, loss = 1.83648936\n",
      "Iteration 992, loss = 1.83637350\n",
      "Iteration 993, loss = 1.83625768\n",
      "Iteration 994, loss = 1.83614189\n",
      "Iteration 995, loss = 1.83602615\n",
      "Iteration 996, loss = 1.83591045\n",
      "Iteration 997, loss = 1.83579479\n",
      "Iteration 998, loss = 1.83567917\n",
      "Iteration 999, loss = 1.83556358\n",
      "Iteration 1000, loss = 1.83544804\n",
      "Iteration 1001, loss = 1.83533254\n",
      "Iteration 1002, loss = 1.83521708\n",
      "Iteration 1003, loss = 1.83510166\n",
      "Iteration 1004, loss = 1.83498628\n",
      "Iteration 1005, loss = 1.83487093\n",
      "Iteration 1006, loss = 1.83475563\n",
      "Iteration 1007, loss = 1.83464037\n",
      "Iteration 1008, loss = 1.83452515\n",
      "Iteration 1009, loss = 1.83440996\n",
      "Iteration 1010, loss = 1.83429482\n",
      "Iteration 1011, loss = 1.83417972\n",
      "Iteration 1012, loss = 1.83406466\n",
      "Iteration 1013, loss = 1.83394963\n",
      "Iteration 1014, loss = 1.83383465\n",
      "Iteration 1015, loss = 1.83371971\n",
      "Iteration 1016, loss = 1.83360480\n",
      "Iteration 1017, loss = 1.83348994\n",
      "Iteration 1018, loss = 1.83337511\n",
      "Iteration 1019, loss = 1.83326033\n",
      "Iteration 1020, loss = 1.83314558\n",
      "Iteration 1021, loss = 1.83303088\n",
      "Iteration 1022, loss = 1.83291621\n",
      "Iteration 1023, loss = 1.83280159\n",
      "Iteration 1024, loss = 1.83268700\n",
      "Iteration 1025, loss = 1.83257246\n",
      "Iteration 1026, loss = 1.83245795\n",
      "Iteration 1027, loss = 1.83234348\n",
      "Iteration 1028, loss = 1.83222905\n",
      "Iteration 1029, loss = 1.83211467\n",
      "Iteration 1030, loss = 1.83200032\n",
      "Iteration 1031, loss = 1.83188601\n",
      "Iteration 1032, loss = 1.83177174\n",
      "Iteration 1033, loss = 1.83165751\n",
      "Iteration 1034, loss = 1.83154332\n",
      "Iteration 1035, loss = 1.83142917\n",
      "Iteration 1036, loss = 1.83131506\n",
      "Iteration 1037, loss = 1.83120099\n",
      "Iteration 1038, loss = 1.83108695\n",
      "Iteration 1039, loss = 1.83097296\n",
      "Iteration 1040, loss = 1.83085901\n",
      "Iteration 1041, loss = 1.83074509\n",
      "Iteration 1042, loss = 1.83063122\n",
      "Iteration 1043, loss = 1.83051738\n",
      "Iteration 1044, loss = 1.83040359\n",
      "Iteration 1045, loss = 1.83028983\n",
      "Iteration 1046, loss = 1.83017611\n",
      "Iteration 1047, loss = 1.83006244\n",
      "Iteration 1048, loss = 1.82994880\n",
      "Iteration 1049, loss = 1.82983520\n",
      "Iteration 1050, loss = 1.82972164\n",
      "Iteration 1051, loss = 1.82960812\n",
      "Iteration 1052, loss = 1.82949464\n",
      "Iteration 1053, loss = 1.82938120\n",
      "Iteration 1054, loss = 1.82926779\n",
      "Iteration 1055, loss = 1.82915443\n",
      "Iteration 1056, loss = 1.82904111\n",
      "Iteration 1057, loss = 1.82892782\n",
      "Iteration 1058, loss = 1.82881457\n",
      "Iteration 1059, loss = 1.82870137\n",
      "Iteration 1060, loss = 1.82858820\n",
      "Iteration 1061, loss = 1.82847507\n",
      "Iteration 1062, loss = 1.82836198\n",
      "Iteration 1063, loss = 1.82824893\n",
      "Iteration 1064, loss = 1.82813592\n",
      "Iteration 1065, loss = 1.82802295\n",
      "Iteration 1066, loss = 1.82791002\n",
      "Iteration 1067, loss = 1.82779712\n",
      "Iteration 1068, loss = 1.82768427\n",
      "Iteration 1069, loss = 1.82757145\n",
      "Iteration 1070, loss = 1.82745868\n",
      "Iteration 1071, loss = 1.82734594\n",
      "Iteration 1072, loss = 1.82723324\n",
      "Iteration 1073, loss = 1.82712058\n",
      "Iteration 1074, loss = 1.82700796\n",
      "Iteration 1075, loss = 1.82689538\n",
      "Iteration 1076, loss = 1.82678284\n",
      "Iteration 1077, loss = 1.82667033\n",
      "Iteration 1078, loss = 1.82655787\n",
      "Iteration 1079, loss = 1.82644544\n",
      "Iteration 1080, loss = 1.82633306\n",
      "Iteration 1081, loss = 1.82622071\n",
      "Iteration 1082, loss = 1.82610840\n",
      "Iteration 1083, loss = 1.82599613\n",
      "Iteration 1084, loss = 1.82588390\n",
      "Iteration 1085, loss = 1.82577170\n",
      "Iteration 1086, loss = 1.82565955\n",
      "Iteration 1087, loss = 1.82554743\n",
      "Iteration 1088, loss = 1.82543536\n",
      "Iteration 1089, loss = 1.82532332\n",
      "Iteration 1090, loss = 1.82521132\n",
      "Iteration 1091, loss = 1.82509936\n",
      "Iteration 1092, loss = 1.82498744\n",
      "Iteration 1093, loss = 1.82487556\n",
      "Iteration 1094, loss = 1.82476371\n",
      "Iteration 1095, loss = 1.82465191\n",
      "Iteration 1096, loss = 1.82454014\n",
      "Iteration 1097, loss = 1.82442841\n",
      "Iteration 1098, loss = 1.82431672\n",
      "Iteration 1099, loss = 1.82420507\n",
      "Iteration 1100, loss = 1.82409346\n",
      "Iteration 1101, loss = 1.82398189\n",
      "Iteration 1102, loss = 1.82387035\n",
      "Iteration 1103, loss = 1.82375886\n",
      "Iteration 1104, loss = 1.82364740\n",
      "Iteration 1105, loss = 1.82353598\n",
      "Iteration 1106, loss = 1.82342460\n",
      "Iteration 1107, loss = 1.82331326\n",
      "Iteration 1108, loss = 1.82320196\n",
      "Iteration 1109, loss = 1.82309069\n",
      "Iteration 1110, loss = 1.82297946\n",
      "Iteration 1111, loss = 1.82286828\n",
      "Iteration 1112, loss = 1.82275713\n",
      "Iteration 1113, loss = 1.82264602\n",
      "Iteration 1114, loss = 1.82253494\n",
      "Iteration 1115, loss = 1.82242391\n",
      "Iteration 1116, loss = 1.82231291\n",
      "Iteration 1117, loss = 1.82220196\n",
      "Iteration 1118, loss = 1.82209104\n",
      "Iteration 1119, loss = 1.82198016\n",
      "Iteration 1120, loss = 1.82186931\n",
      "Iteration 1121, loss = 1.82175851\n",
      "Iteration 1122, loss = 1.82164774\n",
      "Iteration 1123, loss = 1.82153702\n",
      "Iteration 1124, loss = 1.82142633\n",
      "Iteration 1125, loss = 1.82131568\n",
      "Iteration 1126, loss = 1.82120506\n",
      "Iteration 1127, loss = 1.82109449\n",
      "Iteration 1128, loss = 1.82098395\n",
      "Iteration 1129, loss = 1.82087346\n",
      "Iteration 1130, loss = 1.82076300\n",
      "Iteration 1131, loss = 1.82065258\n",
      "Iteration 1132, loss = 1.82054219\n",
      "Iteration 1133, loss = 1.82043185\n",
      "Iteration 1134, loss = 1.82032154\n",
      "Iteration 1135, loss = 1.82021127\n",
      "Iteration 1136, loss = 1.82010104\n",
      "Iteration 1137, loss = 1.81999085\n",
      "Iteration 1138, loss = 1.81988069\n",
      "Iteration 1139, loss = 1.81977058\n",
      "Iteration 1140, loss = 1.81966050\n",
      "Iteration 1141, loss = 1.81955046\n",
      "Iteration 1142, loss = 1.81944046\n",
      "Iteration 1143, loss = 1.81933049\n",
      "Iteration 1144, loss = 1.81922057\n",
      "Iteration 1145, loss = 1.81911068\n",
      "Iteration 1146, loss = 1.81900083\n",
      "Iteration 1147, loss = 1.81889102\n",
      "Iteration 1148, loss = 1.81878125\n",
      "Iteration 1149, loss = 1.81867151\n",
      "Iteration 1150, loss = 1.81856181\n",
      "Iteration 1151, loss = 1.81845215\n",
      "Iteration 1152, loss = 1.81834253\n",
      "Iteration 1153, loss = 1.81823295\n",
      "Iteration 1154, loss = 1.81812340\n",
      "Iteration 1155, loss = 1.81801389\n",
      "Iteration 1156, loss = 1.81790442\n",
      "Iteration 1157, loss = 1.81779499\n",
      "Iteration 1158, loss = 1.81768559\n",
      "Iteration 1159, loss = 1.81757624\n",
      "Iteration 1160, loss = 1.81746692\n",
      "Iteration 1161, loss = 1.81735764\n",
      "Iteration 1162, loss = 1.81724839\n",
      "Iteration 1163, loss = 1.81713919\n",
      "Iteration 1164, loss = 1.81703002\n",
      "Iteration 1165, loss = 1.81692089\n",
      "Iteration 1166, loss = 1.81681180\n",
      "Iteration 1167, loss = 1.81670274\n",
      "Iteration 1168, loss = 1.81659372\n",
      "Iteration 1169, loss = 1.81648474\n",
      "Iteration 1170, loss = 1.81637580\n",
      "Iteration 1171, loss = 1.81626690\n",
      "Iteration 1172, loss = 1.81615803\n",
      "Iteration 1173, loss = 1.81604920\n",
      "Iteration 1174, loss = 1.81594041\n",
      "Iteration 1175, loss = 1.81583166\n",
      "Iteration 1176, loss = 1.81572294\n",
      "Iteration 1177, loss = 1.81561426\n",
      "Iteration 1178, loss = 1.81550562\n",
      "Iteration 1179, loss = 1.81539702\n",
      "Iteration 1180, loss = 1.81528846\n",
      "Iteration 1181, loss = 1.81517993\n",
      "Iteration 1182, loss = 1.81507144\n",
      "Iteration 1183, loss = 1.81496298\n",
      "Iteration 1184, loss = 1.81485457\n",
      "Iteration 1185, loss = 1.81474619\n",
      "Iteration 1186, loss = 1.81463785\n",
      "Iteration 1187, loss = 1.81452955\n",
      "Iteration 1188, loss = 1.81442128\n",
      "Iteration 1189, loss = 1.81431305\n",
      "Iteration 1190, loss = 1.81420486\n",
      "Iteration 1191, loss = 1.81409671\n",
      "Iteration 1192, loss = 1.81398859\n",
      "Iteration 1193, loss = 1.81388052\n",
      "Iteration 1194, loss = 1.81377247\n",
      "Iteration 1195, loss = 1.81366447\n",
      "Iteration 1196, loss = 1.81355650\n",
      "Iteration 1197, loss = 1.81344857\n",
      "Iteration 1198, loss = 1.81334068\n",
      "Iteration 1199, loss = 1.81323283\n",
      "Iteration 1200, loss = 1.81312501\n",
      "Iteration 1201, loss = 1.81301723\n",
      "Iteration 1202, loss = 1.81290949\n",
      "Iteration 1203, loss = 1.81280178\n",
      "Iteration 1204, loss = 1.81269412\n",
      "Iteration 1205, loss = 1.81258648\n",
      "Iteration 1206, loss = 1.81247889\n",
      "Iteration 1207, loss = 1.81237133\n",
      "Iteration 1208, loss = 1.81226382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1209, loss = 1.81215633\n",
      "Iteration 1210, loss = 1.81204889\n",
      "Iteration 1211, loss = 1.81194148\n",
      "Iteration 1212, loss = 1.81183411\n",
      "Iteration 1213, loss = 1.81172678\n",
      "Iteration 1214, loss = 1.81161948\n",
      "Iteration 1215, loss = 1.81151222\n",
      "Iteration 1216, loss = 1.81140500\n",
      "Iteration 1217, loss = 1.81129781\n",
      "Iteration 1218, loss = 1.81119066\n",
      "Iteration 1219, loss = 1.81108355\n",
      "Iteration 1220, loss = 1.81097648\n",
      "Iteration 1221, loss = 1.81086944\n",
      "Iteration 1222, loss = 1.81076244\n",
      "Iteration 1223, loss = 1.81065548\n",
      "Iteration 1224, loss = 1.81054855\n",
      "Iteration 1225, loss = 1.81044166\n",
      "Iteration 1226, loss = 1.81033481\n",
      "Iteration 1227, loss = 1.81022800\n",
      "Iteration 1228, loss = 1.81012122\n",
      "Iteration 1229, loss = 1.81001448\n",
      "Iteration 1230, loss = 1.80990777\n",
      "Iteration 1231, loss = 1.80980110\n",
      "Iteration 1232, loss = 1.80969447\n",
      "Iteration 1233, loss = 1.80958788\n",
      "Iteration 1234, loss = 1.80948132\n",
      "Iteration 1235, loss = 1.80937480\n",
      "Iteration 1236, loss = 1.80926832\n",
      "Iteration 1237, loss = 1.80916187\n",
      "Iteration 1238, loss = 1.80905546\n",
      "Iteration 1239, loss = 1.80894909\n",
      "Iteration 1240, loss = 1.80884275\n",
      "Iteration 1241, loss = 1.80873645\n",
      "Iteration 1242, loss = 1.80863019\n",
      "Iteration 1243, loss = 1.80852397\n",
      "Iteration 1244, loss = 1.80841778\n",
      "Iteration 1245, loss = 1.80831162\n",
      "Iteration 1246, loss = 1.80820551\n",
      "Iteration 1247, loss = 1.80809943\n",
      "Iteration 1248, loss = 1.80799339\n",
      "Iteration 1249, loss = 1.80788738\n",
      "Iteration 1250, loss = 1.80778141\n",
      "Iteration 1251, loss = 1.80767548\n",
      "Iteration 1252, loss = 1.80756958\n",
      "Iteration 1253, loss = 1.80746372\n",
      "Iteration 1254, loss = 1.80735790\n",
      "Iteration 1255, loss = 1.80725211\n",
      "Iteration 1256, loss = 1.80714637\n",
      "Iteration 1257, loss = 1.80704065\n",
      "Iteration 1258, loss = 1.80693498\n",
      "Iteration 1259, loss = 1.80682934\n",
      "Iteration 1260, loss = 1.80672373\n",
      "Iteration 1261, loss = 1.80661817\n",
      "Iteration 1262, loss = 1.80651263\n",
      "Iteration 1263, loss = 1.80640714\n",
      "Iteration 1264, loss = 1.80630168\n",
      "Iteration 1265, loss = 1.80619626\n",
      "Iteration 1266, loss = 1.80609088\n",
      "Iteration 1267, loss = 1.80598553\n",
      "Iteration 1268, loss = 1.80588022\n",
      "Iteration 1269, loss = 1.80577494\n",
      "Iteration 1270, loss = 1.80566970\n",
      "Iteration 1271, loss = 1.80556450\n",
      "Iteration 1272, loss = 1.80545933\n",
      "Iteration 1273, loss = 1.80535420\n",
      "Iteration 1274, loss = 1.80524911\n",
      "Iteration 1275, loss = 1.80514405\n",
      "Iteration 1276, loss = 1.80503903\n",
      "Iteration 1277, loss = 1.80493405\n",
      "Iteration 1278, loss = 1.80482910\n",
      "Iteration 1279, loss = 1.80472419\n",
      "Iteration 1280, loss = 1.80461931\n",
      "Iteration 1281, loss = 1.80451447\n",
      "Iteration 1282, loss = 1.80440967\n",
      "Iteration 1283, loss = 1.80430490\n",
      "Iteration 1284, loss = 1.80420017\n",
      "Iteration 1285, loss = 1.80409547\n",
      "Iteration 1286, loss = 1.80399081\n",
      "Iteration 1287, loss = 1.80388619\n",
      "Iteration 1288, loss = 1.80378161\n",
      "Iteration 1289, loss = 1.80367706\n",
      "Iteration 1290, loss = 1.80357254\n",
      "Iteration 1291, loss = 1.80346806\n",
      "Iteration 1292, loss = 1.80336362\n",
      "Iteration 1293, loss = 1.80325922\n",
      "Iteration 1294, loss = 1.80315485\n",
      "Iteration 1295, loss = 1.80305051\n",
      "Iteration 1296, loss = 1.80294622\n",
      "Iteration 1297, loss = 1.80284195\n",
      "Iteration 1298, loss = 1.80273773\n",
      "Iteration 1299, loss = 1.80263354\n",
      "Iteration 1300, loss = 1.80252938\n",
      "Iteration 1301, loss = 1.80242527\n",
      "Iteration 1302, loss = 1.80232119\n",
      "Iteration 1303, loss = 1.80221714\n",
      "Iteration 1304, loss = 1.80211313\n",
      "Iteration 1305, loss = 1.80200916\n",
      "Iteration 1306, loss = 1.80190522\n",
      "Iteration 1307, loss = 1.80180132\n",
      "Iteration 1308, loss = 1.80169745\n",
      "Iteration 1309, loss = 1.80159362\n",
      "Iteration 1310, loss = 1.80148983\n",
      "Iteration 1311, loss = 1.80138607\n",
      "Iteration 1312, loss = 1.80128235\n",
      "Iteration 1313, loss = 1.80117866\n",
      "Iteration 1314, loss = 1.80107501\n",
      "Iteration 1315, loss = 1.80097139\n",
      "Iteration 1316, loss = 1.80086781\n",
      "Iteration 1317, loss = 1.80076427\n",
      "Iteration 1318, loss = 1.80066076\n",
      "Iteration 1319, loss = 1.80055729\n",
      "Iteration 1320, loss = 1.80045386\n",
      "Iteration 1321, loss = 1.80035045\n",
      "Iteration 1322, loss = 1.80024709\n",
      "Iteration 1323, loss = 1.80014376\n",
      "Iteration 1324, loss = 1.80004047\n",
      "Iteration 1325, loss = 1.79993721\n",
      "Iteration 1326, loss = 1.79983399\n",
      "Iteration 1327, loss = 1.79973080\n",
      "Iteration 1328, loss = 1.79962765\n",
      "Iteration 1329, loss = 1.79952454\n",
      "Iteration 1330, loss = 1.79942146\n",
      "Iteration 1331, loss = 1.79931841\n",
      "Iteration 1332, loss = 1.79921540\n",
      "Iteration 1333, loss = 1.79911243\n",
      "Iteration 1334, loss = 1.79900949\n",
      "Iteration 1335, loss = 1.79890659\n",
      "Iteration 1336, loss = 1.79880373\n",
      "Iteration 1337, loss = 1.79870090\n",
      "Iteration 1338, loss = 1.79859810\n",
      "Iteration 1339, loss = 1.79849534\n",
      "Iteration 1340, loss = 1.79839262\n",
      "Iteration 1341, loss = 1.79828993\n",
      "Iteration 1342, loss = 1.79818728\n",
      "Iteration 1343, loss = 1.79808466\n",
      "Iteration 1344, loss = 1.79798208\n",
      "Iteration 1345, loss = 1.79787953\n",
      "Iteration 1346, loss = 1.79777702\n",
      "Iteration 1347, loss = 1.79767454\n",
      "Iteration 1348, loss = 1.79757210\n",
      "Iteration 1349, loss = 1.79746970\n",
      "Iteration 1350, loss = 1.79736733\n",
      "Iteration 1351, loss = 1.79726499\n",
      "Iteration 1352, loss = 1.79716270\n",
      "Iteration 1353, loss = 1.79706043\n",
      "Iteration 1354, loss = 1.79695820\n",
      "Iteration 1355, loss = 1.79685601\n",
      "Iteration 1356, loss = 1.79675385\n",
      "Iteration 1357, loss = 1.79665173\n",
      "Iteration 1358, loss = 1.79654964\n",
      "Iteration 1359, loss = 1.79644759\n",
      "Iteration 1360, loss = 1.79634557\n",
      "Iteration 1361, loss = 1.79624359\n",
      "Iteration 1362, loss = 1.79614165\n",
      "Iteration 1363, loss = 1.79603974\n",
      "Iteration 1364, loss = 1.79593786\n",
      "Iteration 1365, loss = 1.79583602\n",
      "Iteration 1366, loss = 1.79573421\n",
      "Iteration 1367, loss = 1.79563244\n",
      "Iteration 1368, loss = 1.79553071\n",
      "Iteration 1369, loss = 1.79542901\n",
      "Iteration 1370, loss = 1.79532734\n",
      "Iteration 1371, loss = 1.79522571\n",
      "Iteration 1372, loss = 1.79512412\n",
      "Iteration 1373, loss = 1.79502256\n",
      "Iteration 1374, loss = 1.79492103\n",
      "Iteration 1375, loss = 1.79481955\n",
      "Iteration 1376, loss = 1.79471809\n",
      "Iteration 1377, loss = 1.79461667\n",
      "Iteration 1378, loss = 1.79451529\n",
      "Iteration 1379, loss = 1.79441394\n",
      "Iteration 1380, loss = 1.79431262\n",
      "Iteration 1381, loss = 1.79421134\n",
      "Iteration 1382, loss = 1.79411010\n",
      "Iteration 1383, loss = 1.79400889\n",
      "Iteration 1384, loss = 1.79390772\n",
      "Iteration 1385, loss = 1.79380658\n",
      "Iteration 1386, loss = 1.79370547\n",
      "Iteration 1387, loss = 1.79360440\n",
      "Iteration 1388, loss = 1.79350337\n",
      "Iteration 1389, loss = 1.79340237\n",
      "Iteration 1390, loss = 1.79330140\n",
      "Iteration 1391, loss = 1.79320047\n",
      "Iteration 1392, loss = 1.79309958\n",
      "Iteration 1393, loss = 1.79299871\n",
      "Iteration 1394, loss = 1.79289789\n",
      "Iteration 1395, loss = 1.79279710\n",
      "Iteration 1396, loss = 1.79269634\n",
      "Iteration 1397, loss = 1.79259562\n",
      "Iteration 1398, loss = 1.79249493\n",
      "Iteration 1399, loss = 1.79239428\n",
      "Iteration 1400, loss = 1.79229366\n",
      "Iteration 1401, loss = 1.79219308\n",
      "Iteration 1402, loss = 1.79209253\n",
      "Iteration 1403, loss = 1.79199202\n",
      "Iteration 1404, loss = 1.79189154\n",
      "Iteration 1405, loss = 1.79179110\n",
      "Iteration 1406, loss = 1.79169069\n",
      "Iteration 1407, loss = 1.79159032\n",
      "Iteration 1408, loss = 1.79148998\n",
      "Iteration 1409, loss = 1.79138967\n",
      "Iteration 1410, loss = 1.79128940\n",
      "Iteration 1411, loss = 1.79118917\n",
      "Iteration 1412, loss = 1.79108896\n",
      "Iteration 1413, loss = 1.79098880\n",
      "Iteration 1414, loss = 1.79088867\n",
      "Iteration 1415, loss = 1.79078857\n",
      "Iteration 1416, loss = 1.79068851\n",
      "Iteration 1417, loss = 1.79058848\n",
      "Iteration 1418, loss = 1.79048848\n",
      "Iteration 1419, loss = 1.79038852\n",
      "Iteration 1420, loss = 1.79028860\n",
      "Iteration 1421, loss = 1.79018871\n",
      "Iteration 1422, loss = 1.79008885\n",
      "Iteration 1423, loss = 1.78998903\n",
      "Iteration 1424, loss = 1.78988924\n",
      "Iteration 1425, loss = 1.78978949\n",
      "Iteration 1426, loss = 1.78968977\n",
      "Iteration 1427, loss = 1.78959009\n",
      "Iteration 1428, loss = 1.78949044\n",
      "Iteration 1429, loss = 1.78939083\n",
      "Iteration 1430, loss = 1.78929125\n",
      "Iteration 1431, loss = 1.78919170\n",
      "Iteration 1432, loss = 1.78909219\n",
      "Iteration 1433, loss = 1.78899271\n",
      "Iteration 1434, loss = 1.78889327\n",
      "Iteration 1435, loss = 1.78879386\n",
      "Iteration 1436, loss = 1.78869448\n",
      "Iteration 1437, loss = 1.78859514\n",
      "Iteration 1438, loss = 1.78849584\n",
      "Iteration 1439, loss = 1.78839657\n",
      "Iteration 1440, loss = 1.78829733\n",
      "Iteration 1441, loss = 1.78819813\n",
      "Iteration 1442, loss = 1.78809896\n",
      "Iteration 1443, loss = 1.78799982\n",
      "Iteration 1444, loss = 1.78790072\n",
      "Iteration 1445, loss = 1.78780166\n",
      "Iteration 1446, loss = 1.78770262\n",
      "Iteration 1447, loss = 1.78760363\n",
      "Iteration 1448, loss = 1.78750466\n",
      "Iteration 1449, loss = 1.78740573\n",
      "Iteration 1450, loss = 1.78730684\n",
      "Iteration 1451, loss = 1.78720797\n",
      "Iteration 1452, loss = 1.78710915\n",
      "Iteration 1453, loss = 1.78701035\n",
      "Iteration 1454, loss = 1.78691160\n",
      "Iteration 1455, loss = 1.78681287\n",
      "Iteration 1456, loss = 1.78671418\n",
      "Iteration 1457, loss = 1.78661552\n",
      "Iteration 1458, loss = 1.78651690\n",
      "Iteration 1459, loss = 1.78641831\n",
      "Iteration 1460, loss = 1.78631976\n",
      "Iteration 1461, loss = 1.78622123\n",
      "Iteration 1462, loss = 1.78612275\n",
      "Iteration 1463, loss = 1.78602430\n",
      "Iteration 1464, loss = 1.78592588\n",
      "Iteration 1465, loss = 1.78582749\n",
      "Iteration 1466, loss = 1.78572914\n",
      "Iteration 1467, loss = 1.78563082\n",
      "Iteration 1468, loss = 1.78553254\n",
      "Iteration 1469, loss = 1.78543429\n",
      "Iteration 1470, loss = 1.78533608\n",
      "Iteration 1471, loss = 1.78523789\n",
      "Iteration 1472, loss = 1.78513975\n",
      "Iteration 1473, loss = 1.78504163\n",
      "Iteration 1474, loss = 1.78494355\n",
      "Iteration 1475, loss = 1.78484551\n",
      "Iteration 1476, loss = 1.78474749\n",
      "Iteration 1477, loss = 1.78464952\n",
      "Iteration 1478, loss = 1.78455157\n",
      "Iteration 1479, loss = 1.78445366\n",
      "Iteration 1480, loss = 1.78435578\n",
      "Iteration 1481, loss = 1.78425794\n",
      "Iteration 1482, loss = 1.78416013\n",
      "Iteration 1483, loss = 1.78406235\n",
      "Iteration 1484, loss = 1.78396461\n",
      "Iteration 1485, loss = 1.78386690\n",
      "Iteration 1486, loss = 1.78376923\n",
      "Iteration 1487, loss = 1.78367159\n",
      "Iteration 1488, loss = 1.78357398\n",
      "Iteration 1489, loss = 1.78347641\n",
      "Iteration 1490, loss = 1.78337887\n",
      "Iteration 1491, loss = 1.78328136\n",
      "Iteration 1492, loss = 1.78318389\n",
      "Iteration 1493, loss = 1.78308645\n",
      "Iteration 1494, loss = 1.78298904\n",
      "Iteration 1495, loss = 1.78289167\n",
      "Iteration 1496, loss = 1.78279433\n",
      "Iteration 1497, loss = 1.78269703\n",
      "Iteration 1498, loss = 1.78259975\n",
      "Iteration 1499, loss = 1.78250252\n",
      "Iteration 1500, loss = 1.78240531\n",
      "Iteration 1501, loss = 1.78230814\n",
      "Iteration 1502, loss = 1.78221100\n",
      "Iteration 1503, loss = 1.78211390\n",
      "Iteration 1504, loss = 1.78201683\n",
      "Iteration 1505, loss = 1.78191979\n",
      "Iteration 1506, loss = 1.78182279\n",
      "Iteration 1507, loss = 1.78172582\n",
      "Iteration 1508, loss = 1.78162888\n",
      "Iteration 1509, loss = 1.78153198\n",
      "Iteration 1510, loss = 1.78143511\n",
      "Iteration 1511, loss = 1.78133828\n",
      "Iteration 1512, loss = 1.78124147\n",
      "Iteration 1513, loss = 1.78114470\n",
      "Iteration 1514, loss = 1.78104797\n",
      "Iteration 1515, loss = 1.78095126\n",
      "Iteration 1516, loss = 1.78085459\n",
      "Iteration 1517, loss = 1.78075796\n",
      "Iteration 1518, loss = 1.78066136\n",
      "Iteration 1519, loss = 1.78056479\n",
      "Iteration 1520, loss = 1.78046825\n",
      "Iteration 1521, loss = 1.78037175\n",
      "Iteration 1522, loss = 1.78027528\n",
      "Iteration 1523, loss = 1.78017884\n",
      "Iteration 1524, loss = 1.78008244\n",
      "Iteration 1525, loss = 1.77998607\n",
      "Iteration 1526, loss = 1.77988973\n",
      "Iteration 1527, loss = 1.77979343\n",
      "Iteration 1528, loss = 1.77969716\n",
      "Iteration 1529, loss = 1.77960092\n",
      "Iteration 1530, loss = 1.77950472\n",
      "Iteration 1531, loss = 1.77940855\n",
      "Iteration 1532, loss = 1.77931241\n",
      "Iteration 1533, loss = 1.77921630\n",
      "Iteration 1534, loss = 1.77912023\n",
      "Iteration 1535, loss = 1.77902419\n",
      "Iteration 1536, loss = 1.77892819\n",
      "Iteration 1537, loss = 1.77883222\n",
      "Iteration 1538, loss = 1.77873628\n",
      "Iteration 1539, loss = 1.77864037\n",
      "Iteration 1540, loss = 1.77854450\n",
      "Iteration 1541, loss = 1.77844866\n",
      "Iteration 1542, loss = 1.77835285\n",
      "Iteration 1543, loss = 1.77825708\n",
      "Iteration 1544, loss = 1.77816134\n",
      "Iteration 1545, loss = 1.77806563\n",
      "Iteration 1546, loss = 1.77796996\n",
      "Iteration 1547, loss = 1.77787431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1548, loss = 1.77777871\n",
      "Iteration 1549, loss = 1.77768313\n",
      "Iteration 1550, loss = 1.77758759\n",
      "Iteration 1551, loss = 1.77749208\n",
      "Iteration 1552, loss = 1.77739660\n",
      "Iteration 1553, loss = 1.77730115\n",
      "Iteration 1554, loss = 1.77720574\n",
      "Iteration 1555, loss = 1.77711037\n",
      "Iteration 1556, loss = 1.77701502\n",
      "Iteration 1557, loss = 1.77691971\n",
      "Iteration 1558, loss = 1.77682443\n",
      "Iteration 1559, loss = 1.77672918\n",
      "Iteration 1560, loss = 1.77663397\n",
      "Iteration 1561, loss = 1.77653878\n",
      "Iteration 1562, loss = 1.77644364\n",
      "Iteration 1563, loss = 1.77634852\n",
      "Iteration 1564, loss = 1.77625344\n",
      "Iteration 1565, loss = 1.77615839\n",
      "Iteration 1566, loss = 1.77606337\n",
      "Iteration 1567, loss = 1.77596838\n",
      "Iteration 1568, loss = 1.77587343\n",
      "Iteration 1569, loss = 1.77577851\n",
      "Iteration 1570, loss = 1.77568363\n",
      "Iteration 1571, loss = 1.77558877\n",
      "Iteration 1572, loss = 1.77549395\n",
      "Iteration 1573, loss = 1.77539916\n",
      "Iteration 1574, loss = 1.77530441\n",
      "Iteration 1575, loss = 1.77520968\n",
      "Iteration 1576, loss = 1.77511499\n",
      "Iteration 1577, loss = 1.77502033\n",
      "Iteration 1578, loss = 1.77492571\n",
      "Iteration 1579, loss = 1.77483111\n",
      "Iteration 1580, loss = 1.77473655\n",
      "Iteration 1581, loss = 1.77464203\n",
      "Iteration 1582, loss = 1.77454753\n",
      "Iteration 1583, loss = 1.77445307\n",
      "Iteration 1584, loss = 1.77435864\n",
      "Iteration 1585, loss = 1.77426424\n",
      "Iteration 1586, loss = 1.77416988\n",
      "Iteration 1587, loss = 1.77407554\n",
      "Iteration 1588, loss = 1.77398124\n",
      "Iteration 1589, loss = 1.77388697\n",
      "Iteration 1590, loss = 1.77379274\n",
      "Iteration 1591, loss = 1.77369854\n",
      "Iteration 1592, loss = 1.77360437\n",
      "Iteration 1593, loss = 1.77351023\n",
      "Iteration 1594, loss = 1.77341612\n",
      "Iteration 1595, loss = 1.77332205\n",
      "Iteration 1596, loss = 1.77322801\n",
      "Iteration 1597, loss = 1.77313400\n",
      "Iteration 1598, loss = 1.77304003\n",
      "Iteration 1599, loss = 1.77294608\n",
      "Iteration 1600, loss = 1.77285217\n",
      "Iteration 1601, loss = 1.77275829\n",
      "Iteration 1602, loss = 1.77266445\n",
      "Iteration 1603, loss = 1.77257063\n",
      "Iteration 1604, loss = 1.77247685\n",
      "Iteration 1605, loss = 1.77238310\n",
      "Iteration 1606, loss = 1.77228938\n",
      "Iteration 1607, loss = 1.77219570\n",
      "Iteration 1608, loss = 1.77210205\n",
      "Iteration 1609, loss = 1.77200842\n",
      "Iteration 1610, loss = 1.77191484\n",
      "Iteration 1611, loss = 1.77182128\n",
      "Iteration 1612, loss = 1.77172776\n",
      "Iteration 1613, loss = 1.77163427\n",
      "Iteration 1614, loss = 1.77154081\n",
      "Iteration 1615, loss = 1.77144738\n",
      "Iteration 1616, loss = 1.77135398\n",
      "Iteration 1617, loss = 1.77126062\n",
      "Iteration 1618, loss = 1.77116729\n",
      "Iteration 1619, loss = 1.77107399\n",
      "Iteration 1620, loss = 1.77098072\n",
      "Iteration 1621, loss = 1.77088749\n",
      "Iteration 1622, loss = 1.77079429\n",
      "Iteration 1623, loss = 1.77070112\n",
      "Iteration 1624, loss = 1.77060798\n",
      "Iteration 1625, loss = 1.77051487\n",
      "Iteration 1626, loss = 1.77042180\n",
      "Iteration 1627, loss = 1.77032876\n",
      "Iteration 1628, loss = 1.77023575\n",
      "Iteration 1629, loss = 1.77014277\n",
      "Iteration 1630, loss = 1.77004982\n",
      "Iteration 1631, loss = 1.76995691\n",
      "Iteration 1632, loss = 1.76986403\n",
      "Iteration 1633, loss = 1.76977118\n",
      "Iteration 1634, loss = 1.76967836\n",
      "Iteration 1635, loss = 1.76958557\n",
      "Iteration 1636, loss = 1.76949282\n",
      "Iteration 1637, loss = 1.76940010\n",
      "Iteration 1638, loss = 1.76930741\n",
      "Iteration 1639, loss = 1.76921475\n",
      "Iteration 1640, loss = 1.76912212\n",
      "Iteration 1641, loss = 1.76902953\n",
      "Iteration 1642, loss = 1.76893696\n",
      "Iteration 1643, loss = 1.76884443\n",
      "Iteration 1644, loss = 1.76875193\n",
      "Iteration 1645, loss = 1.76865947\n",
      "Iteration 1646, loss = 1.76856703\n",
      "Iteration 1647, loss = 1.76847463\n",
      "Iteration 1648, loss = 1.76838225\n",
      "Iteration 1649, loss = 1.76828991\n",
      "Iteration 1650, loss = 1.76819761\n",
      "Iteration 1651, loss = 1.76810533\n",
      "Iteration 1652, loss = 1.76801308\n",
      "Iteration 1653, loss = 1.76792087\n",
      "Iteration 1654, loss = 1.76782869\n",
      "Iteration 1655, loss = 1.76773654\n",
      "Iteration 1656, loss = 1.76764442\n",
      "Iteration 1657, loss = 1.76755234\n",
      "Iteration 1658, loss = 1.76746028\n",
      "Iteration 1659, loss = 1.76736826\n",
      "Iteration 1660, loss = 1.76727627\n",
      "Iteration 1661, loss = 1.76718431\n",
      "Iteration 1662, loss = 1.76709238\n",
      "Iteration 1663, loss = 1.76700048\n",
      "Iteration 1664, loss = 1.76690862\n",
      "Iteration 1665, loss = 1.76681679\n",
      "Iteration 1666, loss = 1.76672498\n",
      "Iteration 1667, loss = 1.76663321\n",
      "Iteration 1668, loss = 1.76654148\n",
      "Iteration 1669, loss = 1.76644977\n",
      "Iteration 1670, loss = 1.76635809\n",
      "Iteration 1671, loss = 1.76626645\n",
      "Iteration 1672, loss = 1.76617484\n",
      "Iteration 1673, loss = 1.76608326\n",
      "Iteration 1674, loss = 1.76599171\n",
      "Iteration 1675, loss = 1.76590019\n",
      "Iteration 1676, loss = 1.76580870\n",
      "Iteration 1677, loss = 1.76571725\n",
      "Iteration 1678, loss = 1.76562583\n",
      "Iteration 1679, loss = 1.76553444\n",
      "Iteration 1680, loss = 1.76544307\n",
      "Iteration 1681, loss = 1.76535175\n",
      "Iteration 1682, loss = 1.76526045\n",
      "Iteration 1683, loss = 1.76516918\n",
      "Iteration 1684, loss = 1.76507795\n",
      "Iteration 1685, loss = 1.76498674\n",
      "Iteration 1686, loss = 1.76489557\n",
      "Iteration 1687, loss = 1.76480443\n",
      "Iteration 1688, loss = 1.76471332\n",
      "Iteration 1689, loss = 1.76462224\n",
      "Iteration 1690, loss = 1.76453120\n",
      "Iteration 1691, loss = 1.76444018\n",
      "Iteration 1692, loss = 1.76434920\n",
      "Iteration 1693, loss = 1.76425825\n",
      "Iteration 1694, loss = 1.76416732\n",
      "Iteration 1695, loss = 1.76407643\n",
      "Iteration 1696, loss = 1.76398557\n",
      "Iteration 1697, loss = 1.76389475\n",
      "Iteration 1698, loss = 1.76380395\n",
      "Iteration 1699, loss = 1.76371318\n",
      "Iteration 1700, loss = 1.76362245\n",
      "Iteration 1701, loss = 1.76353175\n",
      "Iteration 1702, loss = 1.76344108\n",
      "Iteration 1703, loss = 1.76335044\n",
      "Iteration 1704, loss = 1.76325983\n",
      "Iteration 1705, loss = 1.76316925\n",
      "Iteration 1706, loss = 1.76307870\n",
      "Iteration 1707, loss = 1.76298819\n",
      "Iteration 1708, loss = 1.76289770\n",
      "Iteration 1709, loss = 1.76280725\n",
      "Iteration 1710, loss = 1.76271682\n",
      "Iteration 1711, loss = 1.76262643\n",
      "Iteration 1712, loss = 1.76253607\n",
      "Iteration 1713, loss = 1.76244574\n",
      "Iteration 1714, loss = 1.76235544\n",
      "Iteration 1715, loss = 1.76226518\n",
      "Iteration 1716, loss = 1.76217494\n",
      "Iteration 1717, loss = 1.76208474\n",
      "Iteration 1718, loss = 1.76199456\n",
      "Iteration 1719, loss = 1.76190442\n",
      "Iteration 1720, loss = 1.76181431\n",
      "Iteration 1721, loss = 1.76172422\n",
      "Iteration 1722, loss = 1.76163417\n",
      "Iteration 1723, loss = 1.76154415\n",
      "Iteration 1724, loss = 1.76145417\n",
      "Iteration 1725, loss = 1.76136421\n",
      "Iteration 1726, loss = 1.76127428\n",
      "Iteration 1727, loss = 1.76118439\n",
      "Iteration 1728, loss = 1.76109452\n",
      "Iteration 1729, loss = 1.76100469\n",
      "Iteration 1730, loss = 1.76091488\n",
      "Iteration 1731, loss = 1.76082511\n",
      "Iteration 1732, loss = 1.76073537\n",
      "Iteration 1733, loss = 1.76064566\n",
      "Iteration 1734, loss = 1.76055598\n",
      "Iteration 1735, loss = 1.76046633\n",
      "Iteration 1736, loss = 1.76037671\n",
      "Iteration 1737, loss = 1.76028713\n",
      "Iteration 1738, loss = 1.76019757\n",
      "Iteration 1739, loss = 1.76010804\n",
      "Iteration 1740, loss = 1.76001855\n",
      "Iteration 1741, loss = 1.75992909\n",
      "Iteration 1742, loss = 1.75983965\n",
      "Iteration 1743, loss = 1.75975025\n",
      "Iteration 1744, loss = 1.75966088\n",
      "Iteration 1745, loss = 1.75957154\n",
      "Iteration 1746, loss = 1.75948223\n",
      "Iteration 1747, loss = 1.75939295\n",
      "Iteration 1748, loss = 1.75930370\n",
      "Iteration 1749, loss = 1.75921448\n",
      "Iteration 1750, loss = 1.75912529\n",
      "Iteration 1751, loss = 1.75903613\n",
      "Iteration 1752, loss = 1.75894701\n",
      "Iteration 1753, loss = 1.75885791\n",
      "Iteration 1754, loss = 1.75876884\n",
      "Iteration 1755, loss = 1.75867981\n",
      "Iteration 1756, loss = 1.75859081\n",
      "Iteration 1757, loss = 1.75850183\n",
      "Iteration 1758, loss = 1.75841289\n",
      "Iteration 1759, loss = 1.75832398\n",
      "Iteration 1760, loss = 1.75823509\n",
      "Iteration 1761, loss = 1.75814624\n",
      "Iteration 1762, loss = 1.75805742\n",
      "Iteration 1763, loss = 1.75796863\n",
      "Iteration 1764, loss = 1.75787987\n",
      "Iteration 1765, loss = 1.75779114\n",
      "Iteration 1766, loss = 1.75770244\n",
      "Iteration 1767, loss = 1.75761377\n",
      "Iteration 1768, loss = 1.75752514\n",
      "Iteration 1769, loss = 1.75743653\n",
      "Iteration 1770, loss = 1.75734795\n",
      "Iteration 1771, loss = 1.75725940\n",
      "Iteration 1772, loss = 1.75717089\n",
      "Iteration 1773, loss = 1.75708240\n",
      "Iteration 1774, loss = 1.75699395\n",
      "Iteration 1775, loss = 1.75690552\n",
      "Iteration 1776, loss = 1.75681713\n",
      "Iteration 1777, loss = 1.75672876\n",
      "Iteration 1778, loss = 1.75664043\n",
      "Iteration 1779, loss = 1.75655212\n",
      "Iteration 1780, loss = 1.75646385\n",
      "Iteration 1781, loss = 1.75637561\n",
      "Iteration 1782, loss = 1.75628739\n",
      "Iteration 1783, loss = 1.75619921\n",
      "Iteration 1784, loss = 1.75611106\n",
      "Iteration 1785, loss = 1.75602294\n",
      "Iteration 1786, loss = 1.75593484\n",
      "Iteration 1787, loss = 1.75584678\n",
      "Iteration 1788, loss = 1.75575875\n",
      "Iteration 1789, loss = 1.75567075\n",
      "Iteration 1790, loss = 1.75558278\n",
      "Iteration 1791, loss = 1.75549484\n",
      "Iteration 1792, loss = 1.75540693\n",
      "Iteration 1793, loss = 1.75531905\n",
      "Iteration 1794, loss = 1.75523120\n",
      "Iteration 1795, loss = 1.75514338\n",
      "Iteration 1796, loss = 1.75505559\n",
      "Iteration 1797, loss = 1.75496783\n",
      "Iteration 1798, loss = 1.75488010\n",
      "Iteration 1799, loss = 1.75479240\n",
      "Iteration 1800, loss = 1.75470473\n",
      "Iteration 1801, loss = 1.75461709\n",
      "Iteration 1802, loss = 1.75452948\n",
      "Iteration 1803, loss = 1.75444190\n",
      "Iteration 1804, loss = 1.75435435\n",
      "Iteration 1805, loss = 1.75426683\n",
      "Iteration 1806, loss = 1.75417934\n",
      "Iteration 1807, loss = 1.75409188\n",
      "Iteration 1808, loss = 1.75400446\n",
      "Iteration 1809, loss = 1.75391706\n",
      "Iteration 1810, loss = 1.75382969\n",
      "Iteration 1811, loss = 1.75374235\n",
      "Iteration 1812, loss = 1.75365504\n",
      "Iteration 1813, loss = 1.75356776\n",
      "Iteration 1814, loss = 1.75348051\n",
      "Iteration 1815, loss = 1.75339329\n",
      "Iteration 1816, loss = 1.75330610\n",
      "Iteration 1817, loss = 1.75321894\n",
      "Iteration 1818, loss = 1.75313181\n",
      "Iteration 1819, loss = 1.75304471\n",
      "Iteration 1820, loss = 1.75295764\n",
      "Iteration 1821, loss = 1.75287060\n",
      "Iteration 1822, loss = 1.75278359\n",
      "Iteration 1823, loss = 1.75269661\n",
      "Iteration 1824, loss = 1.75260966\n",
      "Iteration 1825, loss = 1.75252274\n",
      "Iteration 1826, loss = 1.75243585\n",
      "Iteration 1827, loss = 1.75234899\n",
      "Iteration 1828, loss = 1.75226216\n",
      "Iteration 1829, loss = 1.75217536\n",
      "Iteration 1830, loss = 1.75208859\n",
      "Iteration 1831, loss = 1.75200185\n",
      "Iteration 1832, loss = 1.75191513\n",
      "Iteration 1833, loss = 1.75182845\n",
      "Iteration 1834, loss = 1.75174180\n",
      "Iteration 1835, loss = 1.75165518\n",
      "Iteration 1836, loss = 1.75156858\n",
      "Iteration 1837, loss = 1.75148202\n",
      "Iteration 1838, loss = 1.75139549\n",
      "Iteration 1839, loss = 1.75130898\n",
      "Iteration 1840, loss = 1.75122251\n",
      "Iteration 1841, loss = 1.75113606\n",
      "Iteration 1842, loss = 1.75104965\n",
      "Iteration 1843, loss = 1.75096326\n",
      "Iteration 1844, loss = 1.75087691\n",
      "Iteration 1845, loss = 1.75079058\n",
      "Iteration 1846, loss = 1.75070429\n",
      "Iteration 1847, loss = 1.75061802\n",
      "Iteration 1848, loss = 1.75053178\n",
      "Iteration 1849, loss = 1.75044557\n",
      "Iteration 1850, loss = 1.75035940\n",
      "Iteration 1851, loss = 1.75027325\n",
      "Iteration 1852, loss = 1.75018713\n",
      "Iteration 1853, loss = 1.75010104\n",
      "Iteration 1854, loss = 1.75001498\n",
      "Iteration 1855, loss = 1.74992895\n",
      "Iteration 1856, loss = 1.74984295\n",
      "Iteration 1857, loss = 1.74975697\n",
      "Iteration 1858, loss = 1.74967103\n",
      "Iteration 1859, loss = 1.74958512\n",
      "Iteration 1860, loss = 1.74949924\n",
      "Iteration 1861, loss = 1.74941338\n",
      "Iteration 1862, loss = 1.74932756\n",
      "Iteration 1863, loss = 1.74924176\n",
      "Iteration 1864, loss = 1.74915600\n",
      "Iteration 1865, loss = 1.74907026\n",
      "Iteration 1866, loss = 1.74898456\n",
      "Iteration 1867, loss = 1.74889888\n",
      "Iteration 1868, loss = 1.74881323\n",
      "Iteration 1869, loss = 1.74872761\n",
      "Iteration 1870, loss = 1.74864202\n",
      "Iteration 1871, loss = 1.74855646\n",
      "Iteration 1872, loss = 1.74847093\n",
      "Iteration 1873, loss = 1.74838543\n",
      "Iteration 1874, loss = 1.74829996\n",
      "Iteration 1875, loss = 1.74821452\n",
      "Iteration 1876, loss = 1.74812910\n",
      "Iteration 1877, loss = 1.74804372\n",
      "Iteration 1878, loss = 1.74795836\n",
      "Iteration 1879, loss = 1.74787304\n",
      "Iteration 1880, loss = 1.74778774\n",
      "Iteration 1881, loss = 1.74770248\n",
      "Iteration 1882, loss = 1.74761724\n",
      "Iteration 1883, loss = 1.74753203\n",
      "Iteration 1884, loss = 1.74744685\n",
      "Iteration 1885, loss = 1.74736170\n",
      "Iteration 1886, loss = 1.74727658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1887, loss = 1.74719149\n",
      "Iteration 1888, loss = 1.74710642\n",
      "Iteration 1889, loss = 1.74702139\n",
      "Iteration 1890, loss = 1.74693638\n",
      "Iteration 1891, loss = 1.74685141\n",
      "Iteration 1892, loss = 1.74676646\n",
      "Iteration 1893, loss = 1.74668155\n",
      "Iteration 1894, loss = 1.74659666\n",
      "Iteration 1895, loss = 1.74651180\n",
      "Iteration 1896, loss = 1.74642697\n",
      "Iteration 1897, loss = 1.74634217\n",
      "Iteration 1898, loss = 1.74625740\n",
      "Iteration 1899, loss = 1.74617265\n",
      "Iteration 1900, loss = 1.74608794\n",
      "Iteration 1901, loss = 1.74600326\n",
      "Iteration 1902, loss = 1.74591860\n",
      "Iteration 1903, loss = 1.74583397\n",
      "Iteration 1904, loss = 1.74574938\n",
      "Iteration 1905, loss = 1.74566481\n",
      "Iteration 1906, loss = 1.74558027\n",
      "Iteration 1907, loss = 1.74549576\n",
      "Iteration 1908, loss = 1.74541127\n",
      "Iteration 1909, loss = 1.74532682\n",
      "Iteration 1910, loss = 1.74524240\n",
      "Iteration 1911, loss = 1.74515800\n",
      "Iteration 1912, loss = 1.74507364\n",
      "Iteration 1913, loss = 1.74498930\n",
      "Iteration 1914, loss = 1.74490499\n",
      "Iteration 1915, loss = 1.74482071\n",
      "Iteration 1916, loss = 1.74473646\n",
      "Iteration 1917, loss = 1.74465224\n",
      "Iteration 1918, loss = 1.74456805\n",
      "Iteration 1919, loss = 1.74448388\n",
      "Iteration 1920, loss = 1.74439975\n",
      "Iteration 1921, loss = 1.74431564\n",
      "Iteration 1922, loss = 1.74423156\n",
      "Iteration 1923, loss = 1.74414751\n",
      "Iteration 1924, loss = 1.74406349\n",
      "Iteration 1925, loss = 1.74397950\n",
      "Iteration 1926, loss = 1.74389554\n",
      "Iteration 1927, loss = 1.74381160\n",
      "Iteration 1928, loss = 1.74372770\n",
      "Iteration 1929, loss = 1.74364382\n",
      "Iteration 1930, loss = 1.74355997\n",
      "Iteration 1931, loss = 1.74347616\n",
      "Iteration 1932, loss = 1.74339237\n",
      "Iteration 1933, loss = 1.74330860\n",
      "Iteration 1934, loss = 1.74322487\n",
      "Iteration 1935, loss = 1.74314117\n",
      "Iteration 1936, loss = 1.74305749\n",
      "Iteration 1937, loss = 1.74297384\n",
      "Iteration 1938, loss = 1.74289023\n",
      "Iteration 1939, loss = 1.74280664\n",
      "Iteration 1940, loss = 1.74272307\n",
      "Iteration 1941, loss = 1.74263954\n",
      "Iteration 1942, loss = 1.74255604\n",
      "Iteration 1943, loss = 1.74247256\n",
      "Iteration 1944, loss = 1.74238912\n",
      "Iteration 1945, loss = 1.74230570\n",
      "Iteration 1946, loss = 1.74222231\n",
      "Iteration 1947, loss = 1.74213895\n",
      "Iteration 1948, loss = 1.74205561\n",
      "Iteration 1949, loss = 1.74197231\n",
      "Iteration 1950, loss = 1.74188903\n",
      "Iteration 1951, loss = 1.74180579\n",
      "Iteration 1952, loss = 1.74172257\n",
      "Iteration 1953, loss = 1.74163938\n",
      "Iteration 1954, loss = 1.74155622\n",
      "Iteration 1955, loss = 1.74147308\n",
      "Iteration 1956, loss = 1.74138998\n",
      "Iteration 1957, loss = 1.74130690\n",
      "Iteration 1958, loss = 1.74122385\n",
      "Iteration 1959, loss = 1.74114084\n",
      "Iteration 1960, loss = 1.74105784\n",
      "Iteration 1961, loss = 1.74097488\n",
      "Iteration 1962, loss = 1.74089195\n",
      "Iteration 1963, loss = 1.74080904\n",
      "Iteration 1964, loss = 1.74072616\n",
      "Iteration 1965, loss = 1.74064331\n",
      "Iteration 1966, loss = 1.74056049\n",
      "Iteration 1967, loss = 1.74047770\n",
      "Iteration 1968, loss = 1.74039494\n",
      "Iteration 1969, loss = 1.74031220\n",
      "Iteration 1970, loss = 1.74022949\n",
      "Iteration 1971, loss = 1.74014681\n",
      "Iteration 1972, loss = 1.74006416\n",
      "Iteration 1973, loss = 1.73998154\n",
      "Iteration 1974, loss = 1.73989895\n",
      "Iteration 1975, loss = 1.73981638\n",
      "Iteration 1976, loss = 1.73973384\n",
      "Iteration 1977, loss = 1.73965133\n",
      "Iteration 1978, loss = 1.73956885\n",
      "Iteration 1979, loss = 1.73948640\n",
      "Iteration 1980, loss = 1.73940397\n",
      "Iteration 1981, loss = 1.73932157\n",
      "Iteration 1982, loss = 1.73923921\n",
      "Iteration 1983, loss = 1.73915686\n",
      "Iteration 1984, loss = 1.73907455\n",
      "Iteration 1985, loss = 1.73899227\n",
      "Iteration 1986, loss = 1.73891001\n",
      "Iteration 1987, loss = 1.73882778\n",
      "Iteration 1988, loss = 1.73874558\n",
      "Iteration 1989, loss = 1.73866341\n",
      "Iteration 1990, loss = 1.73858127\n",
      "Iteration 1991, loss = 1.73849915\n",
      "Iteration 1992, loss = 1.73841706\n",
      "Iteration 1993, loss = 1.73833501\n",
      "Iteration 1994, loss = 1.73825297\n",
      "Iteration 1995, loss = 1.73817097\n",
      "Iteration 1996, loss = 1.73808899\n",
      "Iteration 1997, loss = 1.73800705\n",
      "Iteration 1998, loss = 1.73792513\n",
      "Iteration 1999, loss = 1.73784324\n",
      "Iteration 2000, loss = 1.73776137\n",
      "Iteration 2001, loss = 1.73767954\n",
      "Iteration 2002, loss = 1.73759773\n",
      "Iteration 2003, loss = 1.73751595\n",
      "Iteration 2004, loss = 1.73743420\n",
      "Iteration 2005, loss = 1.73735248\n",
      "Iteration 2006, loss = 1.73727078\n",
      "Iteration 2007, loss = 1.73718911\n",
      "Iteration 2008, loss = 1.73710747\n",
      "Iteration 2009, loss = 1.73702586\n",
      "Iteration 2010, loss = 1.73694428\n",
      "Iteration 2011, loss = 1.73686272\n",
      "Iteration 2012, loss = 1.73678119\n",
      "Iteration 2013, loss = 1.73669969\n",
      "Iteration 2014, loss = 1.73661822\n",
      "Iteration 2015, loss = 1.73653677\n",
      "Iteration 2016, loss = 1.73645536\n",
      "Iteration 2017, loss = 1.73637397\n",
      "Iteration 2018, loss = 1.73629261\n",
      "Iteration 2019, loss = 1.73621127\n",
      "Iteration 2020, loss = 1.73612997\n",
      "Iteration 2021, loss = 1.73604869\n",
      "Iteration 2022, loss = 1.73596744\n",
      "Iteration 2023, loss = 1.73588622\n",
      "Iteration 2024, loss = 1.73580502\n",
      "Iteration 2025, loss = 1.73572386\n",
      "Iteration 2026, loss = 1.73564272\n",
      "Iteration 2027, loss = 1.73556161\n",
      "Iteration 2028, loss = 1.73548052\n",
      "Iteration 2029, loss = 1.73539947\n",
      "Iteration 2030, loss = 1.73531844\n",
      "Iteration 2031, loss = 1.73523744\n",
      "Iteration 2032, loss = 1.73515646\n",
      "Iteration 2033, loss = 1.73507552\n",
      "Iteration 2034, loss = 1.73499460\n",
      "Iteration 2035, loss = 1.73491371\n",
      "Iteration 2036, loss = 1.73483285\n",
      "Iteration 2037, loss = 1.73475201\n",
      "Iteration 2038, loss = 1.73467121\n",
      "Iteration 2039, loss = 1.73459043\n",
      "Iteration 2040, loss = 1.73450967\n",
      "Iteration 2041, loss = 1.73442895\n",
      "Iteration 2042, loss = 1.73434825\n",
      "Iteration 2043, loss = 1.73426758\n",
      "Iteration 2044, loss = 1.73418694\n",
      "Iteration 2045, loss = 1.73410633\n",
      "Iteration 2046, loss = 1.73402574\n",
      "Iteration 2047, loss = 1.73394518\n",
      "Iteration 2048, loss = 1.73386465\n",
      "Iteration 2049, loss = 1.73378415\n",
      "Iteration 2050, loss = 1.73370367\n",
      "Iteration 2051, loss = 1.73362322\n",
      "Iteration 2052, loss = 1.73354280\n",
      "Iteration 2053, loss = 1.73346240\n",
      "Iteration 2054, loss = 1.73338204\n",
      "Iteration 2055, loss = 1.73330170\n",
      "Iteration 2056, loss = 1.73322139\n",
      "Iteration 2057, loss = 1.73314110\n",
      "Iteration 2058, loss = 1.73306084\n",
      "Iteration 2059, loss = 1.73298061\n",
      "Iteration 2060, loss = 1.73290041\n",
      "Iteration 2061, loss = 1.73282024\n",
      "Iteration 2062, loss = 1.73274009\n",
      "Iteration 2063, loss = 1.73265997\n",
      "Iteration 2064, loss = 1.73257988\n",
      "Iteration 2065, loss = 1.73249981\n",
      "Iteration 2066, loss = 1.73241977\n",
      "Iteration 2067, loss = 1.73233976\n",
      "Iteration 2068, loss = 1.73225978\n",
      "Iteration 2069, loss = 1.73217982\n",
      "Iteration 2070, loss = 1.73209989\n",
      "Iteration 2071, loss = 1.73201999\n",
      "Iteration 2072, loss = 1.73194012\n",
      "Iteration 2073, loss = 1.73186027\n",
      "Iteration 2074, loss = 1.73178045\n",
      "Iteration 2075, loss = 1.73170066\n",
      "Iteration 2076, loss = 1.73162089\n",
      "Iteration 2077, loss = 1.73154115\n",
      "Iteration 2078, loss = 1.73146144\n",
      "Iteration 2079, loss = 1.73138176\n",
      "Iteration 2080, loss = 1.73130210\n",
      "Iteration 2081, loss = 1.73122247\n",
      "Iteration 2082, loss = 1.73114287\n",
      "Iteration 2083, loss = 1.73106329\n",
      "Iteration 2084, loss = 1.73098375\n",
      "Iteration 2085, loss = 1.73090423\n",
      "Iteration 2086, loss = 1.73082473\n",
      "Iteration 2087, loss = 1.73074526\n",
      "Iteration 2088, loss = 1.73066583\n",
      "Iteration 2089, loss = 1.73058641\n",
      "Iteration 2090, loss = 1.73050703\n",
      "Iteration 2091, loss = 1.73042767\n",
      "Iteration 2092, loss = 1.73034834\n",
      "Iteration 2093, loss = 1.73026903\n",
      "Iteration 2094, loss = 1.73018976\n",
      "Iteration 2095, loss = 1.73011051\n",
      "Iteration 2096, loss = 1.73003128\n",
      "Iteration 2097, loss = 1.72995209\n",
      "Iteration 2098, loss = 1.72987292\n",
      "Iteration 2099, loss = 1.72979378\n",
      "Iteration 2100, loss = 1.72971466\n",
      "Iteration 2101, loss = 1.72963557\n",
      "Iteration 2102, loss = 1.72955651\n",
      "Iteration 2103, loss = 1.72947748\n",
      "Iteration 2104, loss = 1.72939847\n",
      "Iteration 2105, loss = 1.72931949\n",
      "Iteration 2106, loss = 1.72924054\n",
      "Iteration 2107, loss = 1.72916161\n",
      "Iteration 2108, loss = 1.72908271\n",
      "Iteration 2109, loss = 1.72900384\n",
      "Iteration 2110, loss = 1.72892499\n",
      "Iteration 2111, loss = 1.72884617\n",
      "Iteration 2112, loss = 1.72876738\n",
      "Iteration 2113, loss = 1.72868862\n",
      "Iteration 2114, loss = 1.72860988\n",
      "Iteration 2115, loss = 1.72853117\n",
      "Iteration 2116, loss = 1.72845248\n",
      "Iteration 2117, loss = 1.72837383\n",
      "Iteration 2118, loss = 1.72829519\n",
      "Iteration 2119, loss = 1.72821659\n",
      "Iteration 2120, loss = 1.72813801\n",
      "Iteration 2121, loss = 1.72805946\n",
      "Iteration 2122, loss = 1.72798094\n",
      "Iteration 2123, loss = 1.72790244\n",
      "Iteration 2124, loss = 1.72782397\n",
      "Iteration 2125, loss = 1.72774553\n",
      "Iteration 2126, loss = 1.72766711\n",
      "Iteration 2127, loss = 1.72758872\n",
      "Iteration 2128, loss = 1.72751036\n",
      "Iteration 2129, loss = 1.72743202\n",
      "Iteration 2130, loss = 1.72735371\n",
      "Iteration 2131, loss = 1.72727543\n",
      "Iteration 2132, loss = 1.72719717\n",
      "Iteration 2133, loss = 1.72711894\n",
      "Iteration 2134, loss = 1.72704074\n",
      "Iteration 2135, loss = 1.72696256\n",
      "Iteration 2136, loss = 1.72688441\n",
      "Iteration 2137, loss = 1.72680629\n",
      "Iteration 2138, loss = 1.72672819\n",
      "Iteration 2139, loss = 1.72665012\n",
      "Iteration 2140, loss = 1.72657208\n",
      "Iteration 2141, loss = 1.72649406\n",
      "Iteration 2142, loss = 1.72641607\n",
      "Iteration 2143, loss = 1.72633811\n",
      "Iteration 2144, loss = 1.72626017\n",
      "Iteration 2145, loss = 1.72618226\n",
      "Iteration 2146, loss = 1.72610438\n",
      "Iteration 2147, loss = 1.72602652\n",
      "Iteration 2148, loss = 1.72594869\n",
      "Iteration 2149, loss = 1.72587088\n",
      "Iteration 2150, loss = 1.72579310\n",
      "Iteration 2151, loss = 1.72571535\n",
      "Iteration 2152, loss = 1.72563763\n",
      "Iteration 2153, loss = 1.72555993\n",
      "Iteration 2154, loss = 1.72548226\n",
      "Iteration 2155, loss = 1.72540461\n",
      "Iteration 2156, loss = 1.72532699\n",
      "Iteration 2157, loss = 1.72524940\n",
      "Iteration 2158, loss = 1.72517183\n",
      "Iteration 2159, loss = 1.72509429\n",
      "Iteration 2160, loss = 1.72501678\n",
      "Iteration 2161, loss = 1.72493929\n",
      "Iteration 2162, loss = 1.72486183\n",
      "Iteration 2163, loss = 1.72478440\n",
      "Iteration 2164, loss = 1.72470699\n",
      "Iteration 2165, loss = 1.72462961\n",
      "Iteration 2166, loss = 1.72455225\n",
      "Iteration 2167, loss = 1.72447492\n",
      "Iteration 2168, loss = 1.72439762\n",
      "Iteration 2169, loss = 1.72432034\n",
      "Iteration 2170, loss = 1.72424309\n",
      "Iteration 2171, loss = 1.72416587\n",
      "Iteration 2172, loss = 1.72408867\n",
      "Iteration 2173, loss = 1.72401150\n",
      "Iteration 2174, loss = 1.72393436\n",
      "Iteration 2175, loss = 1.72385724\n",
      "Iteration 2176, loss = 1.72378014\n",
      "Iteration 2177, loss = 1.72370308\n",
      "Iteration 2178, loss = 1.72362604\n",
      "Iteration 2179, loss = 1.72354902\n",
      "Iteration 2180, loss = 1.72347204\n",
      "Iteration 2181, loss = 1.72339508\n",
      "Iteration 2182, loss = 1.72331814\n",
      "Iteration 2183, loss = 1.72324123\n",
      "Iteration 2184, loss = 1.72316435\n",
      "Iteration 2185, loss = 1.72308749\n",
      "Iteration 2186, loss = 1.72301066\n",
      "Iteration 2187, loss = 1.72293386\n",
      "Iteration 2188, loss = 1.72285708\n",
      "Iteration 2189, loss = 1.72278033\n",
      "Iteration 2190, loss = 1.72270360\n",
      "Iteration 2191, loss = 1.72262690\n",
      "Iteration 2192, loss = 1.72255023\n",
      "Iteration 2193, loss = 1.72247358\n",
      "Iteration 2194, loss = 1.72239696\n",
      "Iteration 2195, loss = 1.72232036\n",
      "Iteration 2196, loss = 1.72224379\n",
      "Iteration 2197, loss = 1.72216725\n",
      "Iteration 2198, loss = 1.72209073\n",
      "Iteration 2199, loss = 1.72201424\n",
      "Iteration 2200, loss = 1.72193778\n",
      "Iteration 2201, loss = 1.72186134\n",
      "Iteration 2202, loss = 1.72178492\n",
      "Iteration 2203, loss = 1.72170854\n",
      "Iteration 2204, loss = 1.72163217\n",
      "Iteration 2205, loss = 1.72155584\n",
      "Iteration 2206, loss = 1.72147953\n",
      "Iteration 2207, loss = 1.72140325\n",
      "Iteration 2208, loss = 1.72132699\n",
      "Iteration 2209, loss = 1.72125076\n",
      "Iteration 2210, loss = 1.72117455\n",
      "Iteration 2211, loss = 1.72109837\n",
      "Iteration 2212, loss = 1.72102222\n",
      "Iteration 2213, loss = 1.72094609\n",
      "Iteration 2214, loss = 1.72086999\n",
      "Iteration 2215, loss = 1.72079391\n",
      "Iteration 2216, loss = 1.72071786\n",
      "Iteration 2217, loss = 1.72064184\n",
      "Iteration 2218, loss = 1.72056584\n",
      "Iteration 2219, loss = 1.72048987\n",
      "Iteration 2220, loss = 1.72041392\n",
      "Iteration 2221, loss = 1.72033800\n",
      "Iteration 2222, loss = 1.72026210\n",
      "Iteration 2223, loss = 1.72018623\n",
      "Iteration 2224, loss = 1.72011039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2225, loss = 1.72003457\n",
      "Iteration 2226, loss = 1.71995878\n",
      "Iteration 2227, loss = 1.71988301\n",
      "Iteration 2228, loss = 1.71980727\n",
      "Iteration 2229, loss = 1.71973156\n",
      "Iteration 2230, loss = 1.71965587\n",
      "Iteration 2231, loss = 1.71958020\n",
      "Iteration 2232, loss = 1.71950457\n",
      "Iteration 2233, loss = 1.71942895\n",
      "Iteration 2234, loss = 1.71935337\n",
      "Iteration 2235, loss = 1.71927781\n",
      "Iteration 2236, loss = 1.71920227\n",
      "Iteration 2237, loss = 1.71912676\n",
      "Iteration 2238, loss = 1.71905128\n",
      "Iteration 2239, loss = 1.71897582\n",
      "Iteration 2240, loss = 1.71890039\n",
      "Iteration 2241, loss = 1.71882498\n",
      "Iteration 2242, loss = 1.71874960\n",
      "Iteration 2243, loss = 1.71867425\n",
      "Iteration 2244, loss = 1.71859892\n",
      "Iteration 2245, loss = 1.71852361\n",
      "Iteration 2246, loss = 1.71844833\n",
      "Iteration 2247, loss = 1.71837308\n",
      "Iteration 2248, loss = 1.71829785\n",
      "Iteration 2249, loss = 1.71822265\n",
      "Iteration 2250, loss = 1.71814748\n",
      "Iteration 2251, loss = 1.71807232\n",
      "Iteration 2252, loss = 1.71799720\n",
      "Iteration 2253, loss = 1.71792210\n",
      "Iteration 2254, loss = 1.71784702\n",
      "Iteration 2255, loss = 1.71777198\n",
      "Iteration 2256, loss = 1.71769695\n",
      "Iteration 2257, loss = 1.71762195\n",
      "Iteration 2258, loss = 1.71754698\n",
      "Iteration 2259, loss = 1.71747204\n",
      "Iteration 2260, loss = 1.71739711\n",
      "Iteration 2261, loss = 1.71732222\n",
      "Iteration 2262, loss = 1.71724735\n",
      "Iteration 2263, loss = 1.71717250\n",
      "Iteration 2264, loss = 1.71709768\n",
      "Iteration 2265, loss = 1.71702289\n",
      "Iteration 2266, loss = 1.71694812\n",
      "Iteration 2267, loss = 1.71687338\n",
      "Iteration 2268, loss = 1.71679866\n",
      "Iteration 2269, loss = 1.71672396\n",
      "Iteration 2270, loss = 1.71664930\n",
      "Iteration 2271, loss = 1.71657466\n",
      "Iteration 2272, loss = 1.71650004\n",
      "Iteration 2273, loss = 1.71642545\n",
      "Iteration 2274, loss = 1.71635088\n",
      "Iteration 2275, loss = 1.71627634\n",
      "Iteration 2276, loss = 1.71620182\n",
      "Iteration 2277, loss = 1.71612733\n",
      "Iteration 2278, loss = 1.71605287\n",
      "Iteration 2279, loss = 1.71597843\n",
      "Iteration 2280, loss = 1.71590402\n",
      "Iteration 2281, loss = 1.71582963\n",
      "Iteration 2282, loss = 1.71575526\n",
      "Iteration 2283, loss = 1.71568092\n",
      "Iteration 2284, loss = 1.71560661\n",
      "Iteration 2285, loss = 1.71553232\n",
      "Iteration 2286, loss = 1.71545806\n",
      "Iteration 2287, loss = 1.71538382\n",
      "Iteration 2288, loss = 1.71530961\n",
      "Iteration 2289, loss = 1.71523542\n",
      "Iteration 2290, loss = 1.71516126\n",
      "Iteration 2291, loss = 1.71508712\n",
      "Iteration 2292, loss = 1.71501301\n",
      "Iteration 2293, loss = 1.71493892\n",
      "Iteration 2294, loss = 1.71486486\n",
      "Iteration 2295, loss = 1.71479083\n",
      "Iteration 2296, loss = 1.71471681\n",
      "Iteration 2297, loss = 1.71464283\n",
      "Iteration 2298, loss = 1.71456887\n",
      "Iteration 2299, loss = 1.71449493\n",
      "Iteration 2300, loss = 1.71442102\n",
      "Iteration 2301, loss = 1.71434713\n",
      "Iteration 2302, loss = 1.71427327\n",
      "Iteration 2303, loss = 1.71419944\n",
      "Iteration 2304, loss = 1.71412563\n",
      "Iteration 2305, loss = 1.71405184\n",
      "Iteration 2306, loss = 1.71397808\n",
      "Iteration 2307, loss = 1.71390435\n",
      "Iteration 2308, loss = 1.71383063\n",
      "Iteration 2309, loss = 1.71375695\n",
      "Iteration 2310, loss = 1.71368329\n",
      "Iteration 2311, loss = 1.71360965\n",
      "Iteration 2312, loss = 1.71353604\n",
      "Iteration 2313, loss = 1.71346246\n",
      "Iteration 2314, loss = 1.71338890\n",
      "Iteration 2315, loss = 1.71331536\n",
      "Iteration 2316, loss = 1.71324185\n",
      "Iteration 2317, loss = 1.71316836\n",
      "Iteration 2318, loss = 1.71309490\n",
      "Iteration 2319, loss = 1.71302147\n",
      "Iteration 2320, loss = 1.71294805\n",
      "Iteration 2321, loss = 1.71287467\n",
      "Iteration 2322, loss = 1.71280131\n",
      "Iteration 2323, loss = 1.71272797\n",
      "Iteration 2324, loss = 1.71265466\n",
      "Iteration 2325, loss = 1.71258137\n",
      "Iteration 2326, loss = 1.71250811\n",
      "Iteration 2327, loss = 1.71243487\n",
      "Iteration 2328, loss = 1.71236166\n",
      "Iteration 2329, loss = 1.71228847\n",
      "Iteration 2330, loss = 1.71221531\n",
      "Iteration 2331, loss = 1.71214217\n",
      "Iteration 2332, loss = 1.71206906\n",
      "Iteration 2333, loss = 1.71199597\n",
      "Iteration 2334, loss = 1.71192291\n",
      "Iteration 2335, loss = 1.71184987\n",
      "Iteration 2336, loss = 1.71177685\n",
      "Iteration 2337, loss = 1.71170386\n",
      "Iteration 2338, loss = 1.71163090\n",
      "Iteration 2339, loss = 1.71155796\n",
      "Iteration 2340, loss = 1.71148504\n",
      "Iteration 2341, loss = 1.71141215\n",
      "Iteration 2342, loss = 1.71133929\n",
      "Iteration 2343, loss = 1.71126645\n",
      "Iteration 2344, loss = 1.71119363\n",
      "Iteration 2345, loss = 1.71112084\n",
      "Iteration 2346, loss = 1.71104807\n",
      "Iteration 2347, loss = 1.71097533\n",
      "Iteration 2348, loss = 1.71090261\n",
      "Iteration 2349, loss = 1.71082992\n",
      "Iteration 2350, loss = 1.71075725\n",
      "Iteration 2351, loss = 1.71068461\n",
      "Iteration 2352, loss = 1.71061199\n",
      "Iteration 2353, loss = 1.71053939\n",
      "Iteration 2354, loss = 1.71046682\n",
      "Iteration 2355, loss = 1.71039428\n",
      "Iteration 2356, loss = 1.71032176\n",
      "Iteration 2357, loss = 1.71024926\n",
      "Iteration 2358, loss = 1.71017679\n",
      "Iteration 2359, loss = 1.71010434\n",
      "Iteration 2360, loss = 1.71003192\n",
      "Iteration 2361, loss = 1.70995952\n",
      "Iteration 2362, loss = 1.70988714\n",
      "Iteration 2363, loss = 1.70981480\n",
      "Iteration 2364, loss = 1.70974247\n",
      "Iteration 2365, loss = 1.70967017\n",
      "Iteration 2366, loss = 1.70959789\n",
      "Iteration 2367, loss = 1.70952564\n",
      "Iteration 2368, loss = 1.70945342\n",
      "Iteration 2369, loss = 1.70938121\n",
      "Iteration 2370, loss = 1.70930904\n",
      "Iteration 2371, loss = 1.70923688\n",
      "Iteration 2372, loss = 1.70916475\n",
      "Iteration 2373, loss = 1.70909265\n",
      "Iteration 2374, loss = 1.70902057\n",
      "Iteration 2375, loss = 1.70894851\n",
      "Iteration 2376, loss = 1.70887648\n",
      "Iteration 2377, loss = 1.70880447\n",
      "Iteration 2378, loss = 1.70873249\n",
      "Iteration 2379, loss = 1.70866053\n",
      "Iteration 2380, loss = 1.70858860\n",
      "Iteration 2381, loss = 1.70851669\n",
      "Iteration 2382, loss = 1.70844480\n",
      "Iteration 2383, loss = 1.70837294\n",
      "Iteration 2384, loss = 1.70830111\n",
      "Iteration 2385, loss = 1.70822929\n",
      "Iteration 2386, loss = 1.70815751\n",
      "Iteration 2387, loss = 1.70808574\n",
      "Iteration 2388, loss = 1.70801400\n",
      "Iteration 2389, loss = 1.70794229\n",
      "Iteration 2390, loss = 1.70787060\n",
      "Iteration 2391, loss = 1.70779893\n",
      "Iteration 2392, loss = 1.70772729\n",
      "Iteration 2393, loss = 1.70765567\n",
      "Iteration 2394, loss = 1.70758408\n",
      "Iteration 2395, loss = 1.70751251\n",
      "Iteration 2396, loss = 1.70744096\n",
      "Iteration 2397, loss = 1.70736944\n",
      "Iteration 2398, loss = 1.70729794\n",
      "Iteration 2399, loss = 1.70722647\n",
      "Iteration 2400, loss = 1.70715502\n",
      "Iteration 2401, loss = 1.70708360\n",
      "Iteration 2402, loss = 1.70701220\n",
      "Iteration 2403, loss = 1.70694082\n",
      "Iteration 2404, loss = 1.70686947\n",
      "Iteration 2405, loss = 1.70679814\n",
      "Iteration 2406, loss = 1.70672684\n",
      "Iteration 2407, loss = 1.70665556\n",
      "Iteration 2408, loss = 1.70658430\n",
      "Iteration 2409, loss = 1.70651307\n",
      "Iteration 2410, loss = 1.70644186\n",
      "Iteration 2411, loss = 1.70637068\n",
      "Iteration 2412, loss = 1.70629952\n",
      "Iteration 2413, loss = 1.70622838\n",
      "Iteration 2414, loss = 1.70615727\n",
      "Iteration 2415, loss = 1.70608619\n",
      "Iteration 2416, loss = 1.70601512\n",
      "Iteration 2417, loss = 1.70594408\n",
      "Iteration 2418, loss = 1.70587307\n",
      "Iteration 2419, loss = 1.70580208\n",
      "Iteration 2420, loss = 1.70573111\n",
      "Iteration 2421, loss = 1.70566017\n",
      "Iteration 2422, loss = 1.70558925\n",
      "Iteration 2423, loss = 1.70551835\n",
      "Iteration 2424, loss = 1.70544748\n",
      "Iteration 2425, loss = 1.70537663\n",
      "Iteration 2426, loss = 1.70530581\n",
      "Iteration 2427, loss = 1.70523501\n",
      "Iteration 2428, loss = 1.70516424\n",
      "Iteration 2429, loss = 1.70509348\n",
      "Iteration 2430, loss = 1.70502276\n",
      "Iteration 2431, loss = 1.70495205\n",
      "Iteration 2432, loss = 1.70488137\n",
      "Iteration 2433, loss = 1.70481072\n",
      "Iteration 2434, loss = 1.70474009\n",
      "Iteration 2435, loss = 1.70466948\n",
      "Iteration 2436, loss = 1.70459889\n",
      "Iteration 2437, loss = 1.70452833\n",
      "Iteration 2438, loss = 1.70445780\n",
      "Iteration 2439, loss = 1.70438728\n",
      "Iteration 2440, loss = 1.70431679\n",
      "Iteration 2441, loss = 1.70424633\n",
      "Iteration 2442, loss = 1.70417589\n",
      "Iteration 2443, loss = 1.70410547\n",
      "Iteration 2444, loss = 1.70403508\n",
      "Iteration 2445, loss = 1.70396471\n",
      "Iteration 2446, loss = 1.70389436\n",
      "Iteration 2447, loss = 1.70382404\n",
      "Iteration 2448, loss = 1.70375374\n",
      "Iteration 2449, loss = 1.70368346\n",
      "Iteration 2450, loss = 1.70361321\n",
      "Iteration 2451, loss = 1.70354299\n",
      "Iteration 2452, loss = 1.70347278\n",
      "Iteration 2453, loss = 1.70340260\n",
      "Iteration 2454, loss = 1.70333245\n",
      "Iteration 2455, loss = 1.70326231\n",
      "Iteration 2456, loss = 1.70319220\n",
      "Iteration 2457, loss = 1.70312212\n",
      "Iteration 2458, loss = 1.70305206\n",
      "Iteration 2459, loss = 1.70298202\n",
      "Iteration 2460, loss = 1.70291200\n",
      "Iteration 2461, loss = 1.70284201\n",
      "Iteration 2462, loss = 1.70277205\n",
      "Iteration 2463, loss = 1.70270210\n",
      "Iteration 2464, loss = 1.70263218\n",
      "Iteration 2465, loss = 1.70256229\n",
      "Iteration 2466, loss = 1.70249241\n",
      "Iteration 2467, loss = 1.70242257\n",
      "Iteration 2468, loss = 1.70235274\n",
      "Iteration 2469, loss = 1.70228294\n",
      "Iteration 2470, loss = 1.70221316\n",
      "Iteration 2471, loss = 1.70214341\n",
      "Iteration 2472, loss = 1.70207367\n",
      "Iteration 2473, loss = 1.70200397\n",
      "Iteration 2474, loss = 1.70193428\n",
      "Iteration 2475, loss = 1.70186462\n",
      "Iteration 2476, loss = 1.70179498\n",
      "Iteration 2477, loss = 1.70172537\n",
      "Iteration 2478, loss = 1.70165578\n",
      "Iteration 2479, loss = 1.70158621\n",
      "Iteration 2480, loss = 1.70151667\n",
      "Iteration 2481, loss = 1.70144715\n",
      "Iteration 2482, loss = 1.70137765\n",
      "Iteration 2483, loss = 1.70130818\n",
      "Iteration 2484, loss = 1.70123873\n",
      "Iteration 2485, loss = 1.70116931\n",
      "Iteration 2486, loss = 1.70109990\n",
      "Iteration 2487, loss = 1.70103052\n",
      "Iteration 2488, loss = 1.70096117\n",
      "Iteration 2489, loss = 1.70089184\n",
      "Iteration 2490, loss = 1.70082253\n",
      "Iteration 2491, loss = 1.70075324\n",
      "Iteration 2492, loss = 1.70068398\n",
      "Iteration 2493, loss = 1.70061474\n",
      "Iteration 2494, loss = 1.70054553\n",
      "Iteration 2495, loss = 1.70047633\n",
      "Iteration 2496, loss = 1.70040716\n",
      "Iteration 2497, loss = 1.70033802\n",
      "Iteration 2498, loss = 1.70026890\n",
      "Iteration 2499, loss = 1.70019980\n",
      "Iteration 2500, loss = 1.70013072\n",
      "Iteration 2501, loss = 1.70006167\n",
      "Iteration 2502, loss = 1.69999264\n",
      "Iteration 2503, loss = 1.69992364\n",
      "Iteration 2504, loss = 1.69985465\n",
      "Iteration 2505, loss = 1.69978569\n",
      "Iteration 2506, loss = 1.69971676\n",
      "Iteration 2507, loss = 1.69964784\n",
      "Iteration 2508, loss = 1.69957896\n",
      "Iteration 2509, loss = 1.69951009\n",
      "Iteration 2510, loss = 1.69944125\n",
      "Iteration 2511, loss = 1.69937243\n",
      "Iteration 2512, loss = 1.69930363\n",
      "Iteration 2513, loss = 1.69923486\n",
      "Iteration 2514, loss = 1.69916611\n",
      "Iteration 2515, loss = 1.69909738\n",
      "Iteration 2516, loss = 1.69902868\n",
      "Iteration 2517, loss = 1.69895999\n",
      "Iteration 2518, loss = 1.69889134\n",
      "Iteration 2519, loss = 1.69882270\n",
      "Iteration 2520, loss = 1.69875409\n",
      "Iteration 2521, loss = 1.69868550\n",
      "Iteration 2522, loss = 1.69861694\n",
      "Iteration 2523, loss = 1.69854840\n",
      "Iteration 2524, loss = 1.69847988\n",
      "Iteration 2525, loss = 1.69841138\n",
      "Iteration 2526, loss = 1.69834291\n",
      "Iteration 2527, loss = 1.69827446\n",
      "Iteration 2528, loss = 1.69820603\n",
      "Iteration 2529, loss = 1.69813763\n",
      "Iteration 2530, loss = 1.69806925\n",
      "Iteration 2531, loss = 1.69800089\n",
      "Iteration 2532, loss = 1.69793256\n",
      "Iteration 2533, loss = 1.69786424\n",
      "Iteration 2534, loss = 1.69779596\n",
      "Iteration 2535, loss = 1.69772769\n",
      "Iteration 2536, loss = 1.69765945\n",
      "Iteration 2537, loss = 1.69759123\n",
      "Iteration 2538, loss = 1.69752303\n",
      "Iteration 2539, loss = 1.69745486\n",
      "Iteration 2540, loss = 1.69738671\n",
      "Iteration 2541, loss = 1.69731858\n",
      "Iteration 2542, loss = 1.69725048\n",
      "Iteration 2543, loss = 1.69718239\n",
      "Iteration 2544, loss = 1.69711434\n",
      "Iteration 2545, loss = 1.69704630\n",
      "Iteration 2546, loss = 1.69697829\n",
      "Iteration 2547, loss = 1.69691030\n",
      "Iteration 2548, loss = 1.69684233\n",
      "Iteration 2549, loss = 1.69677439\n",
      "Iteration 2550, loss = 1.69670646\n",
      "Iteration 2551, loss = 1.69663857\n",
      "Iteration 2552, loss = 1.69657069\n",
      "Iteration 2553, loss = 1.69650284\n",
      "Iteration 2554, loss = 1.69643501\n",
      "Iteration 2555, loss = 1.69636720\n",
      "Iteration 2556, loss = 1.69629942\n",
      "Iteration 2557, loss = 1.69623165\n",
      "Iteration 2558, loss = 1.69616392\n",
      "Iteration 2559, loss = 1.69609620\n",
      "Iteration 2560, loss = 1.69602851\n",
      "Iteration 2561, loss = 1.69596084\n",
      "Iteration 2562, loss = 1.69589319\n",
      "Iteration 2563, loss = 1.69582556\n",
      "Iteration 2564, loss = 1.69575796\n",
      "Iteration 2565, loss = 1.69569038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2566, loss = 1.69562283\n",
      "Iteration 2567, loss = 1.69555529\n",
      "Iteration 2568, loss = 1.69548778\n",
      "Iteration 2569, loss = 1.69542029\n",
      "Iteration 2570, loss = 1.69535283\n",
      "Iteration 2571, loss = 1.69528538\n",
      "Iteration 2572, loss = 1.69521796\n",
      "Iteration 2573, loss = 1.69515057\n",
      "Iteration 2574, loss = 1.69508319\n",
      "Iteration 2575, loss = 1.69501584\n",
      "Iteration 2576, loss = 1.69494851\n",
      "Iteration 2577, loss = 1.69488120\n",
      "Iteration 2578, loss = 1.69481392\n",
      "Iteration 2579, loss = 1.69474666\n",
      "Iteration 2580, loss = 1.69467942\n",
      "Iteration 2581, loss = 1.69461220\n",
      "Iteration 2582, loss = 1.69454501\n",
      "Iteration 2583, loss = 1.69447784\n",
      "Iteration 2584, loss = 1.69441069\n",
      "Iteration 2585, loss = 1.69434356\n",
      "Iteration 2586, loss = 1.69427646\n",
      "Iteration 2587, loss = 1.69420938\n",
      "Iteration 2588, loss = 1.69414232\n",
      "Iteration 2589, loss = 1.69407528\n",
      "Iteration 2590, loss = 1.69400827\n",
      "Iteration 2591, loss = 1.69394128\n",
      "Iteration 2592, loss = 1.69387431\n",
      "Iteration 2593, loss = 1.69380737\n",
      "Iteration 2594, loss = 1.69374044\n",
      "Iteration 2595, loss = 1.69367354\n",
      "Iteration 2596, loss = 1.69360666\n",
      "Iteration 2597, loss = 1.69353981\n",
      "Iteration 2598, loss = 1.69347297\n",
      "Iteration 2599, loss = 1.69340616\n",
      "Iteration 2600, loss = 1.69333937\n",
      "Iteration 2601, loss = 1.69327261\n",
      "Iteration 2602, loss = 1.69320587\n",
      "Iteration 2603, loss = 1.69313914\n",
      "Iteration 2604, loss = 1.69307245\n",
      "Iteration 2605, loss = 1.69300577\n",
      "Iteration 2606, loss = 1.69293912\n",
      "Iteration 2607, loss = 1.69287248\n",
      "Iteration 2608, loss = 1.69280588\n",
      "Iteration 2609, loss = 1.69273929\n",
      "Iteration 2610, loss = 1.69267272\n",
      "Iteration 2611, loss = 1.69260618\n",
      "Iteration 2612, loss = 1.69253966\n",
      "Iteration 2613, loss = 1.69247317\n",
      "Iteration 2614, loss = 1.69240669\n",
      "Iteration 2615, loss = 1.69234024\n",
      "Iteration 2616, loss = 1.69227381\n",
      "Iteration 2617, loss = 1.69220740\n",
      "Iteration 2618, loss = 1.69214102\n",
      "Iteration 2619, loss = 1.69207465\n",
      "Iteration 2620, loss = 1.69200831\n",
      "Iteration 2621, loss = 1.69194199\n",
      "Iteration 2622, loss = 1.69187570\n",
      "Iteration 2623, loss = 1.69180942\n",
      "Iteration 2624, loss = 1.69174317\n",
      "Iteration 2625, loss = 1.69167694\n",
      "Iteration 2626, loss = 1.69161073\n",
      "Iteration 2627, loss = 1.69154455\n",
      "Iteration 2628, loss = 1.69147838\n",
      "Iteration 2629, loss = 1.69141224\n",
      "Iteration 2630, loss = 1.69134613\n",
      "Iteration 2631, loss = 1.69128003\n",
      "Iteration 2632, loss = 1.69121396\n",
      "Iteration 2633, loss = 1.69114790\n",
      "Iteration 2634, loss = 1.69108187\n",
      "Iteration 2635, loss = 1.69101587\n",
      "Iteration 2636, loss = 1.69094988\n",
      "Iteration 2637, loss = 1.69088392\n",
      "Iteration 2638, loss = 1.69081798\n",
      "Iteration 2639, loss = 1.69075206\n",
      "Iteration 2640, loss = 1.69068616\n",
      "Iteration 2641, loss = 1.69062029\n",
      "Iteration 2642, loss = 1.69055443\n",
      "Iteration 2643, loss = 1.69048860\n",
      "Iteration 2644, loss = 1.69042279\n",
      "Iteration 2645, loss = 1.69035701\n",
      "Iteration 2646, loss = 1.69029124\n",
      "Iteration 2647, loss = 1.69022550\n",
      "Iteration 2648, loss = 1.69015978\n",
      "Iteration 2649, loss = 1.69009408\n",
      "Iteration 2650, loss = 1.69002841\n",
      "Iteration 2651, loss = 1.68996275\n",
      "Iteration 2652, loss = 1.68989712\n",
      "Iteration 2653, loss = 1.68983151\n",
      "Iteration 2654, loss = 1.68976592\n",
      "Iteration 2655, loss = 1.68970036\n",
      "Iteration 2656, loss = 1.68963481\n",
      "Iteration 2657, loss = 1.68956929\n",
      "Iteration 2658, loss = 1.68950379\n",
      "Iteration 2659, loss = 1.68943831\n",
      "Iteration 2660, loss = 1.68937286\n",
      "Iteration 2661, loss = 1.68930742\n",
      "Iteration 2662, loss = 1.68924201\n",
      "Iteration 2663, loss = 1.68917662\n",
      "Iteration 2664, loss = 1.68911125\n",
      "Iteration 2665, loss = 1.68904591\n",
      "Iteration 2666, loss = 1.68898058\n",
      "Iteration 2667, loss = 1.68891528\n",
      "Iteration 2668, loss = 1.68885000\n",
      "Iteration 2669, loss = 1.68878474\n",
      "Iteration 2670, loss = 1.68871950\n",
      "Iteration 2671, loss = 1.68865429\n",
      "Iteration 2672, loss = 1.68858909\n",
      "Iteration 2673, loss = 1.68852392\n",
      "Iteration 2674, loss = 1.68845877\n",
      "Iteration 2675, loss = 1.68839365\n",
      "Iteration 2676, loss = 1.68832854\n",
      "Iteration 2677, loss = 1.68826346\n",
      "Iteration 2678, loss = 1.68819839\n",
      "Iteration 2679, loss = 1.68813335\n",
      "Iteration 2680, loss = 1.68806834\n",
      "Iteration 2681, loss = 1.68800334\n",
      "Iteration 2682, loss = 1.68793836\n",
      "Iteration 2683, loss = 1.68787341\n",
      "Iteration 2684, loss = 1.68780848\n",
      "Iteration 2685, loss = 1.68774357\n",
      "Iteration 2686, loss = 1.68767868\n",
      "Iteration 2687, loss = 1.68761382\n",
      "Iteration 2688, loss = 1.68754897\n",
      "Iteration 2689, loss = 1.68748415\n",
      "Iteration 2690, loss = 1.68741935\n",
      "Iteration 2691, loss = 1.68735457\n",
      "Iteration 2692, loss = 1.68728981\n",
      "Iteration 2693, loss = 1.68722508\n",
      "Iteration 2694, loss = 1.68716036\n",
      "Iteration 2695, loss = 1.68709567\n",
      "Iteration 2696, loss = 1.68703100\n",
      "Iteration 2697, loss = 1.68696635\n",
      "Iteration 2698, loss = 1.68690172\n",
      "Iteration 2699, loss = 1.68683712\n",
      "Iteration 2700, loss = 1.68677253\n",
      "Iteration 2701, loss = 1.68670797\n",
      "Iteration 2702, loss = 1.68664343\n",
      "Iteration 2703, loss = 1.68657891\n",
      "Iteration 2704, loss = 1.68651441\n",
      "Iteration 2705, loss = 1.68644994\n",
      "Iteration 2706, loss = 1.68638548\n",
      "Iteration 2707, loss = 1.68632105\n",
      "Iteration 2708, loss = 1.68625664\n",
      "Iteration 2709, loss = 1.68619225\n",
      "Iteration 2710, loss = 1.68612788\n",
      "Iteration 2711, loss = 1.68606353\n",
      "Iteration 2712, loss = 1.68599921\n",
      "Iteration 2713, loss = 1.68593490\n",
      "Iteration 2714, loss = 1.68587062\n",
      "Iteration 2715, loss = 1.68580636\n",
      "Iteration 2716, loss = 1.68574212\n",
      "Iteration 2717, loss = 1.68567790\n",
      "Iteration 2718, loss = 1.68561371\n",
      "Iteration 2719, loss = 1.68554953\n",
      "Iteration 2720, loss = 1.68548538\n",
      "Iteration 2721, loss = 1.68542125\n",
      "Iteration 2722, loss = 1.68535714\n",
      "Iteration 2723, loss = 1.68529305\n",
      "Iteration 2724, loss = 1.68522898\n",
      "Iteration 2725, loss = 1.68516494\n",
      "Iteration 2726, loss = 1.68510091\n",
      "Iteration 2727, loss = 1.68503691\n",
      "Iteration 2728, loss = 1.68497293\n",
      "Iteration 2729, loss = 1.68490897\n",
      "Iteration 2730, loss = 1.68484503\n",
      "Iteration 2731, loss = 1.68478111\n",
      "Iteration 2732, loss = 1.68471722\n",
      "Iteration 2733, loss = 1.68465334\n",
      "Iteration 2734, loss = 1.68458949\n",
      "Iteration 2735, loss = 1.68452566\n",
      "Iteration 2736, loss = 1.68446185\n",
      "Iteration 2737, loss = 1.68439806\n",
      "Iteration 2738, loss = 1.68433429\n",
      "Iteration 2739, loss = 1.68427055\n",
      "Iteration 2740, loss = 1.68420682\n",
      "Iteration 2741, loss = 1.68414312\n",
      "Iteration 2742, loss = 1.68407943\n",
      "Iteration 2743, loss = 1.68401577\n",
      "Iteration 2744, loss = 1.68395213\n",
      "Iteration 2745, loss = 1.68388851\n",
      "Iteration 2746, loss = 1.68382492\n",
      "Iteration 2747, loss = 1.68376134\n",
      "Iteration 2748, loss = 1.68369779\n",
      "Iteration 2749, loss = 1.68363425\n",
      "Iteration 2750, loss = 1.68357074\n",
      "Iteration 2751, loss = 1.68350725\n",
      "Iteration 2752, loss = 1.68344378\n",
      "Iteration 2753, loss = 1.68338033\n",
      "Iteration 2754, loss = 1.68331690\n",
      "Iteration 2755, loss = 1.68325350\n",
      "Iteration 2756, loss = 1.68319011\n",
      "Iteration 2757, loss = 1.68312675\n",
      "Iteration 2758, loss = 1.68306341\n",
      "Iteration 2759, loss = 1.68300008\n",
      "Iteration 2760, loss = 1.68293678\n",
      "Iteration 2761, loss = 1.68287351\n",
      "Iteration 2762, loss = 1.68281025\n",
      "Iteration 2763, loss = 1.68274701\n",
      "Iteration 2764, loss = 1.68268380\n",
      "Iteration 2765, loss = 1.68262060\n",
      "Iteration 2766, loss = 1.68255743\n",
      "Iteration 2767, loss = 1.68249427\n",
      "Iteration 2768, loss = 1.68243114\n",
      "Iteration 2769, loss = 1.68236803\n",
      "Iteration 2770, loss = 1.68230494\n",
      "Iteration 2771, loss = 1.68224188\n",
      "Iteration 2772, loss = 1.68217883\n",
      "Iteration 2773, loss = 1.68211580\n",
      "Iteration 2774, loss = 1.68205280\n",
      "Iteration 2775, loss = 1.68198982\n",
      "Iteration 2776, loss = 1.68192685\n",
      "Iteration 2777, loss = 1.68186391\n",
      "Iteration 2778, loss = 1.68180099\n",
      "Iteration 2779, loss = 1.68173809\n",
      "Iteration 2780, loss = 1.68167521\n",
      "Iteration 2781, loss = 1.68161235\n",
      "Iteration 2782, loss = 1.68154952\n",
      "Iteration 2783, loss = 1.68148670\n",
      "Iteration 2784, loss = 1.68142391\n",
      "Iteration 2785, loss = 1.68136113\n",
      "Iteration 2786, loss = 1.68129838\n",
      "Iteration 2787, loss = 1.68123565\n",
      "Iteration 2788, loss = 1.68117294\n",
      "Iteration 2789, loss = 1.68111025\n",
      "Iteration 2790, loss = 1.68104758\n",
      "Iteration 2791, loss = 1.68098493\n",
      "Iteration 2792, loss = 1.68092230\n",
      "Iteration 2793, loss = 1.68085970\n",
      "Iteration 2794, loss = 1.68079711\n",
      "Iteration 2795, loss = 1.68073455\n",
      "Iteration 2796, loss = 1.68067201\n",
      "Iteration 2797, loss = 1.68060948\n",
      "Iteration 2798, loss = 1.68054698\n",
      "Iteration 2799, loss = 1.68048450\n",
      "Iteration 2800, loss = 1.68042204\n",
      "Iteration 2801, loss = 1.68035960\n",
      "Iteration 2802, loss = 1.68029718\n",
      "Iteration 2803, loss = 1.68023479\n",
      "Iteration 2804, loss = 1.68017241\n",
      "Iteration 2805, loss = 1.68011005\n",
      "Iteration 2806, loss = 1.68004772\n",
      "Iteration 2807, loss = 1.67998540\n",
      "Iteration 2808, loss = 1.67992311\n",
      "Iteration 2809, loss = 1.67986084\n",
      "Iteration 2810, loss = 1.67979858\n",
      "Iteration 2811, loss = 1.67973635\n",
      "Iteration 2812, loss = 1.67967414\n",
      "Iteration 2813, loss = 1.67961195\n",
      "Iteration 2814, loss = 1.67954978\n",
      "Iteration 2815, loss = 1.67948764\n",
      "Iteration 2816, loss = 1.67942551\n",
      "Iteration 2817, loss = 1.67936340\n",
      "Iteration 2818, loss = 1.67930132\n",
      "Iteration 2819, loss = 1.67923925\n",
      "Iteration 2820, loss = 1.67917721\n",
      "Iteration 2821, loss = 1.67911518\n",
      "Iteration 2822, loss = 1.67905318\n",
      "Iteration 2823, loss = 1.67899120\n",
      "Iteration 2824, loss = 1.67892924\n",
      "Iteration 2825, loss = 1.67886729\n",
      "Iteration 2826, loss = 1.67880537\n",
      "Iteration 2827, loss = 1.67874347\n",
      "Iteration 2828, loss = 1.67868159\n",
      "Iteration 2829, loss = 1.67861974\n",
      "Iteration 2830, loss = 1.67855790\n",
      "Iteration 2831, loss = 1.67849608\n",
      "Iteration 2832, loss = 1.67843428\n",
      "Iteration 2833, loss = 1.67837251\n",
      "Iteration 2834, loss = 1.67831075\n",
      "Iteration 2835, loss = 1.67824902\n",
      "Iteration 2836, loss = 1.67818730\n",
      "Iteration 2837, loss = 1.67812561\n",
      "Iteration 2838, loss = 1.67806393\n",
      "Iteration 2839, loss = 1.67800228\n",
      "Iteration 2840, loss = 1.67794065\n",
      "Iteration 2841, loss = 1.67787904\n",
      "Iteration 2842, loss = 1.67781745\n",
      "Iteration 2843, loss = 1.67775588\n",
      "Iteration 2844, loss = 1.67769433\n",
      "Iteration 2845, loss = 1.67763280\n",
      "Iteration 2846, loss = 1.67757129\n",
      "Iteration 2847, loss = 1.67750980\n",
      "Iteration 2848, loss = 1.67744833\n",
      "Iteration 2849, loss = 1.67738688\n",
      "Iteration 2850, loss = 1.67732546\n",
      "Iteration 2851, loss = 1.67726405\n",
      "Iteration 2852, loss = 1.67720266\n",
      "Iteration 2853, loss = 1.67714130\n",
      "Iteration 2854, loss = 1.67707995\n",
      "Iteration 2855, loss = 1.67701863\n",
      "Iteration 2856, loss = 1.67695732\n",
      "Iteration 2857, loss = 1.67689604\n",
      "Iteration 2858, loss = 1.67683477\n",
      "Iteration 2859, loss = 1.67677353\n",
      "Iteration 2860, loss = 1.67671231\n",
      "Iteration 2861, loss = 1.67665111\n",
      "Iteration 2862, loss = 1.67658992\n",
      "Iteration 2863, loss = 1.67652876\n",
      "Iteration 2864, loss = 1.67646762\n",
      "Iteration 2865, loss = 1.67640650\n",
      "Iteration 2866, loss = 1.67634540\n",
      "Iteration 2867, loss = 1.67628432\n",
      "Iteration 2868, loss = 1.67622326\n",
      "Iteration 2869, loss = 1.67616222\n",
      "Iteration 2870, loss = 1.67610120\n",
      "Iteration 2871, loss = 1.67604020\n",
      "Iteration 2872, loss = 1.67597922\n",
      "Iteration 2873, loss = 1.67591826\n",
      "Iteration 2874, loss = 1.67585733\n",
      "Iteration 2875, loss = 1.67579641\n",
      "Iteration 2876, loss = 1.67573551\n",
      "Iteration 2877, loss = 1.67567463\n",
      "Iteration 2878, loss = 1.67561378\n",
      "Iteration 2879, loss = 1.67555294\n",
      "Iteration 2880, loss = 1.67549212\n",
      "Iteration 2881, loss = 1.67543133\n",
      "Iteration 2882, loss = 1.67537055\n",
      "Iteration 2883, loss = 1.67530979\n",
      "Iteration 2884, loss = 1.67524906\n",
      "Iteration 2885, loss = 1.67518834\n",
      "Iteration 2886, loss = 1.67512765\n",
      "Iteration 2887, loss = 1.67506697\n",
      "Iteration 2888, loss = 1.67500632\n",
      "Iteration 2889, loss = 1.67494568\n",
      "Iteration 2890, loss = 1.67488507\n",
      "Iteration 2891, loss = 1.67482447\n",
      "Iteration 2892, loss = 1.67476390\n",
      "Iteration 2893, loss = 1.67470335\n",
      "Iteration 2894, loss = 1.67464281\n",
      "Iteration 2895, loss = 1.67458230\n",
      "Iteration 2896, loss = 1.67452180\n",
      "Iteration 2897, loss = 1.67446133\n",
      "Iteration 2898, loss = 1.67440088\n",
      "Iteration 2899, loss = 1.67434044\n",
      "Iteration 2900, loss = 1.67428003\n",
      "Iteration 2901, loss = 1.67421964\n",
      "Iteration 2902, loss = 1.67415926\n",
      "Iteration 2903, loss = 1.67409891\n",
      "Iteration 2904, loss = 1.67403858\n",
      "Iteration 2905, loss = 1.67397827\n",
      "Iteration 2906, loss = 1.67391797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2907, loss = 1.67385770\n",
      "Iteration 2908, loss = 1.67379745\n",
      "Iteration 2909, loss = 1.67373722\n",
      "Iteration 2910, loss = 1.67367700\n",
      "Iteration 2911, loss = 1.67361681\n",
      "Iteration 2912, loss = 1.67355664\n",
      "Iteration 2913, loss = 1.67349648\n",
      "Iteration 2914, loss = 1.67343635\n",
      "Iteration 2915, loss = 1.67337624\n",
      "Iteration 2916, loss = 1.67331615\n",
      "Iteration 2917, loss = 1.67325607\n",
      "Iteration 2918, loss = 1.67319602\n",
      "Iteration 2919, loss = 1.67313599\n",
      "Iteration 2920, loss = 1.67307597\n",
      "Iteration 2921, loss = 1.67301598\n",
      "Iteration 2922, loss = 1.67295601\n",
      "Iteration 2923, loss = 1.67289606\n",
      "Iteration 2924, loss = 1.67283612\n",
      "Iteration 2925, loss = 1.67277621\n",
      "Iteration 2926, loss = 1.67271632\n",
      "Iteration 2927, loss = 1.67265644\n",
      "Iteration 2928, loss = 1.67259659\n",
      "Iteration 2929, loss = 1.67253675\n",
      "Iteration 2930, loss = 1.67247694\n",
      "Iteration 2931, loss = 1.67241715\n",
      "Iteration 2932, loss = 1.67235737\n",
      "Iteration 2933, loss = 1.67229762\n",
      "Iteration 2934, loss = 1.67223788\n",
      "Iteration 2935, loss = 1.67217817\n",
      "Iteration 2936, loss = 1.67211847\n",
      "Iteration 2937, loss = 1.67205880\n",
      "Iteration 2938, loss = 1.67199914\n",
      "Iteration 2939, loss = 1.67193951\n",
      "Iteration 2940, loss = 1.67187989\n",
      "Iteration 2941, loss = 1.67182030\n",
      "Iteration 2942, loss = 1.67176072\n",
      "Iteration 2943, loss = 1.67170117\n",
      "Iteration 2944, loss = 1.67164163\n",
      "Iteration 2945, loss = 1.67158211\n",
      "Iteration 2946, loss = 1.67152262\n",
      "Iteration 2947, loss = 1.67146314\n",
      "Iteration 2948, loss = 1.67140368\n",
      "Iteration 2949, loss = 1.67134424\n",
      "Iteration 2950, loss = 1.67128483\n",
      "Iteration 2951, loss = 1.67122543\n",
      "Iteration 2952, loss = 1.67116605\n",
      "Iteration 2953, loss = 1.67110669\n",
      "Iteration 2954, loss = 1.67104735\n",
      "Iteration 2955, loss = 1.67098803\n",
      "Iteration 2956, loss = 1.67092873\n",
      "Iteration 2957, loss = 1.67086945\n",
      "Iteration 2958, loss = 1.67081019\n",
      "Iteration 2959, loss = 1.67075095\n",
      "Iteration 2960, loss = 1.67069173\n",
      "Iteration 2961, loss = 1.67063253\n",
      "Iteration 2962, loss = 1.67057335\n",
      "Iteration 2963, loss = 1.67051418\n",
      "Iteration 2964, loss = 1.67045504\n",
      "Iteration 2965, loss = 1.67039592\n",
      "Iteration 2966, loss = 1.67033681\n",
      "Iteration 2967, loss = 1.67027773\n",
      "Iteration 2968, loss = 1.67021867\n",
      "Iteration 2969, loss = 1.67015962\n",
      "Iteration 2970, loss = 1.67010060\n",
      "Iteration 2971, loss = 1.67004159\n",
      "Iteration 2972, loss = 1.66998261\n",
      "Iteration 2973, loss = 1.66992364\n",
      "Iteration 2974, loss = 1.66986469\n",
      "Iteration 2975, loss = 1.66980576\n",
      "Iteration 2976, loss = 1.66974686\n",
      "Iteration 2977, loss = 1.66968797\n",
      "Iteration 2978, loss = 1.66962910\n",
      "Iteration 2979, loss = 1.66957025\n",
      "Iteration 2980, loss = 1.66951142\n",
      "Iteration 2981, loss = 1.66945261\n",
      "Iteration 2982, loss = 1.66939382\n",
      "Iteration 2983, loss = 1.66933505\n",
      "Iteration 2984, loss = 1.66927630\n",
      "Iteration 2985, loss = 1.66921757\n",
      "Iteration 2986, loss = 1.66915885\n",
      "Iteration 2987, loss = 1.66910016\n",
      "Iteration 2988, loss = 1.66904149\n",
      "Iteration 2989, loss = 1.66898283\n",
      "Iteration 2990, loss = 1.66892420\n",
      "Iteration 2991, loss = 1.66886558\n",
      "Iteration 2992, loss = 1.66880699\n",
      "Iteration 2993, loss = 1.66874841\n",
      "Iteration 2994, loss = 1.66868985\n",
      "Iteration 2995, loss = 1.66863131\n",
      "Iteration 2996, loss = 1.66857279\n",
      "Iteration 2997, loss = 1.66851430\n",
      "Iteration 2998, loss = 1.66845582\n",
      "Iteration 2999, loss = 1.66839736\n",
      "Iteration 3000, loss = 1.66833891\n",
      "Iteration 3001, loss = 1.66828049\n",
      "Iteration 3002, loss = 1.66822209\n",
      "Iteration 3003, loss = 1.66816371\n",
      "Iteration 3004, loss = 1.66810534\n",
      "Iteration 3005, loss = 1.66804700\n",
      "Iteration 3006, loss = 1.66798867\n",
      "Iteration 3007, loss = 1.66793037\n",
      "Iteration 3008, loss = 1.66787208\n",
      "Iteration 3009, loss = 1.66781382\n",
      "Iteration 3010, loss = 1.66775557\n",
      "Iteration 3011, loss = 1.66769734\n",
      "Iteration 3012, loss = 1.66763913\n",
      "Iteration 3013, loss = 1.66758094\n",
      "Iteration 3014, loss = 1.66752277\n",
      "Iteration 3015, loss = 1.66746462\n",
      "Iteration 3016, loss = 1.66740649\n",
      "Iteration 3017, loss = 1.66734837\n",
      "Iteration 3018, loss = 1.66729028\n",
      "Iteration 3019, loss = 1.66723220\n",
      "Iteration 3020, loss = 1.66717415\n",
      "Iteration 3021, loss = 1.66711611\n",
      "Iteration 3022, loss = 1.66705810\n",
      "Iteration 3023, loss = 1.66700010\n",
      "Iteration 3024, loss = 1.66694212\n",
      "Iteration 3025, loss = 1.66688416\n",
      "Iteration 3026, loss = 1.66682622\n",
      "Iteration 3027, loss = 1.66676830\n",
      "Iteration 3028, loss = 1.66671040\n",
      "Iteration 3029, loss = 1.66665252\n",
      "Iteration 3030, loss = 1.66659465\n",
      "Iteration 3031, loss = 1.66653681\n",
      "Iteration 3032, loss = 1.66647898\n",
      "Iteration 3033, loss = 1.66642118\n",
      "Iteration 3034, loss = 1.66636339\n",
      "Iteration 3035, loss = 1.66630562\n",
      "Iteration 3036, loss = 1.66624787\n",
      "Iteration 3037, loss = 1.66619014\n",
      "Iteration 3038, loss = 1.66613243\n",
      "Iteration 3039, loss = 1.66607474\n",
      "Iteration 3040, loss = 1.66601707\n",
      "Iteration 3041, loss = 1.66595942\n",
      "Iteration 3042, loss = 1.66590178\n",
      "Iteration 3043, loss = 1.66584417\n",
      "Iteration 3044, loss = 1.66578657\n",
      "Iteration 3045, loss = 1.66572899\n",
      "Iteration 3046, loss = 1.66567144\n",
      "Iteration 3047, loss = 1.66561390\n",
      "Iteration 3048, loss = 1.66555638\n",
      "Iteration 3049, loss = 1.66549888\n",
      "Iteration 3050, loss = 1.66544139\n",
      "Iteration 3051, loss = 1.66538393\n",
      "Iteration 3052, loss = 1.66532649\n",
      "Iteration 3053, loss = 1.66526906\n",
      "Iteration 3054, loss = 1.66521165\n",
      "Iteration 3055, loss = 1.66515427\n",
      "Iteration 3056, loss = 1.66509690\n",
      "Iteration 3057, loss = 1.66503955\n",
      "Iteration 3058, loss = 1.66498222\n",
      "Iteration 3059, loss = 1.66492491\n",
      "Iteration 3060, loss = 1.66486762\n",
      "Iteration 3061, loss = 1.66481034\n",
      "Iteration 3062, loss = 1.66475309\n",
      "Iteration 3063, loss = 1.66469585\n",
      "Iteration 3064, loss = 1.66463864\n",
      "Iteration 3065, loss = 1.66458144\n",
      "Iteration 3066, loss = 1.66452426\n",
      "Iteration 3067, loss = 1.66446710\n",
      "Iteration 3068, loss = 1.66440996\n",
      "Iteration 3069, loss = 1.66435284\n",
      "Iteration 3070, loss = 1.66429573\n",
      "Iteration 3071, loss = 1.66423865\n",
      "Iteration 3072, loss = 1.66418158\n",
      "Iteration 3073, loss = 1.66412454\n",
      "Iteration 3074, loss = 1.66406751\n",
      "Iteration 3075, loss = 1.66401050\n",
      "Iteration 3076, loss = 1.66395351\n",
      "Iteration 3077, loss = 1.66389654\n",
      "Iteration 3078, loss = 1.66383958\n",
      "Iteration 3079, loss = 1.66378265\n",
      "Iteration 3080, loss = 1.66372574\n",
      "Iteration 3081, loss = 1.66366884\n",
      "Iteration 3082, loss = 1.66361196\n",
      "Iteration 3083, loss = 1.66355510\n",
      "Iteration 3084, loss = 1.66349826\n",
      "Iteration 3085, loss = 1.66344144\n",
      "Iteration 3086, loss = 1.66338464\n",
      "Iteration 3087, loss = 1.66332786\n",
      "Iteration 3088, loss = 1.66327109\n",
      "Iteration 3089, loss = 1.66321434\n",
      "Iteration 3090, loss = 1.66315762\n",
      "Iteration 3091, loss = 1.66310091\n",
      "Iteration 3092, loss = 1.66304422\n",
      "Iteration 3093, loss = 1.66298755\n",
      "Iteration 3094, loss = 1.66293089\n",
      "Iteration 3095, loss = 1.66287426\n",
      "Iteration 3096, loss = 1.66281764\n",
      "Iteration 3097, loss = 1.66276105\n",
      "Iteration 3098, loss = 1.66270447\n",
      "Iteration 3099, loss = 1.66264791\n",
      "Iteration 3100, loss = 1.66259137\n",
      "Iteration 3101, loss = 1.66253485\n",
      "Iteration 3102, loss = 1.66247834\n",
      "Iteration 3103, loss = 1.66242186\n",
      "Iteration 3104, loss = 1.66236539\n",
      "Iteration 3105, loss = 1.66230895\n",
      "Iteration 3106, loss = 1.66225252\n",
      "Iteration 3107, loss = 1.66219611\n",
      "Iteration 3108, loss = 1.66213972\n",
      "Iteration 3109, loss = 1.66208334\n",
      "Iteration 3110, loss = 1.66202699\n",
      "Iteration 3111, loss = 1.66197065\n",
      "Iteration 3112, loss = 1.66191433\n",
      "Iteration 3113, loss = 1.66185804\n",
      "Iteration 3114, loss = 1.66180176\n",
      "Iteration 3115, loss = 1.66174549\n",
      "Iteration 3116, loss = 1.66168925\n",
      "Iteration 3117, loss = 1.66163303\n",
      "Iteration 3118, loss = 1.66157682\n",
      "Iteration 3119, loss = 1.66152063\n",
      "Iteration 3120, loss = 1.66146446\n",
      "Iteration 3121, loss = 1.66140831\n",
      "Iteration 3122, loss = 1.66135218\n",
      "Iteration 3123, loss = 1.66129607\n",
      "Iteration 3124, loss = 1.66123997\n",
      "Iteration 3125, loss = 1.66118390\n",
      "Iteration 3126, loss = 1.66112784\n",
      "Iteration 3127, loss = 1.66107180\n",
      "Iteration 3128, loss = 1.66101578\n",
      "Iteration 3129, loss = 1.66095978\n",
      "Iteration 3130, loss = 1.66090379\n",
      "Iteration 3131, loss = 1.66084783\n",
      "Iteration 3132, loss = 1.66079188\n",
      "Iteration 3133, loss = 1.66073595\n",
      "Iteration 3134, loss = 1.66068004\n",
      "Iteration 3135, loss = 1.66062415\n",
      "Iteration 3136, loss = 1.66056827\n",
      "Iteration 3137, loss = 1.66051242\n",
      "Iteration 3138, loss = 1.66045658\n",
      "Iteration 3139, loss = 1.66040076\n",
      "Iteration 3140, loss = 1.66034496\n",
      "Iteration 3141, loss = 1.66028918\n",
      "Iteration 3142, loss = 1.66023342\n",
      "Iteration 3143, loss = 1.66017767\n",
      "Iteration 3144, loss = 1.66012195\n",
      "Iteration 3145, loss = 1.66006624\n",
      "Iteration 3146, loss = 1.66001055\n",
      "Iteration 3147, loss = 1.65995488\n",
      "Iteration 3148, loss = 1.65989922\n",
      "Iteration 3149, loss = 1.65984359\n",
      "Iteration 3150, loss = 1.65978797\n",
      "Iteration 3151, loss = 1.65973237\n",
      "Iteration 3152, loss = 1.65967679\n",
      "Iteration 3153, loss = 1.65962123\n",
      "Iteration 3154, loss = 1.65956569\n",
      "Iteration 3155, loss = 1.65951016\n",
      "Iteration 3156, loss = 1.65945466\n",
      "Iteration 3157, loss = 1.65939917\n",
      "Iteration 3158, loss = 1.65934370\n",
      "Iteration 3159, loss = 1.65928824\n",
      "Iteration 3160, loss = 1.65923281\n",
      "Iteration 3161, loss = 1.65917740\n",
      "Iteration 3162, loss = 1.65912200\n",
      "Iteration 3163, loss = 1.65906662\n",
      "Iteration 3164, loss = 1.65901126\n",
      "Iteration 3165, loss = 1.65895592\n",
      "Iteration 3166, loss = 1.65890059\n",
      "Iteration 3167, loss = 1.65884528\n",
      "Iteration 3168, loss = 1.65879000\n",
      "Iteration 3169, loss = 1.65873473\n",
      "Iteration 3170, loss = 1.65867947\n",
      "Iteration 3171, loss = 1.65862424\n",
      "Iteration 3172, loss = 1.65856903\n",
      "Iteration 3173, loss = 1.65851383\n",
      "Iteration 3174, loss = 1.65845865\n",
      "Iteration 3175, loss = 1.65840349\n",
      "Iteration 3176, loss = 1.65834834\n",
      "Iteration 3177, loss = 1.65829322\n",
      "Iteration 3178, loss = 1.65823811\n",
      "Iteration 3179, loss = 1.65818302\n",
      "Iteration 3180, loss = 1.65812795\n",
      "Iteration 3181, loss = 1.65807290\n",
      "Iteration 3182, loss = 1.65801787\n",
      "Iteration 3183, loss = 1.65796285\n",
      "Iteration 3184, loss = 1.65790785\n",
      "Iteration 3185, loss = 1.65785287\n",
      "Iteration 3186, loss = 1.65779791\n",
      "Iteration 3187, loss = 1.65774297\n",
      "Iteration 3188, loss = 1.65768804\n",
      "Iteration 3189, loss = 1.65763313\n",
      "Iteration 3190, loss = 1.65757825\n",
      "Iteration 3191, loss = 1.65752337\n",
      "Iteration 3192, loss = 1.65746852\n",
      "Iteration 3193, loss = 1.65741368\n",
      "Iteration 3194, loss = 1.65735887\n",
      "Iteration 3195, loss = 1.65730407\n",
      "Iteration 3196, loss = 1.65724929\n",
      "Iteration 3197, loss = 1.65719452\n",
      "Iteration 3198, loss = 1.65713978\n",
      "Iteration 3199, loss = 1.65708505\n",
      "Iteration 3200, loss = 1.65703034\n",
      "Iteration 3201, loss = 1.65697565\n",
      "Iteration 3202, loss = 1.65692097\n",
      "Iteration 3203, loss = 1.65686632\n",
      "Iteration 3204, loss = 1.65681168\n",
      "Iteration 3205, loss = 1.65675706\n",
      "Iteration 3206, loss = 1.65670246\n",
      "Iteration 3207, loss = 1.65664787\n",
      "Iteration 3208, loss = 1.65659331\n",
      "Iteration 3209, loss = 1.65653876\n",
      "Iteration 3210, loss = 1.65648423\n",
      "Iteration 3211, loss = 1.65642972\n",
      "Iteration 3212, loss = 1.65637522\n",
      "Iteration 3213, loss = 1.65632075\n",
      "Iteration 3214, loss = 1.65626629\n",
      "Iteration 3215, loss = 1.65621185\n",
      "Iteration 3216, loss = 1.65615742\n",
      "Iteration 3217, loss = 1.65610302\n",
      "Iteration 3218, loss = 1.65604863\n",
      "Iteration 3219, loss = 1.65599426\n",
      "Iteration 3220, loss = 1.65593991\n",
      "Iteration 3221, loss = 1.65588558\n",
      "Iteration 3222, loss = 1.65583126\n",
      "Iteration 3223, loss = 1.65577696\n",
      "Iteration 3224, loss = 1.65572268\n",
      "Iteration 3225, loss = 1.65566842\n",
      "Iteration 3226, loss = 1.65561418\n",
      "Iteration 3227, loss = 1.65555995\n",
      "Iteration 3228, loss = 1.65550574\n",
      "Iteration 3229, loss = 1.65545155\n",
      "Iteration 3230, loss = 1.65539738\n",
      "Iteration 3231, loss = 1.65534322\n",
      "Iteration 3232, loss = 1.65528908\n",
      "Iteration 3233, loss = 1.65523496\n",
      "Iteration 3234, loss = 1.65518086\n",
      "Iteration 3235, loss = 1.65512678\n",
      "Iteration 3236, loss = 1.65507271\n",
      "Iteration 3237, loss = 1.65501866\n",
      "Iteration 3238, loss = 1.65496463\n",
      "Iteration 3239, loss = 1.65491061\n",
      "Iteration 3240, loss = 1.65485662\n",
      "Iteration 3241, loss = 1.65480264\n",
      "Iteration 3242, loss = 1.65474868\n",
      "Iteration 3243, loss = 1.65469474\n",
      "Iteration 3244, loss = 1.65464081\n",
      "Iteration 3245, loss = 1.65458690\n",
      "Iteration 3246, loss = 1.65453301\n",
      "Iteration 3247, loss = 1.65447914\n",
      "Iteration 3248, loss = 1.65442529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3249, loss = 1.65437145\n",
      "Iteration 3250, loss = 1.65431763\n",
      "Iteration 3251, loss = 1.65426383\n",
      "Iteration 3252, loss = 1.65421005\n",
      "Iteration 3253, loss = 1.65415628\n",
      "Iteration 3254, loss = 1.65410253\n",
      "Iteration 3255, loss = 1.65404880\n",
      "Iteration 3256, loss = 1.65399509\n",
      "Iteration 3257, loss = 1.65394139\n",
      "Iteration 3258, loss = 1.65388771\n",
      "Iteration 3259, loss = 1.65383405\n",
      "Iteration 3260, loss = 1.65378041\n",
      "Iteration 3261, loss = 1.65372678\n",
      "Iteration 3262, loss = 1.65367317\n",
      "Iteration 3263, loss = 1.65361958\n",
      "Iteration 3264, loss = 1.65356601\n",
      "Iteration 3265, loss = 1.65351245\n",
      "Iteration 3266, loss = 1.65345892\n",
      "Iteration 3267, loss = 1.65340540\n",
      "Iteration 3268, loss = 1.65335189\n",
      "Iteration 3269, loss = 1.65329841\n",
      "Iteration 3270, loss = 1.65324494\n",
      "Iteration 3271, loss = 1.65319149\n",
      "Iteration 3272, loss = 1.65313806\n",
      "Iteration 3273, loss = 1.65308464\n",
      "Iteration 3274, loss = 1.65303124\n",
      "Iteration 3275, loss = 1.65297786\n",
      "Iteration 3276, loss = 1.65292450\n",
      "Iteration 3277, loss = 1.65287116\n",
      "Iteration 3278, loss = 1.65281783\n",
      "Iteration 3279, loss = 1.65276452\n",
      "Iteration 3280, loss = 1.65271122\n",
      "Iteration 3281, loss = 1.65265795\n",
      "Iteration 3282, loss = 1.65260469\n",
      "Iteration 3283, loss = 1.65255145\n",
      "Iteration 3284, loss = 1.65249823\n",
      "Iteration 3285, loss = 1.65244502\n",
      "Iteration 3286, loss = 1.65239183\n",
      "Iteration 3287, loss = 1.65233866\n",
      "Iteration 3288, loss = 1.65228551\n",
      "Iteration 3289, loss = 1.65223237\n",
      "Iteration 3290, loss = 1.65217925\n",
      "Iteration 3291, loss = 1.65212615\n",
      "Iteration 3292, loss = 1.65207307\n",
      "Iteration 3293, loss = 1.65202000\n",
      "Iteration 3294, loss = 1.65196695\n",
      "Iteration 3295, loss = 1.65191392\n",
      "Iteration 3296, loss = 1.65186090\n",
      "Iteration 3297, loss = 1.65180791\n",
      "Iteration 3298, loss = 1.65175493\n",
      "Iteration 3299, loss = 1.65170196\n",
      "Iteration 3300, loss = 1.65164902\n",
      "Iteration 3301, loss = 1.65159609\n",
      "Iteration 3302, loss = 1.65154318\n",
      "Iteration 3303, loss = 1.65149029\n",
      "Iteration 3304, loss = 1.65143741\n",
      "Iteration 3305, loss = 1.65138455\n",
      "Iteration 3306, loss = 1.65133171\n",
      "Iteration 3307, loss = 1.65127889\n",
      "Iteration 3308, loss = 1.65122608\n",
      "Iteration 3309, loss = 1.65117329\n",
      "Iteration 3310, loss = 1.65112052\n",
      "Iteration 3311, loss = 1.65106776\n",
      "Iteration 3312, loss = 1.65101502\n",
      "Iteration 3313, loss = 1.65096230\n",
      "Iteration 3314, loss = 1.65090960\n",
      "Iteration 3315, loss = 1.65085691\n",
      "Iteration 3316, loss = 1.65080424\n",
      "Iteration 3317, loss = 1.65075159\n",
      "Iteration 3318, loss = 1.65069896\n",
      "Iteration 3319, loss = 1.65064634\n",
      "Iteration 3320, loss = 1.65059374\n",
      "Iteration 3321, loss = 1.65054116\n",
      "Iteration 3322, loss = 1.65048859\n",
      "Iteration 3323, loss = 1.65043604\n",
      "Iteration 3324, loss = 1.65038351\n",
      "Iteration 3325, loss = 1.65033099\n",
      "Iteration 3326, loss = 1.65027850\n",
      "Iteration 3327, loss = 1.65022602\n",
      "Iteration 3328, loss = 1.65017355\n",
      "Iteration 3329, loss = 1.65012111\n",
      "Iteration 3330, loss = 1.65006868\n",
      "Iteration 3331, loss = 1.65001627\n",
      "Iteration 3332, loss = 1.64996387\n",
      "Iteration 3333, loss = 1.64991150\n",
      "Iteration 3334, loss = 1.64985914\n",
      "Iteration 3335, loss = 1.64980679\n",
      "Iteration 3336, loss = 1.64975447\n",
      "Iteration 3337, loss = 1.64970216\n",
      "Iteration 3338, loss = 1.64964987\n",
      "Iteration 3339, loss = 1.64959759\n",
      "Iteration 3340, loss = 1.64954533\n",
      "Iteration 3341, loss = 1.64949309\n",
      "Iteration 3342, loss = 1.64944087\n",
      "Iteration 3343, loss = 1.64938866\n",
      "Iteration 3344, loss = 1.64933647\n",
      "Iteration 3345, loss = 1.64928430\n",
      "Iteration 3346, loss = 1.64923214\n",
      "Iteration 3347, loss = 1.64918001\n",
      "Iteration 3348, loss = 1.64912788\n",
      "Iteration 3349, loss = 1.64907578\n",
      "Iteration 3350, loss = 1.64902369\n",
      "Iteration 3351, loss = 1.64897162\n",
      "Iteration 3352, loss = 1.64891957\n",
      "Iteration 3353, loss = 1.64886753\n",
      "Iteration 3354, loss = 1.64881551\n",
      "Iteration 3355, loss = 1.64876351\n",
      "Iteration 3356, loss = 1.64871152\n",
      "Iteration 3357, loss = 1.64865955\n",
      "Iteration 3358, loss = 1.64860760\n",
      "Iteration 3359, loss = 1.64855567\n",
      "Iteration 3360, loss = 1.64850375\n",
      "Iteration 3361, loss = 1.64845185\n",
      "Iteration 3362, loss = 1.64839997\n",
      "Iteration 3363, loss = 1.64834810\n",
      "Iteration 3364, loss = 1.64829625\n",
      "Iteration 3365, loss = 1.64824441\n",
      "Iteration 3366, loss = 1.64819260\n",
      "Iteration 3367, loss = 1.64814080\n",
      "Iteration 3368, loss = 1.64808902\n",
      "Iteration 3369, loss = 1.64803725\n",
      "Iteration 3370, loss = 1.64798550\n",
      "Iteration 3371, loss = 1.64793377\n",
      "Iteration 3372, loss = 1.64788205\n",
      "Iteration 3373, loss = 1.64783036\n",
      "Iteration 3374, loss = 1.64777867\n",
      "Iteration 3375, loss = 1.64772701\n",
      "Iteration 3376, loss = 1.64767536\n",
      "Iteration 3377, loss = 1.64762373\n",
      "Iteration 3378, loss = 1.64757212\n",
      "Iteration 3379, loss = 1.64752052\n",
      "Iteration 3380, loss = 1.64746894\n",
      "Iteration 3381, loss = 1.64741738\n",
      "Iteration 3382, loss = 1.64736583\n",
      "Iteration 3383, loss = 1.64731430\n",
      "Iteration 3384, loss = 1.64726279\n",
      "Iteration 3385, loss = 1.64721129\n",
      "Iteration 3386, loss = 1.64715981\n",
      "Iteration 3387, loss = 1.64710835\n",
      "Iteration 3388, loss = 1.64705690\n",
      "Iteration 3389, loss = 1.64700547\n",
      "Iteration 3390, loss = 1.64695406\n",
      "Iteration 3391, loss = 1.64690266\n",
      "Iteration 3392, loss = 1.64685128\n",
      "Iteration 3393, loss = 1.64679992\n",
      "Iteration 3394, loss = 1.64674858\n",
      "Iteration 3395, loss = 1.64669725\n",
      "Iteration 3396, loss = 1.64664593\n",
      "Iteration 3397, loss = 1.64659464\n",
      "Iteration 3398, loss = 1.64654336\n",
      "Iteration 3399, loss = 1.64649210\n",
      "Iteration 3400, loss = 1.64644085\n",
      "Iteration 3401, loss = 1.64638962\n",
      "Iteration 3402, loss = 1.64633841\n",
      "Iteration 3403, loss = 1.64628722\n",
      "Iteration 3404, loss = 1.64623604\n",
      "Iteration 3405, loss = 1.64618487\n",
      "Iteration 3406, loss = 1.64613373\n",
      "Iteration 3407, loss = 1.64608260\n",
      "Iteration 3408, loss = 1.64603149\n",
      "Iteration 3409, loss = 1.64598039\n",
      "Iteration 3410, loss = 1.64592931\n",
      "Iteration 3411, loss = 1.64587825\n",
      "Iteration 3412, loss = 1.64582721\n",
      "Iteration 3413, loss = 1.64577618\n",
      "Iteration 3414, loss = 1.64572516\n",
      "Iteration 3415, loss = 1.64567417\n",
      "Iteration 3416, loss = 1.64562319\n",
      "Iteration 3417, loss = 1.64557223\n",
      "Iteration 3418, loss = 1.64552128\n",
      "Iteration 3419, loss = 1.64547035\n",
      "Iteration 3420, loss = 1.64541944\n",
      "Iteration 3421, loss = 1.64536854\n",
      "Iteration 3422, loss = 1.64531766\n",
      "Iteration 3423, loss = 1.64526680\n",
      "Iteration 3424, loss = 1.64521595\n",
      "Iteration 3425, loss = 1.64516512\n",
      "Iteration 3426, loss = 1.64511431\n",
      "Iteration 3427, loss = 1.64506351\n",
      "Iteration 3428, loss = 1.64501273\n",
      "Iteration 3429, loss = 1.64496196\n",
      "Iteration 3430, loss = 1.64491122\n",
      "Iteration 3431, loss = 1.64486049\n",
      "Iteration 3432, loss = 1.64480977\n",
      "Iteration 3433, loss = 1.64475907\n",
      "Iteration 3434, loss = 1.64470839\n",
      "Iteration 3435, loss = 1.64465773\n",
      "Iteration 3436, loss = 1.64460708\n",
      "Iteration 3437, loss = 1.64455644\n",
      "Iteration 3438, loss = 1.64450583\n",
      "Iteration 3439, loss = 1.64445523\n",
      "Iteration 3440, loss = 1.64440465\n",
      "Iteration 3441, loss = 1.64435408\n",
      "Iteration 3442, loss = 1.64430353\n",
      "Iteration 3443, loss = 1.64425300\n",
      "Iteration 3444, loss = 1.64420248\n",
      "Iteration 3445, loss = 1.64415198\n",
      "Iteration 3446, loss = 1.64410149\n",
      "Iteration 3447, loss = 1.64405102\n",
      "Iteration 3448, loss = 1.64400057\n",
      "Iteration 3449, loss = 1.64395014\n",
      "Iteration 3450, loss = 1.64389972\n",
      "Iteration 3451, loss = 1.64384932\n",
      "Iteration 3452, loss = 1.64379893\n",
      "Iteration 3453, loss = 1.64374856\n",
      "Iteration 3454, loss = 1.64369821\n",
      "Iteration 3455, loss = 1.64364787\n",
      "Iteration 3456, loss = 1.64359755\n",
      "Iteration 3457, loss = 1.64354724\n",
      "Iteration 3458, loss = 1.64349696\n",
      "Iteration 3459, loss = 1.64344668\n",
      "Iteration 3460, loss = 1.64339643\n",
      "Iteration 3461, loss = 1.64334619\n",
      "Iteration 3462, loss = 1.64329597\n",
      "Iteration 3463, loss = 1.64324576\n",
      "Iteration 3464, loss = 1.64319557\n",
      "Iteration 3465, loss = 1.64314540\n",
      "Iteration 3466, loss = 1.64309524\n",
      "Iteration 3467, loss = 1.64304510\n",
      "Iteration 3468, loss = 1.64299497\n",
      "Iteration 3469, loss = 1.64294486\n",
      "Iteration 3470, loss = 1.64289477\n",
      "Iteration 3471, loss = 1.64284469\n",
      "Iteration 3472, loss = 1.64279463\n",
      "Iteration 3473, loss = 1.64274459\n",
      "Iteration 3474, loss = 1.64269456\n",
      "Iteration 3475, loss = 1.64264455\n",
      "Iteration 3476, loss = 1.64259456\n",
      "Iteration 3477, loss = 1.64254458\n",
      "Iteration 3478, loss = 1.64249461\n",
      "Iteration 3479, loss = 1.64244467\n",
      "Iteration 3480, loss = 1.64239474\n",
      "Iteration 3481, loss = 1.64234482\n",
      "Iteration 3482, loss = 1.64229492\n",
      "Iteration 3483, loss = 1.64224504\n",
      "Iteration 3484, loss = 1.64219518\n",
      "Iteration 3485, loss = 1.64214533\n",
      "Iteration 3486, loss = 1.64209549\n",
      "Iteration 3487, loss = 1.64204568\n",
      "Iteration 3488, loss = 1.64199588\n",
      "Iteration 3489, loss = 1.64194609\n",
      "Iteration 3490, loss = 1.64189632\n",
      "Iteration 3491, loss = 1.64184657\n",
      "Iteration 3492, loss = 1.64179683\n",
      "Iteration 3493, loss = 1.64174711\n",
      "Iteration 3494, loss = 1.64169741\n",
      "Iteration 3495, loss = 1.64164772\n",
      "Iteration 3496, loss = 1.64159805\n",
      "Iteration 3497, loss = 1.64154839\n",
      "Iteration 3498, loss = 1.64149875\n",
      "Iteration 3499, loss = 1.64144913\n",
      "Iteration 3500, loss = 1.64139952\n",
      "Iteration 3501, loss = 1.64134993\n",
      "Iteration 3502, loss = 1.64130035\n",
      "Iteration 3503, loss = 1.64125080\n",
      "Iteration 3504, loss = 1.64120125\n",
      "Iteration 3505, loss = 1.64115172\n",
      "Iteration 3506, loss = 1.64110221\n",
      "Iteration 3507, loss = 1.64105272\n",
      "Iteration 3508, loss = 1.64100324\n",
      "Iteration 3509, loss = 1.64095378\n",
      "Iteration 3510, loss = 1.64090433\n",
      "Iteration 3511, loss = 1.64085490\n",
      "Iteration 3512, loss = 1.64080548\n",
      "Iteration 3513, loss = 1.64075608\n",
      "Iteration 3514, loss = 1.64070670\n",
      "Iteration 3515, loss = 1.64065733\n",
      "Iteration 3516, loss = 1.64060798\n",
      "Iteration 3517, loss = 1.64055865\n",
      "Iteration 3518, loss = 1.64050933\n",
      "Iteration 3519, loss = 1.64046002\n",
      "Iteration 3520, loss = 1.64041074\n",
      "Iteration 3521, loss = 1.64036147\n",
      "Iteration 3522, loss = 1.64031221\n",
      "Iteration 3523, loss = 1.64026297\n",
      "Iteration 3524, loss = 1.64021375\n",
      "Iteration 3525, loss = 1.64016454\n",
      "Iteration 3526, loss = 1.64011535\n",
      "Iteration 3527, loss = 1.64006617\n",
      "Iteration 3528, loss = 1.64001701\n",
      "Iteration 3529, loss = 1.63996787\n",
      "Iteration 3530, loss = 1.63991874\n",
      "Iteration 3531, loss = 1.63986963\n",
      "Iteration 3532, loss = 1.63982053\n",
      "Iteration 3533, loss = 1.63977145\n",
      "Iteration 3534, loss = 1.63972239\n",
      "Iteration 3535, loss = 1.63967334\n",
      "Iteration 3536, loss = 1.63962431\n",
      "Iteration 3537, loss = 1.63957529\n",
      "Iteration 3538, loss = 1.63952629\n",
      "Iteration 3539, loss = 1.63947730\n",
      "Iteration 3540, loss = 1.63942833\n",
      "Iteration 3541, loss = 1.63937938\n",
      "Iteration 3542, loss = 1.63933044\n",
      "Iteration 3543, loss = 1.63928152\n",
      "Iteration 3544, loss = 1.63923261\n",
      "Iteration 3545, loss = 1.63918372\n",
      "Iteration 3546, loss = 1.63913485\n",
      "Iteration 3547, loss = 1.63908599\n",
      "Iteration 3548, loss = 1.63903715\n",
      "Iteration 3549, loss = 1.63898832\n",
      "Iteration 3550, loss = 1.63893951\n",
      "Iteration 3551, loss = 1.63889071\n",
      "Iteration 3552, loss = 1.63884193\n",
      "Iteration 3553, loss = 1.63879317\n",
      "Iteration 3554, loss = 1.63874442\n",
      "Iteration 3555, loss = 1.63869569\n",
      "Iteration 3556, loss = 1.63864697\n",
      "Iteration 3557, loss = 1.63859827\n",
      "Iteration 3558, loss = 1.63854959\n",
      "Iteration 3559, loss = 1.63850092\n",
      "Iteration 3560, loss = 1.63845226\n",
      "Iteration 3561, loss = 1.63840363\n",
      "Iteration 3562, loss = 1.63835500\n",
      "Iteration 3563, loss = 1.63830640\n",
      "Iteration 3564, loss = 1.63825781\n",
      "Iteration 3565, loss = 1.63820923\n",
      "Iteration 3566, loss = 1.63816067\n",
      "Iteration 3567, loss = 1.63811213\n",
      "Iteration 3568, loss = 1.63806360\n",
      "Iteration 3569, loss = 1.63801509\n",
      "Iteration 3570, loss = 1.63796659\n",
      "Iteration 3571, loss = 1.63791811\n",
      "Iteration 3572, loss = 1.63786964\n",
      "Iteration 3573, loss = 1.63782119\n",
      "Iteration 3574, loss = 1.63777276\n",
      "Iteration 3575, loss = 1.63772434\n",
      "Iteration 3576, loss = 1.63767594\n",
      "Iteration 3577, loss = 1.63762755\n",
      "Iteration 3578, loss = 1.63757918\n",
      "Iteration 3579, loss = 1.63753082\n",
      "Iteration 3580, loss = 1.63748248\n",
      "Iteration 3581, loss = 1.63743416\n",
      "Iteration 3582, loss = 1.63738585\n",
      "Iteration 3583, loss = 1.63733756\n",
      "Iteration 3584, loss = 1.63728928\n",
      "Iteration 3585, loss = 1.63724101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3586, loss = 1.63719277\n",
      "Iteration 3587, loss = 1.63714454\n",
      "Iteration 3588, loss = 1.63709632\n",
      "Iteration 3589, loss = 1.63704812\n",
      "Iteration 3590, loss = 1.63699994\n",
      "Iteration 3591, loss = 1.63695177\n",
      "Iteration 3592, loss = 1.63690361\n",
      "Iteration 3593, loss = 1.63685548\n",
      "Iteration 3594, loss = 1.63680735\n",
      "Iteration 3595, loss = 1.63675925\n",
      "Iteration 3596, loss = 1.63671115\n",
      "Iteration 3597, loss = 1.63666308\n",
      "Iteration 3598, loss = 1.63661502\n",
      "Iteration 3599, loss = 1.63656697\n",
      "Iteration 3600, loss = 1.63651894\n",
      "Iteration 3601, loss = 1.63647093\n",
      "Iteration 3602, loss = 1.63642293\n",
      "Iteration 3603, loss = 1.63637495\n",
      "Iteration 3604, loss = 1.63632698\n",
      "Iteration 3605, loss = 1.63627903\n",
      "Iteration 3606, loss = 1.63623109\n",
      "Iteration 3607, loss = 1.63618317\n",
      "Iteration 3608, loss = 1.63613526\n",
      "Iteration 3609, loss = 1.63608737\n",
      "Iteration 3610, loss = 1.63603950\n",
      "Iteration 3611, loss = 1.63599164\n",
      "Iteration 3612, loss = 1.63594380\n",
      "Iteration 3613, loss = 1.63589597\n",
      "Iteration 3614, loss = 1.63584815\n",
      "Iteration 3615, loss = 1.63580036\n",
      "Iteration 3616, loss = 1.63575257\n",
      "Iteration 3617, loss = 1.63570481\n",
      "Iteration 3618, loss = 1.63565705\n",
      "Iteration 3619, loss = 1.63560932\n",
      "Iteration 3620, loss = 1.63556160\n",
      "Iteration 3621, loss = 1.63551389\n",
      "Iteration 3622, loss = 1.63546620\n",
      "Iteration 3623, loss = 1.63541853\n",
      "Iteration 3624, loss = 1.63537087\n",
      "Iteration 3625, loss = 1.63532322\n",
      "Iteration 3626, loss = 1.63527560\n",
      "Iteration 3627, loss = 1.63522798\n",
      "Iteration 3628, loss = 1.63518038\n",
      "Iteration 3629, loss = 1.63513280\n",
      "Iteration 3630, loss = 1.63508523\n",
      "Iteration 3631, loss = 1.63503768\n",
      "Iteration 3632, loss = 1.63499015\n",
      "Iteration 3633, loss = 1.63494262\n",
      "Iteration 3634, loss = 1.63489512\n",
      "Iteration 3635, loss = 1.63484763\n",
      "Iteration 3636, loss = 1.63480015\n",
      "Iteration 3637, loss = 1.63475269\n",
      "Iteration 3638, loss = 1.63470525\n",
      "Iteration 3639, loss = 1.63465782\n",
      "Iteration 3640, loss = 1.63461040\n",
      "Iteration 3641, loss = 1.63456300\n",
      "Iteration 3642, loss = 1.63451562\n",
      "Iteration 3643, loss = 1.63446825\n",
      "Iteration 3644, loss = 1.63442090\n",
      "Iteration 3645, loss = 1.63437356\n",
      "Iteration 3646, loss = 1.63432624\n",
      "Iteration 3647, loss = 1.63427893\n",
      "Iteration 3648, loss = 1.63423163\n",
      "Iteration 3649, loss = 1.63418436\n",
      "Iteration 3650, loss = 1.63413710\n",
      "Iteration 3651, loss = 1.63408985\n",
      "Iteration 3652, loss = 1.63404262\n",
      "Iteration 3653, loss = 1.63399540\n",
      "Iteration 3654, loss = 1.63394820\n",
      "Iteration 3655, loss = 1.63390101\n",
      "Iteration 3656, loss = 1.63385384\n",
      "Iteration 3657, loss = 1.63380669\n",
      "Iteration 3658, loss = 1.63375954\n",
      "Iteration 3659, loss = 1.63371242\n",
      "Iteration 3660, loss = 1.63366531\n",
      "Iteration 3661, loss = 1.63361821\n",
      "Iteration 3662, loss = 1.63357113\n",
      "Iteration 3663, loss = 1.63352407\n",
      "Iteration 3664, loss = 1.63347702\n",
      "Iteration 3665, loss = 1.63342998\n",
      "Iteration 3666, loss = 1.63338296\n",
      "Iteration 3667, loss = 1.63333596\n",
      "Iteration 3668, loss = 1.63328897\n",
      "Iteration 3669, loss = 1.63324200\n",
      "Iteration 3670, loss = 1.63319504\n",
      "Iteration 3671, loss = 1.63314809\n",
      "Iteration 3672, loss = 1.63310116\n",
      "Iteration 3673, loss = 1.63305425\n",
      "Iteration 3674, loss = 1.63300735\n",
      "Iteration 3675, loss = 1.63296047\n",
      "Iteration 3676, loss = 1.63291360\n",
      "Iteration 3677, loss = 1.63286675\n",
      "Iteration 3678, loss = 1.63281991\n",
      "Iteration 3679, loss = 1.63277308\n",
      "Iteration 3680, loss = 1.63272628\n",
      "Iteration 3681, loss = 1.63267948\n",
      "Iteration 3682, loss = 1.63263270\n",
      "Iteration 3683, loss = 1.63258594\n",
      "Iteration 3684, loss = 1.63253919\n",
      "Iteration 3685, loss = 1.63249246\n",
      "Iteration 3686, loss = 1.63244574\n",
      "Iteration 3687, loss = 1.63239904\n",
      "Iteration 3688, loss = 1.63235235\n",
      "Iteration 3689, loss = 1.63230567\n",
      "Iteration 3690, loss = 1.63225902\n",
      "Iteration 3691, loss = 1.63221237\n",
      "Iteration 3692, loss = 1.63216574\n",
      "Iteration 3693, loss = 1.63211913\n",
      "Iteration 3694, loss = 1.63207253\n",
      "Iteration 3695, loss = 1.63202595\n",
      "Iteration 3696, loss = 1.63197938\n",
      "Iteration 3697, loss = 1.63193283\n",
      "Iteration 3698, loss = 1.63188629\n",
      "Iteration 3699, loss = 1.63183976\n",
      "Iteration 3700, loss = 1.63179325\n",
      "Iteration 3701, loss = 1.63174676\n",
      "Iteration 3702, loss = 1.63170028\n",
      "Iteration 3703, loss = 1.63165382\n",
      "Iteration 3704, loss = 1.63160737\n",
      "Iteration 3705, loss = 1.63156093\n",
      "Iteration 3706, loss = 1.63151451\n",
      "Iteration 3707, loss = 1.63146811\n",
      "Iteration 3708, loss = 1.63142172\n",
      "Iteration 3709, loss = 1.63137535\n",
      "Iteration 3710, loss = 1.63132899\n",
      "Iteration 3711, loss = 1.63128264\n",
      "Iteration 3712, loss = 1.63123631\n",
      "Iteration 3713, loss = 1.63119000\n",
      "Iteration 3714, loss = 1.63114369\n",
      "Iteration 3715, loss = 1.63109741\n",
      "Iteration 3716, loss = 1.63105114\n",
      "Iteration 3717, loss = 1.63100488\n",
      "Iteration 3718, loss = 1.63095864\n",
      "Iteration 3719, loss = 1.63091241\n",
      "Iteration 3720, loss = 1.63086620\n",
      "Iteration 3721, loss = 1.63082001\n",
      "Iteration 3722, loss = 1.63077382\n",
      "Iteration 3723, loss = 1.63072766\n",
      "Iteration 3724, loss = 1.63068151\n",
      "Iteration 3725, loss = 1.63063537\n",
      "Iteration 3726, loss = 1.63058925\n",
      "Iteration 3727, loss = 1.63054314\n",
      "Iteration 3728, loss = 1.63049704\n",
      "Iteration 3729, loss = 1.63045097\n",
      "Iteration 3730, loss = 1.63040490\n",
      "Iteration 3731, loss = 1.63035885\n",
      "Iteration 3732, loss = 1.63031282\n",
      "Iteration 3733, loss = 1.63026680\n",
      "Iteration 3734, loss = 1.63022080\n",
      "Iteration 3735, loss = 1.63017481\n",
      "Iteration 3736, loss = 1.63012883\n",
      "Iteration 3737, loss = 1.63008287\n",
      "Iteration 3738, loss = 1.63003692\n",
      "Iteration 3739, loss = 1.62999099\n",
      "Iteration 3740, loss = 1.62994508\n",
      "Iteration 3741, loss = 1.62989918\n",
      "Iteration 3742, loss = 1.62985329\n",
      "Iteration 3743, loss = 1.62980742\n",
      "Iteration 3744, loss = 1.62976156\n",
      "Iteration 3745, loss = 1.62971572\n",
      "Iteration 3746, loss = 1.62966989\n",
      "Iteration 3747, loss = 1.62962407\n",
      "Iteration 3748, loss = 1.62957828\n",
      "Iteration 3749, loss = 1.62953249\n",
      "Iteration 3750, loss = 1.62948672\n",
      "Iteration 3751, loss = 1.62944097\n",
      "Iteration 3752, loss = 1.62939523\n",
      "Iteration 3753, loss = 1.62934950\n",
      "Iteration 3754, loss = 1.62930379\n",
      "Iteration 3755, loss = 1.62925809\n",
      "Iteration 3756, loss = 1.62921241\n",
      "Iteration 3757, loss = 1.62916675\n",
      "Iteration 3758, loss = 1.62912109\n",
      "Iteration 3759, loss = 1.62907546\n",
      "Iteration 3760, loss = 1.62902983\n",
      "Iteration 3761, loss = 1.62898422\n",
      "Iteration 3762, loss = 1.62893863\n",
      "Iteration 3763, loss = 1.62889305\n",
      "Iteration 3764, loss = 1.62884749\n",
      "Iteration 3765, loss = 1.62880194\n",
      "Iteration 3766, loss = 1.62875640\n",
      "Iteration 3767, loss = 1.62871088\n",
      "Iteration 3768, loss = 1.62866537\n",
      "Iteration 3769, loss = 1.62861988\n",
      "Iteration 3770, loss = 1.62857440\n",
      "Iteration 3771, loss = 1.62852894\n",
      "Iteration 3772, loss = 1.62848349\n",
      "Iteration 3773, loss = 1.62843806\n",
      "Iteration 3774, loss = 1.62839264\n",
      "Iteration 3775, loss = 1.62834723\n",
      "Iteration 3776, loss = 1.62830184\n",
      "Iteration 3777, loss = 1.62825647\n",
      "Iteration 3778, loss = 1.62821110\n",
      "Iteration 3779, loss = 1.62816576\n",
      "Iteration 3780, loss = 1.62812043\n",
      "Iteration 3781, loss = 1.62807511\n",
      "Iteration 3782, loss = 1.62802980\n",
      "Iteration 3783, loss = 1.62798452\n",
      "Iteration 3784, loss = 1.62793924\n",
      "Iteration 3785, loss = 1.62789398\n",
      "Iteration 3786, loss = 1.62784874\n",
      "Iteration 3787, loss = 1.62780350\n",
      "Iteration 3788, loss = 1.62775829\n",
      "Iteration 3789, loss = 1.62771309\n",
      "Iteration 3790, loss = 1.62766790\n",
      "Iteration 3791, loss = 1.62762272\n",
      "Iteration 3792, loss = 1.62757757\n",
      "Iteration 3793, loss = 1.62753242\n",
      "Iteration 3794, loss = 1.62748729\n",
      "Iteration 3795, loss = 1.62744218\n",
      "Iteration 3796, loss = 1.62739707\n",
      "Iteration 3797, loss = 1.62735199\n",
      "Iteration 3798, loss = 1.62730691\n",
      "Iteration 3799, loss = 1.62726186\n",
      "Iteration 3800, loss = 1.62721681\n",
      "Iteration 3801, loss = 1.62717178\n",
      "Iteration 3802, loss = 1.62712677\n",
      "Iteration 3803, loss = 1.62708177\n",
      "Iteration 3804, loss = 1.62703678\n",
      "Iteration 3805, loss = 1.62699181\n",
      "Iteration 3806, loss = 1.62694685\n",
      "Iteration 3807, loss = 1.62690191\n",
      "Iteration 3808, loss = 1.62685698\n",
      "Iteration 3809, loss = 1.62681207\n",
      "Iteration 3810, loss = 1.62676717\n",
      "Iteration 3811, loss = 1.62672228\n",
      "Iteration 3812, loss = 1.62667741\n",
      "Iteration 3813, loss = 1.62663255\n",
      "Iteration 3814, loss = 1.62658771\n",
      "Iteration 3815, loss = 1.62654288\n",
      "Iteration 3816, loss = 1.62649807\n",
      "Iteration 3817, loss = 1.62645327\n",
      "Iteration 3818, loss = 1.62640848\n",
      "Iteration 3819, loss = 1.62636371\n",
      "Iteration 3820, loss = 1.62631895\n",
      "Iteration 3821, loss = 1.62627421\n",
      "Iteration 3822, loss = 1.62622948\n",
      "Iteration 3823, loss = 1.62618477\n",
      "Iteration 3824, loss = 1.62614007\n",
      "Iteration 3825, loss = 1.62609538\n",
      "Iteration 3826, loss = 1.62605071\n",
      "Iteration 3827, loss = 1.62600605\n",
      "Iteration 3828, loss = 1.62596141\n",
      "Iteration 3829, loss = 1.62591678\n",
      "Iteration 3830, loss = 1.62587217\n",
      "Iteration 3831, loss = 1.62582757\n",
      "Iteration 3832, loss = 1.62578298\n",
      "Iteration 3833, loss = 1.62573841\n",
      "Iteration 3834, loss = 1.62569385\n",
      "Iteration 3835, loss = 1.62564931\n",
      "Iteration 3836, loss = 1.62560478\n",
      "Iteration 3837, loss = 1.62556026\n",
      "Iteration 3838, loss = 1.62551576\n",
      "Iteration 3839, loss = 1.62547127\n",
      "Iteration 3840, loss = 1.62542680\n",
      "Iteration 3841, loss = 1.62538234\n",
      "Iteration 3842, loss = 1.62533790\n",
      "Iteration 3843, loss = 1.62529347\n",
      "Iteration 3844, loss = 1.62524905\n",
      "Iteration 3845, loss = 1.62520465\n",
      "Iteration 3846, loss = 1.62516026\n",
      "Iteration 3847, loss = 1.62511589\n",
      "Iteration 3848, loss = 1.62507153\n",
      "Iteration 3849, loss = 1.62502719\n",
      "Iteration 3850, loss = 1.62498285\n",
      "Iteration 3851, loss = 1.62493854\n",
      "Iteration 3852, loss = 1.62489423\n",
      "Iteration 3853, loss = 1.62484995\n",
      "Iteration 3854, loss = 1.62480567\n",
      "Iteration 3855, loss = 1.62476141\n",
      "Iteration 3856, loss = 1.62471716\n",
      "Iteration 3857, loss = 1.62467293\n",
      "Iteration 3858, loss = 1.62462871\n",
      "Iteration 3859, loss = 1.62458451\n",
      "Iteration 3860, loss = 1.62454032\n",
      "Iteration 3861, loss = 1.62449614\n",
      "Iteration 3862, loss = 1.62445198\n",
      "Iteration 3863, loss = 1.62440783\n",
      "Iteration 3864, loss = 1.62436370\n",
      "Iteration 3865, loss = 1.62431958\n",
      "Iteration 3866, loss = 1.62427547\n",
      "Iteration 3867, loss = 1.62423138\n",
      "Iteration 3868, loss = 1.62418730\n",
      "Iteration 3869, loss = 1.62414324\n",
      "Iteration 3870, loss = 1.62409919\n",
      "Iteration 3871, loss = 1.62405515\n",
      "Iteration 3872, loss = 1.62401113\n",
      "Iteration 3873, loss = 1.62396712\n",
      "Iteration 3874, loss = 1.62392313\n",
      "Iteration 3875, loss = 1.62387915\n",
      "Iteration 3876, loss = 1.62383518\n",
      "Iteration 3877, loss = 1.62379123\n",
      "Iteration 3878, loss = 1.62374729\n",
      "Iteration 3879, loss = 1.62370337\n",
      "Iteration 3880, loss = 1.62365946\n",
      "Iteration 3881, loss = 1.62361556\n",
      "Iteration 3882, loss = 1.62357168\n",
      "Iteration 3883, loss = 1.62352781\n",
      "Iteration 3884, loss = 1.62348396\n",
      "Iteration 3885, loss = 1.62344012\n",
      "Iteration 3886, loss = 1.62339629\n",
      "Iteration 3887, loss = 1.62335248\n",
      "Iteration 3888, loss = 1.62330868\n",
      "Iteration 3889, loss = 1.62326490\n",
      "Iteration 3890, loss = 1.62322113\n",
      "Iteration 3891, loss = 1.62317737\n",
      "Iteration 3892, loss = 1.62313363\n",
      "Iteration 3893, loss = 1.62308990\n",
      "Iteration 3894, loss = 1.62304618\n",
      "Iteration 3895, loss = 1.62300248\n",
      "Iteration 3896, loss = 1.62295880\n",
      "Iteration 3897, loss = 1.62291512\n",
      "Iteration 3898, loss = 1.62287146\n",
      "Iteration 3899, loss = 1.62282782\n",
      "Iteration 3900, loss = 1.62278419\n",
      "Iteration 3901, loss = 1.62274057\n",
      "Iteration 3902, loss = 1.62269696\n",
      "Iteration 3903, loss = 1.62265337\n",
      "Iteration 3904, loss = 1.62260980\n",
      "Iteration 3905, loss = 1.62256623\n",
      "Iteration 3906, loss = 1.62252269\n",
      "Iteration 3907, loss = 1.62247915\n",
      "Iteration 3908, loss = 1.62243563\n",
      "Iteration 3909, loss = 1.62239212\n",
      "Iteration 3910, loss = 1.62234863\n",
      "Iteration 3911, loss = 1.62230515\n",
      "Iteration 3912, loss = 1.62226168\n",
      "Iteration 3913, loss = 1.62221823\n",
      "Iteration 3914, loss = 1.62217479\n",
      "Iteration 3915, loss = 1.62213137\n",
      "Iteration 3916, loss = 1.62208796\n",
      "Iteration 3917, loss = 1.62204456\n",
      "Iteration 3918, loss = 1.62200118\n",
      "Iteration 3919, loss = 1.62195781\n",
      "Iteration 3920, loss = 1.62191445\n",
      "Iteration 3921, loss = 1.62187111\n",
      "Iteration 3922, loss = 1.62182778\n",
      "Iteration 3923, loss = 1.62178447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3924, loss = 1.62174117\n",
      "Iteration 3925, loss = 1.62169788\n",
      "Iteration 3926, loss = 1.62165461\n",
      "Iteration 3927, loss = 1.62161135\n",
      "Iteration 3928, loss = 1.62156810\n",
      "Iteration 3929, loss = 1.62152487\n",
      "Iteration 3930, loss = 1.62148165\n",
      "Iteration 3931, loss = 1.62143845\n",
      "Iteration 3932, loss = 1.62139526\n",
      "Iteration 3933, loss = 1.62135208\n",
      "Iteration 3934, loss = 1.62130892\n",
      "Iteration 3935, loss = 1.62126577\n",
      "Iteration 3936, loss = 1.62122263\n",
      "Iteration 3937, loss = 1.62117951\n",
      "Iteration 3938, loss = 1.62113640\n",
      "Iteration 3939, loss = 1.62109331\n",
      "Iteration 3940, loss = 1.62105022\n",
      "Iteration 3941, loss = 1.62100716\n",
      "Iteration 3942, loss = 1.62096410\n",
      "Iteration 3943, loss = 1.62092106\n",
      "Iteration 3944, loss = 1.62087803\n",
      "Iteration 3945, loss = 1.62083502\n",
      "Iteration 3946, loss = 1.62079202\n",
      "Iteration 3947, loss = 1.62074904\n",
      "Iteration 3948, loss = 1.62070606\n",
      "Iteration 3949, loss = 1.62066310\n",
      "Iteration 3950, loss = 1.62062016\n",
      "Iteration 3951, loss = 1.62057723\n",
      "Iteration 3952, loss = 1.62053431\n",
      "Iteration 3953, loss = 1.62049141\n",
      "Iteration 3954, loss = 1.62044852\n",
      "Iteration 3955, loss = 1.62040564\n",
      "Iteration 3956, loss = 1.62036277\n",
      "Iteration 3957, loss = 1.62031992\n",
      "Iteration 3958, loss = 1.62027709\n",
      "Iteration 3959, loss = 1.62023426\n",
      "Iteration 3960, loss = 1.62019146\n",
      "Iteration 3961, loss = 1.62014866\n",
      "Iteration 3962, loss = 1.62010588\n",
      "Iteration 3963, loss = 1.62006311\n",
      "Iteration 3964, loss = 1.62002035\n",
      "Iteration 3965, loss = 1.61997761\n",
      "Iteration 3966, loss = 1.61993488\n",
      "Iteration 3967, loss = 1.61989217\n",
      "Iteration 3968, loss = 1.61984947\n",
      "Iteration 3969, loss = 1.61980678\n",
      "Iteration 3970, loss = 1.61976411\n",
      "Iteration 3971, loss = 1.61972145\n",
      "Iteration 3972, loss = 1.61967880\n",
      "Iteration 3973, loss = 1.61963617\n",
      "Iteration 3974, loss = 1.61959355\n",
      "Iteration 3975, loss = 1.61955094\n",
      "Iteration 3976, loss = 1.61950835\n",
      "Iteration 3977, loss = 1.61946577\n",
      "Iteration 3978, loss = 1.61942320\n",
      "Iteration 3979, loss = 1.61938065\n",
      "Iteration 3980, loss = 1.61933811\n",
      "Iteration 3981, loss = 1.61929558\n",
      "Iteration 3982, loss = 1.61925307\n",
      "Iteration 3983, loss = 1.61921057\n",
      "Iteration 3984, loss = 1.61916809\n",
      "Iteration 3985, loss = 1.61912561\n",
      "Iteration 3986, loss = 1.61908315\n",
      "Iteration 3987, loss = 1.61904071\n",
      "Iteration 3988, loss = 1.61899828\n",
      "Iteration 3989, loss = 1.61895586\n",
      "Iteration 3990, loss = 1.61891345\n",
      "Iteration 3991, loss = 1.61887106\n",
      "Iteration 3992, loss = 1.61882868\n",
      "Iteration 3993, loss = 1.61878632\n",
      "Iteration 3994, loss = 1.61874397\n",
      "Iteration 3995, loss = 1.61870163\n",
      "Iteration 3996, loss = 1.61865931\n",
      "Iteration 3997, loss = 1.61861699\n",
      "Iteration 3998, loss = 1.61857470\n",
      "Iteration 3999, loss = 1.61853241\n",
      "Iteration 4000, loss = 1.61849014\n",
      "Iteration 4001, loss = 1.61844788\n",
      "Iteration 4002, loss = 1.61840564\n",
      "Iteration 4003, loss = 1.61836341\n",
      "Iteration 4004, loss = 1.61832119\n",
      "Iteration 4005, loss = 1.61827899\n",
      "Iteration 4006, loss = 1.61823679\n",
      "Iteration 4007, loss = 1.61819462\n",
      "Iteration 4008, loss = 1.61815245\n",
      "Iteration 4009, loss = 1.61811030\n",
      "Iteration 4010, loss = 1.61806816\n",
      "Iteration 4011, loss = 1.61802604\n",
      "Iteration 4012, loss = 1.61798393\n",
      "Iteration 4013, loss = 1.61794183\n",
      "Iteration 4014, loss = 1.61789975\n",
      "Iteration 4015, loss = 1.61785767\n",
      "Iteration 4016, loss = 1.61781562\n",
      "Iteration 4017, loss = 1.61777357\n",
      "Iteration 4018, loss = 1.61773154\n",
      "Iteration 4019, loss = 1.61768952\n",
      "Iteration 4020, loss = 1.61764752\n",
      "Iteration 4021, loss = 1.61760552\n",
      "Iteration 4022, loss = 1.61756355\n",
      "Iteration 4023, loss = 1.61752158\n",
      "Iteration 4024, loss = 1.61747963\n",
      "Iteration 4025, loss = 1.61743769\n",
      "Iteration 4026, loss = 1.61739576\n",
      "Iteration 4027, loss = 1.61735385\n",
      "Iteration 4028, loss = 1.61731195\n",
      "Iteration 4029, loss = 1.61727007\n",
      "Iteration 4030, loss = 1.61722819\n",
      "Iteration 4031, loss = 1.61718634\n",
      "Iteration 4032, loss = 1.61714449\n",
      "Iteration 4033, loss = 1.61710266\n",
      "Iteration 4034, loss = 1.61706084\n",
      "Iteration 4035, loss = 1.61701903\n",
      "Iteration 4036, loss = 1.61697724\n",
      "Iteration 4037, loss = 1.61693546\n",
      "Iteration 4038, loss = 1.61689369\n",
      "Iteration 4039, loss = 1.61685193\n",
      "Iteration 4040, loss = 1.61681019\n",
      "Iteration 4041, loss = 1.61676847\n",
      "Iteration 4042, loss = 1.61672675\n",
      "Iteration 4043, loss = 1.61668505\n",
      "Iteration 4044, loss = 1.61664336\n",
      "Iteration 4045, loss = 1.61660169\n",
      "Iteration 4046, loss = 1.61656002\n",
      "Iteration 4047, loss = 1.61651838\n",
      "Iteration 4048, loss = 1.61647674\n",
      "Iteration 4049, loss = 1.61643512\n",
      "Iteration 4050, loss = 1.61639351\n",
      "Iteration 4051, loss = 1.61635191\n",
      "Iteration 4052, loss = 1.61631033\n",
      "Iteration 4053, loss = 1.61626876\n",
      "Iteration 4054, loss = 1.61622720\n",
      "Iteration 4055, loss = 1.61618566\n",
      "Iteration 4056, loss = 1.61614413\n",
      "Iteration 4057, loss = 1.61610261\n",
      "Iteration 4058, loss = 1.61606110\n",
      "Iteration 4059, loss = 1.61601961\n",
      "Iteration 4060, loss = 1.61597813\n",
      "Iteration 4061, loss = 1.61593667\n",
      "Iteration 4062, loss = 1.61589522\n",
      "Iteration 4063, loss = 1.61585378\n",
      "Iteration 4064, loss = 1.61581235\n",
      "Iteration 4065, loss = 1.61577094\n",
      "Iteration 4066, loss = 1.61572954\n",
      "Iteration 4067, loss = 1.61568815\n",
      "Iteration 4068, loss = 1.61564677\n",
      "Iteration 4069, loss = 1.61560541\n",
      "Iteration 4070, loss = 1.61556406\n",
      "Iteration 4071, loss = 1.61552273\n",
      "Iteration 4072, loss = 1.61548141\n",
      "Iteration 4073, loss = 1.61544010\n",
      "Iteration 4074, loss = 1.61539880\n",
      "Iteration 4075, loss = 1.61535752\n",
      "Iteration 4076, loss = 1.61531625\n",
      "Iteration 4077, loss = 1.61527499\n",
      "Iteration 4078, loss = 1.61523375\n",
      "Iteration 4079, loss = 1.61519252\n",
      "Iteration 4080, loss = 1.61515130\n",
      "Iteration 4081, loss = 1.61511009\n",
      "Iteration 4082, loss = 1.61506890\n",
      "Iteration 4083, loss = 1.61502772\n",
      "Iteration 4084, loss = 1.61498655\n",
      "Iteration 4085, loss = 1.61494540\n",
      "Iteration 4086, loss = 1.61490426\n",
      "Iteration 4087, loss = 1.61486313\n",
      "Iteration 4088, loss = 1.61482202\n",
      "Iteration 4089, loss = 1.61478092\n",
      "Iteration 4090, loss = 1.61473983\n",
      "Iteration 4091, loss = 1.61469875\n",
      "Iteration 4092, loss = 1.61465769\n",
      "Iteration 4093, loss = 1.61461664\n",
      "Iteration 4094, loss = 1.61457560\n",
      "Iteration 4095, loss = 1.61453458\n",
      "Iteration 4096, loss = 1.61449356\n",
      "Iteration 4097, loss = 1.61445257\n",
      "Iteration 4098, loss = 1.61441158\n",
      "Iteration 4099, loss = 1.61437061\n",
      "Iteration 4100, loss = 1.61432965\n",
      "Iteration 4101, loss = 1.61428870\n",
      "Iteration 4102, loss = 1.61424777\n",
      "Iteration 4103, loss = 1.61420684\n",
      "Iteration 4104, loss = 1.61416594\n",
      "Iteration 4105, loss = 1.61412504\n",
      "Iteration 4106, loss = 1.61408416\n",
      "Iteration 4107, loss = 1.61404329\n",
      "Iteration 4108, loss = 1.61400243\n",
      "Iteration 4109, loss = 1.61396159\n",
      "Iteration 4110, loss = 1.61392075\n",
      "Iteration 4111, loss = 1.61387994\n",
      "Iteration 4112, loss = 1.61383913\n",
      "Iteration 4113, loss = 1.61379834\n",
      "Iteration 4114, loss = 1.61375756\n",
      "Iteration 4115, loss = 1.61371679\n",
      "Iteration 4116, loss = 1.61367604\n",
      "Iteration 4117, loss = 1.61363529\n",
      "Iteration 4118, loss = 1.61359456\n",
      "Iteration 4119, loss = 1.61355385\n",
      "Iteration 4120, loss = 1.61351314\n",
      "Iteration 4121, loss = 1.61347245\n",
      "Iteration 4122, loss = 1.61343178\n",
      "Iteration 4123, loss = 1.61339111\n",
      "Iteration 4124, loss = 1.61335046\n",
      "Iteration 4125, loss = 1.61330982\n",
      "Iteration 4126, loss = 1.61326919\n",
      "Iteration 4127, loss = 1.61322858\n",
      "Iteration 4128, loss = 1.61318798\n",
      "Iteration 4129, loss = 1.61314739\n",
      "Iteration 4130, loss = 1.61310681\n",
      "Iteration 4131, loss = 1.61306625\n",
      "Iteration 4132, loss = 1.61302570\n",
      "Iteration 4133, loss = 1.61298516\n",
      "Iteration 4134, loss = 1.61294464\n",
      "Iteration 4135, loss = 1.61290412\n",
      "Iteration 4136, loss = 1.61286362\n",
      "Iteration 4137, loss = 1.61282314\n",
      "Iteration 4138, loss = 1.61278266\n",
      "Iteration 4139, loss = 1.61274220\n",
      "Iteration 4140, loss = 1.61270175\n",
      "Iteration 4141, loss = 1.61266132\n",
      "Iteration 4142, loss = 1.61262089\n",
      "Iteration 4143, loss = 1.61258048\n",
      "Iteration 4144, loss = 1.61254008\n",
      "Iteration 4145, loss = 1.61249970\n",
      "Iteration 4146, loss = 1.61245933\n",
      "Iteration 4147, loss = 1.61241896\n",
      "Iteration 4148, loss = 1.61237862\n",
      "Iteration 4149, loss = 1.61233828\n",
      "Iteration 4150, loss = 1.61229796\n",
      "Iteration 4151, loss = 1.61225765\n",
      "Iteration 4152, loss = 1.61221735\n",
      "Iteration 4153, loss = 1.61217707\n",
      "Iteration 4154, loss = 1.61213680\n",
      "Iteration 4155, loss = 1.61209654\n",
      "Iteration 4156, loss = 1.61205629\n",
      "Iteration 4157, loss = 1.61201606\n",
      "Iteration 4158, loss = 1.61197584\n",
      "Iteration 4159, loss = 1.61193563\n",
      "Iteration 4160, loss = 1.61189543\n",
      "Iteration 4161, loss = 1.61185525\n",
      "Iteration 4162, loss = 1.61181508\n",
      "Iteration 4163, loss = 1.61177492\n",
      "Iteration 4164, loss = 1.61173477\n",
      "Iteration 4165, loss = 1.61169464\n",
      "Iteration 4166, loss = 1.61165452\n",
      "Iteration 4167, loss = 1.61161441\n",
      "Iteration 4168, loss = 1.61157432\n",
      "Iteration 4169, loss = 1.61153423\n",
      "Iteration 4170, loss = 1.61149416\n",
      "Iteration 4171, loss = 1.61145410\n",
      "Iteration 4172, loss = 1.61141406\n",
      "Iteration 4173, loss = 1.61137403\n",
      "Iteration 4174, loss = 1.61133401\n",
      "Iteration 4175, loss = 1.61129400\n",
      "Iteration 4176, loss = 1.61125400\n",
      "Iteration 4177, loss = 1.61121402\n",
      "Iteration 4178, loss = 1.61117405\n",
      "Iteration 4179, loss = 1.61113409\n",
      "Iteration 4180, loss = 1.61109415\n",
      "Iteration 4181, loss = 1.61105421\n",
      "Iteration 4182, loss = 1.61101429\n",
      "Iteration 4183, loss = 1.61097439\n",
      "Iteration 4184, loss = 1.61093449\n",
      "Iteration 4185, loss = 1.61089461\n",
      "Iteration 4186, loss = 1.61085474\n",
      "Iteration 4187, loss = 1.61081488\n",
      "Iteration 4188, loss = 1.61077503\n",
      "Iteration 4189, loss = 1.61073520\n",
      "Iteration 4190, loss = 1.61069538\n",
      "Iteration 4191, loss = 1.61065557\n",
      "Iteration 4192, loss = 1.61061578\n",
      "Iteration 4193, loss = 1.61057599\n",
      "Iteration 4194, loss = 1.61053622\n",
      "Iteration 4195, loss = 1.61049646\n",
      "Iteration 4196, loss = 1.61045672\n",
      "Iteration 4197, loss = 1.61041698\n",
      "Iteration 4198, loss = 1.61037726\n",
      "Iteration 4199, loss = 1.61033755\n",
      "Iteration 4200, loss = 1.61029786\n",
      "Iteration 4201, loss = 1.61025817\n",
      "Iteration 4202, loss = 1.61021850\n",
      "Iteration 4203, loss = 1.61017884\n",
      "Iteration 4204, loss = 1.61013920\n",
      "Iteration 4205, loss = 1.61009956\n",
      "Iteration 4206, loss = 1.61005994\n",
      "Iteration 4207, loss = 1.61002033\n",
      "Iteration 4208, loss = 1.60998073\n",
      "Iteration 4209, loss = 1.60994115\n",
      "Iteration 4210, loss = 1.60990158\n",
      "Iteration 4211, loss = 1.60986202\n",
      "Iteration 4212, loss = 1.60982247\n",
      "Iteration 4213, loss = 1.60978293\n",
      "Iteration 4214, loss = 1.60974341\n",
      "Iteration 4215, loss = 1.60970390\n",
      "Iteration 4216, loss = 1.60966440\n",
      "Iteration 4217, loss = 1.60962492\n",
      "Iteration 4218, loss = 1.60958544\n",
      "Iteration 4219, loss = 1.60954598\n",
      "Iteration 4220, loss = 1.60950653\n",
      "Iteration 4221, loss = 1.60946710\n",
      "Iteration 4222, loss = 1.60942767\n",
      "Iteration 4223, loss = 1.60938826\n",
      "Iteration 4224, loss = 1.60934886\n",
      "Iteration 4225, loss = 1.60930947\n",
      "Iteration 4226, loss = 1.60927010\n",
      "Iteration 4227, loss = 1.60923073\n",
      "Iteration 4228, loss = 1.60919138\n",
      "Iteration 4229, loss = 1.60915204\n",
      "Iteration 4230, loss = 1.60911272\n",
      "Iteration 4231, loss = 1.60907340\n",
      "Iteration 4232, loss = 1.60903410\n",
      "Iteration 4233, loss = 1.60899481\n",
      "Iteration 4234, loss = 1.60895554\n",
      "Iteration 4235, loss = 1.60891627\n",
      "Iteration 4236, loss = 1.60887702\n",
      "Iteration 4237, loss = 1.60883778\n",
      "Iteration 4238, loss = 1.60879855\n",
      "Iteration 4239, loss = 1.60875933\n",
      "Iteration 4240, loss = 1.60872013\n",
      "Iteration 4241, loss = 1.60868094\n",
      "Iteration 4242, loss = 1.60864176\n",
      "Iteration 4243, loss = 1.60860259\n",
      "Iteration 4244, loss = 1.60856344\n",
      "Iteration 4245, loss = 1.60852430\n",
      "Iteration 4246, loss = 1.60848517\n",
      "Iteration 4247, loss = 1.60844605\n",
      "Iteration 4248, loss = 1.60840694\n",
      "Iteration 4249, loss = 1.60836785\n",
      "Iteration 4250, loss = 1.60832877\n",
      "Iteration 4251, loss = 1.60828970\n",
      "Iteration 4252, loss = 1.60825064\n",
      "Iteration 4253, loss = 1.60821159\n",
      "Iteration 4254, loss = 1.60817256\n",
      "Iteration 4255, loss = 1.60813354\n",
      "Iteration 4256, loss = 1.60809453\n",
      "Iteration 4257, loss = 1.60805553\n",
      "Iteration 4258, loss = 1.60801655\n",
      "Iteration 4259, loss = 1.60797758\n",
      "Iteration 4260, loss = 1.60793862\n",
      "Iteration 4261, loss = 1.60789967\n",
      "Iteration 4262, loss = 1.60786073\n",
      "Iteration 4263, loss = 1.60782181\n",
      "Iteration 4264, loss = 1.60778290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4265, loss = 1.60774400\n",
      "Iteration 4266, loss = 1.60770511\n",
      "Iteration 4267, loss = 1.60766624\n",
      "Iteration 4268, loss = 1.60762737\n",
      "Iteration 4269, loss = 1.60758852\n",
      "Iteration 4270, loss = 1.60754968\n",
      "Iteration 4271, loss = 1.60751086\n",
      "Iteration 4272, loss = 1.60747204\n",
      "Iteration 4273, loss = 1.60743324\n",
      "Iteration 4274, loss = 1.60739445\n",
      "Iteration 4275, loss = 1.60735567\n",
      "Iteration 4276, loss = 1.60731690\n",
      "Iteration 4277, loss = 1.60727815\n",
      "Iteration 4278, loss = 1.60723941\n",
      "Iteration 4279, loss = 1.60720068\n",
      "Iteration 4280, loss = 1.60716196\n",
      "Iteration 4281, loss = 1.60712325\n",
      "Iteration 4282, loss = 1.60708456\n",
      "Iteration 4283, loss = 1.60704587\n",
      "Iteration 4284, loss = 1.60700720\n",
      "Iteration 4285, loss = 1.60696855\n",
      "Iteration 4286, loss = 1.60692990\n",
      "Iteration 4287, loss = 1.60689127\n",
      "Iteration 4288, loss = 1.60685264\n",
      "Iteration 4289, loss = 1.60681403\n",
      "Iteration 4290, loss = 1.60677544\n",
      "Iteration 4291, loss = 1.60673685\n",
      "Iteration 4292, loss = 1.60669828\n",
      "Iteration 4293, loss = 1.60665971\n",
      "Iteration 4294, loss = 1.60662116\n",
      "Iteration 4295, loss = 1.60658263\n",
      "Iteration 4296, loss = 1.60654410\n",
      "Iteration 4297, loss = 1.60650559\n",
      "Iteration 4298, loss = 1.60646708\n",
      "Iteration 4299, loss = 1.60642859\n",
      "Iteration 4300, loss = 1.60639011\n",
      "Iteration 4301, loss = 1.60635165\n",
      "Iteration 4302, loss = 1.60631319\n",
      "Iteration 4303, loss = 1.60627475\n",
      "Iteration 4304, loss = 1.60623632\n",
      "Iteration 4305, loss = 1.60619790\n",
      "Iteration 4306, loss = 1.60615950\n",
      "Iteration 4307, loss = 1.60612110\n",
      "Iteration 4308, loss = 1.60608272\n",
      "Iteration 4309, loss = 1.60604435\n",
      "Iteration 4310, loss = 1.60600599\n",
      "Iteration 4311, loss = 1.60596764\n",
      "Iteration 4312, loss = 1.60592931\n",
      "Iteration 4313, loss = 1.60589098\n",
      "Iteration 4314, loss = 1.60585267\n",
      "Iteration 4315, loss = 1.60581437\n",
      "Iteration 4316, loss = 1.60577608\n",
      "Iteration 4317, loss = 1.60573781\n",
      "Iteration 4318, loss = 1.60569955\n",
      "Iteration 4319, loss = 1.60566129\n",
      "Iteration 4320, loss = 1.60562305\n",
      "Iteration 4321, loss = 1.60558482\n",
      "Iteration 4322, loss = 1.60554661\n",
      "Iteration 4323, loss = 1.60550840\n",
      "Iteration 4324, loss = 1.60547021\n",
      "Iteration 4325, loss = 1.60543203\n",
      "Iteration 4326, loss = 1.60539386\n",
      "Iteration 4327, loss = 1.60535570\n",
      "Iteration 4328, loss = 1.60531756\n",
      "Iteration 4329, loss = 1.60527943\n",
      "Iteration 4330, loss = 1.60524130\n",
      "Iteration 4331, loss = 1.60520320\n",
      "Iteration 4332, loss = 1.60516510\n",
      "Iteration 4333, loss = 1.60512701\n",
      "Iteration 4334, loss = 1.60508894\n",
      "Iteration 4335, loss = 1.60505087\n",
      "Iteration 4336, loss = 1.60501282\n",
      "Iteration 4337, loss = 1.60497478\n",
      "Iteration 4338, loss = 1.60493676\n",
      "Iteration 4339, loss = 1.60489874\n",
      "Iteration 4340, loss = 1.60486074\n",
      "Iteration 4341, loss = 1.60482275\n",
      "Iteration 4342, loss = 1.60478477\n",
      "Iteration 4343, loss = 1.60474680\n",
      "Iteration 4344, loss = 1.60470884\n",
      "Iteration 4345, loss = 1.60467090\n",
      "Iteration 4346, loss = 1.60463296\n",
      "Iteration 4347, loss = 1.60459504\n",
      "Iteration 4348, loss = 1.60455713\n",
      "Iteration 4349, loss = 1.60451924\n",
      "Iteration 4350, loss = 1.60448135\n",
      "Iteration 4351, loss = 1.60444348\n",
      "Iteration 4352, loss = 1.60440561\n",
      "Iteration 4353, loss = 1.60436776\n",
      "Iteration 4354, loss = 1.60432992\n",
      "Iteration 4355, loss = 1.60429210\n",
      "Iteration 4356, loss = 1.60425428\n",
      "Iteration 4357, loss = 1.60421648\n",
      "Iteration 4358, loss = 1.60417869\n",
      "Iteration 4359, loss = 1.60414091\n",
      "Iteration 4360, loss = 1.60410314\n",
      "Iteration 4361, loss = 1.60406538\n",
      "Iteration 4362, loss = 1.60402763\n",
      "Iteration 4363, loss = 1.60398990\n",
      "Iteration 4364, loss = 1.60395218\n",
      "Iteration 4365, loss = 1.60391447\n",
      "Iteration 4366, loss = 1.60387677\n",
      "Iteration 4367, loss = 1.60383908\n",
      "Iteration 4368, loss = 1.60380141\n",
      "Iteration 4369, loss = 1.60376374\n",
      "Iteration 4370, loss = 1.60372609\n",
      "Iteration 4371, loss = 1.60368845\n",
      "Iteration 4372, loss = 1.60365082\n",
      "Iteration 4373, loss = 1.60361321\n",
      "Iteration 4374, loss = 1.60357560\n",
      "Iteration 4375, loss = 1.60353801\n",
      "Iteration 4376, loss = 1.60350042\n",
      "Iteration 4377, loss = 1.60346285\n",
      "Iteration 4378, loss = 1.60342529\n",
      "Iteration 4379, loss = 1.60338775\n",
      "Iteration 4380, loss = 1.60335021\n",
      "Iteration 4381, loss = 1.60331269\n",
      "Iteration 4382, loss = 1.60327517\n",
      "Iteration 4383, loss = 1.60323767\n",
      "Iteration 4384, loss = 1.60320018\n",
      "Iteration 4385, loss = 1.60316271\n",
      "Iteration 4386, loss = 1.60312524\n",
      "Iteration 4387, loss = 1.60308779\n",
      "Iteration 4388, loss = 1.60305034\n",
      "Iteration 4389, loss = 1.60301291\n",
      "Iteration 4390, loss = 1.60297549\n",
      "Iteration 4391, loss = 1.60293808\n",
      "Iteration 4392, loss = 1.60290069\n",
      "Iteration 4393, loss = 1.60286330\n",
      "Iteration 4394, loss = 1.60282593\n",
      "Iteration 4395, loss = 1.60278857\n",
      "Iteration 4396, loss = 1.60275122\n",
      "Iteration 4397, loss = 1.60271388\n",
      "Iteration 4398, loss = 1.60267655\n",
      "Iteration 4399, loss = 1.60263923\n",
      "Iteration 4400, loss = 1.60260193\n",
      "Iteration 4401, loss = 1.60256464\n",
      "Iteration 4402, loss = 1.60252736\n",
      "Iteration 4403, loss = 1.60249009\n",
      "Iteration 4404, loss = 1.60245283\n",
      "Iteration 4405, loss = 1.60241558\n",
      "Iteration 4406, loss = 1.60237835\n",
      "Iteration 4407, loss = 1.60234112\n",
      "Iteration 4408, loss = 1.60230391\n",
      "Iteration 4409, loss = 1.60226671\n",
      "Iteration 4410, loss = 1.60222952\n",
      "Iteration 4411, loss = 1.60219234\n",
      "Iteration 4412, loss = 1.60215518\n",
      "Iteration 4413, loss = 1.60211802\n",
      "Iteration 4414, loss = 1.60208088\n",
      "Iteration 4415, loss = 1.60204375\n",
      "Iteration 4416, loss = 1.60200662\n",
      "Iteration 4417, loss = 1.60196952\n",
      "Iteration 4418, loss = 1.60193242\n",
      "Iteration 4419, loss = 1.60189533\n",
      "Iteration 4420, loss = 1.60185826\n",
      "Iteration 4421, loss = 1.60182119\n",
      "Iteration 4422, loss = 1.60178414\n",
      "Iteration 4423, loss = 1.60174710\n",
      "Iteration 4424, loss = 1.60171007\n",
      "Iteration 4425, loss = 1.60167306\n",
      "Iteration 4426, loss = 1.60163605\n",
      "Iteration 4427, loss = 1.60159905\n",
      "Iteration 4428, loss = 1.60156207\n",
      "Iteration 4429, loss = 1.60152510\n",
      "Iteration 4430, loss = 1.60148814\n",
      "Iteration 4431, loss = 1.60145119\n",
      "Iteration 4432, loss = 1.60141425\n",
      "Iteration 4433, loss = 1.60137733\n",
      "Iteration 4434, loss = 1.60134041\n",
      "Iteration 4435, loss = 1.60130351\n",
      "Iteration 4436, loss = 1.60126662\n",
      "Iteration 4437, loss = 1.60122973\n",
      "Iteration 4438, loss = 1.60119287\n",
      "Iteration 4439, loss = 1.60115601\n",
      "Iteration 4440, loss = 1.60111916\n",
      "Iteration 4441, loss = 1.60108233\n",
      "Iteration 4442, loss = 1.60104550\n",
      "Iteration 4443, loss = 1.60100869\n",
      "Iteration 4444, loss = 1.60097189\n",
      "Iteration 4445, loss = 1.60093510\n",
      "Iteration 4446, loss = 1.60089832\n",
      "Iteration 4447, loss = 1.60086155\n",
      "Iteration 4448, loss = 1.60082480\n",
      "Iteration 4449, loss = 1.60078805\n",
      "Iteration 4450, loss = 1.60075132\n",
      "Iteration 4451, loss = 1.60071460\n",
      "Iteration 4452, loss = 1.60067788\n",
      "Iteration 4453, loss = 1.60064119\n",
      "Iteration 4454, loss = 1.60060450\n",
      "Iteration 4455, loss = 1.60056782\n",
      "Iteration 4456, loss = 1.60053116\n",
      "Iteration 4457, loss = 1.60049450\n",
      "Iteration 4458, loss = 1.60045786\n",
      "Iteration 4459, loss = 1.60042123\n",
      "Iteration 4460, loss = 1.60038461\n",
      "Iteration 4461, loss = 1.60034800\n",
      "Iteration 4462, loss = 1.60031140\n",
      "Iteration 4463, loss = 1.60027481\n",
      "Iteration 4464, loss = 1.60023824\n",
      "Iteration 4465, loss = 1.60020167\n",
      "Iteration 4466, loss = 1.60016512\n",
      "Iteration 4467, loss = 1.60012858\n",
      "Iteration 4468, loss = 1.60009205\n",
      "Iteration 4469, loss = 1.60005553\n",
      "Iteration 4470, loss = 1.60001902\n",
      "Iteration 4471, loss = 1.59998253\n",
      "Iteration 4472, loss = 1.59994604\n",
      "Iteration 4473, loss = 1.59990957\n",
      "Iteration 4474, loss = 1.59987310\n",
      "Iteration 4475, loss = 1.59983665\n",
      "Iteration 4476, loss = 1.59980021\n",
      "Iteration 4477, loss = 1.59976378\n",
      "Iteration 4478, loss = 1.59972736\n",
      "Iteration 4479, loss = 1.59969096\n",
      "Iteration 4480, loss = 1.59965456\n",
      "Iteration 4481, loss = 1.59961818\n",
      "Iteration 4482, loss = 1.59958180\n",
      "Iteration 4483, loss = 1.59954544\n",
      "Iteration 4484, loss = 1.59950909\n",
      "Iteration 4485, loss = 1.59947275\n",
      "Iteration 4486, loss = 1.59943642\n",
      "Iteration 4487, loss = 1.59940010\n",
      "Iteration 4488, loss = 1.59936380\n",
      "Iteration 4489, loss = 1.59932750\n",
      "Iteration 4490, loss = 1.59929122\n",
      "Iteration 4491, loss = 1.59925495\n",
      "Iteration 4492, loss = 1.59921868\n",
      "Iteration 4493, loss = 1.59918243\n",
      "Iteration 4494, loss = 1.59914619\n",
      "Iteration 4495, loss = 1.59910997\n",
      "Iteration 4496, loss = 1.59907375\n",
      "Iteration 4497, loss = 1.59903754\n",
      "Iteration 4498, loss = 1.59900135\n",
      "Iteration 4499, loss = 1.59896516\n",
      "Iteration 4500, loss = 1.59892899\n",
      "Iteration 4501, loss = 1.59889283\n",
      "Iteration 4502, loss = 1.59885668\n",
      "Iteration 4503, loss = 1.59882054\n",
      "Iteration 4504, loss = 1.59878441\n",
      "Iteration 4505, loss = 1.59874829\n",
      "Iteration 4506, loss = 1.59871219\n",
      "Iteration 4507, loss = 1.59867609\n",
      "Iteration 4508, loss = 1.59864001\n",
      "Iteration 4509, loss = 1.59860394\n",
      "Iteration 4510, loss = 1.59856787\n",
      "Iteration 4511, loss = 1.59853182\n",
      "Iteration 4512, loss = 1.59849578\n",
      "Iteration 4513, loss = 1.59845976\n",
      "Iteration 4514, loss = 1.59842374\n",
      "Iteration 4515, loss = 1.59838773\n",
      "Iteration 4516, loss = 1.59835174\n",
      "Iteration 4517, loss = 1.59831575\n",
      "Iteration 4518, loss = 1.59827978\n",
      "Iteration 4519, loss = 1.59824382\n",
      "Iteration 4520, loss = 1.59820786\n",
      "Iteration 4521, loss = 1.59817192\n",
      "Iteration 4522, loss = 1.59813599\n",
      "Iteration 4523, loss = 1.59810008\n",
      "Iteration 4524, loss = 1.59806417\n",
      "Iteration 4525, loss = 1.59802827\n",
      "Iteration 4526, loss = 1.59799239\n",
      "Iteration 4527, loss = 1.59795651\n",
      "Iteration 4528, loss = 1.59792065\n",
      "Iteration 4529, loss = 1.59788480\n",
      "Iteration 4530, loss = 1.59784896\n",
      "Iteration 4531, loss = 1.59781313\n",
      "Iteration 4532, loss = 1.59777731\n",
      "Iteration 4533, loss = 1.59774150\n",
      "Iteration 4534, loss = 1.59770570\n",
      "Iteration 4535, loss = 1.59766992\n",
      "Iteration 4536, loss = 1.59763414\n",
      "Iteration 4537, loss = 1.59759838\n",
      "Iteration 4538, loss = 1.59756262\n",
      "Iteration 4539, loss = 1.59752688\n",
      "Iteration 4540, loss = 1.59749115\n",
      "Iteration 4541, loss = 1.59745543\n",
      "Iteration 4542, loss = 1.59741972\n",
      "Iteration 4543, loss = 1.59738402\n",
      "Iteration 4544, loss = 1.59734833\n",
      "Iteration 4545, loss = 1.59731266\n",
      "Iteration 4546, loss = 1.59727699\n",
      "Iteration 4547, loss = 1.59724133\n",
      "Iteration 4548, loss = 1.59720569\n",
      "Iteration 4549, loss = 1.59717006\n",
      "Iteration 4550, loss = 1.59713443\n",
      "Iteration 4551, loss = 1.59709882\n",
      "Iteration 4552, loss = 1.59706322\n",
      "Iteration 4553, loss = 1.59702763\n",
      "Iteration 4554, loss = 1.59699205\n",
      "Iteration 4555, loss = 1.59695649\n",
      "Iteration 4556, loss = 1.59692093\n",
      "Iteration 4557, loss = 1.59688538\n",
      "Iteration 4558, loss = 1.59684985\n",
      "Iteration 4559, loss = 1.59681433\n",
      "Iteration 4560, loss = 1.59677881\n",
      "Iteration 4561, loss = 1.59674331\n",
      "Iteration 4562, loss = 1.59670782\n",
      "Iteration 4563, loss = 1.59667234\n",
      "Iteration 4564, loss = 1.59663687\n",
      "Iteration 4565, loss = 1.59660141\n",
      "Iteration 4566, loss = 1.59656596\n",
      "Iteration 4567, loss = 1.59653052\n",
      "Iteration 4568, loss = 1.59649510\n",
      "Iteration 4569, loss = 1.59645968\n",
      "Iteration 4570, loss = 1.59642428\n",
      "Iteration 4571, loss = 1.59638888\n",
      "Iteration 4572, loss = 1.59635350\n",
      "Iteration 4573, loss = 1.59631813\n",
      "Iteration 4574, loss = 1.59628276\n",
      "Iteration 4575, loss = 1.59624741\n",
      "Iteration 4576, loss = 1.59621207\n",
      "Iteration 4577, loss = 1.59617674\n",
      "Iteration 4578, loss = 1.59614143\n",
      "Iteration 4579, loss = 1.59610612\n",
      "Iteration 4580, loss = 1.59607082\n",
      "Iteration 4581, loss = 1.59603554\n",
      "Iteration 4582, loss = 1.59600026\n",
      "Iteration 4583, loss = 1.59596500\n",
      "Iteration 4584, loss = 1.59592974\n",
      "Iteration 4585, loss = 1.59589450\n",
      "Iteration 4586, loss = 1.59585927\n",
      "Iteration 4587, loss = 1.59582405\n",
      "Iteration 4588, loss = 1.59578884\n",
      "Iteration 4589, loss = 1.59575364\n",
      "Iteration 4590, loss = 1.59571845\n",
      "Iteration 4591, loss = 1.59568327\n",
      "Iteration 4592, loss = 1.59564810\n",
      "Iteration 4593, loss = 1.59561294\n",
      "Iteration 4594, loss = 1.59557780\n",
      "Iteration 4595, loss = 1.59554266\n",
      "Iteration 4596, loss = 1.59550754\n",
      "Iteration 4597, loss = 1.59547242\n",
      "Iteration 4598, loss = 1.59543732\n",
      "Iteration 4599, loss = 1.59540223\n",
      "Iteration 4600, loss = 1.59536715\n",
      "Iteration 4601, loss = 1.59533208\n",
      "Iteration 4602, loss = 1.59529702\n",
      "Iteration 4603, loss = 1.59526197\n",
      "Iteration 4604, loss = 1.59522693\n",
      "Iteration 4605, loss = 1.59519190\n",
      "Iteration 4606, loss = 1.59515688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4607, loss = 1.59512187\n",
      "Iteration 4608, loss = 1.59508688\n",
      "Iteration 4609, loss = 1.59505189\n",
      "Iteration 4610, loss = 1.59501692\n",
      "Iteration 4611, loss = 1.59498195\n",
      "Iteration 4612, loss = 1.59494700\n",
      "Iteration 4613, loss = 1.59491206\n",
      "Iteration 4614, loss = 1.59487712\n",
      "Iteration 4615, loss = 1.59484220\n",
      "Iteration 4616, loss = 1.59480729\n",
      "Iteration 4617, loss = 1.59477239\n",
      "Iteration 4618, loss = 1.59473750\n",
      "Iteration 4619, loss = 1.59470262\n",
      "Iteration 4620, loss = 1.59466776\n",
      "Iteration 4621, loss = 1.59463290\n",
      "Iteration 4622, loss = 1.59459805\n",
      "Iteration 4623, loss = 1.59456321\n",
      "Iteration 4624, loss = 1.59452839\n",
      "Iteration 4625, loss = 1.59449357\n",
      "Iteration 4626, loss = 1.59445877\n",
      "Iteration 4627, loss = 1.59442397\n",
      "Iteration 4628, loss = 1.59438919\n",
      "Iteration 4629, loss = 1.59435442\n",
      "Iteration 4630, loss = 1.59431966\n",
      "Iteration 4631, loss = 1.59428490\n",
      "Iteration 4632, loss = 1.59425016\n",
      "Iteration 4633, loss = 1.59421543\n",
      "Iteration 4634, loss = 1.59418071\n",
      "Iteration 4635, loss = 1.59414600\n",
      "Iteration 4636, loss = 1.59411131\n",
      "Iteration 4637, loss = 1.59407662\n",
      "Iteration 4638, loss = 1.59404194\n",
      "Iteration 4639, loss = 1.59400727\n",
      "Iteration 4640, loss = 1.59397262\n",
      "Iteration 4641, loss = 1.59393797\n",
      "Iteration 4642, loss = 1.59390334\n",
      "Iteration 4643, loss = 1.59386871\n",
      "Iteration 4644, loss = 1.59383410\n",
      "Iteration 4645, loss = 1.59379949\n",
      "Iteration 4646, loss = 1.59376490\n",
      "Iteration 4647, loss = 1.59373032\n",
      "Iteration 4648, loss = 1.59369574\n",
      "Iteration 4649, loss = 1.59366118\n",
      "Iteration 4650, loss = 1.59362663\n",
      "Iteration 4651, loss = 1.59359209\n",
      "Iteration 4652, loss = 1.59355756\n",
      "Iteration 4653, loss = 1.59352304\n",
      "Iteration 4654, loss = 1.59348853\n",
      "Iteration 4655, loss = 1.59345403\n",
      "Iteration 4656, loss = 1.59341955\n",
      "Iteration 4657, loss = 1.59338507\n",
      "Iteration 4658, loss = 1.59335060\n",
      "Iteration 4659, loss = 1.59331615\n",
      "Iteration 4660, loss = 1.59328170\n",
      "Iteration 4661, loss = 1.59324726\n",
      "Iteration 4662, loss = 1.59321284\n",
      "Iteration 4663, loss = 1.59317842\n",
      "Iteration 4664, loss = 1.59314402\n",
      "Iteration 4665, loss = 1.59310963\n",
      "Iteration 4666, loss = 1.59307524\n",
      "Iteration 4667, loss = 1.59304087\n",
      "Iteration 4668, loss = 1.59300651\n",
      "Iteration 4669, loss = 1.59297216\n",
      "Iteration 4670, loss = 1.59293781\n",
      "Iteration 4671, loss = 1.59290348\n",
      "Iteration 4672, loss = 1.59286916\n",
      "Iteration 4673, loss = 1.59283485\n",
      "Iteration 4674, loss = 1.59280055\n",
      "Iteration 4675, loss = 1.59276626\n",
      "Iteration 4676, loss = 1.59273198\n",
      "Iteration 4677, loss = 1.59269772\n",
      "Iteration 4678, loss = 1.59266346\n",
      "Iteration 4679, loss = 1.59262921\n",
      "Iteration 4680, loss = 1.59259497\n",
      "Iteration 4681, loss = 1.59256075\n",
      "Iteration 4682, loss = 1.59252653\n",
      "Iteration 4683, loss = 1.59249232\n",
      "Iteration 4684, loss = 1.59245813\n",
      "Iteration 4685, loss = 1.59242394\n",
      "Iteration 4686, loss = 1.59238977\n",
      "Iteration 4687, loss = 1.59235560\n",
      "Iteration 4688, loss = 1.59232145\n",
      "Iteration 4689, loss = 1.59228730\n",
      "Iteration 4690, loss = 1.59225317\n",
      "Iteration 4691, loss = 1.59221905\n",
      "Iteration 4692, loss = 1.59218494\n",
      "Iteration 4693, loss = 1.59215083\n",
      "Iteration 4694, loss = 1.59211674\n",
      "Iteration 4695, loss = 1.59208266\n",
      "Iteration 4696, loss = 1.59204859\n",
      "Iteration 4697, loss = 1.59201453\n",
      "Iteration 4698, loss = 1.59198048\n",
      "Iteration 4699, loss = 1.59194644\n",
      "Iteration 4700, loss = 1.59191241\n",
      "Iteration 4701, loss = 1.59187839\n",
      "Iteration 4702, loss = 1.59184438\n",
      "Iteration 4703, loss = 1.59181038\n",
      "Iteration 4704, loss = 1.59177639\n",
      "Iteration 4705, loss = 1.59174241\n",
      "Iteration 4706, loss = 1.59170844\n",
      "Iteration 4707, loss = 1.59167449\n",
      "Iteration 4708, loss = 1.59164054\n",
      "Iteration 4709, loss = 1.59160660\n",
      "Iteration 4710, loss = 1.59157267\n",
      "Iteration 4711, loss = 1.59153876\n",
      "Iteration 4712, loss = 1.59150485\n",
      "Iteration 4713, loss = 1.59147095\n",
      "Iteration 4714, loss = 1.59143707\n",
      "Iteration 4715, loss = 1.59140319\n",
      "Iteration 4716, loss = 1.59136933\n",
      "Iteration 4717, loss = 1.59133547\n",
      "Iteration 4718, loss = 1.59130163\n",
      "Iteration 4719, loss = 1.59126779\n",
      "Iteration 4720, loss = 1.59123397\n",
      "Iteration 4721, loss = 1.59120016\n",
      "Iteration 4722, loss = 1.59116635\n",
      "Iteration 4723, loss = 1.59113256\n",
      "Iteration 4724, loss = 1.59109878\n",
      "Iteration 4725, loss = 1.59106500\n",
      "Iteration 4726, loss = 1.59103124\n",
      "Iteration 4727, loss = 1.59099749\n",
      "Iteration 4728, loss = 1.59096375\n",
      "Iteration 4729, loss = 1.59093001\n",
      "Iteration 4730, loss = 1.59089629\n",
      "Iteration 4731, loss = 1.59086258\n",
      "Iteration 4732, loss = 1.59082888\n",
      "Iteration 4733, loss = 1.59079519\n",
      "Iteration 4734, loss = 1.59076151\n",
      "Iteration 4735, loss = 1.59072784\n",
      "Iteration 4736, loss = 1.59069417\n",
      "Iteration 4737, loss = 1.59066052\n",
      "Iteration 4738, loss = 1.59062688\n",
      "Iteration 4739, loss = 1.59059325\n",
      "Iteration 4740, loss = 1.59055963\n",
      "Iteration 4741, loss = 1.59052602\n",
      "Iteration 4742, loss = 1.59049242\n",
      "Iteration 4743, loss = 1.59045884\n",
      "Iteration 4744, loss = 1.59042526\n",
      "Iteration 4745, loss = 1.59039169\n",
      "Iteration 4746, loss = 1.59035813\n",
      "Iteration 4747, loss = 1.59032458\n",
      "Iteration 4748, loss = 1.59029104\n",
      "Iteration 4749, loss = 1.59025751\n",
      "Iteration 4750, loss = 1.59022399\n",
      "Iteration 4751, loss = 1.59019049\n",
      "Iteration 4752, loss = 1.59015699\n",
      "Iteration 4753, loss = 1.59012350\n",
      "Iteration 4754, loss = 1.59009002\n",
      "Iteration 4755, loss = 1.59005655\n",
      "Iteration 4756, loss = 1.59002310\n",
      "Iteration 4757, loss = 1.58998965\n",
      "Iteration 4758, loss = 1.58995621\n",
      "Iteration 4759, loss = 1.58992279\n",
      "Iteration 4760, loss = 1.58988937\n",
      "Iteration 4761, loss = 1.58985596\n",
      "Iteration 4762, loss = 1.58982256\n",
      "Iteration 4763, loss = 1.58978918\n",
      "Iteration 4764, loss = 1.58975580\n",
      "Iteration 4765, loss = 1.58972243\n",
      "Iteration 4766, loss = 1.58968908\n",
      "Iteration 4767, loss = 1.58965573\n",
      "Iteration 4768, loss = 1.58962239\n",
      "Iteration 4769, loss = 1.58958907\n",
      "Iteration 4770, loss = 1.58955575\n",
      "Iteration 4771, loss = 1.58952245\n",
      "Iteration 4772, loss = 1.58948915\n",
      "Iteration 4773, loss = 1.58945586\n",
      "Iteration 4774, loss = 1.58942259\n",
      "Iteration 4775, loss = 1.58938932\n",
      "Iteration 4776, loss = 1.58935607\n",
      "Iteration 4777, loss = 1.58932282\n",
      "Iteration 4778, loss = 1.58928958\n",
      "Iteration 4779, loss = 1.58925636\n",
      "Iteration 4780, loss = 1.58922314\n",
      "Iteration 4781, loss = 1.58918994\n",
      "Iteration 4782, loss = 1.58915674\n",
      "Iteration 4783, loss = 1.58912356\n",
      "Iteration 4784, loss = 1.58909038\n",
      "Iteration 4785, loss = 1.58905721\n",
      "Iteration 4786, loss = 1.58902406\n",
      "Iteration 4787, loss = 1.58899091\n",
      "Iteration 4788, loss = 1.58895778\n",
      "Iteration 4789, loss = 1.58892465\n",
      "Iteration 4790, loss = 1.58889154\n",
      "Iteration 4791, loss = 1.58885843\n",
      "Iteration 4792, loss = 1.58882534\n",
      "Iteration 4793, loss = 1.58879225\n",
      "Iteration 4794, loss = 1.58875917\n",
      "Iteration 4795, loss = 1.58872611\n",
      "Iteration 4796, loss = 1.58869305\n",
      "Iteration 4797, loss = 1.58866001\n",
      "Iteration 4798, loss = 1.58862697\n",
      "Iteration 4799, loss = 1.58859395\n",
      "Iteration 4800, loss = 1.58856093\n",
      "Iteration 4801, loss = 1.58852792\n",
      "Iteration 4802, loss = 1.58849493\n",
      "Iteration 4803, loss = 1.58846194\n",
      "Iteration 4804, loss = 1.58842897\n",
      "Iteration 4805, loss = 1.58839600\n",
      "Iteration 4806, loss = 1.58836304\n",
      "Iteration 4807, loss = 1.58833010\n",
      "Iteration 4808, loss = 1.58829716\n",
      "Iteration 4809, loss = 1.58826423\n",
      "Iteration 4810, loss = 1.58823132\n",
      "Iteration 4811, loss = 1.58819841\n",
      "Iteration 4812, loss = 1.58816551\n",
      "Iteration 4813, loss = 1.58813263\n",
      "Iteration 4814, loss = 1.58809975\n",
      "Iteration 4815, loss = 1.58806688\n",
      "Iteration 4816, loss = 1.58803403\n",
      "Iteration 4817, loss = 1.58800118\n",
      "Iteration 4818, loss = 1.58796834\n",
      "Iteration 4819, loss = 1.58793552\n",
      "Iteration 4820, loss = 1.58790270\n",
      "Iteration 4821, loss = 1.58786989\n",
      "Iteration 4822, loss = 1.58783709\n",
      "Iteration 4823, loss = 1.58780431\n",
      "Iteration 4824, loss = 1.58777153\n",
      "Iteration 4825, loss = 1.58773876\n",
      "Iteration 4826, loss = 1.58770600\n",
      "Iteration 4827, loss = 1.58767325\n",
      "Iteration 4828, loss = 1.58764051\n",
      "Iteration 4829, loss = 1.58760779\n",
      "Iteration 4830, loss = 1.58757507\n",
      "Iteration 4831, loss = 1.58754236\n",
      "Iteration 4832, loss = 1.58750966\n",
      "Iteration 4833, loss = 1.58747697\n",
      "Iteration 4834, loss = 1.58744429\n",
      "Iteration 4835, loss = 1.58741162\n",
      "Iteration 4836, loss = 1.58737896\n",
      "Iteration 4837, loss = 1.58734631\n",
      "Iteration 4838, loss = 1.58731367\n",
      "Iteration 4839, loss = 1.58728104\n",
      "Iteration 4840, loss = 1.58724842\n",
      "Iteration 4841, loss = 1.58721581\n",
      "Iteration 4842, loss = 1.58718321\n",
      "Iteration 4843, loss = 1.58715062\n",
      "Iteration 4844, loss = 1.58711804\n",
      "Iteration 4845, loss = 1.58708547\n",
      "Iteration 4846, loss = 1.58705290\n",
      "Iteration 4847, loss = 1.58702035\n",
      "Iteration 4848, loss = 1.58698781\n",
      "Iteration 4849, loss = 1.58695528\n",
      "Iteration 4850, loss = 1.58692276\n",
      "Iteration 4851, loss = 1.58689024\n",
      "Iteration 4852, loss = 1.58685774\n",
      "Iteration 4853, loss = 1.58682525\n",
      "Iteration 4854, loss = 1.58679276\n",
      "Iteration 4855, loss = 1.58676029\n",
      "Iteration 4856, loss = 1.58672783\n",
      "Iteration 4857, loss = 1.58669537\n",
      "Iteration 4858, loss = 1.58666293\n",
      "Iteration 4859, loss = 1.58663049\n",
      "Iteration 4860, loss = 1.58659807\n",
      "Iteration 4861, loss = 1.58656565\n",
      "Iteration 4862, loss = 1.58653325\n",
      "Iteration 4863, loss = 1.58650085\n",
      "Iteration 4864, loss = 1.58646847\n",
      "Iteration 4865, loss = 1.58643609\n",
      "Iteration 4866, loss = 1.58640372\n",
      "Iteration 4867, loss = 1.58637137\n",
      "Iteration 4868, loss = 1.58633902\n",
      "Iteration 4869, loss = 1.58630668\n",
      "Iteration 4870, loss = 1.58627436\n",
      "Iteration 4871, loss = 1.58624204\n",
      "Iteration 4872, loss = 1.58620973\n",
      "Iteration 4873, loss = 1.58617743\n",
      "Iteration 4874, loss = 1.58614514\n",
      "Iteration 4875, loss = 1.58611286\n",
      "Iteration 4876, loss = 1.58608059\n",
      "Iteration 4877, loss = 1.58604833\n",
      "Iteration 4878, loss = 1.58601608\n",
      "Iteration 4879, loss = 1.58598384\n",
      "Iteration 4880, loss = 1.58595161\n",
      "Iteration 4881, loss = 1.58591939\n",
      "Iteration 4882, loss = 1.58588718\n",
      "Iteration 4883, loss = 1.58585498\n",
      "Iteration 4884, loss = 1.58582279\n",
      "Iteration 4885, loss = 1.58579060\n",
      "Iteration 4886, loss = 1.58575843\n",
      "Iteration 4887, loss = 1.58572627\n",
      "Iteration 4888, loss = 1.58569411\n",
      "Iteration 4889, loss = 1.58566197\n",
      "Iteration 4890, loss = 1.58562984\n",
      "Iteration 4891, loss = 1.58559771\n",
      "Iteration 4892, loss = 1.58556560\n",
      "Iteration 4893, loss = 1.58553349\n",
      "Iteration 4894, loss = 1.58550140\n",
      "Iteration 4895, loss = 1.58546931\n",
      "Iteration 4896, loss = 1.58543723\n",
      "Iteration 4897, loss = 1.58540517\n",
      "Iteration 4898, loss = 1.58537311\n",
      "Iteration 4899, loss = 1.58534106\n",
      "Iteration 4900, loss = 1.58530902\n",
      "Iteration 4901, loss = 1.58527700\n",
      "Iteration 4902, loss = 1.58524498\n",
      "Iteration 4903, loss = 1.58521297\n",
      "Iteration 4904, loss = 1.58518097\n",
      "Iteration 4905, loss = 1.58514898\n",
      "Iteration 4906, loss = 1.58511700\n",
      "Iteration 4907, loss = 1.58508503\n",
      "Iteration 4908, loss = 1.58505307\n",
      "Iteration 4909, loss = 1.58502111\n",
      "Iteration 4910, loss = 1.58498917\n",
      "Iteration 4911, loss = 1.58495724\n",
      "Iteration 4912, loss = 1.58492532\n",
      "Iteration 4913, loss = 1.58489340\n",
      "Iteration 4914, loss = 1.58486150\n",
      "Iteration 4915, loss = 1.58482961\n",
      "Iteration 4916, loss = 1.58479772\n",
      "Iteration 4917, loss = 1.58476585\n",
      "Iteration 4918, loss = 1.58473398\n",
      "Iteration 4919, loss = 1.58470212\n",
      "Iteration 4920, loss = 1.58467028\n",
      "Iteration 4921, loss = 1.58463844\n",
      "Iteration 4922, loss = 1.58460661\n",
      "Iteration 4923, loss = 1.58457479\n",
      "Iteration 4924, loss = 1.58454299\n",
      "Iteration 4925, loss = 1.58451119\n",
      "Iteration 4926, loss = 1.58447940\n",
      "Iteration 4927, loss = 1.58444762\n",
      "Iteration 4928, loss = 1.58441585\n",
      "Iteration 4929, loss = 1.58438409\n",
      "Iteration 4930, loss = 1.58435234\n",
      "Iteration 4931, loss = 1.58432059\n",
      "Iteration 4932, loss = 1.58428886\n",
      "Iteration 4933, loss = 1.58425714\n",
      "Iteration 4934, loss = 1.58422542\n",
      "Iteration 4935, loss = 1.58419372\n",
      "Iteration 4936, loss = 1.58416203\n",
      "Iteration 4937, loss = 1.58413034\n",
      "Iteration 4938, loss = 1.58409867\n",
      "Iteration 4939, loss = 1.58406700\n",
      "Iteration 4940, loss = 1.58403534\n",
      "Iteration 4941, loss = 1.58400370\n",
      "Iteration 4942, loss = 1.58397206\n",
      "Iteration 4943, loss = 1.58394043\n",
      "Iteration 4944, loss = 1.58390881\n",
      "Iteration 4945, loss = 1.58387720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4946, loss = 1.58384560\n",
      "Iteration 4947, loss = 1.58381401\n",
      "Iteration 4948, loss = 1.58378243\n",
      "Iteration 4949, loss = 1.58375086\n",
      "Iteration 4950, loss = 1.58371930\n",
      "Iteration 4951, loss = 1.58368774\n",
      "Iteration 4952, loss = 1.58365620\n",
      "Iteration 4953, loss = 1.58362467\n",
      "Iteration 4954, loss = 1.58359314\n",
      "Iteration 4955, loss = 1.58356163\n",
      "Iteration 4956, loss = 1.58353012\n",
      "Iteration 4957, loss = 1.58349863\n",
      "Iteration 4958, loss = 1.58346714\n",
      "Iteration 4959, loss = 1.58343566\n",
      "Iteration 4960, loss = 1.58340419\n",
      "Iteration 4961, loss = 1.58337273\n",
      "Iteration 4962, loss = 1.58334129\n",
      "Iteration 4963, loss = 1.58330985\n",
      "Iteration 4964, loss = 1.58327842\n",
      "Iteration 4965, loss = 1.58324699\n",
      "Iteration 4966, loss = 1.58321558\n",
      "Iteration 4967, loss = 1.58318418\n",
      "Iteration 4968, loss = 1.58315279\n",
      "Iteration 4969, loss = 1.58312140\n",
      "Iteration 4970, loss = 1.58309003\n",
      "Iteration 4971, loss = 1.58305866\n",
      "Iteration 4972, loss = 1.58302731\n",
      "Iteration 4973, loss = 1.58299596\n",
      "Iteration 4974, loss = 1.58296463\n",
      "Iteration 4975, loss = 1.58293330\n",
      "Iteration 4976, loss = 1.58290198\n",
      "Iteration 4977, loss = 1.58287067\n",
      "Iteration 4978, loss = 1.58283937\n",
      "Iteration 4979, loss = 1.58280808\n",
      "Iteration 4980, loss = 1.58277680\n",
      "Iteration 4981, loss = 1.58274553\n",
      "Iteration 4982, loss = 1.58271427\n",
      "Iteration 4983, loss = 1.58268302\n",
      "Iteration 4984, loss = 1.58265177\n",
      "Iteration 4985, loss = 1.58262054\n",
      "Iteration 4986, loss = 1.58258931\n",
      "Iteration 4987, loss = 1.58255810\n",
      "Iteration 4988, loss = 1.58252689\n",
      "Iteration 4989, loss = 1.58249569\n",
      "Iteration 4990, loss = 1.58246451\n",
      "Iteration 4991, loss = 1.58243333\n",
      "Iteration 4992, loss = 1.58240216\n",
      "Iteration 4993, loss = 1.58237100\n",
      "Iteration 4994, loss = 1.58233985\n",
      "Iteration 4995, loss = 1.58230871\n",
      "Iteration 4996, loss = 1.58227758\n",
      "Iteration 4997, loss = 1.58224645\n",
      "Iteration 4998, loss = 1.58221534\n",
      "Iteration 4999, loss = 1.58218424\n",
      "Iteration 5000, loss = 1.58215314\n",
      "Iteration 5001, loss = 1.58212205\n",
      "Iteration 5002, loss = 1.58209098\n",
      "Iteration 5003, loss = 1.58205991\n",
      "Iteration 5004, loss = 1.58202885\n",
      "Iteration 5005, loss = 1.58199780\n",
      "Iteration 5006, loss = 1.58196677\n",
      "Iteration 5007, loss = 1.58193574\n",
      "Iteration 5008, loss = 1.58190471\n",
      "Iteration 5009, loss = 1.58187370\n",
      "Iteration 5010, loss = 1.58184270\n",
      "Iteration 5011, loss = 1.58181171\n",
      "Iteration 5012, loss = 1.58178072\n",
      "Iteration 5013, loss = 1.58174975\n",
      "Iteration 5014, loss = 1.58171878\n",
      "Iteration 5015, loss = 1.58168783\n",
      "Iteration 5016, loss = 1.58165688\n",
      "Iteration 5017, loss = 1.58162594\n",
      "Iteration 5018, loss = 1.58159501\n",
      "Iteration 5019, loss = 1.58156409\n",
      "Iteration 5020, loss = 1.58153318\n",
      "Iteration 5021, loss = 1.58150228\n",
      "Iteration 5022, loss = 1.58147139\n",
      "Iteration 5023, loss = 1.58144051\n",
      "Iteration 5024, loss = 1.58140963\n",
      "Iteration 5025, loss = 1.58137877\n",
      "Iteration 5026, loss = 1.58134791\n",
      "Iteration 5027, loss = 1.58131707\n",
      "Iteration 5028, loss = 1.58128623\n",
      "Iteration 5029, loss = 1.58125540\n",
      "Iteration 5030, loss = 1.58122459\n",
      "Iteration 5031, loss = 1.58119378\n",
      "Iteration 5032, loss = 1.58116298\n",
      "Iteration 5033, loss = 1.58113218\n",
      "Iteration 5034, loss = 1.58110140\n",
      "Iteration 5035, loss = 1.58107063\n",
      "Iteration 5036, loss = 1.58103987\n",
      "Iteration 5037, loss = 1.58100911\n",
      "Iteration 5038, loss = 1.58097837\n",
      "Iteration 5039, loss = 1.58094763\n",
      "Iteration 5040, loss = 1.58091690\n",
      "Iteration 5041, loss = 1.58088619\n",
      "Iteration 5042, loss = 1.58085548\n",
      "Iteration 5043, loss = 1.58082478\n",
      "Iteration 5044, loss = 1.58079409\n",
      "Iteration 5045, loss = 1.58076340\n",
      "Iteration 5046, loss = 1.58073273\n",
      "Iteration 5047, loss = 1.58070207\n",
      "Iteration 5048, loss = 1.58067141\n",
      "Iteration 5049, loss = 1.58064077\n",
      "Iteration 5050, loss = 1.58061013\n",
      "Iteration 5051, loss = 1.58057951\n",
      "Iteration 5052, loss = 1.58054889\n",
      "Iteration 5053, loss = 1.58051828\n",
      "Iteration 5054, loss = 1.58048768\n",
      "Iteration 5055, loss = 1.58045709\n",
      "Iteration 5056, loss = 1.58042651\n",
      "Iteration 5057, loss = 1.58039594\n",
      "Iteration 5058, loss = 1.58036537\n",
      "Iteration 5059, loss = 1.58033482\n",
      "Iteration 5060, loss = 1.58030427\n",
      "Iteration 5061, loss = 1.58027374\n",
      "Iteration 5062, loss = 1.58024321\n",
      "Iteration 5063, loss = 1.58021269\n",
      "Iteration 5064, loss = 1.58018218\n",
      "Iteration 5065, loss = 1.58015168\n",
      "Iteration 5066, loss = 1.58012119\n",
      "Iteration 5067, loss = 1.58009071\n",
      "Iteration 5068, loss = 1.58006024\n",
      "Iteration 5069, loss = 1.58002977\n",
      "Iteration 5070, loss = 1.57999932\n",
      "Iteration 5071, loss = 1.57996887\n",
      "Iteration 5072, loss = 1.57993844\n",
      "Iteration 5073, loss = 1.57990801\n",
      "Iteration 5074, loss = 1.57987759\n",
      "Iteration 5075, loss = 1.57984718\n",
      "Iteration 5076, loss = 1.57981678\n",
      "Iteration 5077, loss = 1.57978639\n",
      "Iteration 5078, loss = 1.57975601\n",
      "Iteration 5079, loss = 1.57972563\n",
      "Iteration 5080, loss = 1.57969527\n",
      "Iteration 5081, loss = 1.57966491\n",
      "Iteration 5082, loss = 1.57963457\n",
      "Iteration 5083, loss = 1.57960423\n",
      "Iteration 5084, loss = 1.57957390\n",
      "Iteration 5085, loss = 1.57954358\n",
      "Iteration 5086, loss = 1.57951327\n",
      "Iteration 5087, loss = 1.57948297\n",
      "Iteration 5088, loss = 1.57945268\n",
      "Iteration 5089, loss = 1.57942239\n",
      "Iteration 5090, loss = 1.57939212\n",
      "Iteration 5091, loss = 1.57936185\n",
      "Iteration 5092, loss = 1.57933159\n",
      "Iteration 5093, loss = 1.57930135\n",
      "Iteration 5094, loss = 1.57927111\n",
      "Iteration 5095, loss = 1.57924088\n",
      "Iteration 5096, loss = 1.57921066\n",
      "Iteration 5097, loss = 1.57918044\n",
      "Iteration 5098, loss = 1.57915024\n",
      "Iteration 5099, loss = 1.57912005\n",
      "Iteration 5100, loss = 1.57908986\n",
      "Iteration 5101, loss = 1.57905969\n",
      "Iteration 5102, loss = 1.57902952\n",
      "Iteration 5103, loss = 1.57899936\n",
      "Iteration 5104, loss = 1.57896921\n",
      "Iteration 5105, loss = 1.57893907\n",
      "Iteration 5106, loss = 1.57890894\n",
      "Iteration 5107, loss = 1.57887881\n",
      "Iteration 5108, loss = 1.57884870\n",
      "Iteration 5109, loss = 1.57881860\n",
      "Iteration 5110, loss = 1.57878850\n",
      "Iteration 5111, loss = 1.57875841\n",
      "Iteration 5112, loss = 1.57872833\n",
      "Iteration 5113, loss = 1.57869826\n",
      "Iteration 5114, loss = 1.57866820\n",
      "Iteration 5115, loss = 1.57863815\n",
      "Iteration 5116, loss = 1.57860811\n",
      "Iteration 5117, loss = 1.57857808\n",
      "Iteration 5118, loss = 1.57854805\n",
      "Iteration 5119, loss = 1.57851803\n",
      "Iteration 5120, loss = 1.57848803\n",
      "Iteration 5121, loss = 1.57845803\n",
      "Iteration 5122, loss = 1.57842804\n",
      "Iteration 5123, loss = 1.57839806\n",
      "Iteration 5124, loss = 1.57836809\n",
      "Iteration 5125, loss = 1.57833812\n",
      "Iteration 5126, loss = 1.57830817\n",
      "Iteration 5127, loss = 1.57827822\n",
      "Iteration 5128, loss = 1.57824829\n",
      "Iteration 5129, loss = 1.57821836\n",
      "Iteration 5130, loss = 1.57818844\n",
      "Iteration 5131, loss = 1.57815853\n",
      "Iteration 5132, loss = 1.57812863\n",
      "Iteration 5133, loss = 1.57809874\n",
      "Iteration 5134, loss = 1.57806886\n",
      "Iteration 5135, loss = 1.57803898\n",
      "Iteration 5136, loss = 1.57800912\n",
      "Iteration 5137, loss = 1.57797926\n",
      "Iteration 5138, loss = 1.57794941\n",
      "Iteration 5139, loss = 1.57791957\n",
      "Iteration 5140, loss = 1.57788974\n",
      "Iteration 5141, loss = 1.57785992\n",
      "Iteration 5142, loss = 1.57783011\n",
      "Iteration 5143, loss = 1.57780030\n",
      "Iteration 5144, loss = 1.57777051\n",
      "Iteration 5145, loss = 1.57774072\n",
      "Iteration 5146, loss = 1.57771094\n",
      "Iteration 5147, loss = 1.57768117\n",
      "Iteration 5148, loss = 1.57765141\n",
      "Iteration 5149, loss = 1.57762166\n",
      "Iteration 5150, loss = 1.57759192\n",
      "Iteration 5151, loss = 1.57756218\n",
      "Iteration 5152, loss = 1.57753246\n",
      "Iteration 5153, loss = 1.57750274\n",
      "Iteration 5154, loss = 1.57747304\n",
      "Iteration 5155, loss = 1.57744334\n",
      "Iteration 5156, loss = 1.57741365\n",
      "Iteration 5157, loss = 1.57738397\n",
      "Iteration 5158, loss = 1.57735429\n",
      "Iteration 5159, loss = 1.57732463\n",
      "Iteration 5160, loss = 1.57729497\n",
      "Iteration 5161, loss = 1.57726533\n",
      "Iteration 5162, loss = 1.57723569\n",
      "Iteration 5163, loss = 1.57720606\n",
      "Iteration 5164, loss = 1.57717644\n",
      "Iteration 5165, loss = 1.57714683\n",
      "Iteration 5166, loss = 1.57711723\n",
      "Iteration 5167, loss = 1.57708763\n",
      "Iteration 5168, loss = 1.57705805\n",
      "Iteration 5169, loss = 1.57702847\n",
      "Iteration 5170, loss = 1.57699890\n",
      "Iteration 5171, loss = 1.57696934\n",
      "Iteration 5172, loss = 1.57693979\n",
      "Iteration 5173, loss = 1.57691025\n",
      "Iteration 5174, loss = 1.57688072\n",
      "Iteration 5175, loss = 1.57685120\n",
      "Iteration 5176, loss = 1.57682168\n",
      "Iteration 5177, loss = 1.57679217\n",
      "Iteration 5178, loss = 1.57676267\n",
      "Iteration 5179, loss = 1.57673319\n",
      "Iteration 5180, loss = 1.57670370\n",
      "Iteration 5181, loss = 1.57667423\n",
      "Iteration 5182, loss = 1.57664477\n",
      "Iteration 5183, loss = 1.57661531\n",
      "Iteration 5184, loss = 1.57658587\n",
      "Iteration 5185, loss = 1.57655643\n",
      "Iteration 5186, loss = 1.57652700\n",
      "Iteration 5187, loss = 1.57649758\n",
      "Iteration 5188, loss = 1.57646817\n",
      "Iteration 5189, loss = 1.57643877\n",
      "Iteration 5190, loss = 1.57640937\n",
      "Iteration 5191, loss = 1.57637999\n",
      "Iteration 5192, loss = 1.57635061\n",
      "Iteration 5193, loss = 1.57632124\n",
      "Iteration 5194, loss = 1.57629188\n",
      "Iteration 5195, loss = 1.57626253\n",
      "Iteration 5196, loss = 1.57623319\n",
      "Iteration 5197, loss = 1.57620385\n",
      "Iteration 5198, loss = 1.57617453\n",
      "Iteration 5199, loss = 1.57614521\n",
      "Iteration 5200, loss = 1.57611591\n",
      "Iteration 5201, loss = 1.57608661\n",
      "Iteration 5202, loss = 1.57605732\n",
      "Iteration 5203, loss = 1.57602803\n",
      "Iteration 5204, loss = 1.57599876\n",
      "Iteration 5205, loss = 1.57596950\n",
      "Iteration 5206, loss = 1.57594024\n",
      "Iteration 5207, loss = 1.57591099\n",
      "Iteration 5208, loss = 1.57588175\n",
      "Iteration 5209, loss = 1.57585252\n",
      "Iteration 5210, loss = 1.57582330\n",
      "Iteration 5211, loss = 1.57579409\n",
      "Iteration 5212, loss = 1.57576488\n",
      "Iteration 5213, loss = 1.57573569\n",
      "Iteration 5214, loss = 1.57570650\n",
      "Iteration 5215, loss = 1.57567732\n",
      "Iteration 5216, loss = 1.57564815\n",
      "Iteration 5217, loss = 1.57561899\n",
      "Iteration 5218, loss = 1.57558983\n",
      "Iteration 5219, loss = 1.57556069\n",
      "Iteration 5220, loss = 1.57553155\n",
      "Iteration 5221, loss = 1.57550242\n",
      "Iteration 5222, loss = 1.57547331\n",
      "Iteration 5223, loss = 1.57544420\n",
      "Iteration 5224, loss = 1.57541509\n",
      "Iteration 5225, loss = 1.57538600\n",
      "Iteration 5226, loss = 1.57535692\n",
      "Iteration 5227, loss = 1.57532784\n",
      "Iteration 5228, loss = 1.57529877\n",
      "Iteration 5229, loss = 1.57526971\n",
      "Iteration 5230, loss = 1.57524066\n",
      "Iteration 5231, loss = 1.57521162\n",
      "Iteration 5232, loss = 1.57518258\n",
      "Iteration 5233, loss = 1.57515356\n",
      "Iteration 5234, loss = 1.57512454\n",
      "Iteration 5235, loss = 1.57509553\n",
      "Iteration 5236, loss = 1.57506654\n",
      "Iteration 5237, loss = 1.57503754\n",
      "Iteration 5238, loss = 1.57500856\n",
      "Iteration 5239, loss = 1.57497959\n",
      "Iteration 5240, loss = 1.57495062\n",
      "Iteration 5241, loss = 1.57492166\n",
      "Iteration 5242, loss = 1.57489272\n",
      "Iteration 5243, loss = 1.57486378\n",
      "Iteration 5244, loss = 1.57483484\n",
      "Iteration 5245, loss = 1.57480592\n",
      "Iteration 5246, loss = 1.57477701\n",
      "Iteration 5247, loss = 1.57474810\n",
      "Iteration 5248, loss = 1.57471920\n",
      "Iteration 5249, loss = 1.57469031\n",
      "Iteration 5250, loss = 1.57466143\n",
      "Iteration 5251, loss = 1.57463256\n",
      "Iteration 5252, loss = 1.57460369\n",
      "Iteration 5253, loss = 1.57457484\n",
      "Iteration 5254, loss = 1.57454599\n",
      "Iteration 5255, loss = 1.57451715\n",
      "Iteration 5256, loss = 1.57448832\n",
      "Iteration 5257, loss = 1.57445950\n",
      "Iteration 5258, loss = 1.57443069\n",
      "Iteration 5259, loss = 1.57440188\n",
      "Iteration 5260, loss = 1.57437309\n",
      "Iteration 5261, loss = 1.57434430\n",
      "Iteration 5262, loss = 1.57431552\n",
      "Iteration 5263, loss = 1.57428675\n",
      "Iteration 5264, loss = 1.57425799\n",
      "Iteration 5265, loss = 1.57422923\n",
      "Iteration 5266, loss = 1.57420049\n",
      "Iteration 5267, loss = 1.57417175\n",
      "Iteration 5268, loss = 1.57414302\n",
      "Iteration 5269, loss = 1.57411430\n",
      "Iteration 5270, loss = 1.57408559\n",
      "Iteration 5271, loss = 1.57405688\n",
      "Iteration 5272, loss = 1.57402819\n",
      "Iteration 5273, loss = 1.57399950\n",
      "Iteration 5274, loss = 1.57397082\n",
      "Iteration 5275, loss = 1.57394215\n",
      "Iteration 5276, loss = 1.57391349\n",
      "Iteration 5277, loss = 1.57388484\n",
      "Iteration 5278, loss = 1.57385619\n",
      "Iteration 5279, loss = 1.57382755\n",
      "Iteration 5280, loss = 1.57379893\n",
      "Iteration 5281, loss = 1.57377031\n",
      "Iteration 5282, loss = 1.57374169\n",
      "Iteration 5283, loss = 1.57371309\n",
      "Iteration 5284, loss = 1.57368450\n",
      "Iteration 5285, loss = 1.57365591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5286, loss = 1.57362733\n",
      "Iteration 5287, loss = 1.57359876\n",
      "Iteration 5288, loss = 1.57357020\n",
      "Iteration 5289, loss = 1.57354165\n",
      "Iteration 5290, loss = 1.57351310\n",
      "Iteration 5291, loss = 1.57348457\n",
      "Iteration 5292, loss = 1.57345604\n",
      "Iteration 5293, loss = 1.57342752\n",
      "Iteration 5294, loss = 1.57339901\n",
      "Iteration 5295, loss = 1.57337050\n",
      "Iteration 5296, loss = 1.57334201\n",
      "Iteration 5297, loss = 1.57331352\n",
      "Iteration 5298, loss = 1.57328504\n",
      "Iteration 5299, loss = 1.57325657\n",
      "Iteration 5300, loss = 1.57322811\n",
      "Iteration 5301, loss = 1.57319966\n",
      "Iteration 5302, loss = 1.57317121\n",
      "Iteration 5303, loss = 1.57314278\n",
      "Iteration 5304, loss = 1.57311435\n",
      "Iteration 5305, loss = 1.57308593\n",
      "Iteration 5306, loss = 1.57305752\n",
      "Iteration 5307, loss = 1.57302912\n",
      "Iteration 5308, loss = 1.57300072\n",
      "Iteration 5309, loss = 1.57297233\n",
      "Iteration 5310, loss = 1.57294395\n",
      "Iteration 5311, loss = 1.57291558\n",
      "Iteration 5312, loss = 1.57288722\n",
      "Iteration 5313, loss = 1.57285887\n",
      "Iteration 5314, loss = 1.57283052\n",
      "Iteration 5315, loss = 1.57280219\n",
      "Iteration 5316, loss = 1.57277386\n",
      "Iteration 5317, loss = 1.57274554\n",
      "Iteration 5318, loss = 1.57271722\n",
      "Iteration 5319, loss = 1.57268892\n",
      "Iteration 5320, loss = 1.57266062\n",
      "Iteration 5321, loss = 1.57263234\n",
      "Iteration 5322, loss = 1.57260406\n",
      "Iteration 5323, loss = 1.57257579\n",
      "Iteration 5324, loss = 1.57254752\n",
      "Iteration 5325, loss = 1.57251927\n",
      "Iteration 5326, loss = 1.57249102\n",
      "Iteration 5327, loss = 1.57246279\n",
      "Iteration 5328, loss = 1.57243456\n",
      "Iteration 5329, loss = 1.57240633\n",
      "Iteration 5330, loss = 1.57237812\n",
      "Iteration 5331, loss = 1.57234991\n",
      "Iteration 5332, loss = 1.57232172\n",
      "Iteration 5333, loss = 1.57229353\n",
      "Iteration 5334, loss = 1.57226535\n",
      "Iteration 5335, loss = 1.57223718\n",
      "Iteration 5336, loss = 1.57220901\n",
      "Iteration 5337, loss = 1.57218086\n",
      "Iteration 5338, loss = 1.57215271\n",
      "Iteration 5339, loss = 1.57212457\n",
      "Iteration 5340, loss = 1.57209644\n",
      "Iteration 5341, loss = 1.57206831\n",
      "Iteration 5342, loss = 1.57204020\n",
      "Iteration 5343, loss = 1.57201209\n",
      "Iteration 5344, loss = 1.57198399\n",
      "Iteration 5345, loss = 1.57195590\n",
      "Iteration 5346, loss = 1.57192782\n",
      "Iteration 5347, loss = 1.57189975\n",
      "Iteration 5348, loss = 1.57187168\n",
      "Iteration 5349, loss = 1.57184362\n",
      "Iteration 5350, loss = 1.57181557\n",
      "Iteration 5351, loss = 1.57178753\n",
      "Iteration 5352, loss = 1.57175950\n",
      "Iteration 5353, loss = 1.57173148\n",
      "Iteration 5354, loss = 1.57170346\n",
      "Iteration 5355, loss = 1.57167545\n",
      "Iteration 5356, loss = 1.57164745\n",
      "Iteration 5357, loss = 1.57161946\n",
      "Iteration 5358, loss = 1.57159147\n",
      "Iteration 5359, loss = 1.57156350\n",
      "Iteration 5360, loss = 1.57153553\n",
      "Iteration 5361, loss = 1.57150757\n",
      "Iteration 5362, loss = 1.57147962\n",
      "Iteration 5363, loss = 1.57145167\n",
      "Iteration 5364, loss = 1.57142374\n",
      "Iteration 5365, loss = 1.57139581\n",
      "Iteration 5366, loss = 1.57136789\n",
      "Iteration 5367, loss = 1.57133998\n",
      "Iteration 5368, loss = 1.57131208\n",
      "Iteration 5369, loss = 1.57128418\n",
      "Iteration 5370, loss = 1.57125630\n",
      "Iteration 5371, loss = 1.57122842\n",
      "Iteration 5372, loss = 1.57120055\n",
      "Iteration 5373, loss = 1.57117268\n",
      "Iteration 5374, loss = 1.57114483\n",
      "Iteration 5375, loss = 1.57111698\n",
      "Iteration 5376, loss = 1.57108914\n",
      "Iteration 5377, loss = 1.57106131\n",
      "Iteration 5378, loss = 1.57103349\n",
      "Iteration 5379, loss = 1.57100568\n",
      "Iteration 5380, loss = 1.57097787\n",
      "Iteration 5381, loss = 1.57095007\n",
      "Iteration 5382, loss = 1.57092229\n",
      "Iteration 5383, loss = 1.57089450\n",
      "Iteration 5384, loss = 1.57086673\n",
      "Iteration 5385, loss = 1.57083896\n",
      "Iteration 5386, loss = 1.57081121\n",
      "Iteration 5387, loss = 1.57078346\n",
      "Iteration 5388, loss = 1.57075572\n",
      "Iteration 5389, loss = 1.57072798\n",
      "Iteration 5390, loss = 1.57070026\n",
      "Iteration 5391, loss = 1.57067254\n",
      "Iteration 5392, loss = 1.57064483\n",
      "Iteration 5393, loss = 1.57061713\n",
      "Iteration 5394, loss = 1.57058944\n",
      "Iteration 5395, loss = 1.57056175\n",
      "Iteration 5396, loss = 1.57053408\n",
      "Iteration 5397, loss = 1.57050641\n",
      "Iteration 5398, loss = 1.57047875\n",
      "Iteration 5399, loss = 1.57045109\n",
      "Iteration 5400, loss = 1.57042345\n",
      "Iteration 5401, loss = 1.57039581\n",
      "Iteration 5402, loss = 1.57036818\n",
      "Iteration 5403, loss = 1.57034056\n",
      "Iteration 5404, loss = 1.57031295\n",
      "Iteration 5405, loss = 1.57028534\n",
      "Iteration 5406, loss = 1.57025775\n",
      "Iteration 5407, loss = 1.57023016\n",
      "Iteration 5408, loss = 1.57020258\n",
      "Iteration 5409, loss = 1.57017501\n",
      "Iteration 5410, loss = 1.57014744\n",
      "Iteration 5411, loss = 1.57011988\n",
      "Iteration 5412, loss = 1.57009233\n",
      "Iteration 5413, loss = 1.57006479\n",
      "Iteration 5414, loss = 1.57003726\n",
      "Iteration 5415, loss = 1.57000974\n",
      "Iteration 5416, loss = 1.56998222\n",
      "Iteration 5417, loss = 1.56995471\n",
      "Iteration 5418, loss = 1.56992721\n",
      "Iteration 5419, loss = 1.56989972\n",
      "Iteration 5420, loss = 1.56987223\n",
      "Iteration 5421, loss = 1.56984475\n",
      "Iteration 5422, loss = 1.56981729\n",
      "Iteration 5423, loss = 1.56978982\n",
      "Iteration 5424, loss = 1.56976237\n",
      "Iteration 5425, loss = 1.56973493\n",
      "Iteration 5426, loss = 1.56970749\n",
      "Iteration 5427, loss = 1.56968006\n",
      "Iteration 5428, loss = 1.56965264\n",
      "Iteration 5429, loss = 1.56962522\n",
      "Iteration 5430, loss = 1.56959782\n",
      "Iteration 5431, loss = 1.56957042\n",
      "Iteration 5432, loss = 1.56954303\n",
      "Iteration 5433, loss = 1.56951565\n",
      "Iteration 5434, loss = 1.56948828\n",
      "Iteration 5435, loss = 1.56946091\n",
      "Iteration 5436, loss = 1.56943355\n",
      "Iteration 5437, loss = 1.56940620\n",
      "Iteration 5438, loss = 1.56937886\n",
      "Iteration 5439, loss = 1.56935153\n",
      "Iteration 5440, loss = 1.56932420\n",
      "Iteration 5441, loss = 1.56929688\n",
      "Iteration 5442, loss = 1.56926957\n",
      "Iteration 5443, loss = 1.56924227\n",
      "Iteration 5444, loss = 1.56921497\n",
      "Iteration 5445, loss = 1.56918769\n",
      "Iteration 5446, loss = 1.56916041\n",
      "Iteration 5447, loss = 1.56913314\n",
      "Iteration 5448, loss = 1.56910587\n",
      "Iteration 5449, loss = 1.56907862\n",
      "Iteration 5450, loss = 1.56905137\n",
      "Iteration 5451, loss = 1.56902413\n",
      "Iteration 5452, loss = 1.56899690\n",
      "Iteration 5453, loss = 1.56896968\n",
      "Iteration 5454, loss = 1.56894246\n",
      "Iteration 5455, loss = 1.56891525\n",
      "Iteration 5456, loss = 1.56888805\n",
      "Iteration 5457, loss = 1.56886086\n",
      "Iteration 5458, loss = 1.56883368\n",
      "Iteration 5459, loss = 1.56880650\n",
      "Iteration 5460, loss = 1.56877933\n",
      "Iteration 5461, loss = 1.56875217\n",
      "Iteration 5462, loss = 1.56872502\n",
      "Iteration 5463, loss = 1.56869787\n",
      "Iteration 5464, loss = 1.56867074\n",
      "Iteration 5465, loss = 1.56864361\n",
      "Iteration 5466, loss = 1.56861648\n",
      "Iteration 5467, loss = 1.56858937\n",
      "Iteration 5468, loss = 1.56856226\n",
      "Iteration 5469, loss = 1.56853517\n",
      "Iteration 5470, loss = 1.56850808\n",
      "Iteration 5471, loss = 1.56848099\n",
      "Iteration 5472, loss = 1.56845392\n",
      "Iteration 5473, loss = 1.56842685\n",
      "Iteration 5474, loss = 1.56839979\n",
      "Iteration 5475, loss = 1.56837274\n",
      "Iteration 5476, loss = 1.56834570\n",
      "Iteration 5477, loss = 1.56831866\n",
      "Iteration 5478, loss = 1.56829163\n",
      "Iteration 5479, loss = 1.56826461\n",
      "Iteration 5480, loss = 1.56823760\n",
      "Iteration 5481, loss = 1.56821060\n",
      "Iteration 5482, loss = 1.56818360\n",
      "Iteration 5483, loss = 1.56815661\n",
      "Iteration 5484, loss = 1.56812963\n",
      "Iteration 5485, loss = 1.56810266\n",
      "Iteration 5486, loss = 1.56807569\n",
      "Iteration 5487, loss = 1.56804874\n",
      "Iteration 5488, loss = 1.56802179\n",
      "Iteration 5489, loss = 1.56799484\n",
      "Iteration 5490, loss = 1.56796791\n",
      "Iteration 5491, loss = 1.56794098\n",
      "Iteration 5492, loss = 1.56791406\n",
      "Iteration 5493, loss = 1.56788715\n",
      "Iteration 5494, loss = 1.56786025\n",
      "Iteration 5495, loss = 1.56783335\n",
      "Iteration 5496, loss = 1.56780647\n",
      "Iteration 5497, loss = 1.56777959\n",
      "Iteration 5498, loss = 1.56775271\n",
      "Iteration 5499, loss = 1.56772585\n",
      "Iteration 5500, loss = 1.56769899\n",
      "Iteration 5501, loss = 1.56767214\n",
      "Iteration 5502, loss = 1.56764530\n",
      "Iteration 5503, loss = 1.56761847\n",
      "Iteration 5504, loss = 1.56759164\n",
      "Iteration 5505, loss = 1.56756482\n",
      "Iteration 5506, loss = 1.56753801\n",
      "Iteration 5507, loss = 1.56751121\n",
      "Iteration 5508, loss = 1.56748441\n",
      "Iteration 5509, loss = 1.56745763\n",
      "Iteration 5510, loss = 1.56743085\n",
      "Iteration 5511, loss = 1.56740408\n",
      "Iteration 5512, loss = 1.56737731\n",
      "Iteration 5513, loss = 1.56735056\n",
      "Iteration 5514, loss = 1.56732381\n",
      "Iteration 5515, loss = 1.56729707\n",
      "Iteration 5516, loss = 1.56727033\n",
      "Iteration 5517, loss = 1.56724361\n",
      "Iteration 5518, loss = 1.56721689\n",
      "Iteration 5519, loss = 1.56719018\n",
      "Iteration 5520, loss = 1.56716348\n",
      "Iteration 5521, loss = 1.56713678\n",
      "Iteration 5522, loss = 1.56711009\n",
      "Iteration 5523, loss = 1.56708341\n",
      "Iteration 5524, loss = 1.56705674\n",
      "Iteration 5525, loss = 1.56703008\n",
      "Iteration 5526, loss = 1.56700342\n",
      "Iteration 5527, loss = 1.56697677\n",
      "Iteration 5528, loss = 1.56695013\n",
      "Iteration 5529, loss = 1.56692350\n",
      "Iteration 5530, loss = 1.56689687\n",
      "Iteration 5531, loss = 1.56687025\n",
      "Iteration 5532, loss = 1.56684364\n",
      "Iteration 5533, loss = 1.56681704\n",
      "Iteration 5534, loss = 1.56679045\n",
      "Iteration 5535, loss = 1.56676386\n",
      "Iteration 5536, loss = 1.56673728\n",
      "Iteration 5537, loss = 1.56671071\n",
      "Iteration 5538, loss = 1.56668414\n",
      "Iteration 5539, loss = 1.56665758\n",
      "Iteration 5540, loss = 1.56663103\n",
      "Iteration 5541, loss = 1.56660449\n",
      "Iteration 5542, loss = 1.56657796\n",
      "Iteration 5543, loss = 1.56655143\n",
      "Iteration 5544, loss = 1.56652491\n",
      "Iteration 5545, loss = 1.56649840\n",
      "Iteration 5546, loss = 1.56647190\n",
      "Iteration 5547, loss = 1.56644540\n",
      "Iteration 5548, loss = 1.56641891\n",
      "Iteration 5549, loss = 1.56639243\n",
      "Iteration 5550, loss = 1.56636596\n",
      "Iteration 5551, loss = 1.56633949\n",
      "Iteration 5552, loss = 1.56631304\n",
      "Iteration 5553, loss = 1.56628659\n",
      "Iteration 5554, loss = 1.56626014\n",
      "Iteration 5555, loss = 1.56623371\n",
      "Iteration 5556, loss = 1.56620728\n",
      "Iteration 5557, loss = 1.56618086\n",
      "Iteration 5558, loss = 1.56615445\n",
      "Iteration 5559, loss = 1.56612804\n",
      "Iteration 5560, loss = 1.56610164\n",
      "Iteration 5561, loss = 1.56607525\n",
      "Iteration 5562, loss = 1.56604887\n",
      "Iteration 5563, loss = 1.56602250\n",
      "Iteration 5564, loss = 1.56599613\n",
      "Iteration 5565, loss = 1.56596977\n",
      "Iteration 5566, loss = 1.56594342\n",
      "Iteration 5567, loss = 1.56591708\n",
      "Iteration 5568, loss = 1.56589074\n",
      "Iteration 5569, loss = 1.56586441\n",
      "Iteration 5570, loss = 1.56583809\n",
      "Iteration 5571, loss = 1.56581177\n",
      "Iteration 5572, loss = 1.56578547\n",
      "Iteration 5573, loss = 1.56575917\n",
      "Iteration 5574, loss = 1.56573288\n",
      "Iteration 5575, loss = 1.56570659\n",
      "Iteration 5576, loss = 1.56568032\n",
      "Iteration 5577, loss = 1.56565405\n",
      "Iteration 5578, loss = 1.56562779\n",
      "Iteration 5579, loss = 1.56560153\n",
      "Iteration 5580, loss = 1.56557529\n",
      "Iteration 5581, loss = 1.56554905\n",
      "Iteration 5582, loss = 1.56552282\n",
      "Iteration 5583, loss = 1.56549659\n",
      "Iteration 5584, loss = 1.56547037\n",
      "Iteration 5585, loss = 1.56544417\n",
      "Iteration 5586, loss = 1.56541797\n",
      "Iteration 5587, loss = 1.56539177\n",
      "Iteration 5588, loss = 1.56536559\n",
      "Iteration 5589, loss = 1.56533941\n",
      "Iteration 5590, loss = 1.56531324\n",
      "Iteration 5591, loss = 1.56528707\n",
      "Iteration 5592, loss = 1.56526092\n",
      "Iteration 5593, loss = 1.56523477\n",
      "Iteration 5594, loss = 1.56520863\n",
      "Iteration 5595, loss = 1.56518249\n",
      "Iteration 5596, loss = 1.56515637\n",
      "Iteration 5597, loss = 1.56513025\n",
      "Iteration 5598, loss = 1.56510413\n",
      "Iteration 5599, loss = 1.56507803\n",
      "Iteration 5600, loss = 1.56505193\n",
      "Iteration 5601, loss = 1.56502585\n",
      "Iteration 5602, loss = 1.56499976\n",
      "Iteration 5603, loss = 1.56497369\n",
      "Iteration 5604, loss = 1.56494762\n",
      "Iteration 5605, loss = 1.56492156\n",
      "Iteration 5606, loss = 1.56489551\n",
      "Iteration 5607, loss = 1.56486947\n",
      "Iteration 5608, loss = 1.56484343\n",
      "Iteration 5609, loss = 1.56481740\n",
      "Iteration 5610, loss = 1.56479138\n",
      "Iteration 5611, loss = 1.56476537\n",
      "Iteration 5612, loss = 1.56473936\n",
      "Iteration 5613, loss = 1.56471336\n",
      "Iteration 5614, loss = 1.56468737\n",
      "Iteration 5615, loss = 1.56466138\n",
      "Iteration 5616, loss = 1.56463541\n",
      "Iteration 5617, loss = 1.56460944\n",
      "Iteration 5618, loss = 1.56458347\n",
      "Iteration 5619, loss = 1.56455752\n",
      "Iteration 5620, loss = 1.56453157\n",
      "Iteration 5621, loss = 1.56450563\n",
      "Iteration 5622, loss = 1.56447970\n",
      "Iteration 5623, loss = 1.56445377\n",
      "Iteration 5624, loss = 1.56442786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5625, loss = 1.56440195\n",
      "Iteration 5626, loss = 1.56437604\n",
      "Iteration 5627, loss = 1.56435015\n",
      "Iteration 5628, loss = 1.56432426\n",
      "Iteration 5629, loss = 1.56429838\n",
      "Iteration 5630, loss = 1.56427250\n",
      "Iteration 5631, loss = 1.56424664\n",
      "Iteration 5632, loss = 1.56422078\n",
      "Iteration 5633, loss = 1.56419493\n",
      "Iteration 5634, loss = 1.56416908\n",
      "Iteration 5635, loss = 1.56414325\n",
      "Iteration 5636, loss = 1.56411742\n",
      "Iteration 5637, loss = 1.56409160\n",
      "Iteration 5638, loss = 1.56406578\n",
      "Iteration 5639, loss = 1.56403998\n",
      "Iteration 5640, loss = 1.56401418\n",
      "Iteration 5641, loss = 1.56398838\n",
      "Iteration 5642, loss = 1.56396260\n",
      "Iteration 5643, loss = 1.56393682\n",
      "Iteration 5644, loss = 1.56391105\n",
      "Iteration 5645, loss = 1.56388529\n",
      "Iteration 5646, loss = 1.56385953\n",
      "Iteration 5647, loss = 1.56383378\n",
      "Iteration 5648, loss = 1.56380804\n",
      "Iteration 5649, loss = 1.56378231\n",
      "Iteration 5650, loss = 1.56375658\n",
      "Iteration 5651, loss = 1.56373086\n",
      "Iteration 5652, loss = 1.56370515\n",
      "Iteration 5653, loss = 1.56367945\n",
      "Iteration 5654, loss = 1.56365375\n",
      "Iteration 5655, loss = 1.56362806\n",
      "Iteration 5656, loss = 1.56360238\n",
      "Iteration 5657, loss = 1.56357671\n",
      "Iteration 5658, loss = 1.56355104\n",
      "Iteration 5659, loss = 1.56352538\n",
      "Iteration 5660, loss = 1.56349973\n",
      "Iteration 5661, loss = 1.56347408\n",
      "Iteration 5662, loss = 1.56344844\n",
      "Iteration 5663, loss = 1.56342281\n",
      "Iteration 5664, loss = 1.56339719\n",
      "Iteration 5665, loss = 1.56337157\n",
      "Iteration 5666, loss = 1.56334596\n",
      "Iteration 5667, loss = 1.56332036\n",
      "Iteration 5668, loss = 1.56329477\n",
      "Iteration 5669, loss = 1.56326918\n",
      "Iteration 5670, loss = 1.56324360\n",
      "Iteration 5671, loss = 1.56321803\n",
      "Iteration 5672, loss = 1.56319246\n",
      "Iteration 5673, loss = 1.56316690\n",
      "Iteration 5674, loss = 1.56314135\n",
      "Iteration 5675, loss = 1.56311581\n",
      "Iteration 5676, loss = 1.56309027\n",
      "Iteration 5677, loss = 1.56306474\n",
      "Iteration 5678, loss = 1.56303922\n",
      "Iteration 5679, loss = 1.56301371\n",
      "Iteration 5680, loss = 1.56298820\n",
      "Iteration 5681, loss = 1.56296270\n",
      "Iteration 5682, loss = 1.56293721\n",
      "Iteration 5683, loss = 1.56291172\n",
      "Iteration 5684, loss = 1.56288624\n",
      "Iteration 5685, loss = 1.56286077\n",
      "Iteration 5686, loss = 1.56283531\n",
      "Iteration 5687, loss = 1.56280985\n",
      "Iteration 5688, loss = 1.56278440\n",
      "Iteration 5689, loss = 1.56275896\n",
      "Iteration 5690, loss = 1.56273353\n",
      "Iteration 5691, loss = 1.56270810\n",
      "Iteration 5692, loss = 1.56268268\n",
      "Iteration 5693, loss = 1.56265727\n",
      "Iteration 5694, loss = 1.56263186\n",
      "Iteration 5695, loss = 1.56260646\n",
      "Iteration 5696, loss = 1.56258107\n",
      "Iteration 5697, loss = 1.56255569\n",
      "Iteration 5698, loss = 1.56253031\n",
      "Iteration 5699, loss = 1.56250494\n",
      "Iteration 5700, loss = 1.56247958\n",
      "Iteration 5701, loss = 1.56245422\n",
      "Iteration 5702, loss = 1.56242888\n",
      "Iteration 5703, loss = 1.56240353\n",
      "Iteration 5704, loss = 1.56237820\n",
      "Iteration 5705, loss = 1.56235287\n",
      "Iteration 5706, loss = 1.56232756\n",
      "Iteration 5707, loss = 1.56230224\n",
      "Iteration 5708, loss = 1.56227694\n",
      "Iteration 5709, loss = 1.56225164\n",
      "Iteration 5710, loss = 1.56222635\n",
      "Iteration 5711, loss = 1.56220107\n",
      "Iteration 5712, loss = 1.56217579\n",
      "Iteration 5713, loss = 1.56215052\n",
      "Iteration 5714, loss = 1.56212526\n",
      "Iteration 5715, loss = 1.56210001\n",
      "Iteration 5716, loss = 1.56207476\n",
      "Iteration 5717, loss = 1.56204952\n",
      "Iteration 5718, loss = 1.56202429\n",
      "Iteration 5719, loss = 1.56199906\n",
      "Iteration 5720, loss = 1.56197384\n",
      "Iteration 5721, loss = 1.56194863\n",
      "Iteration 5722, loss = 1.56192342\n",
      "Iteration 5723, loss = 1.56189823\n",
      "Iteration 5724, loss = 1.56187304\n",
      "Iteration 5725, loss = 1.56184785\n",
      "Iteration 5726, loss = 1.56182268\n",
      "Iteration 5727, loss = 1.56179751\n",
      "Iteration 5728, loss = 1.56177235\n",
      "Iteration 5729, loss = 1.56174719\n",
      "Iteration 5730, loss = 1.56172205\n",
      "Iteration 5731, loss = 1.56169691\n",
      "Iteration 5732, loss = 1.56167177\n",
      "Iteration 5733, loss = 1.56164665\n",
      "Iteration 5734, loss = 1.56162153\n",
      "Iteration 5735, loss = 1.56159642\n",
      "Iteration 5736, loss = 1.56157131\n",
      "Iteration 5737, loss = 1.56154622\n",
      "Iteration 5738, loss = 1.56152113\n",
      "Iteration 5739, loss = 1.56149604\n",
      "Iteration 5740, loss = 1.56147097\n",
      "Iteration 5741, loss = 1.56144590\n",
      "Iteration 5742, loss = 1.56142084\n",
      "Iteration 5743, loss = 1.56139578\n",
      "Iteration 5744, loss = 1.56137073\n",
      "Iteration 5745, loss = 1.56134569\n",
      "Iteration 5746, loss = 1.56132066\n",
      "Iteration 5747, loss = 1.56129563\n",
      "Iteration 5748, loss = 1.56127061\n",
      "Iteration 5749, loss = 1.56124560\n",
      "Iteration 5750, loss = 1.56122060\n",
      "Iteration 5751, loss = 1.56119560\n",
      "Iteration 5752, loss = 1.56117061\n",
      "Iteration 5753, loss = 1.56114562\n",
      "Iteration 5754, loss = 1.56112065\n",
      "Iteration 5755, loss = 1.56109568\n",
      "Iteration 5756, loss = 1.56107072\n",
      "Iteration 5757, loss = 1.56104576\n",
      "Iteration 5758, loss = 1.56102081\n",
      "Iteration 5759, loss = 1.56099587\n",
      "Iteration 5760, loss = 1.56097094\n",
      "Iteration 5761, loss = 1.56094601\n",
      "Iteration 5762, loss = 1.56092109\n",
      "Iteration 5763, loss = 1.56089618\n",
      "Iteration 5764, loss = 1.56087127\n",
      "Iteration 5765, loss = 1.56084637\n",
      "Iteration 5766, loss = 1.56082148\n",
      "Iteration 5767, loss = 1.56079659\n",
      "Iteration 5768, loss = 1.56077172\n",
      "Iteration 5769, loss = 1.56074684\n",
      "Iteration 5770, loss = 1.56072198\n",
      "Iteration 5771, loss = 1.56069712\n",
      "Iteration 5772, loss = 1.56067227\n",
      "Iteration 5773, loss = 1.56064743\n",
      "Iteration 5774, loss = 1.56062260\n",
      "Iteration 5775, loss = 1.56059777\n",
      "Iteration 5776, loss = 1.56057295\n",
      "Iteration 5777, loss = 1.56054813\n",
      "Iteration 5778, loss = 1.56052332\n",
      "Iteration 5779, loss = 1.56049852\n",
      "Iteration 5780, loss = 1.56047373\n",
      "Iteration 5781, loss = 1.56044894\n",
      "Iteration 5782, loss = 1.56042416\n",
      "Iteration 5783, loss = 1.56039939\n",
      "Iteration 5784, loss = 1.56037462\n",
      "Iteration 5785, loss = 1.56034987\n",
      "Iteration 5786, loss = 1.56032511\n",
      "Iteration 5787, loss = 1.56030037\n",
      "Iteration 5788, loss = 1.56027563\n",
      "Iteration 5789, loss = 1.56025090\n",
      "Iteration 5790, loss = 1.56022618\n",
      "Iteration 5791, loss = 1.56020146\n",
      "Iteration 5792, loss = 1.56017675\n",
      "Iteration 5793, loss = 1.56015205\n",
      "Iteration 5794, loss = 1.56012735\n",
      "Iteration 5795, loss = 1.56010266\n",
      "Iteration 5796, loss = 1.56007798\n",
      "Iteration 5797, loss = 1.56005331\n",
      "Iteration 5798, loss = 1.56002864\n",
      "Iteration 5799, loss = 1.56000398\n",
      "Iteration 5800, loss = 1.55997933\n",
      "Iteration 5801, loss = 1.55995468\n",
      "Iteration 5802, loss = 1.55993004\n",
      "Iteration 5803, loss = 1.55990541\n",
      "Iteration 5804, loss = 1.55988078\n",
      "Iteration 5805, loss = 1.55985616\n",
      "Iteration 5806, loss = 1.55983155\n",
      "Iteration 5807, loss = 1.55980694\n",
      "Iteration 5808, loss = 1.55978234\n",
      "Iteration 5809, loss = 1.55975775\n",
      "Iteration 5810, loss = 1.55973317\n",
      "Iteration 5811, loss = 1.55970859\n",
      "Iteration 5812, loss = 1.55968402\n",
      "Iteration 5813, loss = 1.55965946\n",
      "Iteration 5814, loss = 1.55963490\n",
      "Iteration 5815, loss = 1.55961035\n",
      "Iteration 5816, loss = 1.55958581\n",
      "Iteration 5817, loss = 1.55956127\n",
      "Iteration 5818, loss = 1.55953674\n",
      "Iteration 5819, loss = 1.55951222\n",
      "Iteration 5820, loss = 1.55948771\n",
      "Iteration 5821, loss = 1.55946320\n",
      "Iteration 5822, loss = 1.55943870\n",
      "Iteration 5823, loss = 1.55941420\n",
      "Iteration 5824, loss = 1.55938971\n",
      "Iteration 5825, loss = 1.55936523\n",
      "Iteration 5826, loss = 1.55934076\n",
      "Iteration 5827, loss = 1.55931629\n",
      "Iteration 5828, loss = 1.55929183\n",
      "Iteration 5829, loss = 1.55926738\n",
      "Iteration 5830, loss = 1.55924293\n",
      "Iteration 5831, loss = 1.55921850\n",
      "Iteration 5832, loss = 1.55919406\n",
      "Iteration 5833, loss = 1.55916964\n",
      "Iteration 5834, loss = 1.55914522\n",
      "Iteration 5835, loss = 1.55912081\n",
      "Iteration 5836, loss = 1.55909640\n",
      "Iteration 5837, loss = 1.55907200\n",
      "Iteration 5838, loss = 1.55904761\n",
      "Iteration 5839, loss = 1.55902323\n",
      "Iteration 5840, loss = 1.55899885\n",
      "Iteration 5841, loss = 1.55897448\n",
      "Iteration 5842, loss = 1.55895012\n",
      "Iteration 5843, loss = 1.55892576\n",
      "Iteration 5844, loss = 1.55890141\n",
      "Iteration 5845, loss = 1.55887707\n",
      "Iteration 5846, loss = 1.55885273\n",
      "Iteration 5847, loss = 1.55882840\n",
      "Iteration 5848, loss = 1.55880408\n",
      "Iteration 5849, loss = 1.55877976\n",
      "Iteration 5850, loss = 1.55875545\n",
      "Iteration 5851, loss = 1.55873115\n",
      "Iteration 5852, loss = 1.55870686\n",
      "Iteration 5853, loss = 1.55868257\n",
      "Iteration 5854, loss = 1.55865829\n",
      "Iteration 5855, loss = 1.55863401\n",
      "Iteration 5856, loss = 1.55860974\n",
      "Iteration 5857, loss = 1.55858548\n",
      "Iteration 5858, loss = 1.55856123\n",
      "Iteration 5859, loss = 1.55853698\n",
      "Iteration 5860, loss = 1.55851274\n",
      "Iteration 5861, loss = 1.55848851\n",
      "Iteration 5862, loss = 1.55846428\n",
      "Iteration 5863, loss = 1.55844006\n",
      "Iteration 5864, loss = 1.55841585\n",
      "Iteration 5865, loss = 1.55839164\n",
      "Iteration 5866, loss = 1.55836744\n",
      "Iteration 5867, loss = 1.55834325\n",
      "Iteration 5868, loss = 1.55831906\n",
      "Iteration 5869, loss = 1.55829488\n",
      "Iteration 5870, loss = 1.55827071\n",
      "Iteration 5871, loss = 1.55824654\n",
      "Iteration 5872, loss = 1.55822239\n",
      "Iteration 5873, loss = 1.55819823\n",
      "Iteration 5874, loss = 1.55817409\n",
      "Iteration 5875, loss = 1.55814995\n",
      "Iteration 5876, loss = 1.55812582\n",
      "Iteration 5877, loss = 1.55810169\n",
      "Iteration 5878, loss = 1.55807757\n",
      "Iteration 5879, loss = 1.55805346\n",
      "Iteration 5880, loss = 1.55802936\n",
      "Iteration 5881, loss = 1.55800526\n",
      "Iteration 5882, loss = 1.55798117\n",
      "Iteration 5883, loss = 1.55795708\n",
      "Iteration 5884, loss = 1.55793301\n",
      "Iteration 5885, loss = 1.55790894\n",
      "Iteration 5886, loss = 1.55788487\n",
      "Iteration 5887, loss = 1.55786081\n",
      "Iteration 5888, loss = 1.55783676\n",
      "Iteration 5889, loss = 1.55781272\n",
      "Iteration 5890, loss = 1.55778868\n",
      "Iteration 5891, loss = 1.55776465\n",
      "Iteration 5892, loss = 1.55774063\n",
      "Iteration 5893, loss = 1.55771661\n",
      "Iteration 5894, loss = 1.55769260\n",
      "Iteration 5895, loss = 1.55766860\n",
      "Iteration 5896, loss = 1.55764460\n",
      "Iteration 5897, loss = 1.55762061\n",
      "Iteration 5898, loss = 1.55759663\n",
      "Iteration 5899, loss = 1.55757265\n",
      "Iteration 5900, loss = 1.55754868\n",
      "Iteration 5901, loss = 1.55752472\n",
      "Iteration 5902, loss = 1.55750076\n",
      "Iteration 5903, loss = 1.55747681\n",
      "Iteration 5904, loss = 1.55745287\n",
      "Iteration 5905, loss = 1.55742893\n",
      "Iteration 5906, loss = 1.55740500\n",
      "Iteration 5907, loss = 1.55738108\n",
      "Iteration 5908, loss = 1.55735716\n",
      "Iteration 5909, loss = 1.55733325\n",
      "Iteration 5910, loss = 1.55730935\n",
      "Iteration 5911, loss = 1.55728545\n",
      "Iteration 5912, loss = 1.55726157\n",
      "Iteration 5913, loss = 1.55723768\n",
      "Iteration 5914, loss = 1.55721381\n",
      "Iteration 5915, loss = 1.55718994\n",
      "Iteration 5916, loss = 1.55716607\n",
      "Iteration 5917, loss = 1.55714222\n",
      "Iteration 5918, loss = 1.55711837\n",
      "Iteration 5919, loss = 1.55709453\n",
      "Iteration 5920, loss = 1.55707069\n",
      "Iteration 5921, loss = 1.55704686\n",
      "Iteration 5922, loss = 1.55702304\n",
      "Iteration 5923, loss = 1.55699922\n",
      "Iteration 5924, loss = 1.55697541\n",
      "Iteration 5925, loss = 1.55695161\n",
      "Iteration 5926, loss = 1.55692782\n",
      "Iteration 5927, loss = 1.55690403\n",
      "Iteration 5928, loss = 1.55688024\n",
      "Iteration 5929, loss = 1.55685647\n",
      "Iteration 5930, loss = 1.55683270\n",
      "Iteration 5931, loss = 1.55680894\n",
      "Iteration 5932, loss = 1.55678518\n",
      "Iteration 5933, loss = 1.55676143\n",
      "Iteration 5934, loss = 1.55673769\n",
      "Iteration 5935, loss = 1.55671395\n",
      "Iteration 5936, loss = 1.55669022\n",
      "Iteration 5937, loss = 1.55666650\n",
      "Iteration 5938, loss = 1.55664278\n",
      "Iteration 5939, loss = 1.55661907\n",
      "Iteration 5940, loss = 1.55659537\n",
      "Iteration 5941, loss = 1.55657168\n",
      "Iteration 5942, loss = 1.55654799\n",
      "Iteration 5943, loss = 1.55652430\n",
      "Iteration 5944, loss = 1.55650063\n",
      "Iteration 5945, loss = 1.55647696\n",
      "Iteration 5946, loss = 1.55645329\n",
      "Iteration 5947, loss = 1.55642964\n",
      "Iteration 5948, loss = 1.55640599\n",
      "Iteration 5949, loss = 1.55638234\n",
      "Iteration 5950, loss = 1.55635871\n",
      "Iteration 5951, loss = 1.55633508\n",
      "Iteration 5952, loss = 1.55631145\n",
      "Iteration 5953, loss = 1.55628784\n",
      "Iteration 5954, loss = 1.55626423\n",
      "Iteration 5955, loss = 1.55624062\n",
      "Iteration 5956, loss = 1.55621702\n",
      "Iteration 5957, loss = 1.55619343\n",
      "Iteration 5958, loss = 1.55616985\n",
      "Iteration 5959, loss = 1.55614627\n",
      "Iteration 5960, loss = 1.55612270\n",
      "Iteration 5961, loss = 1.55609914\n",
      "Iteration 5962, loss = 1.55607558\n",
      "Iteration 5963, loss = 1.55605203\n",
      "Iteration 5964, loss = 1.55602848\n",
      "Iteration 5965, loss = 1.55600495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5966, loss = 1.55598142\n",
      "Iteration 5967, loss = 1.55595789\n",
      "Iteration 5968, loss = 1.55593437\n",
      "Iteration 5969, loss = 1.55591086\n",
      "Iteration 5970, loss = 1.55588736\n",
      "Iteration 5971, loss = 1.55586386\n",
      "Iteration 5972, loss = 1.55584037\n",
      "Iteration 5973, loss = 1.55581688\n",
      "Iteration 5974, loss = 1.55579340\n",
      "Iteration 5975, loss = 1.55576993\n",
      "Iteration 5976, loss = 1.55574646\n",
      "Iteration 5977, loss = 1.55572300\n",
      "Iteration 5978, loss = 1.55569955\n",
      "Iteration 5979, loss = 1.55567610\n",
      "Iteration 5980, loss = 1.55565266\n",
      "Iteration 5981, loss = 1.55562923\n",
      "Iteration 5982, loss = 1.55560581\n",
      "Iteration 5983, loss = 1.55558239\n",
      "Iteration 5984, loss = 1.55555897\n",
      "Iteration 5985, loss = 1.55553556\n",
      "Iteration 5986, loss = 1.55551216\n",
      "Iteration 5987, loss = 1.55548877\n",
      "Iteration 5988, loss = 1.55546538\n",
      "Iteration 5989, loss = 1.55544200\n",
      "Iteration 5990, loss = 1.55541863\n",
      "Iteration 5991, loss = 1.55539526\n",
      "Iteration 5992, loss = 1.55537190\n",
      "Iteration 5993, loss = 1.55534854\n",
      "Iteration 5994, loss = 1.55532520\n",
      "Iteration 5995, loss = 1.55530185\n",
      "Iteration 5996, loss = 1.55527852\n",
      "Iteration 5997, loss = 1.55525519\n",
      "Iteration 5998, loss = 1.55523187\n",
      "Iteration 5999, loss = 1.55520855\n",
      "Iteration 6000, loss = 1.55518524\n",
      "Iteration 6001, loss = 1.55516194\n",
      "Iteration 6002, loss = 1.55513864\n",
      "Iteration 6003, loss = 1.55511535\n",
      "Iteration 6004, loss = 1.55509207\n",
      "Iteration 6005, loss = 1.55506879\n",
      "Iteration 6006, loss = 1.55504552\n",
      "Iteration 6007, loss = 1.55502226\n",
      "Iteration 6008, loss = 1.55499900\n",
      "Iteration 6009, loss = 1.55497575\n",
      "Iteration 6010, loss = 1.55495250\n",
      "Iteration 6011, loss = 1.55492927\n",
      "Iteration 6012, loss = 1.55490603\n",
      "Iteration 6013, loss = 1.55488281\n",
      "Iteration 6014, loss = 1.55485959\n",
      "Iteration 6015, loss = 1.55483638\n",
      "Iteration 6016, loss = 1.55481317\n",
      "Iteration 6017, loss = 1.55478997\n",
      "Iteration 6018, loss = 1.55476678\n",
      "Iteration 6019, loss = 1.55474359\n",
      "Iteration 6020, loss = 1.55472041\n",
      "Iteration 6021, loss = 1.55469724\n",
      "Iteration 6022, loss = 1.55467407\n",
      "Iteration 6023, loss = 1.55465091\n",
      "Iteration 6024, loss = 1.55462776\n",
      "Iteration 6025, loss = 1.55460461\n",
      "Iteration 6026, loss = 1.55458147\n",
      "Iteration 6027, loss = 1.55455833\n",
      "Iteration 6028, loss = 1.55453521\n",
      "Iteration 6029, loss = 1.55451208\n",
      "Iteration 6030, loss = 1.55448897\n",
      "Iteration 6031, loss = 1.55446586\n",
      "Iteration 6032, loss = 1.55444276\n",
      "Iteration 6033, loss = 1.55441966\n",
      "Iteration 6034, loss = 1.55439657\n",
      "Iteration 6035, loss = 1.55437349\n",
      "Iteration 6036, loss = 1.55435041\n",
      "Iteration 6037, loss = 1.55432734\n",
      "Iteration 6038, loss = 1.55430427\n",
      "Iteration 6039, loss = 1.55428121\n",
      "Iteration 6040, loss = 1.55425816\n",
      "Iteration 6041, loss = 1.55423512\n",
      "Iteration 6042, loss = 1.55421208\n",
      "Iteration 6043, loss = 1.55418905\n",
      "Iteration 6044, loss = 1.55416602\n",
      "Iteration 6045, loss = 1.55414300\n",
      "Iteration 6046, loss = 1.55411999\n",
      "Iteration 6047, loss = 1.55409698\n",
      "Iteration 6048, loss = 1.55407398\n",
      "Iteration 6049, loss = 1.55405099\n",
      "Iteration 6050, loss = 1.55402800\n",
      "Iteration 6051, loss = 1.55400502\n",
      "Iteration 6052, loss = 1.55398204\n",
      "Iteration 6053, loss = 1.55395907\n",
      "Iteration 6054, loss = 1.55393611\n",
      "Iteration 6055, loss = 1.55391316\n",
      "Iteration 6056, loss = 1.55389021\n",
      "Iteration 6057, loss = 1.55386726\n",
      "Iteration 6058, loss = 1.55384433\n",
      "Iteration 6059, loss = 1.55382140\n",
      "Iteration 6060, loss = 1.55379847\n",
      "Iteration 6061, loss = 1.55377556\n",
      "Iteration 6062, loss = 1.55375264\n",
      "Iteration 6063, loss = 1.55372974\n",
      "Iteration 6064, loss = 1.55370684\n",
      "Iteration 6065, loss = 1.55368395\n",
      "Iteration 6066, loss = 1.55366106\n",
      "Iteration 6067, loss = 1.55363818\n",
      "Iteration 6068, loss = 1.55361531\n",
      "Iteration 6069, loss = 1.55359244\n",
      "Iteration 6070, loss = 1.55356958\n",
      "Iteration 6071, loss = 1.55354673\n",
      "Iteration 6072, loss = 1.55352388\n",
      "Iteration 6073, loss = 1.55350104\n",
      "Iteration 6074, loss = 1.55347820\n",
      "Iteration 6075, loss = 1.55345537\n",
      "Iteration 6076, loss = 1.55343255\n",
      "Iteration 6077, loss = 1.55340973\n",
      "Iteration 6078, loss = 1.55338692\n",
      "Iteration 6079, loss = 1.55336412\n",
      "Iteration 6080, loss = 1.55334132\n",
      "Iteration 6081, loss = 1.55331853\n",
      "Iteration 6082, loss = 1.55329575\n",
      "Iteration 6083, loss = 1.55327297\n",
      "Iteration 6084, loss = 1.55325020\n",
      "Iteration 6085, loss = 1.55322743\n",
      "Iteration 6086, loss = 1.55320467\n",
      "Iteration 6087, loss = 1.55318192\n",
      "Iteration 6088, loss = 1.55315917\n",
      "Iteration 6089, loss = 1.55313643\n",
      "Iteration 6090, loss = 1.55311370\n",
      "Iteration 6091, loss = 1.55309097\n",
      "Iteration 6092, loss = 1.55306825\n",
      "Iteration 6093, loss = 1.55304553\n",
      "Iteration 6094, loss = 1.55302282\n",
      "Iteration 6095, loss = 1.55300012\n",
      "Iteration 6096, loss = 1.55297742\n",
      "Iteration 6097, loss = 1.55295473\n",
      "Iteration 6098, loss = 1.55293205\n",
      "Iteration 6099, loss = 1.55290937\n",
      "Iteration 6100, loss = 1.55288670\n",
      "Iteration 6101, loss = 1.55286403\n",
      "Iteration 6102, loss = 1.55284138\n",
      "Iteration 6103, loss = 1.55281872\n",
      "Iteration 6104, loss = 1.55279608\n",
      "Iteration 6105, loss = 1.55277344\n",
      "Iteration 6106, loss = 1.55275080\n",
      "Iteration 6107, loss = 1.55272817\n",
      "Iteration 6108, loss = 1.55270555\n",
      "Iteration 6109, loss = 1.55268294\n",
      "Iteration 6110, loss = 1.55266033\n",
      "Iteration 6111, loss = 1.55263773\n",
      "Iteration 6112, loss = 1.55261513\n",
      "Iteration 6113, loss = 1.55259254\n",
      "Iteration 6114, loss = 1.55256995\n",
      "Iteration 6115, loss = 1.55254738\n",
      "Iteration 6116, loss = 1.55252481\n",
      "Iteration 6117, loss = 1.55250224\n",
      "Iteration 6118, loss = 1.55247968\n",
      "Iteration 6119, loss = 1.55245713\n",
      "Iteration 6120, loss = 1.55243458\n",
      "Iteration 6121, loss = 1.55241204\n",
      "Iteration 6122, loss = 1.55238951\n",
      "Iteration 6123, loss = 1.55236698\n",
      "Iteration 6124, loss = 1.55234446\n",
      "Iteration 6125, loss = 1.55232194\n",
      "Iteration 6126, loss = 1.55229943\n",
      "Iteration 6127, loss = 1.55227693\n",
      "Iteration 6128, loss = 1.55225443\n",
      "Iteration 6129, loss = 1.55223194\n",
      "Iteration 6130, loss = 1.55220946\n",
      "Iteration 6131, loss = 1.55218698\n",
      "Iteration 6132, loss = 1.55216451\n",
      "Iteration 6133, loss = 1.55214204\n",
      "Iteration 6134, loss = 1.55211958\n",
      "Iteration 6135, loss = 1.55209713\n",
      "Iteration 6136, loss = 1.55207468\n",
      "Iteration 6137, loss = 1.55205224\n",
      "Iteration 6138, loss = 1.55202980\n",
      "Iteration 6139, loss = 1.55200738\n",
      "Iteration 6140, loss = 1.55198495\n",
      "Iteration 6141, loss = 1.55196254\n",
      "Iteration 6142, loss = 1.55194013\n",
      "Iteration 6143, loss = 1.55191772\n",
      "Iteration 6144, loss = 1.55189533\n",
      "Iteration 6145, loss = 1.55187293\n",
      "Iteration 6146, loss = 1.55185055\n",
      "Iteration 6147, loss = 1.55182817\n",
      "Iteration 6148, loss = 1.55180580\n",
      "Iteration 6149, loss = 1.55178343\n",
      "Iteration 6150, loss = 1.55176107\n",
      "Iteration 6151, loss = 1.55173871\n",
      "Iteration 6152, loss = 1.55171636\n",
      "Iteration 6153, loss = 1.55169402\n",
      "Iteration 6154, loss = 1.55167169\n",
      "Iteration 6155, loss = 1.55164936\n",
      "Iteration 6156, loss = 1.55162703\n",
      "Iteration 6157, loss = 1.55160471\n",
      "Iteration 6158, loss = 1.55158240\n",
      "Iteration 6159, loss = 1.55156010\n",
      "Iteration 6160, loss = 1.55153780\n",
      "Iteration 6161, loss = 1.55151550\n",
      "Iteration 6162, loss = 1.55149322\n",
      "Iteration 6163, loss = 1.55147094\n",
      "Iteration 6164, loss = 1.55144866\n",
      "Iteration 6165, loss = 1.55142639\n",
      "Iteration 6166, loss = 1.55140413\n",
      "Iteration 6167, loss = 1.55138187\n",
      "Iteration 6168, loss = 1.55135962\n",
      "Iteration 6169, loss = 1.55133738\n",
      "Iteration 6170, loss = 1.55131514\n",
      "Iteration 6171, loss = 1.55129291\n",
      "Iteration 6172, loss = 1.55127068\n",
      "Iteration 6173, loss = 1.55124846\n",
      "Iteration 6174, loss = 1.55122625\n",
      "Iteration 6175, loss = 1.55120404\n",
      "Iteration 6176, loss = 1.55118184\n",
      "Iteration 6177, loss = 1.55115965\n",
      "Iteration 6178, loss = 1.55113746\n",
      "Iteration 6179, loss = 1.55111527\n",
      "Iteration 6180, loss = 1.55109310\n",
      "Iteration 6181, loss = 1.55107093\n",
      "Iteration 6182, loss = 1.55104876\n",
      "Iteration 6183, loss = 1.55102660\n",
      "Iteration 6184, loss = 1.55100445\n",
      "Iteration 6185, loss = 1.55098230\n",
      "Iteration 6186, loss = 1.55096016\n",
      "Iteration 6187, loss = 1.55093803\n",
      "Iteration 6188, loss = 1.55091590\n",
      "Iteration 6189, loss = 1.55089378\n",
      "Iteration 6190, loss = 1.55087166\n",
      "Iteration 6191, loss = 1.55084955\n",
      "Iteration 6192, loss = 1.55082745\n",
      "Iteration 6193, loss = 1.55080535\n",
      "Iteration 6194, loss = 1.55078326\n",
      "Iteration 6195, loss = 1.55076117\n",
      "Iteration 6196, loss = 1.55073909\n",
      "Iteration 6197, loss = 1.55071702\n",
      "Iteration 6198, loss = 1.55069495\n",
      "Iteration 6199, loss = 1.55067289\n",
      "Iteration 6200, loss = 1.55065083\n",
      "Iteration 6201, loss = 1.55062878\n",
      "Iteration 6202, loss = 1.55060674\n",
      "Iteration 6203, loss = 1.55058470\n",
      "Iteration 6204, loss = 1.55056267\n",
      "Iteration 6205, loss = 1.55054065\n",
      "Iteration 6206, loss = 1.55051863\n",
      "Iteration 6207, loss = 1.55049662\n",
      "Iteration 6208, loss = 1.55047461\n",
      "Iteration 6209, loss = 1.55045261\n",
      "Iteration 6210, loss = 1.55043061\n",
      "Iteration 6211, loss = 1.55040862\n",
      "Iteration 6212, loss = 1.55038664\n",
      "Iteration 6213, loss = 1.55036466\n",
      "Iteration 6214, loss = 1.55034269\n",
      "Iteration 6215, loss = 1.55032073\n",
      "Iteration 6216, loss = 1.55029877\n",
      "Iteration 6217, loss = 1.55027682\n",
      "Iteration 6218, loss = 1.55025487\n",
      "Iteration 6219, loss = 1.55023293\n",
      "Iteration 6220, loss = 1.55021099\n",
      "Iteration 6221, loss = 1.55018906\n",
      "Iteration 6222, loss = 1.55016714\n",
      "Iteration 6223, loss = 1.55014522\n",
      "Iteration 6224, loss = 1.55012331\n",
      "Iteration 6225, loss = 1.55010141\n",
      "Iteration 6226, loss = 1.55007951\n",
      "Iteration 6227, loss = 1.55005762\n",
      "Iteration 6228, loss = 1.55003573\n",
      "Iteration 6229, loss = 1.55001385\n",
      "Iteration 6230, loss = 1.54999197\n",
      "Iteration 6231, loss = 1.54997011\n",
      "Iteration 6232, loss = 1.54994824\n",
      "Iteration 6233, loss = 1.54992639\n",
      "Iteration 6234, loss = 1.54990454\n",
      "Iteration 6235, loss = 1.54988269\n",
      "Iteration 6236, loss = 1.54986085\n",
      "Iteration 6237, loss = 1.54983902\n",
      "Iteration 6238, loss = 1.54981719\n",
      "Iteration 6239, loss = 1.54979537\n",
      "Iteration 6240, loss = 1.54977356\n",
      "Iteration 6241, loss = 1.54975175\n",
      "Iteration 6242, loss = 1.54972994\n",
      "Iteration 6243, loss = 1.54970815\n",
      "Iteration 6244, loss = 1.54968636\n",
      "Iteration 6245, loss = 1.54966457\n",
      "Iteration 6246, loss = 1.54964279\n",
      "Iteration 6247, loss = 1.54962102\n",
      "Iteration 6248, loss = 1.54959925\n",
      "Iteration 6249, loss = 1.54957749\n",
      "Iteration 6250, loss = 1.54955573\n",
      "Iteration 6251, loss = 1.54953398\n",
      "Iteration 6252, loss = 1.54951224\n",
      "Iteration 6253, loss = 1.54949050\n",
      "Iteration 6254, loss = 1.54946877\n",
      "Iteration 6255, loss = 1.54944705\n",
      "Iteration 6256, loss = 1.54942533\n",
      "Iteration 6257, loss = 1.54940361\n",
      "Iteration 6258, loss = 1.54938191\n",
      "Iteration 6259, loss = 1.54936020\n",
      "Iteration 6260, loss = 1.54933851\n",
      "Iteration 6261, loss = 1.54931682\n",
      "Iteration 6262, loss = 1.54929513\n",
      "Iteration 6263, loss = 1.54927346\n",
      "Iteration 6264, loss = 1.54925178\n",
      "Iteration 6265, loss = 1.54923012\n",
      "Iteration 6266, loss = 1.54920846\n",
      "Iteration 6267, loss = 1.54918680\n",
      "Iteration 6268, loss = 1.54916515\n",
      "Iteration 6269, loss = 1.54914351\n",
      "Iteration 6270, loss = 1.54912187\n",
      "Iteration 6271, loss = 1.54910024\n",
      "Iteration 6272, loss = 1.54907862\n",
      "Iteration 6273, loss = 1.54905700\n",
      "Iteration 6274, loss = 1.54903539\n",
      "Iteration 6275, loss = 1.54901378\n",
      "Iteration 6276, loss = 1.54899218\n",
      "Iteration 6277, loss = 1.54897058\n",
      "Iteration 6278, loss = 1.54894899\n",
      "Iteration 6279, loss = 1.54892741\n",
      "Iteration 6280, loss = 1.54890583\n",
      "Iteration 6281, loss = 1.54888426\n",
      "Iteration 6282, loss = 1.54886269\n",
      "Iteration 6283, loss = 1.54884113\n",
      "Iteration 6284, loss = 1.54881958\n",
      "Iteration 6285, loss = 1.54879803\n",
      "Iteration 6286, loss = 1.54877649\n",
      "Iteration 6287, loss = 1.54875495\n",
      "Iteration 6288, loss = 1.54873342\n",
      "Iteration 6289, loss = 1.54871190\n",
      "Iteration 6290, loss = 1.54869038\n",
      "Iteration 6291, loss = 1.54866886\n",
      "Iteration 6292, loss = 1.54864736\n",
      "Iteration 6293, loss = 1.54862585\n",
      "Iteration 6294, loss = 1.54860436\n",
      "Iteration 6295, loss = 1.54858287\n",
      "Iteration 6296, loss = 1.54856139\n",
      "Iteration 6297, loss = 1.54853991\n",
      "Iteration 6298, loss = 1.54851844\n",
      "Iteration 6299, loss = 1.54849697\n",
      "Iteration 6300, loss = 1.54847551\n",
      "Iteration 6301, loss = 1.54845405\n",
      "Iteration 6302, loss = 1.54843261\n",
      "Iteration 6303, loss = 1.54841116\n",
      "Iteration 6304, loss = 1.54838973\n",
      "Iteration 6305, loss = 1.54836829\n",
      "Iteration 6306, loss = 1.54834687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6307, loss = 1.54832545\n",
      "Iteration 6308, loss = 1.54830404\n",
      "Iteration 6309, loss = 1.54828263\n",
      "Iteration 6310, loss = 1.54826123\n",
      "Iteration 6311, loss = 1.54823983\n",
      "Iteration 6312, loss = 1.54821844\n",
      "Iteration 6313, loss = 1.54819706\n",
      "Iteration 6314, loss = 1.54817568\n",
      "Iteration 6315, loss = 1.54815430\n",
      "Iteration 6316, loss = 1.54813294\n",
      "Iteration 6317, loss = 1.54811158\n",
      "Iteration 6318, loss = 1.54809022\n",
      "Iteration 6319, loss = 1.54806887\n",
      "Iteration 6320, loss = 1.54804753\n",
      "Iteration 6321, loss = 1.54802619\n",
      "Iteration 6322, loss = 1.54800486\n",
      "Iteration 6323, loss = 1.54798353\n",
      "Iteration 6324, loss = 1.54796221\n",
      "Iteration 6325, loss = 1.54794089\n",
      "Iteration 6326, loss = 1.54791959\n",
      "Iteration 6327, loss = 1.54789828\n",
      "Iteration 6328, loss = 1.54787699\n",
      "Iteration 6329, loss = 1.54785569\n",
      "Iteration 6330, loss = 1.54783441\n",
      "Iteration 6331, loss = 1.54781313\n",
      "Iteration 6332, loss = 1.54779185\n",
      "Iteration 6333, loss = 1.54777059\n",
      "Iteration 6334, loss = 1.54774932\n",
      "Iteration 6335, loss = 1.54772807\n",
      "Iteration 6336, loss = 1.54770681\n",
      "Iteration 6337, loss = 1.54768557\n",
      "Iteration 6338, loss = 1.54766433\n",
      "Iteration 6339, loss = 1.54764310\n",
      "Iteration 6340, loss = 1.54762187\n",
      "Iteration 6341, loss = 1.54760065\n",
      "Iteration 6342, loss = 1.54757943\n",
      "Iteration 6343, loss = 1.54755822\n",
      "Iteration 6344, loss = 1.54753701\n",
      "Iteration 6345, loss = 1.54751581\n",
      "Iteration 6346, loss = 1.54749462\n",
      "Iteration 6347, loss = 1.54747343\n",
      "Iteration 6348, loss = 1.54745225\n",
      "Iteration 6349, loss = 1.54743107\n",
      "Iteration 6350, loss = 1.54740990\n",
      "Iteration 6351, loss = 1.54738874\n",
      "Iteration 6352, loss = 1.54736758\n",
      "Iteration 6353, loss = 1.54734643\n",
      "Iteration 6354, loss = 1.54732528\n",
      "Iteration 6355, loss = 1.54730414\n",
      "Iteration 6356, loss = 1.54728300\n",
      "Iteration 6357, loss = 1.54726187\n",
      "Iteration 6358, loss = 1.54724075\n",
      "Iteration 6359, loss = 1.54721963\n",
      "Iteration 6360, loss = 1.54719852\n",
      "Iteration 6361, loss = 1.54717741\n",
      "Iteration 6362, loss = 1.54715631\n",
      "Iteration 6363, loss = 1.54713521\n",
      "Iteration 6364, loss = 1.54711412\n",
      "Iteration 6365, loss = 1.54709304\n",
      "Iteration 6366, loss = 1.54707196\n",
      "Iteration 6367, loss = 1.54705088\n",
      "Iteration 6368, loss = 1.54702982\n",
      "Iteration 6369, loss = 1.54700876\n",
      "Iteration 6370, loss = 1.54698770\n",
      "Iteration 6371, loss = 1.54696665\n",
      "Iteration 6372, loss = 1.54694561\n",
      "Iteration 6373, loss = 1.54692457\n",
      "Iteration 6374, loss = 1.54690353\n",
      "Iteration 6375, loss = 1.54688251\n",
      "Iteration 6376, loss = 1.54686148\n",
      "Iteration 6377, loss = 1.54684047\n",
      "Iteration 6378, loss = 1.54681946\n",
      "Iteration 6379, loss = 1.54679845\n",
      "Iteration 6380, loss = 1.54677745\n",
      "Iteration 6381, loss = 1.54675646\n",
      "Iteration 6382, loss = 1.54673547\n",
      "Iteration 6383, loss = 1.54671449\n",
      "Iteration 6384, loss = 1.54669352\n",
      "Iteration 6385, loss = 1.54667255\n",
      "Iteration 6386, loss = 1.54665158\n",
      "Iteration 6387, loss = 1.54663062\n",
      "Iteration 6388, loss = 1.54660967\n",
      "Iteration 6389, loss = 1.54658872\n",
      "Iteration 6390, loss = 1.54656778\n",
      "Iteration 6391, loss = 1.54654684\n",
      "Iteration 6392, loss = 1.54652591\n",
      "Iteration 6393, loss = 1.54650499\n",
      "Iteration 6394, loss = 1.54648407\n",
      "Iteration 6395, loss = 1.54646315\n",
      "Iteration 6396, loss = 1.54644225\n",
      "Iteration 6397, loss = 1.54642134\n",
      "Iteration 6398, loss = 1.54640045\n",
      "Iteration 6399, loss = 1.54637956\n",
      "Iteration 6400, loss = 1.54635867\n",
      "Iteration 6401, loss = 1.54633779\n",
      "Iteration 6402, loss = 1.54631692\n",
      "Iteration 6403, loss = 1.54629605\n",
      "Iteration 6404, loss = 1.54627518\n",
      "Iteration 6405, loss = 1.54625433\n",
      "Iteration 6406, loss = 1.54623348\n",
      "Iteration 6407, loss = 1.54621263\n",
      "Iteration 6408, loss = 1.54619179\n",
      "Iteration 6409, loss = 1.54617096\n",
      "Iteration 6410, loss = 1.54615013\n",
      "Iteration 6411, loss = 1.54612930\n",
      "Iteration 6412, loss = 1.54610849\n",
      "Iteration 6413, loss = 1.54608767\n",
      "Iteration 6414, loss = 1.54606687\n",
      "Iteration 6415, loss = 1.54604607\n",
      "Iteration 6416, loss = 1.54602527\n",
      "Iteration 6417, loss = 1.54600448\n",
      "Iteration 6418, loss = 1.54598370\n",
      "Iteration 6419, loss = 1.54596292\n",
      "Iteration 6420, loss = 1.54594215\n",
      "Iteration 6421, loss = 1.54592138\n",
      "Iteration 6422, loss = 1.54590062\n",
      "Iteration 6423, loss = 1.54587986\n",
      "Iteration 6424, loss = 1.54585911\n",
      "Iteration 6425, loss = 1.54583837\n",
      "Iteration 6426, loss = 1.54581763\n",
      "Iteration 6427, loss = 1.54579689\n",
      "Iteration 6428, loss = 1.54577616\n",
      "Iteration 6429, loss = 1.54575544\n",
      "Iteration 6430, loss = 1.54573473\n",
      "Iteration 6431, loss = 1.54571401\n",
      "Iteration 6432, loss = 1.54569331\n",
      "Iteration 6433, loss = 1.54567261\n",
      "Iteration 6434, loss = 1.54565191\n",
      "Iteration 6435, loss = 1.54563123\n",
      "Iteration 6436, loss = 1.54561054\n",
      "Iteration 6437, loss = 1.54558986\n",
      "Iteration 6438, loss = 1.54556919\n",
      "Iteration 6439, loss = 1.54554853\n",
      "Iteration 6440, loss = 1.54552787\n",
      "Iteration 6441, loss = 1.54550721\n",
      "Iteration 6442, loss = 1.54548656\n",
      "Iteration 6443, loss = 1.54546592\n",
      "Iteration 6444, loss = 1.54544528\n",
      "Iteration 6445, loss = 1.54542464\n",
      "Iteration 6446, loss = 1.54540402\n",
      "Iteration 6447, loss = 1.54538339\n",
      "Iteration 6448, loss = 1.54536278\n",
      "Iteration 6449, loss = 1.54534217\n",
      "Iteration 6450, loss = 1.54532156\n",
      "Iteration 6451, loss = 1.54530096\n",
      "Iteration 6452, loss = 1.54528037\n",
      "Iteration 6453, loss = 1.54525978\n",
      "Iteration 6454, loss = 1.54523920\n",
      "Iteration 6455, loss = 1.54521862\n",
      "Iteration 6456, loss = 1.54519805\n",
      "Iteration 6457, loss = 1.54517748\n",
      "Iteration 6458, loss = 1.54515692\n",
      "Iteration 6459, loss = 1.54513636\n",
      "Iteration 6460, loss = 1.54511581\n",
      "Iteration 6461, loss = 1.54509527\n",
      "Iteration 6462, loss = 1.54507473\n",
      "Iteration 6463, loss = 1.54505420\n",
      "Iteration 6464, loss = 1.54503367\n",
      "Iteration 6465, loss = 1.54501315\n",
      "Iteration 6466, loss = 1.54499263\n",
      "Iteration 6467, loss = 1.54497212\n",
      "Iteration 6468, loss = 1.54495162\n",
      "Iteration 6469, loss = 1.54493112\n",
      "Iteration 6470, loss = 1.54491062\n",
      "Iteration 6471, loss = 1.54489013\n",
      "Iteration 6472, loss = 1.54486965\n",
      "Iteration 6473, loss = 1.54484917\n",
      "Iteration 6474, loss = 1.54482870\n",
      "Iteration 6475, loss = 1.54480823\n",
      "Iteration 6476, loss = 1.54478777\n",
      "Iteration 6477, loss = 1.54476731\n",
      "Iteration 6478, loss = 1.54474686\n",
      "Iteration 6479, loss = 1.54472642\n",
      "Iteration 6480, loss = 1.54470598\n",
      "Iteration 6481, loss = 1.54468554\n",
      "Iteration 6482, loss = 1.54466512\n",
      "Iteration 6483, loss = 1.54464469\n",
      "Iteration 6484, loss = 1.54462428\n",
      "Iteration 6485, loss = 1.54460386\n",
      "Iteration 6486, loss = 1.54458346\n",
      "Iteration 6487, loss = 1.54456306\n",
      "Iteration 6488, loss = 1.54454266\n",
      "Iteration 6489, loss = 1.54452227\n",
      "Iteration 6490, loss = 1.54450189\n",
      "Iteration 6491, loss = 1.54448151\n",
      "Iteration 6492, loss = 1.54446113\n",
      "Iteration 6493, loss = 1.54444076\n",
      "Iteration 6494, loss = 1.54442040\n",
      "Iteration 6495, loss = 1.54440004\n",
      "Iteration 6496, loss = 1.54437969\n",
      "Iteration 6497, loss = 1.54435935\n",
      "Iteration 6498, loss = 1.54433901\n",
      "Iteration 6499, loss = 1.54431867\n",
      "Iteration 6500, loss = 1.54429834\n",
      "Iteration 6501, loss = 1.54427802\n",
      "Iteration 6502, loss = 1.54425770\n",
      "Iteration 6503, loss = 1.54423738\n",
      "Iteration 6504, loss = 1.54421708\n",
      "Iteration 6505, loss = 1.54419677\n",
      "Iteration 6506, loss = 1.54417648\n",
      "Iteration 6507, loss = 1.54415618\n",
      "Iteration 6508, loss = 1.54413590\n",
      "Iteration 6509, loss = 1.54411562\n",
      "Iteration 6510, loss = 1.54409534\n",
      "Iteration 6511, loss = 1.54407507\n",
      "Iteration 6512, loss = 1.54405481\n",
      "Iteration 6513, loss = 1.54403455\n",
      "Iteration 6514, loss = 1.54401430\n",
      "Iteration 6515, loss = 1.54399405\n",
      "Iteration 6516, loss = 1.54397380\n",
      "Iteration 6517, loss = 1.54395357\n",
      "Iteration 6518, loss = 1.54393334\n",
      "Iteration 6519, loss = 1.54391311\n",
      "Iteration 6520, loss = 1.54389289\n",
      "Iteration 6521, loss = 1.54387267\n",
      "Iteration 6522, loss = 1.54385246\n",
      "Iteration 6523, loss = 1.54383226\n",
      "Iteration 6524, loss = 1.54381206\n",
      "Iteration 6525, loss = 1.54379186\n",
      "Iteration 6526, loss = 1.54377168\n",
      "Iteration 6527, loss = 1.54375149\n",
      "Iteration 6528, loss = 1.54373131\n",
      "Iteration 6529, loss = 1.54371114\n",
      "Iteration 6530, loss = 1.54369098\n",
      "Iteration 6531, loss = 1.54367081\n",
      "Iteration 6532, loss = 1.54365066\n",
      "Iteration 6533, loss = 1.54363051\n",
      "Iteration 6534, loss = 1.54361036\n",
      "Iteration 6535, loss = 1.54359022\n",
      "Iteration 6536, loss = 1.54357009\n",
      "Iteration 6537, loss = 1.54354996\n",
      "Iteration 6538, loss = 1.54352984\n",
      "Iteration 6539, loss = 1.54350972\n",
      "Iteration 6540, loss = 1.54348960\n",
      "Iteration 6541, loss = 1.54346950\n",
      "Iteration 6542, loss = 1.54344939\n",
      "Iteration 6543, loss = 1.54342930\n",
      "Iteration 6544, loss = 1.54340921\n",
      "Iteration 6545, loss = 1.54338912\n",
      "Iteration 6546, loss = 1.54336904\n",
      "Iteration 6547, loss = 1.54334896\n",
      "Iteration 6548, loss = 1.54332889\n",
      "Iteration 6549, loss = 1.54330883\n",
      "Iteration 6550, loss = 1.54328877\n",
      "Iteration 6551, loss = 1.54326872\n",
      "Iteration 6552, loss = 1.54324867\n",
      "Iteration 6553, loss = 1.54322863\n",
      "Iteration 6554, loss = 1.54320859\n",
      "Iteration 6555, loss = 1.54318856\n",
      "Iteration 6556, loss = 1.54316853\n",
      "Iteration 6557, loss = 1.54314851\n",
      "Iteration 6558, loss = 1.54312849\n",
      "Iteration 6559, loss = 1.54310848\n",
      "Iteration 6560, loss = 1.54308847\n",
      "Iteration 6561, loss = 1.54306847\n",
      "Iteration 6562, loss = 1.54304848\n",
      "Iteration 6563, loss = 1.54302849\n",
      "Iteration 6564, loss = 1.54300851\n",
      "Iteration 6565, loss = 1.54298853\n",
      "Iteration 6566, loss = 1.54296855\n",
      "Iteration 6567, loss = 1.54294859\n",
      "Iteration 6568, loss = 1.54292862\n",
      "Iteration 6569, loss = 1.54290867\n",
      "Iteration 6570, loss = 1.54288871\n",
      "Iteration 6571, loss = 1.54286877\n",
      "Iteration 6572, loss = 1.54284882\n",
      "Iteration 6573, loss = 1.54282889\n",
      "Iteration 6574, loss = 1.54280896\n",
      "Iteration 6575, loss = 1.54278903\n",
      "Iteration 6576, loss = 1.54276911\n",
      "Iteration 6577, loss = 1.54274920\n",
      "Iteration 6578, loss = 1.54272929\n",
      "Iteration 6579, loss = 1.54270938\n",
      "Iteration 6580, loss = 1.54268948\n",
      "Iteration 6581, loss = 1.54266959\n",
      "Iteration 6582, loss = 1.54264970\n",
      "Iteration 6583, loss = 1.54262982\n",
      "Iteration 6584, loss = 1.54260994\n",
      "Iteration 6585, loss = 1.54259007\n",
      "Iteration 6586, loss = 1.54257020\n",
      "Iteration 6587, loss = 1.54255034\n",
      "Iteration 6588, loss = 1.54253048\n",
      "Iteration 6589, loss = 1.54251063\n",
      "Iteration 6590, loss = 1.54249079\n",
      "Iteration 6591, loss = 1.54247095\n",
      "Iteration 6592, loss = 1.54245111\n",
      "Iteration 6593, loss = 1.54243128\n",
      "Iteration 6594, loss = 1.54241146\n",
      "Iteration 6595, loss = 1.54239164\n",
      "Iteration 6596, loss = 1.54237182\n",
      "Iteration 6597, loss = 1.54235202\n",
      "Iteration 6598, loss = 1.54233221\n",
      "Iteration 6599, loss = 1.54231241\n",
      "Iteration 6600, loss = 1.54229262\n",
      "Iteration 6601, loss = 1.54227283\n",
      "Iteration 6602, loss = 1.54225305\n",
      "Iteration 6603, loss = 1.54223327\n",
      "Iteration 6604, loss = 1.54221350\n",
      "Iteration 6605, loss = 1.54219374\n",
      "Iteration 6606, loss = 1.54217397\n",
      "Iteration 6607, loss = 1.54215422\n",
      "Iteration 6608, loss = 1.54213447\n",
      "Iteration 6609, loss = 1.54211472\n",
      "Iteration 6610, loss = 1.54209498\n",
      "Iteration 6611, loss = 1.54207525\n",
      "Iteration 6612, loss = 1.54205552\n",
      "Iteration 6613, loss = 1.54203579\n",
      "Iteration 6614, loss = 1.54201607\n",
      "Iteration 6615, loss = 1.54199636\n",
      "Iteration 6616, loss = 1.54197665\n",
      "Iteration 6617, loss = 1.54195695\n",
      "Iteration 6618, loss = 1.54193725\n",
      "Iteration 6619, loss = 1.54191756\n",
      "Iteration 6620, loss = 1.54189787\n",
      "Iteration 6621, loss = 1.54187819\n",
      "Iteration 6622, loss = 1.54185851\n",
      "Iteration 6623, loss = 1.54183884\n",
      "Iteration 6624, loss = 1.54181917\n",
      "Iteration 6625, loss = 1.54179951\n",
      "Iteration 6626, loss = 1.54177985\n",
      "Iteration 6627, loss = 1.54176020\n",
      "Iteration 6628, loss = 1.54174055\n",
      "Iteration 6629, loss = 1.54172091\n",
      "Iteration 6630, loss = 1.54170128\n",
      "Iteration 6631, loss = 1.54168165\n",
      "Iteration 6632, loss = 1.54166202\n",
      "Iteration 6633, loss = 1.54164240\n",
      "Iteration 6634, loss = 1.54162279\n",
      "Iteration 6635, loss = 1.54160318\n",
      "Iteration 6636, loss = 1.54158358\n",
      "Iteration 6637, loss = 1.54156398\n",
      "Iteration 6638, loss = 1.54154438\n",
      "Iteration 6639, loss = 1.54152479\n",
      "Iteration 6640, loss = 1.54150521\n",
      "Iteration 6641, loss = 1.54148563\n",
      "Iteration 6642, loss = 1.54146606\n",
      "Iteration 6643, loss = 1.54144649\n",
      "Iteration 6644, loss = 1.54142693\n",
      "Iteration 6645, loss = 1.54140737\n",
      "Iteration 6646, loss = 1.54138782\n",
      "Iteration 6647, loss = 1.54136827\n",
      "Iteration 6648, loss = 1.54134873\n",
      "Iteration 6649, loss = 1.54132920\n",
      "Iteration 6650, loss = 1.54130966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6651, loss = 1.54129014\n",
      "Iteration 6652, loss = 1.54127062\n",
      "Iteration 6653, loss = 1.54125110\n",
      "Iteration 6654, loss = 1.54123159\n",
      "Iteration 6655, loss = 1.54121209\n",
      "Iteration 6656, loss = 1.54119259\n",
      "Iteration 6657, loss = 1.54117309\n",
      "Iteration 6658, loss = 1.54115360\n",
      "Iteration 6659, loss = 1.54113412\n",
      "Iteration 6660, loss = 1.54111464\n",
      "Iteration 6661, loss = 1.54109516\n",
      "Iteration 6662, loss = 1.54107569\n",
      "Iteration 6663, loss = 1.54105623\n",
      "Iteration 6664, loss = 1.54103677\n",
      "Iteration 6665, loss = 1.54101732\n",
      "Iteration 6666, loss = 1.54099787\n",
      "Iteration 6667, loss = 1.54097843\n",
      "Iteration 6668, loss = 1.54095899\n",
      "Iteration 6669, loss = 1.54093955\n",
      "Iteration 6670, loss = 1.54092013\n",
      "Iteration 6671, loss = 1.54090070\n",
      "Iteration 6672, loss = 1.54088129\n",
      "Iteration 6673, loss = 1.54086187\n",
      "Iteration 6674, loss = 1.54084247\n",
      "Iteration 6675, loss = 1.54082306\n",
      "Iteration 6676, loss = 1.54080367\n",
      "Iteration 6677, loss = 1.54078428\n",
      "Iteration 6678, loss = 1.54076489\n",
      "Iteration 6679, loss = 1.54074551\n",
      "Iteration 6680, loss = 1.54072613\n",
      "Iteration 6681, loss = 1.54070676\n",
      "Iteration 6682, loss = 1.54068739\n",
      "Iteration 6683, loss = 1.54066803\n",
      "Iteration 6684, loss = 1.54064868\n",
      "Iteration 6685, loss = 1.54062933\n",
      "Iteration 6686, loss = 1.54060998\n",
      "Iteration 6687, loss = 1.54059064\n",
      "Iteration 6688, loss = 1.54057131\n",
      "Iteration 6689, loss = 1.54055198\n",
      "Iteration 6690, loss = 1.54053265\n",
      "Iteration 6691, loss = 1.54051333\n",
      "Iteration 6692, loss = 1.54049402\n",
      "Iteration 6693, loss = 1.54047471\n",
      "Iteration 6694, loss = 1.54045540\n",
      "Iteration 6695, loss = 1.54043610\n",
      "Iteration 6696, loss = 1.54041681\n",
      "Iteration 6697, loss = 1.54039752\n",
      "Iteration 6698, loss = 1.54037823\n",
      "Iteration 6699, loss = 1.54035895\n",
      "Iteration 6700, loss = 1.54033968\n",
      "Iteration 6701, loss = 1.54032041\n",
      "Iteration 6702, loss = 1.54030115\n",
      "Iteration 6703, loss = 1.54028189\n",
      "Iteration 6704, loss = 1.54026264\n",
      "Iteration 6705, loss = 1.54024339\n",
      "Iteration 6706, loss = 1.54022414\n",
      "Iteration 6707, loss = 1.54020491\n",
      "Iteration 6708, loss = 1.54018567\n",
      "Iteration 6709, loss = 1.54016644\n",
      "Iteration 6710, loss = 1.54014722\n",
      "Iteration 6711, loss = 1.54012800\n",
      "Iteration 6712, loss = 1.54010879\n",
      "Iteration 6713, loss = 1.54008958\n",
      "Iteration 6714, loss = 1.54007038\n",
      "Iteration 6715, loss = 1.54005118\n",
      "Iteration 6716, loss = 1.54003199\n",
      "Iteration 6717, loss = 1.54001280\n",
      "Iteration 6718, loss = 1.53999362\n",
      "Iteration 6719, loss = 1.53997444\n",
      "Iteration 6720, loss = 1.53995527\n",
      "Iteration 6721, loss = 1.53993610\n",
      "Iteration 6722, loss = 1.53991694\n",
      "Iteration 6723, loss = 1.53989778\n",
      "Iteration 6724, loss = 1.53987863\n",
      "Iteration 6725, loss = 1.53985948\n",
      "Iteration 6726, loss = 1.53984034\n",
      "Iteration 6727, loss = 1.53982121\n",
      "Iteration 6728, loss = 1.53980207\n",
      "Iteration 6729, loss = 1.53978295\n",
      "Iteration 6730, loss = 1.53976383\n",
      "Iteration 6731, loss = 1.53974471\n",
      "Iteration 6732, loss = 1.53972560\n",
      "Iteration 6733, loss = 1.53970649\n",
      "Iteration 6734, loss = 1.53968739\n",
      "Iteration 6735, loss = 1.53966829\n",
      "Iteration 6736, loss = 1.53964920\n",
      "Iteration 6737, loss = 1.53963011\n",
      "Iteration 6738, loss = 1.53961103\n",
      "Iteration 6739, loss = 1.53959196\n",
      "Iteration 6740, loss = 1.53957289\n",
      "Iteration 6741, loss = 1.53955382\n",
      "Iteration 6742, loss = 1.53953476\n",
      "Iteration 6743, loss = 1.53951570\n",
      "Iteration 6744, loss = 1.53949665\n",
      "Iteration 6745, loss = 1.53947761\n",
      "Iteration 6746, loss = 1.53945856\n",
      "Iteration 6747, loss = 1.53943953\n",
      "Iteration 6748, loss = 1.53942050\n",
      "Iteration 6749, loss = 1.53940147\n",
      "Iteration 6750, loss = 1.53938245\n",
      "Iteration 6751, loss = 1.53936343\n",
      "Iteration 6752, loss = 1.53934442\n",
      "Iteration 6753, loss = 1.53932542\n",
      "Iteration 6754, loss = 1.53930642\n",
      "Iteration 6755, loss = 1.53928742\n",
      "Iteration 6756, loss = 1.53926843\n",
      "Iteration 6757, loss = 1.53924944\n",
      "Iteration 6758, loss = 1.53923046\n",
      "Iteration 6759, loss = 1.53921149\n",
      "Iteration 6760, loss = 1.53919252\n",
      "Iteration 6761, loss = 1.53917355\n",
      "Iteration 6762, loss = 1.53915459\n",
      "Iteration 6763, loss = 1.53913563\n",
      "Iteration 6764, loss = 1.53911668\n",
      "Iteration 6765, loss = 1.53909774\n",
      "Iteration 6766, loss = 1.53907880\n",
      "Iteration 6767, loss = 1.53905986\n",
      "Iteration 6768, loss = 1.53904093\n",
      "Iteration 6769, loss = 1.53902200\n",
      "Iteration 6770, loss = 1.53900308\n",
      "Iteration 6771, loss = 1.53898417\n",
      "Iteration 6772, loss = 1.53896525\n",
      "Iteration 6773, loss = 1.53894635\n",
      "Iteration 6774, loss = 1.53892745\n",
      "Iteration 6775, loss = 1.53890855\n",
      "Iteration 6776, loss = 1.53888966\n",
      "Iteration 6777, loss = 1.53887077\n",
      "Iteration 6778, loss = 1.53885189\n",
      "Iteration 6779, loss = 1.53883302\n",
      "Iteration 6780, loss = 1.53881414\n",
      "Iteration 6781, loss = 1.53879528\n",
      "Iteration 6782, loss = 1.53877642\n",
      "Iteration 6783, loss = 1.53875756\n",
      "Iteration 6784, loss = 1.53873871\n",
      "Iteration 6785, loss = 1.53871986\n",
      "Iteration 6786, loss = 1.53870102\n",
      "Iteration 6787, loss = 1.53868218\n",
      "Iteration 6788, loss = 1.53866335\n",
      "Iteration 6789, loss = 1.53864453\n",
      "Iteration 6790, loss = 1.53862571\n",
      "Iteration 6791, loss = 1.53860689\n",
      "Iteration 6792, loss = 1.53858808\n",
      "Iteration 6793, loss = 1.53856927\n",
      "Iteration 6794, loss = 1.53855047\n",
      "Iteration 6795, loss = 1.53853167\n",
      "Iteration 6796, loss = 1.53851288\n",
      "Iteration 6797, loss = 1.53849409\n",
      "Iteration 6798, loss = 1.53847531\n",
      "Iteration 6799, loss = 1.53845653\n",
      "Iteration 6800, loss = 1.53843776\n",
      "Iteration 6801, loss = 1.53841899\n",
      "Iteration 6802, loss = 1.53840023\n",
      "Iteration 6803, loss = 1.53838147\n",
      "Iteration 6804, loss = 1.53836272\n",
      "Iteration 6805, loss = 1.53834397\n",
      "Iteration 6806, loss = 1.53832523\n",
      "Iteration 6807, loss = 1.53830649\n",
      "Iteration 6808, loss = 1.53828776\n",
      "Iteration 6809, loss = 1.53826903\n",
      "Iteration 6810, loss = 1.53825031\n",
      "Iteration 6811, loss = 1.53823159\n",
      "Iteration 6812, loss = 1.53821288\n",
      "Iteration 6813, loss = 1.53819417\n",
      "Iteration 6814, loss = 1.53817547\n",
      "Iteration 6815, loss = 1.53815677\n",
      "Iteration 6816, loss = 1.53813807\n",
      "Iteration 6817, loss = 1.53811938\n",
      "Iteration 6818, loss = 1.53810070\n",
      "Iteration 6819, loss = 1.53808202\n",
      "Iteration 6820, loss = 1.53806335\n",
      "Iteration 6821, loss = 1.53804468\n",
      "Iteration 6822, loss = 1.53802602\n",
      "Iteration 6823, loss = 1.53800736\n",
      "Iteration 6824, loss = 1.53798870\n",
      "Iteration 6825, loss = 1.53797005\n",
      "Iteration 6826, loss = 1.53795141\n",
      "Iteration 6827, loss = 1.53793277\n",
      "Iteration 6828, loss = 1.53791413\n",
      "Iteration 6829, loss = 1.53789550\n",
      "Iteration 6830, loss = 1.53787688\n",
      "Iteration 6831, loss = 1.53785826\n",
      "Iteration 6832, loss = 1.53783964\n",
      "Iteration 6833, loss = 1.53782103\n",
      "Iteration 6834, loss = 1.53780243\n",
      "Iteration 6835, loss = 1.53778383\n",
      "Iteration 6836, loss = 1.53776523\n",
      "Iteration 6837, loss = 1.53774664\n",
      "Iteration 6838, loss = 1.53772805\n",
      "Iteration 6839, loss = 1.53770947\n",
      "Iteration 6840, loss = 1.53769090\n",
      "Iteration 6841, loss = 1.53767233\n",
      "Iteration 6842, loss = 1.53765376\n",
      "Iteration 6843, loss = 1.53763520\n",
      "Iteration 6844, loss = 1.53761664\n",
      "Iteration 6845, loss = 1.53759809\n",
      "Iteration 6846, loss = 1.53757954\n",
      "Iteration 6847, loss = 1.53756100\n",
      "Iteration 6848, loss = 1.53754246\n",
      "Iteration 6849, loss = 1.53752393\n",
      "Iteration 6850, loss = 1.53750540\n",
      "Iteration 6851, loss = 1.53748688\n",
      "Iteration 6852, loss = 1.53746836\n",
      "Iteration 6853, loss = 1.53744985\n",
      "Iteration 6854, loss = 1.53743134\n",
      "Iteration 6855, loss = 1.53741284\n",
      "Iteration 6856, loss = 1.53739434\n",
      "Iteration 6857, loss = 1.53737584\n",
      "Iteration 6858, loss = 1.53735735\n",
      "Iteration 6859, loss = 1.53733887\n",
      "Iteration 6860, loss = 1.53732039\n",
      "Iteration 6861, loss = 1.53730192\n",
      "Iteration 6862, loss = 1.53728345\n",
      "Iteration 6863, loss = 1.53726498\n",
      "Iteration 6864, loss = 1.53724652\n",
      "Iteration 6865, loss = 1.53722807\n",
      "Iteration 6866, loss = 1.53720962\n",
      "Iteration 6867, loss = 1.53719117\n",
      "Iteration 6868, loss = 1.53717273\n",
      "Iteration 6869, loss = 1.53715429\n",
      "Iteration 6870, loss = 1.53713586\n",
      "Iteration 6871, loss = 1.53711744\n",
      "Iteration 6872, loss = 1.53709902\n",
      "Iteration 6873, loss = 1.53708060\n",
      "Iteration 6874, loss = 1.53706219\n",
      "Iteration 6875, loss = 1.53704378\n",
      "Iteration 6876, loss = 1.53702538\n",
      "Iteration 6877, loss = 1.53700698\n",
      "Iteration 6878, loss = 1.53698859\n",
      "Iteration 6879, loss = 1.53697020\n",
      "Iteration 6880, loss = 1.53695182\n",
      "Iteration 6881, loss = 1.53693344\n",
      "Iteration 6882, loss = 1.53691507\n",
      "Iteration 6883, loss = 1.53689670\n",
      "Iteration 6884, loss = 1.53687834\n",
      "Iteration 6885, loss = 1.53685998\n",
      "Iteration 6886, loss = 1.53684162\n",
      "Iteration 6887, loss = 1.53682327\n",
      "Iteration 6888, loss = 1.53680493\n",
      "Iteration 6889, loss = 1.53678659\n",
      "Iteration 6890, loss = 1.53676825\n",
      "Iteration 6891, loss = 1.53674992\n",
      "Iteration 6892, loss = 1.53673160\n",
      "Iteration 6893, loss = 1.53671328\n",
      "Iteration 6894, loss = 1.53669496\n",
      "Iteration 6895, loss = 1.53667665\n",
      "Iteration 6896, loss = 1.53665834\n",
      "Iteration 6897, loss = 1.53664004\n",
      "Iteration 6898, loss = 1.53662175\n",
      "Iteration 6899, loss = 1.53660345\n",
      "Iteration 6900, loss = 1.53658517\n",
      "Iteration 6901, loss = 1.53656688\n",
      "Iteration 6902, loss = 1.53654861\n",
      "Iteration 6903, loss = 1.53653033\n",
      "Iteration 6904, loss = 1.53651207\n",
      "Iteration 6905, loss = 1.53649380\n",
      "Iteration 6906, loss = 1.53647554\n",
      "Iteration 6907, loss = 1.53645729\n",
      "Iteration 6908, loss = 1.53643904\n",
      "Iteration 6909, loss = 1.53642080\n",
      "Iteration 6910, loss = 1.53640256\n",
      "Iteration 6911, loss = 1.53638432\n",
      "Iteration 6912, loss = 1.53636609\n",
      "Iteration 6913, loss = 1.53634787\n",
      "Iteration 6914, loss = 1.53632965\n",
      "Iteration 6915, loss = 1.53631143\n",
      "Iteration 6916, loss = 1.53629322\n",
      "Iteration 6917, loss = 1.53627501\n",
      "Iteration 6918, loss = 1.53625681\n",
      "Iteration 6919, loss = 1.53623862\n",
      "Iteration 6920, loss = 1.53622042\n",
      "Iteration 6921, loss = 1.53620224\n",
      "Iteration 6922, loss = 1.53618405\n",
      "Iteration 6923, loss = 1.53616588\n",
      "Iteration 6924, loss = 1.53614770\n",
      "Iteration 6925, loss = 1.53612953\n",
      "Iteration 6926, loss = 1.53611137\n",
      "Iteration 6927, loss = 1.53609321\n",
      "Iteration 6928, loss = 1.53607506\n",
      "Iteration 6929, loss = 1.53605691\n",
      "Iteration 6930, loss = 1.53603876\n",
      "Iteration 6931, loss = 1.53602062\n",
      "Iteration 6932, loss = 1.53600249\n",
      "Iteration 6933, loss = 1.53598436\n",
      "Iteration 6934, loss = 1.53596623\n",
      "Iteration 6935, loss = 1.53594811\n",
      "Iteration 6936, loss = 1.53592999\n",
      "Iteration 6937, loss = 1.53591188\n",
      "Iteration 6938, loss = 1.53589377\n",
      "Iteration 6939, loss = 1.53587567\n",
      "Iteration 6940, loss = 1.53585757\n",
      "Iteration 6941, loss = 1.53583948\n",
      "Iteration 6942, loss = 1.53582139\n",
      "Iteration 6943, loss = 1.53580331\n",
      "Iteration 6944, loss = 1.53578523\n",
      "Iteration 6945, loss = 1.53576716\n",
      "Iteration 6946, loss = 1.53574909\n",
      "Iteration 6947, loss = 1.53573102\n",
      "Iteration 6948, loss = 1.53571296\n",
      "Iteration 6949, loss = 1.53569491\n",
      "Iteration 6950, loss = 1.53567686\n",
      "Iteration 6951, loss = 1.53565881\n",
      "Iteration 6952, loss = 1.53564077\n",
      "Iteration 6953, loss = 1.53562273\n",
      "Iteration 6954, loss = 1.53560470\n",
      "Iteration 6955, loss = 1.53558667\n",
      "Iteration 6956, loss = 1.53556865\n",
      "Iteration 6957, loss = 1.53555063\n",
      "Iteration 6958, loss = 1.53553262\n",
      "Iteration 6959, loss = 1.53551461\n",
      "Iteration 6960, loss = 1.53549661\n",
      "Iteration 6961, loss = 1.53547861\n",
      "Iteration 6962, loss = 1.53546061\n",
      "Iteration 6963, loss = 1.53544262\n",
      "Iteration 6964, loss = 1.53542464\n",
      "Iteration 6965, loss = 1.53540666\n",
      "Iteration 6966, loss = 1.53538868\n",
      "Iteration 6967, loss = 1.53537071\n",
      "Iteration 6968, loss = 1.53535275\n",
      "Iteration 6969, loss = 1.53533478\n",
      "Iteration 6970, loss = 1.53531683\n",
      "Iteration 6971, loss = 1.53529887\n",
      "Iteration 6972, loss = 1.53528093\n",
      "Iteration 6973, loss = 1.53526298\n",
      "Iteration 6974, loss = 1.53524505\n",
      "Iteration 6975, loss = 1.53522711\n",
      "Iteration 6976, loss = 1.53520918\n",
      "Iteration 6977, loss = 1.53519126\n",
      "Iteration 6978, loss = 1.53517334\n",
      "Iteration 6979, loss = 1.53515542\n",
      "Iteration 6980, loss = 1.53513751\n",
      "Iteration 6981, loss = 1.53511961\n",
      "Iteration 6982, loss = 1.53510170\n",
      "Iteration 6983, loss = 1.53508381\n",
      "Iteration 6984, loss = 1.53506592\n",
      "Iteration 6985, loss = 1.53504803\n",
      "Iteration 6986, loss = 1.53503015\n",
      "Iteration 6987, loss = 1.53501227\n",
      "Iteration 6988, loss = 1.53499439\n",
      "Iteration 6989, loss = 1.53497652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6990, loss = 1.53495866\n",
      "Iteration 6991, loss = 1.53494080\n",
      "Iteration 6992, loss = 1.53492295\n",
      "Iteration 6993, loss = 1.53490510\n",
      "Iteration 6994, loss = 1.53488725\n",
      "Iteration 6995, loss = 1.53486941\n",
      "Iteration 6996, loss = 1.53485157\n",
      "Iteration 6997, loss = 1.53483374\n",
      "Iteration 6998, loss = 1.53481591\n",
      "Iteration 6999, loss = 1.53479809\n",
      "Iteration 7000, loss = 1.53478027\n",
      "Iteration 7001, loss = 1.53476246\n",
      "Iteration 7002, loss = 1.53474465\n",
      "Iteration 7003, loss = 1.53472685\n",
      "Iteration 7004, loss = 1.53470905\n",
      "Iteration 7005, loss = 1.53469125\n",
      "Iteration 7006, loss = 1.53467346\n",
      "Iteration 7007, loss = 1.53465568\n",
      "Iteration 7008, loss = 1.53463789\n",
      "Iteration 7009, loss = 1.53462012\n",
      "Iteration 7010, loss = 1.53460235\n",
      "Iteration 7011, loss = 1.53458458\n",
      "Iteration 7012, loss = 1.53456682\n",
      "Iteration 7013, loss = 1.53454906\n",
      "Iteration 7014, loss = 1.53453130\n",
      "Iteration 7015, loss = 1.53451355\n",
      "Iteration 7016, loss = 1.53449581\n",
      "Iteration 7017, loss = 1.53447807\n",
      "Iteration 7018, loss = 1.53446034\n",
      "Iteration 7019, loss = 1.53444260\n",
      "Iteration 7020, loss = 1.53442488\n",
      "Iteration 7021, loss = 1.53440716\n",
      "Iteration 7022, loss = 1.53438944\n",
      "Iteration 7023, loss = 1.53437173\n",
      "Iteration 7024, loss = 1.53435402\n",
      "Iteration 7025, loss = 1.53433632\n",
      "Iteration 7026, loss = 1.53431862\n",
      "Iteration 7027, loss = 1.53430092\n",
      "Iteration 7028, loss = 1.53428323\n",
      "Iteration 7029, loss = 1.53426555\n",
      "Iteration 7030, loss = 1.53424787\n",
      "Iteration 7031, loss = 1.53423019\n",
      "Iteration 7032, loss = 1.53421252\n",
      "Iteration 7033, loss = 1.53419486\n",
      "Iteration 7034, loss = 1.53417719\n",
      "Iteration 7035, loss = 1.53415954\n",
      "Iteration 7036, loss = 1.53414188\n",
      "Iteration 7037, loss = 1.53412423\n",
      "Iteration 7038, loss = 1.53410659\n",
      "Iteration 7039, loss = 1.53408895\n",
      "Iteration 7040, loss = 1.53407132\n",
      "Iteration 7041, loss = 1.53405369\n",
      "Iteration 7042, loss = 1.53403606\n",
      "Iteration 7043, loss = 1.53401844\n",
      "Iteration 7044, loss = 1.53400082\n",
      "Iteration 7045, loss = 1.53398321\n",
      "Iteration 7046, loss = 1.53396560\n",
      "Iteration 7047, loss = 1.53394800\n",
      "Iteration 7048, loss = 1.53393040\n",
      "Iteration 7049, loss = 1.53391281\n",
      "Iteration 7050, loss = 1.53389522\n",
      "Iteration 7051, loss = 1.53387763\n",
      "Iteration 7052, loss = 1.53386005\n",
      "Iteration 7053, loss = 1.53384248\n",
      "Iteration 7054, loss = 1.53382491\n",
      "Iteration 7055, loss = 1.53380734\n",
      "Iteration 7056, loss = 1.53378978\n",
      "Iteration 7057, loss = 1.53377222\n",
      "Iteration 7058, loss = 1.53375467\n",
      "Iteration 7059, loss = 1.53373712\n",
      "Iteration 7060, loss = 1.53371958\n",
      "Iteration 7061, loss = 1.53370204\n",
      "Iteration 7062, loss = 1.53368450\n",
      "Iteration 7063, loss = 1.53366697\n",
      "Iteration 7064, loss = 1.53364945\n",
      "Iteration 7065, loss = 1.53363192\n",
      "Iteration 7066, loss = 1.53361441\n",
      "Iteration 7067, loss = 1.53359689\n",
      "Iteration 7068, loss = 1.53357939\n",
      "Iteration 7069, loss = 1.53356188\n",
      "Iteration 7070, loss = 1.53354438\n",
      "Iteration 7071, loss = 1.53352689\n",
      "Iteration 7072, loss = 1.53350940\n",
      "Iteration 7073, loss = 1.53349191\n",
      "Iteration 7074, loss = 1.53347443\n",
      "Iteration 7075, loss = 1.53345696\n",
      "Iteration 7076, loss = 1.53343949\n",
      "Iteration 7077, loss = 1.53342202\n",
      "Iteration 7078, loss = 1.53340456\n",
      "Iteration 7079, loss = 1.53338710\n",
      "Iteration 7080, loss = 1.53336964\n",
      "Iteration 7081, loss = 1.53335219\n",
      "Iteration 7082, loss = 1.53333475\n",
      "Iteration 7083, loss = 1.53331731\n",
      "Iteration 7084, loss = 1.53329987\n",
      "Iteration 7085, loss = 1.53328244\n",
      "Iteration 7086, loss = 1.53326501\n",
      "Iteration 7087, loss = 1.53324759\n",
      "Iteration 7088, loss = 1.53323017\n",
      "Iteration 7089, loss = 1.53321276\n",
      "Iteration 7090, loss = 1.53319535\n",
      "Iteration 7091, loss = 1.53317795\n",
      "Iteration 7092, loss = 1.53316055\n",
      "Iteration 7093, loss = 1.53314315\n",
      "Iteration 7094, loss = 1.53312576\n",
      "Iteration 7095, loss = 1.53310837\n",
      "Iteration 7096, loss = 1.53309099\n",
      "Iteration 7097, loss = 1.53307361\n",
      "Iteration 7098, loss = 1.53305624\n",
      "Iteration 7099, loss = 1.53303887\n",
      "Iteration 7100, loss = 1.53302151\n",
      "Iteration 7101, loss = 1.53300415\n",
      "Iteration 7102, loss = 1.53298679\n",
      "Iteration 7103, loss = 1.53296944\n",
      "Iteration 7104, loss = 1.53295210\n",
      "Iteration 7105, loss = 1.53293475\n",
      "Iteration 7106, loss = 1.53291742\n",
      "Iteration 7107, loss = 1.53290008\n",
      "Iteration 7108, loss = 1.53288275\n",
      "Iteration 7109, loss = 1.53286543\n",
      "Iteration 7110, loss = 1.53284811\n",
      "Iteration 7111, loss = 1.53283079\n",
      "Iteration 7112, loss = 1.53281348\n",
      "Iteration 7113, loss = 1.53279618\n",
      "Iteration 7114, loss = 1.53277888\n",
      "Iteration 7115, loss = 1.53276158\n",
      "Iteration 7116, loss = 1.53274428\n",
      "Iteration 7117, loss = 1.53272700\n",
      "Iteration 7118, loss = 1.53270971\n",
      "Iteration 7119, loss = 1.53269243\n",
      "Iteration 7120, loss = 1.53267516\n",
      "Iteration 7121, loss = 1.53265789\n",
      "Iteration 7122, loss = 1.53264062\n",
      "Iteration 7123, loss = 1.53262336\n",
      "Iteration 7124, loss = 1.53260610\n",
      "Iteration 7125, loss = 1.53258885\n",
      "Iteration 7126, loss = 1.53257160\n",
      "Iteration 7127, loss = 1.53255435\n",
      "Iteration 7128, loss = 1.53253711\n",
      "Iteration 7129, loss = 1.53251988\n",
      "Iteration 7130, loss = 1.53250265\n",
      "Iteration 7131, loss = 1.53248542\n",
      "Iteration 7132, loss = 1.53246820\n",
      "Iteration 7133, loss = 1.53245098\n",
      "Iteration 7134, loss = 1.53243377\n",
      "Iteration 7135, loss = 1.53241656\n",
      "Iteration 7136, loss = 1.53239935\n",
      "Iteration 7137, loss = 1.53238215\n",
      "Iteration 7138, loss = 1.53236496\n",
      "Iteration 7139, loss = 1.53234777\n",
      "Iteration 7140, loss = 1.53233058\n",
      "Iteration 7141, loss = 1.53231340\n",
      "Iteration 7142, loss = 1.53229622\n",
      "Iteration 7143, loss = 1.53227904\n",
      "Iteration 7144, loss = 1.53226187\n",
      "Iteration 7145, loss = 1.53224471\n",
      "Iteration 7146, loss = 1.53222755\n",
      "Iteration 7147, loss = 1.53221039\n",
      "Iteration 7148, loss = 1.53219324\n",
      "Iteration 7149, loss = 1.53217609\n",
      "Iteration 7150, loss = 1.53215895\n",
      "Iteration 7151, loss = 1.53214181\n",
      "Iteration 7152, loss = 1.53212468\n",
      "Iteration 7153, loss = 1.53210755\n",
      "Iteration 7154, loss = 1.53209042\n",
      "Iteration 7155, loss = 1.53207330\n",
      "Iteration 7156, loss = 1.53205618\n",
      "Iteration 7157, loss = 1.53203907\n",
      "Iteration 7158, loss = 1.53202196\n",
      "Iteration 7159, loss = 1.53200486\n",
      "Iteration 7160, loss = 1.53198776\n",
      "Iteration 7161, loss = 1.53197067\n",
      "Iteration 7162, loss = 1.53195358\n",
      "Iteration 7163, loss = 1.53193649\n",
      "Iteration 7164, loss = 1.53191941\n",
      "Iteration 7165, loss = 1.53190233\n",
      "Iteration 7166, loss = 1.53188526\n",
      "Iteration 7167, loss = 1.53186819\n",
      "Iteration 7168, loss = 1.53185113\n",
      "Iteration 7169, loss = 1.53183407\n",
      "Iteration 7170, loss = 1.53181701\n",
      "Iteration 7171, loss = 1.53179996\n",
      "Iteration 7172, loss = 1.53178291\n",
      "Iteration 7173, loss = 1.53176587\n",
      "Iteration 7174, loss = 1.53174883\n",
      "Iteration 7175, loss = 1.53173180\n",
      "Iteration 7176, loss = 1.53171477\n",
      "Iteration 7177, loss = 1.53169774\n",
      "Iteration 7178, loss = 1.53168072\n",
      "Iteration 7179, loss = 1.53166371\n",
      "Iteration 7180, loss = 1.53164669\n",
      "Iteration 7181, loss = 1.53162969\n",
      "Iteration 7182, loss = 1.53161268\n",
      "Iteration 7183, loss = 1.53159568\n",
      "Iteration 7184, loss = 1.53157869\n",
      "Iteration 7185, loss = 1.53156170\n",
      "Iteration 7186, loss = 1.53154471\n",
      "Iteration 7187, loss = 1.53152773\n",
      "Iteration 7188, loss = 1.53151075\n",
      "Iteration 7189, loss = 1.53149378\n",
      "Iteration 7190, loss = 1.53147681\n",
      "Iteration 7191, loss = 1.53145985\n",
      "Iteration 7192, loss = 1.53144289\n",
      "Iteration 7193, loss = 1.53142593\n",
      "Iteration 7194, loss = 1.53140898\n",
      "Iteration 7195, loss = 1.53139203\n",
      "Iteration 7196, loss = 1.53137509\n",
      "Iteration 7197, loss = 1.53135815\n",
      "Iteration 7198, loss = 1.53134122\n",
      "Iteration 7199, loss = 1.53132429\n",
      "Iteration 7200, loss = 1.53130736\n",
      "Iteration 7201, loss = 1.53129044\n",
      "Iteration 7202, loss = 1.53127353\n",
      "Iteration 7203, loss = 1.53125661\n",
      "Iteration 7204, loss = 1.53123971\n",
      "Iteration 7205, loss = 1.53122280\n",
      "Iteration 7206, loss = 1.53120590\n",
      "Iteration 7207, loss = 1.53118901\n",
      "Iteration 7208, loss = 1.53117212\n",
      "Iteration 7209, loss = 1.53115523\n",
      "Iteration 7210, loss = 1.53113835\n",
      "Iteration 7211, loss = 1.53112147\n",
      "Iteration 7212, loss = 1.53110460\n",
      "Iteration 7213, loss = 1.53108773\n",
      "Iteration 7214, loss = 1.53107086\n",
      "Iteration 7215, loss = 1.53105400\n",
      "Iteration 7216, loss = 1.53103714\n",
      "Iteration 7217, loss = 1.53102029\n",
      "Iteration 7218, loss = 1.53100344\n",
      "Iteration 7219, loss = 1.53098660\n",
      "Iteration 7220, loss = 1.53096976\n",
      "Iteration 7221, loss = 1.53095293\n",
      "Iteration 7222, loss = 1.53093610\n",
      "Iteration 7223, loss = 1.53091927\n",
      "Iteration 7224, loss = 1.53090245\n",
      "Iteration 7225, loss = 1.53088563\n",
      "Iteration 7226, loss = 1.53086882\n",
      "Iteration 7227, loss = 1.53085201\n",
      "Iteration 7228, loss = 1.53083520\n",
      "Iteration 7229, loss = 1.53081840\n",
      "Iteration 7230, loss = 1.53080160\n",
      "Iteration 7231, loss = 1.53078481\n",
      "Iteration 7232, loss = 1.53076802\n",
      "Iteration 7233, loss = 1.53075124\n",
      "Iteration 7234, loss = 1.53073446\n",
      "Iteration 7235, loss = 1.53071769\n",
      "Iteration 7236, loss = 1.53070091\n",
      "Iteration 7237, loss = 1.53068415\n",
      "Iteration 7238, loss = 1.53066739\n",
      "Iteration 7239, loss = 1.53065063\n",
      "Iteration 7240, loss = 1.53063387\n",
      "Iteration 7241, loss = 1.53061712\n",
      "Iteration 7242, loss = 1.53060038\n",
      "Iteration 7243, loss = 1.53058364\n",
      "Iteration 7244, loss = 1.53056690\n",
      "Iteration 7245, loss = 1.53055017\n",
      "Iteration 7246, loss = 1.53053344\n",
      "Iteration 7247, loss = 1.53051672\n",
      "Iteration 7248, loss = 1.53050000\n",
      "Iteration 7249, loss = 1.53048328\n",
      "Iteration 7250, loss = 1.53046657\n",
      "Iteration 7251, loss = 1.53044986\n",
      "Iteration 7252, loss = 1.53043316\n",
      "Iteration 7253, loss = 1.53041646\n",
      "Iteration 7254, loss = 1.53039977\n",
      "Iteration 7255, loss = 1.53038308\n",
      "Iteration 7256, loss = 1.53036639\n",
      "Iteration 7257, loss = 1.53034971\n",
      "Iteration 7258, loss = 1.53033303\n",
      "Iteration 7259, loss = 1.53031636\n",
      "Iteration 7260, loss = 1.53029969\n",
      "Iteration 7261, loss = 1.53028303\n",
      "Iteration 7262, loss = 1.53026636\n",
      "Iteration 7263, loss = 1.53024971\n",
      "Iteration 7264, loss = 1.53023306\n",
      "Iteration 7265, loss = 1.53021641\n",
      "Iteration 7266, loss = 1.53019977\n",
      "Iteration 7267, loss = 1.53018313\n",
      "Iteration 7268, loss = 1.53016649\n",
      "Iteration 7269, loss = 1.53014986\n",
      "Iteration 7270, loss = 1.53013323\n",
      "Iteration 7271, loss = 1.53011661\n",
      "Iteration 7272, loss = 1.53009999\n",
      "Iteration 7273, loss = 1.53008338\n",
      "Iteration 7274, loss = 1.53006677\n",
      "Iteration 7275, loss = 1.53005016\n",
      "Iteration 7276, loss = 1.53003356\n",
      "Iteration 7277, loss = 1.53001696\n",
      "Iteration 7278, loss = 1.53000037\n",
      "Iteration 7279, loss = 1.52998378\n",
      "Iteration 7280, loss = 1.52996720\n",
      "Iteration 7281, loss = 1.52995062\n",
      "Iteration 7282, loss = 1.52993404\n",
      "Iteration 7283, loss = 1.52991747\n",
      "Iteration 7284, loss = 1.52990090\n",
      "Iteration 7285, loss = 1.52988434\n",
      "Iteration 7286, loss = 1.52986778\n",
      "Iteration 7287, loss = 1.52985122\n",
      "Iteration 7288, loss = 1.52983467\n",
      "Iteration 7289, loss = 1.52981813\n",
      "Iteration 7290, loss = 1.52980158\n",
      "Iteration 7291, loss = 1.52978505\n",
      "Iteration 7292, loss = 1.52976851\n",
      "Iteration 7293, loss = 1.52975198\n",
      "Iteration 7294, loss = 1.52973546\n",
      "Iteration 7295, loss = 1.52971893\n",
      "Iteration 7296, loss = 1.52970242\n",
      "Iteration 7297, loss = 1.52968590\n",
      "Iteration 7298, loss = 1.52966940\n",
      "Iteration 7299, loss = 1.52965289\n",
      "Iteration 7300, loss = 1.52963639\n",
      "Iteration 7301, loss = 1.52961989\n",
      "Iteration 7302, loss = 1.52960340\n",
      "Iteration 7303, loss = 1.52958691\n",
      "Iteration 7304, loss = 1.52957043\n",
      "Iteration 7305, loss = 1.52955395\n",
      "Iteration 7306, loss = 1.52953747\n",
      "Iteration 7307, loss = 1.52952100\n",
      "Iteration 7308, loss = 1.52950453\n",
      "Iteration 7309, loss = 1.52948807\n",
      "Iteration 7310, loss = 1.52947161\n",
      "Iteration 7311, loss = 1.52945516\n",
      "Iteration 7312, loss = 1.52943871\n",
      "Iteration 7313, loss = 1.52942226\n",
      "Iteration 7314, loss = 1.52940582\n",
      "Iteration 7315, loss = 1.52938938\n",
      "Iteration 7316, loss = 1.52937295\n",
      "Iteration 7317, loss = 1.52935652\n",
      "Iteration 7318, loss = 1.52934009\n",
      "Iteration 7319, loss = 1.52932367\n",
      "Iteration 7320, loss = 1.52930725\n",
      "Iteration 7321, loss = 1.52929084\n",
      "Iteration 7322, loss = 1.52927443\n",
      "Iteration 7323, loss = 1.52925802\n",
      "Iteration 7324, loss = 1.52924162\n",
      "Iteration 7325, loss = 1.52922523\n",
      "Iteration 7326, loss = 1.52920883\n",
      "Iteration 7327, loss = 1.52919244\n",
      "Iteration 7328, loss = 1.52917606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7329, loss = 1.52915968\n",
      "Iteration 7330, loss = 1.52914330\n",
      "Iteration 7331, loss = 1.52912693\n",
      "Iteration 7332, loss = 1.52911056\n",
      "Iteration 7333, loss = 1.52909420\n",
      "Iteration 7334, loss = 1.52907784\n",
      "Iteration 7335, loss = 1.52906149\n",
      "Iteration 7336, loss = 1.52904514\n",
      "Iteration 7337, loss = 1.52902879\n",
      "Iteration 7338, loss = 1.52901245\n",
      "Iteration 7339, loss = 1.52899611\n",
      "Iteration 7340, loss = 1.52897977\n",
      "Iteration 7341, loss = 1.52896344\n",
      "Iteration 7342, loss = 1.52894711\n",
      "Iteration 7343, loss = 1.52893079\n",
      "Iteration 7344, loss = 1.52891447\n",
      "Iteration 7345, loss = 1.52889816\n",
      "Iteration 7346, loss = 1.52888185\n",
      "Iteration 7347, loss = 1.52886554\n",
      "Iteration 7348, loss = 1.52884924\n",
      "Iteration 7349, loss = 1.52883294\n",
      "Iteration 7350, loss = 1.52881665\n",
      "Iteration 7351, loss = 1.52880036\n",
      "Iteration 7352, loss = 1.52878408\n",
      "Iteration 7353, loss = 1.52876780\n",
      "Iteration 7354, loss = 1.52875152\n",
      "Iteration 7355, loss = 1.52873525\n",
      "Iteration 7356, loss = 1.52871898\n",
      "Iteration 7357, loss = 1.52870271\n",
      "Iteration 7358, loss = 1.52868645\n",
      "Iteration 7359, loss = 1.52867019\n",
      "Iteration 7360, loss = 1.52865394\n",
      "Iteration 7361, loss = 1.52863769\n",
      "Iteration 7362, loss = 1.52862145\n",
      "Iteration 7363, loss = 1.52860521\n",
      "Iteration 7364, loss = 1.52858897\n",
      "Iteration 7365, loss = 1.52857274\n",
      "Iteration 7366, loss = 1.52855651\n",
      "Iteration 7367, loss = 1.52854029\n",
      "Iteration 7368, loss = 1.52852407\n",
      "Iteration 7369, loss = 1.52850785\n",
      "Iteration 7370, loss = 1.52849164\n",
      "Iteration 7371, loss = 1.52847543\n",
      "Iteration 7372, loss = 1.52845923\n",
      "Iteration 7373, loss = 1.52844303\n",
      "Iteration 7374, loss = 1.52842684\n",
      "Iteration 7375, loss = 1.52841064\n",
      "Iteration 7376, loss = 1.52839446\n",
      "Iteration 7377, loss = 1.52837827\n",
      "Iteration 7378, loss = 1.52836210\n",
      "Iteration 7379, loss = 1.52834592\n",
      "Iteration 7380, loss = 1.52832975\n",
      "Iteration 7381, loss = 1.52831358\n",
      "Iteration 7382, loss = 1.52829742\n",
      "Iteration 7383, loss = 1.52828126\n",
      "Iteration 7384, loss = 1.52826511\n",
      "Iteration 7385, loss = 1.52824896\n",
      "Iteration 7386, loss = 1.52823281\n",
      "Iteration 7387, loss = 1.52821667\n",
      "Iteration 7388, loss = 1.52820053\n",
      "Iteration 7389, loss = 1.52818439\n",
      "Iteration 7390, loss = 1.52816826\n",
      "Iteration 7391, loss = 1.52815214\n",
      "Iteration 7392, loss = 1.52813602\n",
      "Iteration 7393, loss = 1.52811990\n",
      "Iteration 7394, loss = 1.52810378\n",
      "Iteration 7395, loss = 1.52808767\n",
      "Iteration 7396, loss = 1.52807157\n",
      "Iteration 7397, loss = 1.52805547\n",
      "Iteration 7398, loss = 1.52803937\n",
      "Iteration 7399, loss = 1.52802327\n",
      "Iteration 7400, loss = 1.52800719\n",
      "Iteration 7401, loss = 1.52799110\n",
      "Iteration 7402, loss = 1.52797502\n",
      "Iteration 7403, loss = 1.52795894\n",
      "Iteration 7404, loss = 1.52794287\n",
      "Iteration 7405, loss = 1.52792680\n",
      "Iteration 7406, loss = 1.52791073\n",
      "Iteration 7407, loss = 1.52789467\n",
      "Iteration 7408, loss = 1.52787861\n",
      "Iteration 7409, loss = 1.52786256\n",
      "Iteration 7410, loss = 1.52784651\n",
      "Iteration 7411, loss = 1.52783046\n",
      "Iteration 7412, loss = 1.52781442\n",
      "Iteration 7413, loss = 1.52779839\n",
      "Iteration 7414, loss = 1.52778235\n",
      "Iteration 7415, loss = 1.52776632\n",
      "Iteration 7416, loss = 1.52775030\n",
      "Iteration 7417, loss = 1.52773428\n",
      "Iteration 7418, loss = 1.52771826\n",
      "Iteration 7419, loss = 1.52770225\n",
      "Iteration 7420, loss = 1.52768624\n",
      "Iteration 7421, loss = 1.52767023\n",
      "Iteration 7422, loss = 1.52765423\n",
      "Iteration 7423, loss = 1.52763823\n",
      "Iteration 7424, loss = 1.52762224\n",
      "Iteration 7425, loss = 1.52760625\n",
      "Iteration 7426, loss = 1.52759027\n",
      "Iteration 7427, loss = 1.52757429\n",
      "Iteration 7428, loss = 1.52755831\n",
      "Iteration 7429, loss = 1.52754234\n",
      "Iteration 7430, loss = 1.52752637\n",
      "Iteration 7431, loss = 1.52751040\n",
      "Iteration 7432, loss = 1.52749444\n",
      "Iteration 7433, loss = 1.52747848\n",
      "Iteration 7434, loss = 1.52746253\n",
      "Iteration 7435, loss = 1.52744658\n",
      "Iteration 7436, loss = 1.52743064\n",
      "Iteration 7437, loss = 1.52741470\n",
      "Iteration 7438, loss = 1.52739876\n",
      "Iteration 7439, loss = 1.52738282\n",
      "Iteration 7440, loss = 1.52736690\n",
      "Iteration 7441, loss = 1.52735097\n",
      "Iteration 7442, loss = 1.52733505\n",
      "Iteration 7443, loss = 1.52731913\n",
      "Iteration 7444, loss = 1.52730322\n",
      "Iteration 7445, loss = 1.52728731\n",
      "Iteration 7446, loss = 1.52727141\n",
      "Iteration 7447, loss = 1.52725550\n",
      "Iteration 7448, loss = 1.52723961\n",
      "Iteration 7449, loss = 1.52722371\n",
      "Iteration 7450, loss = 1.52720782\n",
      "Iteration 7451, loss = 1.52719194\n",
      "Iteration 7452, loss = 1.52717606\n",
      "Iteration 7453, loss = 1.52716018\n",
      "Iteration 7454, loss = 1.52714431\n",
      "Iteration 7455, loss = 1.52712844\n",
      "Iteration 7456, loss = 1.52711257\n",
      "Iteration 7457, loss = 1.52709671\n",
      "Iteration 7458, loss = 1.52708085\n",
      "Iteration 7459, loss = 1.52706500\n",
      "Iteration 7460, loss = 1.52704915\n",
      "Iteration 7461, loss = 1.52703330\n",
      "Iteration 7462, loss = 1.52701746\n",
      "Iteration 7463, loss = 1.52700163\n",
      "Iteration 7464, loss = 1.52698579\n",
      "Iteration 7465, loss = 1.52696996\n",
      "Iteration 7466, loss = 1.52695414\n",
      "Iteration 7467, loss = 1.52693831\n",
      "Iteration 7468, loss = 1.52692250\n",
      "Iteration 7469, loss = 1.52690668\n",
      "Iteration 7470, loss = 1.52689087\n",
      "Iteration 7471, loss = 1.52687507\n",
      "Iteration 7472, loss = 1.52685926\n",
      "Iteration 7473, loss = 1.52684347\n",
      "Iteration 7474, loss = 1.52682767\n",
      "Iteration 7475, loss = 1.52681188\n",
      "Iteration 7476, loss = 1.52679610\n",
      "Iteration 7477, loss = 1.52678031\n",
      "Iteration 7478, loss = 1.52676454\n",
      "Iteration 7479, loss = 1.52674876\n",
      "Iteration 7480, loss = 1.52673299\n",
      "Iteration 7481, loss = 1.52671722\n",
      "Iteration 7482, loss = 1.52670146\n",
      "Iteration 7483, loss = 1.52668570\n",
      "Iteration 7484, loss = 1.52666995\n",
      "Iteration 7485, loss = 1.52665420\n",
      "Iteration 7486, loss = 1.52663845\n",
      "Iteration 7487, loss = 1.52662271\n",
      "Iteration 7488, loss = 1.52660697\n",
      "Iteration 7489, loss = 1.52659123\n",
      "Iteration 7490, loss = 1.52657550\n",
      "Iteration 7491, loss = 1.52655977\n",
      "Iteration 7492, loss = 1.52654405\n",
      "Iteration 7493, loss = 1.52652833\n",
      "Iteration 7494, loss = 1.52651262\n",
      "Iteration 7495, loss = 1.52649690\n",
      "Iteration 7496, loss = 1.52648120\n",
      "Iteration 7497, loss = 1.52646549\n",
      "Iteration 7498, loss = 1.52644979\n",
      "Iteration 7499, loss = 1.52643410\n",
      "Iteration 7500, loss = 1.52641841\n",
      "Iteration 7501, loss = 1.52640272\n",
      "Iteration 7502, loss = 1.52638703\n",
      "Iteration 7503, loss = 1.52637135\n",
      "Iteration 7504, loss = 1.52635568\n",
      "Iteration 7505, loss = 1.52634000\n",
      "Iteration 7506, loss = 1.52632434\n",
      "Iteration 7507, loss = 1.52630867\n",
      "Iteration 7508, loss = 1.52629301\n",
      "Iteration 7509, loss = 1.52627735\n",
      "Iteration 7510, loss = 1.52626170\n",
      "Iteration 7511, loss = 1.52624605\n",
      "Iteration 7512, loss = 1.52623041\n",
      "Iteration 7513, loss = 1.52621476\n",
      "Iteration 7514, loss = 1.52619913\n",
      "Iteration 7515, loss = 1.52618349\n",
      "Iteration 7516, loss = 1.52616786\n",
      "Iteration 7517, loss = 1.52615224\n",
      "Iteration 7518, loss = 1.52613662\n",
      "Iteration 7519, loss = 1.52612100\n",
      "Iteration 7520, loss = 1.52610539\n",
      "Iteration 7521, loss = 1.52608978\n",
      "Iteration 7522, loss = 1.52607417\n",
      "Iteration 7523, loss = 1.52605857\n",
      "Iteration 7524, loss = 1.52604297\n",
      "Iteration 7525, loss = 1.52602737\n",
      "Iteration 7526, loss = 1.52601178\n",
      "Iteration 7527, loss = 1.52599620\n",
      "Iteration 7528, loss = 1.52598061\n",
      "Iteration 7529, loss = 1.52596503\n",
      "Iteration 7530, loss = 1.52594946\n",
      "Iteration 7531, loss = 1.52593389\n",
      "Iteration 7532, loss = 1.52591832\n",
      "Iteration 7533, loss = 1.52590276\n",
      "Iteration 7534, loss = 1.52588720\n",
      "Iteration 7535, loss = 1.52587164\n",
      "Iteration 7536, loss = 1.52585609\n",
      "Iteration 7537, loss = 1.52584054\n",
      "Iteration 7538, loss = 1.52582500\n",
      "Iteration 7539, loss = 1.52580946\n",
      "Iteration 7540, loss = 1.52579392\n",
      "Iteration 7541, loss = 1.52577839\n",
      "Iteration 7542, loss = 1.52576286\n",
      "Iteration 7543, loss = 1.52574733\n",
      "Iteration 7544, loss = 1.52573181\n",
      "Iteration 7545, loss = 1.52571630\n",
      "Iteration 7546, loss = 1.52570078\n",
      "Iteration 7547, loss = 1.52568527\n",
      "Iteration 7548, loss = 1.52566977\n",
      "Iteration 7549, loss = 1.52565427\n",
      "Iteration 7550, loss = 1.52563877\n",
      "Iteration 7551, loss = 1.52562327\n",
      "Iteration 7552, loss = 1.52560778\n",
      "Iteration 7553, loss = 1.52559230\n",
      "Iteration 7554, loss = 1.52557681\n",
      "Iteration 7555, loss = 1.52556134\n",
      "Iteration 7556, loss = 1.52554586\n",
      "Iteration 7557, loss = 1.52553039\n",
      "Iteration 7558, loss = 1.52551492\n",
      "Iteration 7559, loss = 1.52549946\n",
      "Iteration 7560, loss = 1.52548400\n",
      "Iteration 7561, loss = 1.52546854\n",
      "Iteration 7562, loss = 1.52545309\n",
      "Iteration 7563, loss = 1.52543764\n",
      "Iteration 7564, loss = 1.52542220\n",
      "Iteration 7565, loss = 1.52540676\n",
      "Iteration 7566, loss = 1.52539132\n",
      "Iteration 7567, loss = 1.52537589\n",
      "Iteration 7568, loss = 1.52536046\n",
      "Iteration 7569, loss = 1.52534504\n",
      "Iteration 7570, loss = 1.52532962\n",
      "Iteration 7571, loss = 1.52531420\n",
      "Iteration 7572, loss = 1.52529878\n",
      "Iteration 7573, loss = 1.52528338\n",
      "Iteration 7574, loss = 1.52526797\n",
      "Iteration 7575, loss = 1.52525257\n",
      "Iteration 7576, loss = 1.52523717\n",
      "Iteration 7577, loss = 1.52522177\n",
      "Iteration 7578, loss = 1.52520638\n",
      "Iteration 7579, loss = 1.52519100\n",
      "Iteration 7580, loss = 1.52517561\n",
      "Iteration 7581, loss = 1.52516023\n",
      "Iteration 7582, loss = 1.52514486\n",
      "Iteration 7583, loss = 1.52512949\n",
      "Iteration 7584, loss = 1.52511412\n",
      "Iteration 7585, loss = 1.52509876\n",
      "Iteration 7586, loss = 1.52508340\n",
      "Iteration 7587, loss = 1.52506804\n",
      "Iteration 7588, loss = 1.52505269\n",
      "Iteration 7589, loss = 1.52503734\n",
      "Iteration 7590, loss = 1.52502199\n",
      "Iteration 7591, loss = 1.52500665\n",
      "Iteration 7592, loss = 1.52499132\n",
      "Iteration 7593, loss = 1.52497598\n",
      "Iteration 7594, loss = 1.52496065\n",
      "Iteration 7595, loss = 1.52494533\n",
      "Iteration 7596, loss = 1.52493000\n",
      "Iteration 7597, loss = 1.52491469\n",
      "Iteration 7598, loss = 1.52489937\n",
      "Iteration 7599, loss = 1.52488406\n",
      "Iteration 7600, loss = 1.52486875\n",
      "Iteration 7601, loss = 1.52485345\n",
      "Iteration 7602, loss = 1.52483815\n",
      "Iteration 7603, loss = 1.52482286\n",
      "Iteration 7604, loss = 1.52480756\n",
      "Iteration 7605, loss = 1.52479228\n",
      "Iteration 7606, loss = 1.52477699\n",
      "Iteration 7607, loss = 1.52476171\n",
      "Iteration 7608, loss = 1.52474643\n",
      "Iteration 7609, loss = 1.52473116\n",
      "Iteration 7610, loss = 1.52471589\n",
      "Iteration 7611, loss = 1.52470063\n",
      "Iteration 7612, loss = 1.52468536\n",
      "Iteration 7613, loss = 1.52467011\n",
      "Iteration 7614, loss = 1.52465485\n",
      "Iteration 7615, loss = 1.52463960\n",
      "Iteration 7616, loss = 1.52462436\n",
      "Iteration 7617, loss = 1.52460911\n",
      "Iteration 7618, loss = 1.52459387\n",
      "Iteration 7619, loss = 1.52457864\n",
      "Iteration 7620, loss = 1.52456341\n",
      "Iteration 7621, loss = 1.52454818\n",
      "Iteration 7622, loss = 1.52453296\n",
      "Iteration 7623, loss = 1.52451774\n",
      "Iteration 7624, loss = 1.52450252\n",
      "Iteration 7625, loss = 1.52448731\n",
      "Iteration 7626, loss = 1.52447210\n",
      "Iteration 7627, loss = 1.52445689\n",
      "Iteration 7628, loss = 1.52444169\n",
      "Iteration 7629, loss = 1.52442649\n",
      "Iteration 7630, loss = 1.52441130\n",
      "Iteration 7631, loss = 1.52439611\n",
      "Iteration 7632, loss = 1.52438092\n",
      "Iteration 7633, loss = 1.52436574\n",
      "Iteration 7634, loss = 1.52435056\n",
      "Iteration 7635, loss = 1.52433539\n",
      "Iteration 7636, loss = 1.52432022\n",
      "Iteration 7637, loss = 1.52430505\n",
      "Iteration 7638, loss = 1.52428988\n",
      "Iteration 7639, loss = 1.52427472\n",
      "Iteration 7640, loss = 1.52425957\n",
      "Iteration 7641, loss = 1.52424441\n",
      "Iteration 7642, loss = 1.52422927\n",
      "Iteration 7643, loss = 1.52421412\n",
      "Iteration 7644, loss = 1.52419898\n",
      "Iteration 7645, loss = 1.52418384\n",
      "Iteration 7646, loss = 1.52416871\n",
      "Iteration 7647, loss = 1.52415358\n",
      "Iteration 7648, loss = 1.52413845\n",
      "Iteration 7649, loss = 1.52412333\n",
      "Iteration 7650, loss = 1.52410821\n",
      "Iteration 7651, loss = 1.52409309\n",
      "Iteration 7652, loss = 1.52407798\n",
      "Iteration 7653, loss = 1.52406287\n",
      "Iteration 7654, loss = 1.52404777\n",
      "Iteration 7655, loss = 1.52403267\n",
      "Iteration 7656, loss = 1.52401757\n",
      "Iteration 7657, loss = 1.52400248\n",
      "Iteration 7658, loss = 1.52398739\n",
      "Iteration 7659, loss = 1.52397230\n",
      "Iteration 7660, loss = 1.52395722\n",
      "Iteration 7661, loss = 1.52394214\n",
      "Iteration 7662, loss = 1.52392707\n",
      "Iteration 7663, loss = 1.52391200\n",
      "Iteration 7664, loss = 1.52389693\n",
      "Iteration 7665, loss = 1.52388187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7666, loss = 1.52386681\n",
      "Iteration 7667, loss = 1.52385175\n",
      "Iteration 7668, loss = 1.52383670\n",
      "Iteration 7669, loss = 1.52382165\n",
      "Iteration 7670, loss = 1.52380661\n",
      "Iteration 7671, loss = 1.52379157\n",
      "Iteration 7672, loss = 1.52377653\n",
      "Iteration 7673, loss = 1.52376150\n",
      "Iteration 7674, loss = 1.52374647\n",
      "Iteration 7675, loss = 1.52373144\n",
      "Iteration 7676, loss = 1.52371642\n",
      "Iteration 7677, loss = 1.52370140\n",
      "Iteration 7678, loss = 1.52368638\n",
      "Iteration 7679, loss = 1.52367137\n",
      "Iteration 7680, loss = 1.52365636\n",
      "Iteration 7681, loss = 1.52364136\n",
      "Iteration 7682, loss = 1.52362636\n",
      "Iteration 7683, loss = 1.52361136\n",
      "Iteration 7684, loss = 1.52359637\n",
      "Iteration 7685, loss = 1.52358138\n",
      "Iteration 7686, loss = 1.52356639\n",
      "Iteration 7687, loss = 1.52355141\n",
      "Iteration 7688, loss = 1.52353643\n",
      "Iteration 7689, loss = 1.52352146\n",
      "Iteration 7690, loss = 1.52350649\n",
      "Iteration 7691, loss = 1.52349152\n",
      "Iteration 7692, loss = 1.52347656\n",
      "Iteration 7693, loss = 1.52346160\n",
      "Iteration 7694, loss = 1.52344664\n",
      "Iteration 7695, loss = 1.52343169\n",
      "Iteration 7696, loss = 1.52341674\n",
      "Iteration 7697, loss = 1.52340180\n",
      "Iteration 7698, loss = 1.52338685\n",
      "Iteration 7699, loss = 1.52337192\n",
      "Iteration 7700, loss = 1.52335698\n",
      "Iteration 7701, loss = 1.52334205\n",
      "Iteration 7702, loss = 1.52332712\n",
      "Iteration 7703, loss = 1.52331220\n",
      "Iteration 7704, loss = 1.52329728\n",
      "Iteration 7705, loss = 1.52328237\n",
      "Iteration 7706, loss = 1.52326745\n",
      "Iteration 7707, loss = 1.52325255\n",
      "Iteration 7708, loss = 1.52323764\n",
      "Iteration 7709, loss = 1.52322274\n",
      "Iteration 7710, loss = 1.52320784\n",
      "Iteration 7711, loss = 1.52319295\n",
      "Iteration 7712, loss = 1.52317806\n",
      "Iteration 7713, loss = 1.52316317\n",
      "Iteration 7714, loss = 1.52314829\n",
      "Iteration 7715, loss = 1.52313341\n",
      "Iteration 7716, loss = 1.52311853\n",
      "Iteration 7717, loss = 1.52310366\n",
      "Iteration 7718, loss = 1.52308879\n",
      "Iteration 7719, loss = 1.52307393\n",
      "Iteration 7720, loss = 1.52305907\n",
      "Iteration 7721, loss = 1.52304421\n",
      "Iteration 7722, loss = 1.52302936\n",
      "Iteration 7723, loss = 1.52301451\n",
      "Iteration 7724, loss = 1.52299966\n",
      "Iteration 7725, loss = 1.52298482\n",
      "Iteration 7726, loss = 1.52296998\n",
      "Iteration 7727, loss = 1.52295514\n",
      "Iteration 7728, loss = 1.52294031\n",
      "Iteration 7729, loss = 1.52292548\n",
      "Iteration 7730, loss = 1.52291066\n",
      "Iteration 7731, loss = 1.52289584\n",
      "Iteration 7732, loss = 1.52288102\n",
      "Iteration 7733, loss = 1.52286620\n",
      "Iteration 7734, loss = 1.52285139\n",
      "Iteration 7735, loss = 1.52283659\n",
      "Iteration 7736, loss = 1.52282178\n",
      "Iteration 7737, loss = 1.52280699\n",
      "Iteration 7738, loss = 1.52279219\n",
      "Iteration 7739, loss = 1.52277740\n",
      "Iteration 7740, loss = 1.52276261\n",
      "Iteration 7741, loss = 1.52274782\n",
      "Iteration 7742, loss = 1.52273304\n",
      "Iteration 7743, loss = 1.52271827\n",
      "Iteration 7744, loss = 1.52270349\n",
      "Iteration 7745, loss = 1.52268872\n",
      "Iteration 7746, loss = 1.52267395\n",
      "Iteration 7747, loss = 1.52265919\n",
      "Iteration 7748, loss = 1.52264443\n",
      "Iteration 7749, loss = 1.52262968\n",
      "Iteration 7750, loss = 1.52261492\n",
      "Iteration 7751, loss = 1.52260018\n",
      "Iteration 7752, loss = 1.52258543\n",
      "Iteration 7753, loss = 1.52257069\n",
      "Iteration 7754, loss = 1.52255595\n",
      "Iteration 7755, loss = 1.52254122\n",
      "Iteration 7756, loss = 1.52252649\n",
      "Iteration 7757, loss = 1.52251176\n",
      "Iteration 7758, loss = 1.52249704\n",
      "Iteration 7759, loss = 1.52248232\n",
      "Iteration 7760, loss = 1.52246760\n",
      "Iteration 7761, loss = 1.52245289\n",
      "Iteration 7762, loss = 1.52243818\n",
      "Iteration 7763, loss = 1.52242347\n",
      "Iteration 7764, loss = 1.52240877\n",
      "Iteration 7765, loss = 1.52239407\n",
      "Iteration 7766, loss = 1.52237938\n",
      "Iteration 7767, loss = 1.52236469\n",
      "Iteration 7768, loss = 1.52235000\n",
      "Iteration 7769, loss = 1.52233532\n",
      "Iteration 7770, loss = 1.52232064\n",
      "Iteration 7771, loss = 1.52230596\n",
      "Iteration 7772, loss = 1.52229129\n",
      "Iteration 7773, loss = 1.52227662\n",
      "Iteration 7774, loss = 1.52226195\n",
      "Iteration 7775, loss = 1.52224729\n",
      "Iteration 7776, loss = 1.52223263\n",
      "Iteration 7777, loss = 1.52221797\n",
      "Iteration 7778, loss = 1.52220332\n",
      "Iteration 7779, loss = 1.52218867\n",
      "Iteration 7780, loss = 1.52217403\n",
      "Iteration 7781, loss = 1.52215939\n",
      "Iteration 7782, loss = 1.52214475\n",
      "Iteration 7783, loss = 1.52213012\n",
      "Iteration 7784, loss = 1.52211549\n",
      "Iteration 7785, loss = 1.52210086\n",
      "Iteration 7786, loss = 1.52208624\n",
      "Iteration 7787, loss = 1.52207162\n",
      "Iteration 7788, loss = 1.52205700\n",
      "Iteration 7789, loss = 1.52204239\n",
      "Iteration 7790, loss = 1.52202778\n",
      "Iteration 7791, loss = 1.52201318\n",
      "Iteration 7792, loss = 1.52199857\n",
      "Iteration 7793, loss = 1.52198398\n",
      "Iteration 7794, loss = 1.52196938\n",
      "Iteration 7795, loss = 1.52195479\n",
      "Iteration 7796, loss = 1.52194020\n",
      "Iteration 7797, loss = 1.52192562\n",
      "Iteration 7798, loss = 1.52191104\n",
      "Iteration 7799, loss = 1.52189646\n",
      "Iteration 7800, loss = 1.52188189\n",
      "Iteration 7801, loss = 1.52186732\n",
      "Iteration 7802, loss = 1.52185275\n",
      "Iteration 7803, loss = 1.52183819\n",
      "Iteration 7804, loss = 1.52182363\n",
      "Iteration 7805, loss = 1.52180908\n",
      "Iteration 7806, loss = 1.52179452\n",
      "Iteration 7807, loss = 1.52177998\n",
      "Iteration 7808, loss = 1.52176543\n",
      "Iteration 7809, loss = 1.52175089\n",
      "Iteration 7810, loss = 1.52173635\n",
      "Iteration 7811, loss = 1.52172182\n",
      "Iteration 7812, loss = 1.52170729\n",
      "Iteration 7813, loss = 1.52169276\n",
      "Iteration 7814, loss = 1.52167824\n",
      "Iteration 7815, loss = 1.52166372\n",
      "Iteration 7816, loss = 1.52164920\n",
      "Iteration 7817, loss = 1.52163469\n",
      "Iteration 7818, loss = 1.52162018\n",
      "Iteration 7819, loss = 1.52160567\n",
      "Iteration 7820, loss = 1.52159117\n",
      "Iteration 7821, loss = 1.52157667\n",
      "Iteration 7822, loss = 1.52156217\n",
      "Iteration 7823, loss = 1.52154768\n",
      "Iteration 7824, loss = 1.52153319\n",
      "Iteration 7825, loss = 1.52151871\n",
      "Iteration 7826, loss = 1.52150423\n",
      "Iteration 7827, loss = 1.52148975\n",
      "Iteration 7828, loss = 1.52147528\n",
      "Iteration 7829, loss = 1.52146081\n",
      "Iteration 7830, loss = 1.52144634\n",
      "Iteration 7831, loss = 1.52143188\n",
      "Iteration 7832, loss = 1.52141742\n",
      "Iteration 7833, loss = 1.52140296\n",
      "Iteration 7834, loss = 1.52138851\n",
      "Iteration 7835, loss = 1.52137406\n",
      "Iteration 7836, loss = 1.52135961\n",
      "Iteration 7837, loss = 1.52134517\n",
      "Iteration 7838, loss = 1.52133073\n",
      "Iteration 7839, loss = 1.52131629\n",
      "Iteration 7840, loss = 1.52130186\n",
      "Iteration 7841, loss = 1.52128743\n",
      "Iteration 7842, loss = 1.52127301\n",
      "Iteration 7843, loss = 1.52125859\n",
      "Iteration 7844, loss = 1.52124417\n",
      "Iteration 7845, loss = 1.52122975\n",
      "Iteration 7846, loss = 1.52121534\n",
      "Iteration 7847, loss = 1.52120093\n",
      "Iteration 7848, loss = 1.52118653\n",
      "Iteration 7849, loss = 1.52117213\n",
      "Iteration 7850, loss = 1.52115773\n",
      "Iteration 7851, loss = 1.52114334\n",
      "Iteration 7852, loss = 1.52112895\n",
      "Iteration 7853, loss = 1.52111456\n",
      "Iteration 7854, loss = 1.52110018\n",
      "Iteration 7855, loss = 1.52108580\n",
      "Iteration 7856, loss = 1.52107142\n",
      "Iteration 7857, loss = 1.52105705\n",
      "Iteration 7858, loss = 1.52104268\n",
      "Iteration 7859, loss = 1.52102832\n",
      "Iteration 7860, loss = 1.52101396\n",
      "Iteration 7861, loss = 1.52099960\n",
      "Iteration 7862, loss = 1.52098524\n",
      "Iteration 7863, loss = 1.52097089\n",
      "Iteration 7864, loss = 1.52095654\n",
      "Iteration 7865, loss = 1.52094220\n",
      "Iteration 7866, loss = 1.52092786\n",
      "Iteration 7867, loss = 1.52091352\n",
      "Iteration 7868, loss = 1.52089918\n",
      "Iteration 7869, loss = 1.52088485\n",
      "Iteration 7870, loss = 1.52087053\n",
      "Iteration 7871, loss = 1.52085620\n",
      "Iteration 7872, loss = 1.52084188\n",
      "Iteration 7873, loss = 1.52082756\n",
      "Iteration 7874, loss = 1.52081325\n",
      "Iteration 7875, loss = 1.52079894\n",
      "Iteration 7876, loss = 1.52078463\n",
      "Iteration 7877, loss = 1.52077033\n",
      "Iteration 7878, loss = 1.52075603\n",
      "Iteration 7879, loss = 1.52074174\n",
      "Iteration 7880, loss = 1.52072744\n",
      "Iteration 7881, loss = 1.52071315\n",
      "Iteration 7882, loss = 1.52069887\n",
      "Iteration 7883, loss = 1.52068459\n",
      "Iteration 7884, loss = 1.52067031\n",
      "Iteration 7885, loss = 1.52065603\n",
      "Iteration 7886, loss = 1.52064176\n",
      "Iteration 7887, loss = 1.52062749\n",
      "Iteration 7888, loss = 1.52061323\n",
      "Iteration 7889, loss = 1.52059897\n",
      "Iteration 7890, loss = 1.52058471\n",
      "Iteration 7891, loss = 1.52057045\n",
      "Iteration 7892, loss = 1.52055620\n",
      "Iteration 7893, loss = 1.52054195\n",
      "Iteration 7894, loss = 1.52052771\n",
      "Iteration 7895, loss = 1.52051347\n",
      "Iteration 7896, loss = 1.52049923\n",
      "Iteration 7897, loss = 1.52048500\n",
      "Iteration 7898, loss = 1.52047077\n",
      "Iteration 7899, loss = 1.52045654\n",
      "Iteration 7900, loss = 1.52044232\n",
      "Iteration 7901, loss = 1.52042810\n",
      "Iteration 7902, loss = 1.52041388\n",
      "Iteration 7903, loss = 1.52039967\n",
      "Iteration 7904, loss = 1.52038546\n",
      "Iteration 7905, loss = 1.52037125\n",
      "Iteration 7906, loss = 1.52035705\n",
      "Iteration 7907, loss = 1.52034285\n",
      "Iteration 7908, loss = 1.52032865\n",
      "Iteration 7909, loss = 1.52031446\n",
      "Iteration 7910, loss = 1.52030027\n",
      "Iteration 7911, loss = 1.52028608\n",
      "Iteration 7912, loss = 1.52027190\n",
      "Iteration 7913, loss = 1.52025772\n",
      "Iteration 7914, loss = 1.52024355\n",
      "Iteration 7915, loss = 1.52022937\n",
      "Iteration 7916, loss = 1.52021521\n",
      "Iteration 7917, loss = 1.52020104\n",
      "Iteration 7918, loss = 1.52018688\n",
      "Iteration 7919, loss = 1.52017272\n",
      "Iteration 7920, loss = 1.52015857\n",
      "Iteration 7921, loss = 1.52014441\n",
      "Iteration 7922, loss = 1.52013027\n",
      "Iteration 7923, loss = 1.52011612\n",
      "Iteration 7924, loss = 1.52010198\n",
      "Iteration 7925, loss = 1.52008784\n",
      "Iteration 7926, loss = 1.52007371\n",
      "Iteration 7927, loss = 1.52005958\n",
      "Iteration 7928, loss = 1.52004545\n",
      "Iteration 7929, loss = 1.52003132\n",
      "Iteration 7930, loss = 1.52001720\n",
      "Iteration 7931, loss = 1.52000309\n",
      "Iteration 7932, loss = 1.51998897\n",
      "Iteration 7933, loss = 1.51997486\n",
      "Iteration 7934, loss = 1.51996075\n",
      "Iteration 7935, loss = 1.51994665\n",
      "Iteration 7936, loss = 1.51993255\n",
      "Iteration 7937, loss = 1.51991845\n",
      "Iteration 7938, loss = 1.51990436\n",
      "Iteration 7939, loss = 1.51989027\n",
      "Iteration 7940, loss = 1.51987618\n",
      "Iteration 7941, loss = 1.51986210\n",
      "Iteration 7942, loss = 1.51984802\n",
      "Iteration 7943, loss = 1.51983394\n",
      "Iteration 7944, loss = 1.51981987\n",
      "Iteration 7945, loss = 1.51980580\n",
      "Iteration 7946, loss = 1.51979173\n",
      "Iteration 7947, loss = 1.51977767\n",
      "Iteration 7948, loss = 1.51976361\n",
      "Iteration 7949, loss = 1.51974955\n",
      "Iteration 7950, loss = 1.51973550\n",
      "Iteration 7951, loss = 1.51972145\n",
      "Iteration 7952, loss = 1.51970740\n",
      "Iteration 7953, loss = 1.51969336\n",
      "Iteration 7954, loss = 1.51967932\n",
      "Iteration 7955, loss = 1.51966529\n",
      "Iteration 7956, loss = 1.51965125\n",
      "Iteration 7957, loss = 1.51963722\n",
      "Iteration 7958, loss = 1.51962320\n",
      "Iteration 7959, loss = 1.51960918\n",
      "Iteration 7960, loss = 1.51959516\n",
      "Iteration 7961, loss = 1.51958114\n",
      "Iteration 7962, loss = 1.51956713\n",
      "Iteration 7963, loss = 1.51955312\n",
      "Iteration 7964, loss = 1.51953911\n",
      "Iteration 7965, loss = 1.51952511\n",
      "Iteration 7966, loss = 1.51951111\n",
      "Iteration 7967, loss = 1.51949712\n",
      "Iteration 7968, loss = 1.51948313\n",
      "Iteration 7969, loss = 1.51946914\n",
      "Iteration 7970, loss = 1.51945515\n",
      "Iteration 7971, loss = 1.51944117\n",
      "Iteration 7972, loss = 1.51942719\n",
      "Iteration 7973, loss = 1.51941322\n",
      "Iteration 7974, loss = 1.51939924\n",
      "Iteration 7975, loss = 1.51938527\n",
      "Iteration 7976, loss = 1.51937131\n",
      "Iteration 7977, loss = 1.51935735\n",
      "Iteration 7978, loss = 1.51934339\n",
      "Iteration 7979, loss = 1.51932943\n",
      "Iteration 7980, loss = 1.51931548\n",
      "Iteration 7981, loss = 1.51930153\n",
      "Iteration 7982, loss = 1.51928759\n",
      "Iteration 7983, loss = 1.51927365\n",
      "Iteration 7984, loss = 1.51925971\n",
      "Iteration 7985, loss = 1.51924577\n",
      "Iteration 7986, loss = 1.51923184\n",
      "Iteration 7987, loss = 1.51921791\n",
      "Iteration 7988, loss = 1.51920399\n",
      "Iteration 7989, loss = 1.51919007\n",
      "Iteration 7990, loss = 1.51917615\n",
      "Iteration 7991, loss = 1.51916223\n",
      "Iteration 7992, loss = 1.51914832\n",
      "Iteration 7993, loss = 1.51913441\n",
      "Iteration 7994, loss = 1.51912051\n",
      "Iteration 7995, loss = 1.51910661\n",
      "Iteration 7996, loss = 1.51909271\n",
      "Iteration 7997, loss = 1.51907881\n",
      "Iteration 7998, loss = 1.51906492\n",
      "Iteration 7999, loss = 1.51905103\n",
      "Iteration 8000, loss = 1.51903715\n",
      "Iteration 8001, loss = 1.51902327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8002, loss = 1.51900939\n",
      "Iteration 8003, loss = 1.51899551\n",
      "Iteration 8004, loss = 1.51898164\n",
      "Iteration 8005, loss = 1.51896777\n",
      "Iteration 8006, loss = 1.51895391\n",
      "Iteration 8007, loss = 1.51894005\n",
      "Iteration 8008, loss = 1.51892619\n",
      "Iteration 8009, loss = 1.51891233\n",
      "Iteration 8010, loss = 1.51889848\n",
      "Iteration 8011, loss = 1.51888463\n",
      "Iteration 8012, loss = 1.51887079\n",
      "Iteration 8013, loss = 1.51885695\n",
      "Iteration 8014, loss = 1.51884311\n",
      "Iteration 8015, loss = 1.51882927\n",
      "Iteration 8016, loss = 1.51881544\n",
      "Iteration 8017, loss = 1.51880161\n",
      "Iteration 8018, loss = 1.51878779\n",
      "Iteration 8019, loss = 1.51877397\n",
      "Iteration 8020, loss = 1.51876015\n",
      "Iteration 8021, loss = 1.51874633\n",
      "Iteration 8022, loss = 1.51873252\n",
      "Iteration 8023, loss = 1.51871871\n",
      "Iteration 8024, loss = 1.51870491\n",
      "Iteration 8025, loss = 1.51869111\n",
      "Iteration 8026, loss = 1.51867731\n",
      "Iteration 8027, loss = 1.51866351\n",
      "Iteration 8028, loss = 1.51864972\n",
      "Iteration 8029, loss = 1.51863593\n",
      "Iteration 8030, loss = 1.51862215\n",
      "Iteration 8031, loss = 1.51860836\n",
      "Iteration 8032, loss = 1.51859459\n",
      "Iteration 8033, loss = 1.51858081\n",
      "Iteration 8034, loss = 1.51856704\n",
      "Iteration 8035, loss = 1.51855327\n",
      "Iteration 8036, loss = 1.51853950\n",
      "Iteration 8037, loss = 1.51852574\n",
      "Iteration 8038, loss = 1.51851198\n",
      "Iteration 8039, loss = 1.51849823\n",
      "Iteration 8040, loss = 1.51848447\n",
      "Iteration 8041, loss = 1.51847072\n",
      "Iteration 8042, loss = 1.51845698\n",
      "Iteration 8043, loss = 1.51844324\n",
      "Iteration 8044, loss = 1.51842950\n",
      "Iteration 8045, loss = 1.51841576\n",
      "Iteration 8046, loss = 1.51840203\n",
      "Iteration 8047, loss = 1.51838830\n",
      "Iteration 8048, loss = 1.51837457\n",
      "Iteration 8049, loss = 1.51836085\n",
      "Iteration 8050, loss = 1.51834713\n",
      "Iteration 8051, loss = 1.51833341\n",
      "Iteration 8052, loss = 1.51831970\n",
      "Iteration 8053, loss = 1.51830599\n",
      "Iteration 8054, loss = 1.51829228\n",
      "Iteration 8055, loss = 1.51827858\n",
      "Iteration 8056, loss = 1.51826488\n",
      "Iteration 8057, loss = 1.51825118\n",
      "Iteration 8058, loss = 1.51823749\n",
      "Iteration 8059, loss = 1.51822380\n",
      "Iteration 8060, loss = 1.51821011\n",
      "Iteration 8061, loss = 1.51819643\n",
      "Iteration 8062, loss = 1.51818275\n",
      "Iteration 8063, loss = 1.51816907\n",
      "Iteration 8064, loss = 1.51815540\n",
      "Iteration 8065, loss = 1.51814173\n",
      "Iteration 8066, loss = 1.51812806\n",
      "Iteration 8067, loss = 1.51811440\n",
      "Iteration 8068, loss = 1.51810074\n",
      "Iteration 8069, loss = 1.51808708\n",
      "Iteration 8070, loss = 1.51807343\n",
      "Iteration 8071, loss = 1.51805978\n",
      "Iteration 8072, loss = 1.51804613\n",
      "Iteration 8073, loss = 1.51803248\n",
      "Iteration 8074, loss = 1.51801884\n",
      "Iteration 8075, loss = 1.51800520\n",
      "Iteration 8076, loss = 1.51799157\n",
      "Iteration 8077, loss = 1.51797794\n",
      "Iteration 8078, loss = 1.51796431\n",
      "Iteration 8079, loss = 1.51795069\n",
      "Iteration 8080, loss = 1.51793706\n",
      "Iteration 8081, loss = 1.51792345\n",
      "Iteration 8082, loss = 1.51790983\n",
      "Iteration 8083, loss = 1.51789622\n",
      "Iteration 8084, loss = 1.51788261\n",
      "Iteration 8085, loss = 1.51786900\n",
      "Iteration 8086, loss = 1.51785540\n",
      "Iteration 8087, loss = 1.51784180\n",
      "Iteration 8088, loss = 1.51782821\n",
      "Iteration 8089, loss = 1.51781462\n",
      "Iteration 8090, loss = 1.51780103\n",
      "Iteration 8091, loss = 1.51778744\n",
      "Iteration 8092, loss = 1.51777386\n",
      "Iteration 8093, loss = 1.51776028\n",
      "Iteration 8094, loss = 1.51774670\n",
      "Iteration 8095, loss = 1.51773313\n",
      "Iteration 8096, loss = 1.51771956\n",
      "Iteration 8097, loss = 1.51770599\n",
      "Iteration 8098, loss = 1.51769243\n",
      "Iteration 8099, loss = 1.51767887\n",
      "Iteration 8100, loss = 1.51766531\n",
      "Iteration 8101, loss = 1.51765176\n",
      "Iteration 8102, loss = 1.51763821\n",
      "Iteration 8103, loss = 1.51762466\n",
      "Iteration 8104, loss = 1.51761112\n",
      "Iteration 8105, loss = 1.51759758\n",
      "Iteration 8106, loss = 1.51758404\n",
      "Iteration 8107, loss = 1.51757051\n",
      "Iteration 8108, loss = 1.51755697\n",
      "Iteration 8109, loss = 1.51754345\n",
      "Iteration 8110, loss = 1.51752992\n",
      "Iteration 8111, loss = 1.51751640\n",
      "Iteration 8112, loss = 1.51750288\n",
      "Iteration 8113, loss = 1.51748937\n",
      "Iteration 8114, loss = 1.51747586\n",
      "Iteration 8115, loss = 1.51746235\n",
      "Iteration 8116, loss = 1.51744884\n",
      "Iteration 8117, loss = 1.51743534\n",
      "Iteration 8118, loss = 1.51742184\n",
      "Iteration 8119, loss = 1.51740835\n",
      "Iteration 8120, loss = 1.51739485\n",
      "Iteration 8121, loss = 1.51738136\n",
      "Iteration 8122, loss = 1.51736788\n",
      "Iteration 8123, loss = 1.51735440\n",
      "Iteration 8124, loss = 1.51734092\n",
      "Iteration 8125, loss = 1.51732744\n",
      "Iteration 8126, loss = 1.51731397\n",
      "Iteration 8127, loss = 1.51730050\n",
      "Iteration 8128, loss = 1.51728703\n",
      "Iteration 8129, loss = 1.51727357\n",
      "Iteration 8130, loss = 1.51726011\n",
      "Iteration 8131, loss = 1.51724665\n",
      "Iteration 8132, loss = 1.51723319\n",
      "Iteration 8133, loss = 1.51721974\n",
      "Iteration 8134, loss = 1.51720630\n",
      "Iteration 8135, loss = 1.51719285\n",
      "Iteration 8136, loss = 1.51717941\n",
      "Iteration 8137, loss = 1.51716597\n",
      "Iteration 8138, loss = 1.51715254\n",
      "Iteration 8139, loss = 1.51713911\n",
      "Iteration 8140, loss = 1.51712568\n",
      "Iteration 8141, loss = 1.51711225\n",
      "Iteration 8142, loss = 1.51709883\n",
      "Iteration 8143, loss = 1.51708541\n",
      "Iteration 8144, loss = 1.51707200\n",
      "Iteration 8145, loss = 1.51705858\n",
      "Iteration 8146, loss = 1.51704517\n",
      "Iteration 8147, loss = 1.51703177\n",
      "Iteration 8148, loss = 1.51701837\n",
      "Iteration 8149, loss = 1.51700497\n",
      "Iteration 8150, loss = 1.51699157\n",
      "Iteration 8151, loss = 1.51697818\n",
      "Iteration 8152, loss = 1.51696479\n",
      "Iteration 8153, loss = 1.51695140\n",
      "Iteration 8154, loss = 1.51693801\n",
      "Iteration 8155, loss = 1.51692463\n",
      "Iteration 8156, loss = 1.51691126\n",
      "Iteration 8157, loss = 1.51689788\n",
      "Iteration 8158, loss = 1.51688451\n",
      "Iteration 8159, loss = 1.51687114\n",
      "Iteration 8160, loss = 1.51685778\n",
      "Iteration 8161, loss = 1.51684442\n",
      "Iteration 8162, loss = 1.51683106\n",
      "Iteration 8163, loss = 1.51681770\n",
      "Iteration 8164, loss = 1.51680435\n",
      "Iteration 8165, loss = 1.51679100\n",
      "Iteration 8166, loss = 1.51677766\n",
      "Iteration 8167, loss = 1.51676431\n",
      "Iteration 8168, loss = 1.51675097\n",
      "Iteration 8169, loss = 1.51673764\n",
      "Iteration 8170, loss = 1.51672430\n",
      "Iteration 8171, loss = 1.51671097\n",
      "Iteration 8172, loss = 1.51669765\n",
      "Iteration 8173, loss = 1.51668432\n",
      "Iteration 8174, loss = 1.51667100\n",
      "Iteration 8175, loss = 1.51665769\n",
      "Iteration 8176, loss = 1.51664437\n",
      "Iteration 8177, loss = 1.51663106\n",
      "Iteration 8178, loss = 1.51661775\n",
      "Iteration 8179, loss = 1.51660445\n",
      "Iteration 8180, loss = 1.51659115\n",
      "Iteration 8181, loss = 1.51657785\n",
      "Iteration 8182, loss = 1.51656455\n",
      "Iteration 8183, loss = 1.51655126\n",
      "Iteration 8184, loss = 1.51653797\n",
      "Iteration 8185, loss = 1.51652468\n",
      "Iteration 8186, loss = 1.51651140\n",
      "Iteration 8187, loss = 1.51649812\n",
      "Iteration 8188, loss = 1.51648485\n",
      "Iteration 8189, loss = 1.51647157\n",
      "Iteration 8190, loss = 1.51645830\n",
      "Iteration 8191, loss = 1.51644504\n",
      "Iteration 8192, loss = 1.51643177\n",
      "Iteration 8193, loss = 1.51641851\n",
      "Iteration 8194, loss = 1.51640525\n",
      "Iteration 8195, loss = 1.51639200\n",
      "Iteration 8196, loss = 1.51637875\n",
      "Iteration 8197, loss = 1.51636550\n",
      "Iteration 8198, loss = 1.51635225\n",
      "Iteration 8199, loss = 1.51633901\n",
      "Iteration 8200, loss = 1.51632577\n",
      "Iteration 8201, loss = 1.51631254\n",
      "Iteration 8202, loss = 1.51629930\n",
      "Iteration 8203, loss = 1.51628608\n",
      "Iteration 8204, loss = 1.51627285\n",
      "Iteration 8205, loss = 1.51625963\n",
      "Iteration 8206, loss = 1.51624641\n",
      "Iteration 8207, loss = 1.51623319\n",
      "Iteration 8208, loss = 1.51621997\n",
      "Iteration 8209, loss = 1.51620676\n",
      "Iteration 8210, loss = 1.51619356\n",
      "Iteration 8211, loss = 1.51618035\n",
      "Iteration 8212, loss = 1.51616715\n",
      "Iteration 8213, loss = 1.51615395\n",
      "Iteration 8214, loss = 1.51614076\n",
      "Iteration 8215, loss = 1.51612757\n",
      "Iteration 8216, loss = 1.51611438\n",
      "Iteration 8217, loss = 1.51610119\n",
      "Iteration 8218, loss = 1.51608801\n",
      "Iteration 8219, loss = 1.51607483\n",
      "Iteration 8220, loss = 1.51606165\n",
      "Iteration 8221, loss = 1.51604848\n",
      "Iteration 8222, loss = 1.51603531\n",
      "Iteration 8223, loss = 1.51602214\n",
      "Iteration 8224, loss = 1.51600898\n",
      "Iteration 8225, loss = 1.51599582\n",
      "Iteration 8226, loss = 1.51598266\n",
      "Iteration 8227, loss = 1.51596950\n",
      "Iteration 8228, loss = 1.51595635\n",
      "Iteration 8229, loss = 1.51594320\n",
      "Iteration 8230, loss = 1.51593006\n",
      "Iteration 8231, loss = 1.51591692\n",
      "Iteration 8232, loss = 1.51590378\n",
      "Iteration 8233, loss = 1.51589064\n",
      "Iteration 8234, loss = 1.51587751\n",
      "Iteration 8235, loss = 1.51586438\n",
      "Iteration 8236, loss = 1.51585125\n",
      "Iteration 8237, loss = 1.51583813\n",
      "Iteration 8238, loss = 1.51582501\n",
      "Iteration 8239, loss = 1.51581189\n",
      "Iteration 8240, loss = 1.51579877\n",
      "Iteration 8241, loss = 1.51578566\n",
      "Iteration 8242, loss = 1.51577256\n",
      "Iteration 8243, loss = 1.51575945\n",
      "Iteration 8244, loss = 1.51574635\n",
      "Iteration 8245, loss = 1.51573325\n",
      "Iteration 8246, loss = 1.51572015\n",
      "Iteration 8247, loss = 1.51570706\n",
      "Iteration 8248, loss = 1.51569397\n",
      "Iteration 8249, loss = 1.51568088\n",
      "Iteration 8250, loss = 1.51566780\n",
      "Iteration 8251, loss = 1.51565472\n",
      "Iteration 8252, loss = 1.51564164\n",
      "Iteration 8253, loss = 1.51562857\n",
      "Iteration 8254, loss = 1.51561550\n",
      "Iteration 8255, loss = 1.51560243\n",
      "Iteration 8256, loss = 1.51558936\n",
      "Iteration 8257, loss = 1.51557630\n",
      "Iteration 8258, loss = 1.51556324\n",
      "Iteration 8259, loss = 1.51555019\n",
      "Iteration 8260, loss = 1.51553713\n",
      "Iteration 8261, loss = 1.51552408\n",
      "Iteration 8262, loss = 1.51551104\n",
      "Iteration 8263, loss = 1.51549799\n",
      "Iteration 8264, loss = 1.51548495\n",
      "Iteration 8265, loss = 1.51547192\n",
      "Iteration 8266, loss = 1.51545888\n",
      "Iteration 8267, loss = 1.51544585\n",
      "Iteration 8268, loss = 1.51543282\n",
      "Iteration 8269, loss = 1.51541980\n",
      "Iteration 8270, loss = 1.51540678\n",
      "Iteration 8271, loss = 1.51539376\n",
      "Iteration 8272, loss = 1.51538074\n",
      "Iteration 8273, loss = 1.51536773\n",
      "Iteration 8274, loss = 1.51535472\n",
      "Iteration 8275, loss = 1.51534171\n",
      "Iteration 8276, loss = 1.51532871\n",
      "Iteration 8277, loss = 1.51531571\n",
      "Iteration 8278, loss = 1.51530271\n",
      "Iteration 8279, loss = 1.51528971\n",
      "Iteration 8280, loss = 1.51527672\n",
      "Iteration 8281, loss = 1.51526373\n",
      "Iteration 8282, loss = 1.51525075\n",
      "Iteration 8283, loss = 1.51523777\n",
      "Iteration 8284, loss = 1.51522479\n",
      "Iteration 8285, loss = 1.51521181\n",
      "Iteration 8286, loss = 1.51519884\n",
      "Iteration 8287, loss = 1.51518587\n",
      "Iteration 8288, loss = 1.51517290\n",
      "Iteration 8289, loss = 1.51515994\n",
      "Iteration 8290, loss = 1.51514697\n",
      "Iteration 8291, loss = 1.51513402\n",
      "Iteration 8292, loss = 1.51512106\n",
      "Iteration 8293, loss = 1.51510811\n",
      "Iteration 8294, loss = 1.51509516\n",
      "Iteration 8295, loss = 1.51508221\n",
      "Iteration 8296, loss = 1.51506927\n",
      "Iteration 8297, loss = 1.51505633\n",
      "Iteration 8298, loss = 1.51504340\n",
      "Iteration 8299, loss = 1.51503046\n",
      "Iteration 8300, loss = 1.51501753\n",
      "Iteration 8301, loss = 1.51500460\n",
      "Iteration 8302, loss = 1.51499168\n",
      "Iteration 8303, loss = 1.51497876\n",
      "Iteration 8304, loss = 1.51496584\n",
      "Iteration 8305, loss = 1.51495292\n",
      "Iteration 8306, loss = 1.51494001\n",
      "Iteration 8307, loss = 1.51492710\n",
      "Iteration 8308, loss = 1.51491419\n",
      "Iteration 8309, loss = 1.51490129\n",
      "Iteration 8310, loss = 1.51488839\n",
      "Iteration 8311, loss = 1.51487549\n",
      "Iteration 8312, loss = 1.51486260\n",
      "Iteration 8313, loss = 1.51484971\n",
      "Iteration 8314, loss = 1.51483682\n",
      "Iteration 8315, loss = 1.51482393\n",
      "Iteration 8316, loss = 1.51481105\n",
      "Iteration 8317, loss = 1.51479817\n",
      "Iteration 8318, loss = 1.51478530\n",
      "Iteration 8319, loss = 1.51477242\n",
      "Iteration 8320, loss = 1.51475955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8321, loss = 1.51474669\n",
      "Iteration 8322, loss = 1.51473382\n",
      "Iteration 8323, loss = 1.51472096\n",
      "Iteration 8324, loss = 1.51470810\n",
      "Iteration 8325, loss = 1.51469525\n",
      "Iteration 8326, loss = 1.51468240\n",
      "Iteration 8327, loss = 1.51466955\n",
      "Iteration 8328, loss = 1.51465670\n",
      "Iteration 8329, loss = 1.51464386\n",
      "Iteration 8330, loss = 1.51463102\n",
      "Iteration 8331, loss = 1.51461818\n",
      "Iteration 8332, loss = 1.51460535\n",
      "Iteration 8333, loss = 1.51459251\n",
      "Iteration 8334, loss = 1.51457969\n",
      "Iteration 8335, loss = 1.51456686\n",
      "Iteration 8336, loss = 1.51455404\n",
      "Iteration 8337, loss = 1.51454122\n",
      "Iteration 8338, loss = 1.51452840\n",
      "Iteration 8339, loss = 1.51451559\n",
      "Iteration 8340, loss = 1.51450278\n",
      "Iteration 8341, loss = 1.51448997\n",
      "Iteration 8342, loss = 1.51447717\n",
      "Iteration 8343, loss = 1.51446437\n",
      "Iteration 8344, loss = 1.51445157\n",
      "Iteration 8345, loss = 1.51443878\n",
      "Iteration 8346, loss = 1.51442598\n",
      "Iteration 8347, loss = 1.51441319\n",
      "Iteration 8348, loss = 1.51440041\n",
      "Iteration 8349, loss = 1.51438763\n",
      "Iteration 8350, loss = 1.51437485\n",
      "Iteration 8351, loss = 1.51436207\n",
      "Iteration 8352, loss = 1.51434929\n",
      "Iteration 8353, loss = 1.51433652\n",
      "Iteration 8354, loss = 1.51432375\n",
      "Iteration 8355, loss = 1.51431099\n",
      "Iteration 8356, loss = 1.51429823\n",
      "Iteration 8357, loss = 1.51428547\n",
      "Iteration 8358, loss = 1.51427271\n",
      "Iteration 8359, loss = 1.51425996\n",
      "Iteration 8360, loss = 1.51424721\n",
      "Iteration 8361, loss = 1.51423446\n",
      "Iteration 8362, loss = 1.51422172\n",
      "Iteration 8363, loss = 1.51420897\n",
      "Iteration 8364, loss = 1.51419624\n",
      "Iteration 8365, loss = 1.51418350\n",
      "Iteration 8366, loss = 1.51417077\n",
      "Iteration 8367, loss = 1.51415804\n",
      "Iteration 8368, loss = 1.51414531\n",
      "Iteration 8369, loss = 1.51413259\n",
      "Iteration 8370, loss = 1.51411987\n",
      "Iteration 8371, loss = 1.51410715\n",
      "Iteration 8372, loss = 1.51409444\n",
      "Iteration 8373, loss = 1.51408172\n",
      "Iteration 8374, loss = 1.51406902\n",
      "Iteration 8375, loss = 1.51405631\n",
      "Iteration 8376, loss = 1.51404361\n",
      "Iteration 8377, loss = 1.51403091\n",
      "Iteration 8378, loss = 1.51401821\n",
      "Iteration 8379, loss = 1.51400552\n",
      "Iteration 8380, loss = 1.51399283\n",
      "Iteration 8381, loss = 1.51398014\n",
      "Iteration 8382, loss = 1.51396745\n",
      "Iteration 8383, loss = 1.51395477\n",
      "Iteration 8384, loss = 1.51394209\n",
      "Iteration 8385, loss = 1.51392941\n",
      "Iteration 8386, loss = 1.51391674\n",
      "Iteration 8387, loss = 1.51390407\n",
      "Iteration 8388, loss = 1.51389140\n",
      "Iteration 8389, loss = 1.51387874\n",
      "Iteration 8390, loss = 1.51386608\n",
      "Iteration 8391, loss = 1.51385342\n",
      "Iteration 8392, loss = 1.51384076\n",
      "Iteration 8393, loss = 1.51382811\n",
      "Iteration 8394, loss = 1.51381546\n",
      "Iteration 8395, loss = 1.51380282\n",
      "Iteration 8396, loss = 1.51379017\n",
      "Iteration 8397, loss = 1.51377753\n",
      "Iteration 8398, loss = 1.51376489\n",
      "Iteration 8399, loss = 1.51375226\n",
      "Iteration 8400, loss = 1.51373963\n",
      "Iteration 8401, loss = 1.51372700\n",
      "Iteration 8402, loss = 1.51371437\n",
      "Iteration 8403, loss = 1.51370175\n",
      "Iteration 8404, loss = 1.51368913\n",
      "Iteration 8405, loss = 1.51367651\n",
      "Iteration 8406, loss = 1.51366390\n",
      "Iteration 8407, loss = 1.51365128\n",
      "Iteration 8408, loss = 1.51363868\n",
      "Iteration 8409, loss = 1.51362607\n",
      "Iteration 8410, loss = 1.51361347\n",
      "Iteration 8411, loss = 1.51360087\n",
      "Iteration 8412, loss = 1.51358827\n",
      "Iteration 8413, loss = 1.51357568\n",
      "Iteration 8414, loss = 1.51356309\n",
      "Iteration 8415, loss = 1.51355050\n",
      "Iteration 8416, loss = 1.51353791\n",
      "Iteration 8417, loss = 1.51352533\n",
      "Iteration 8418, loss = 1.51351275\n",
      "Iteration 8419, loss = 1.51350018\n",
      "Iteration 8420, loss = 1.51348760\n",
      "Iteration 8421, loss = 1.51347503\n",
      "Iteration 8422, loss = 1.51346247\n",
      "Iteration 8423, loss = 1.51344990\n",
      "Iteration 8424, loss = 1.51343734\n",
      "Iteration 8425, loss = 1.51342478\n",
      "Iteration 8426, loss = 1.51341223\n",
      "Iteration 8427, loss = 1.51339967\n",
      "Iteration 8428, loss = 1.51338712\n",
      "Iteration 8429, loss = 1.51337458\n",
      "Iteration 8430, loss = 1.51336203\n",
      "Iteration 8431, loss = 1.51334949\n",
      "Iteration 8432, loss = 1.51333695\n",
      "Iteration 8433, loss = 1.51332442\n",
      "Iteration 8434, loss = 1.51331188\n",
      "Iteration 8435, loss = 1.51329935\n",
      "Iteration 8436, loss = 1.51328683\n",
      "Iteration 8437, loss = 1.51327430\n",
      "Iteration 8438, loss = 1.51326178\n",
      "Iteration 8439, loss = 1.51324927\n",
      "Iteration 8440, loss = 1.51323675\n",
      "Iteration 8441, loss = 1.51322424\n",
      "Iteration 8442, loss = 1.51321173\n",
      "Iteration 8443, loss = 1.51319922\n",
      "Iteration 8444, loss = 1.51318672\n",
      "Iteration 8445, loss = 1.51317422\n",
      "Iteration 8446, loss = 1.51316172\n",
      "Iteration 8447, loss = 1.51314923\n",
      "Iteration 8448, loss = 1.51313673\n",
      "Iteration 8449, loss = 1.51312425\n",
      "Iteration 8450, loss = 1.51311176\n",
      "Iteration 8451, loss = 1.51309928\n",
      "Iteration 8452, loss = 1.51308680\n",
      "Iteration 8453, loss = 1.51307432\n",
      "Iteration 8454, loss = 1.51306184\n",
      "Iteration 8455, loss = 1.51304937\n",
      "Iteration 8456, loss = 1.51303690\n",
      "Iteration 8457, loss = 1.51302444\n",
      "Iteration 8458, loss = 1.51301198\n",
      "Iteration 8459, loss = 1.51299952\n",
      "Iteration 8460, loss = 1.51298706\n",
      "Iteration 8461, loss = 1.51297460\n",
      "Iteration 8462, loss = 1.51296215\n",
      "Iteration 8463, loss = 1.51294970\n",
      "Iteration 8464, loss = 1.51293726\n",
      "Iteration 8465, loss = 1.51292482\n",
      "Iteration 8466, loss = 1.51291238\n",
      "Iteration 8467, loss = 1.51289994\n",
      "Iteration 8468, loss = 1.51288751\n",
      "Iteration 8469, loss = 1.51287507\n",
      "Iteration 8470, loss = 1.51286265\n",
      "Iteration 8471, loss = 1.51285022\n",
      "Iteration 8472, loss = 1.51283780\n",
      "Iteration 8473, loss = 1.51282538\n",
      "Iteration 8474, loss = 1.51281296\n",
      "Iteration 8475, loss = 1.51280055\n",
      "Iteration 8476, loss = 1.51278814\n",
      "Iteration 8477, loss = 1.51277573\n",
      "Iteration 8478, loss = 1.51276332\n",
      "Iteration 8479, loss = 1.51275092\n",
      "Iteration 8480, loss = 1.51273852\n",
      "Iteration 8481, loss = 1.51272612\n",
      "Iteration 8482, loss = 1.51271373\n",
      "Iteration 8483, loss = 1.51270134\n",
      "Iteration 8484, loss = 1.51268895\n",
      "Iteration 8485, loss = 1.51267657\n",
      "Iteration 8486, loss = 1.51266418\n",
      "Iteration 8487, loss = 1.51265180\n",
      "Iteration 8488, loss = 1.51263943\n",
      "Iteration 8489, loss = 1.51262705\n",
      "Iteration 8490, loss = 1.51261468\n",
      "Iteration 8491, loss = 1.51260232\n",
      "Iteration 8492, loss = 1.51258995\n",
      "Iteration 8493, loss = 1.51257759\n",
      "Iteration 8494, loss = 1.51256523\n",
      "Iteration 8495, loss = 1.51255287\n",
      "Iteration 8496, loss = 1.51254052\n",
      "Iteration 8497, loss = 1.51252817\n",
      "Iteration 8498, loss = 1.51251582\n",
      "Iteration 8499, loss = 1.51250347\n",
      "Iteration 8500, loss = 1.51249113\n",
      "Iteration 8501, loss = 1.51247879\n",
      "Iteration 8502, loss = 1.51246646\n",
      "Iteration 8503, loss = 1.51245412\n",
      "Iteration 8504, loss = 1.51244179\n",
      "Iteration 8505, loss = 1.51242946\n",
      "Iteration 8506, loss = 1.51241714\n",
      "Iteration 8507, loss = 1.51240482\n",
      "Iteration 8508, loss = 1.51239250\n",
      "Iteration 8509, loss = 1.51238018\n",
      "Iteration 8510, loss = 1.51236787\n",
      "Iteration 8511, loss = 1.51235555\n",
      "Iteration 8512, loss = 1.51234325\n",
      "Iteration 8513, loss = 1.51233094\n",
      "Iteration 8514, loss = 1.51231864\n",
      "Iteration 8515, loss = 1.51230634\n",
      "Iteration 8516, loss = 1.51229404\n",
      "Iteration 8517, loss = 1.51228175\n",
      "Iteration 8518, loss = 1.51226946\n",
      "Iteration 8519, loss = 1.51225717\n",
      "Iteration 8520, loss = 1.51224488\n",
      "Iteration 8521, loss = 1.51223260\n",
      "Iteration 8522, loss = 1.51222032\n",
      "Iteration 8523, loss = 1.51220804\n",
      "Iteration 8524, loss = 1.51219577\n",
      "Iteration 8525, loss = 1.51218350\n",
      "Iteration 8526, loss = 1.51217123\n",
      "Iteration 8527, loss = 1.51215896\n",
      "Iteration 8528, loss = 1.51214670\n",
      "Iteration 8529, loss = 1.51213444\n",
      "Iteration 8530, loss = 1.51212218\n",
      "Iteration 8531, loss = 1.51210993\n",
      "Iteration 8532, loss = 1.51209768\n",
      "Iteration 8533, loss = 1.51208543\n",
      "Iteration 8534, loss = 1.51207318\n",
      "Iteration 8535, loss = 1.51206094\n",
      "Iteration 8536, loss = 1.51204870\n",
      "Iteration 8537, loss = 1.51203646\n",
      "Iteration 8538, loss = 1.51202423\n",
      "Iteration 8539, loss = 1.51201200\n",
      "Iteration 8540, loss = 1.51199977\n",
      "Iteration 8541, loss = 1.51198754\n",
      "Iteration 8542, loss = 1.51197532\n",
      "Iteration 8543, loss = 1.51196310\n",
      "Iteration 8544, loss = 1.51195088\n",
      "Iteration 8545, loss = 1.51193867\n",
      "Iteration 8546, loss = 1.51192645\n",
      "Iteration 8547, loss = 1.51191424\n",
      "Iteration 8548, loss = 1.51190204\n",
      "Iteration 8549, loss = 1.51188983\n",
      "Iteration 8550, loss = 1.51187763\n",
      "Iteration 8551, loss = 1.51186544\n",
      "Iteration 8552, loss = 1.51185324\n",
      "Iteration 8553, loss = 1.51184105\n",
      "Iteration 8554, loss = 1.51182886\n",
      "Iteration 8555, loss = 1.51181667\n",
      "Iteration 8556, loss = 1.51180449\n",
      "Iteration 8557, loss = 1.51179231\n",
      "Iteration 8558, loss = 1.51178013\n",
      "Iteration 8559, loss = 1.51176795\n",
      "Iteration 8560, loss = 1.51175578\n",
      "Iteration 8561, loss = 1.51174361\n",
      "Iteration 8562, loss = 1.51173144\n",
      "Iteration 8563, loss = 1.51171928\n",
      "Iteration 8564, loss = 1.51170712\n",
      "Iteration 8565, loss = 1.51169496\n",
      "Iteration 8566, loss = 1.51168280\n",
      "Iteration 8567, loss = 1.51167065\n",
      "Iteration 8568, loss = 1.51165850\n",
      "Iteration 8569, loss = 1.51164635\n",
      "Iteration 8570, loss = 1.51163421\n",
      "Iteration 8571, loss = 1.51162206\n",
      "Iteration 8572, loss = 1.51160992\n",
      "Iteration 8573, loss = 1.51159779\n",
      "Iteration 8574, loss = 1.51158565\n",
      "Iteration 8575, loss = 1.51157352\n",
      "Iteration 8576, loss = 1.51156140\n",
      "Iteration 8577, loss = 1.51154927\n",
      "Iteration 8578, loss = 1.51153715\n",
      "Iteration 8579, loss = 1.51152503\n",
      "Iteration 8580, loss = 1.51151291\n",
      "Iteration 8581, loss = 1.51150080\n",
      "Iteration 8582, loss = 1.51148869\n",
      "Iteration 8583, loss = 1.51147658\n",
      "Iteration 8584, loss = 1.51146447\n",
      "Iteration 8585, loss = 1.51145237\n",
      "Iteration 8586, loss = 1.51144027\n",
      "Iteration 8587, loss = 1.51142817\n",
      "Iteration 8588, loss = 1.51141608\n",
      "Iteration 8589, loss = 1.51140398\n",
      "Iteration 8590, loss = 1.51139189\n",
      "Iteration 8591, loss = 1.51137981\n",
      "Iteration 8592, loss = 1.51136772\n",
      "Iteration 8593, loss = 1.51135564\n",
      "Iteration 8594, loss = 1.51134357\n",
      "Iteration 8595, loss = 1.51133149\n",
      "Iteration 8596, loss = 1.51131942\n",
      "Iteration 8597, loss = 1.51130735\n",
      "Iteration 8598, loss = 1.51129528\n",
      "Iteration 8599, loss = 1.51128322\n",
      "Iteration 8600, loss = 1.51127115\n",
      "Iteration 8601, loss = 1.51125910\n",
      "Iteration 8602, loss = 1.51124704\n",
      "Iteration 8603, loss = 1.51123499\n",
      "Iteration 8604, loss = 1.51122294\n",
      "Iteration 8605, loss = 1.51121089\n",
      "Iteration 8606, loss = 1.51119884\n",
      "Iteration 8607, loss = 1.51118680\n",
      "Iteration 8608, loss = 1.51117476\n",
      "Iteration 8609, loss = 1.51116273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8610, loss = 1.51115069\n",
      "Iteration 8611, loss = 1.51113866\n",
      "Iteration 8612, loss = 1.51112663\n",
      "Iteration 8613, loss = 1.51111461\n",
      "Iteration 8614, loss = 1.51110258\n",
      "Iteration 8615, loss = 1.51109056\n",
      "Iteration 8616, loss = 1.51107855\n",
      "Iteration 8617, loss = 1.51106653\n",
      "Iteration 8618, loss = 1.51105452\n",
      "Iteration 8619, loss = 1.51104251\n",
      "Iteration 8620, loss = 1.51103051\n",
      "Iteration 8621, loss = 1.51101850\n",
      "Iteration 8622, loss = 1.51100650\n",
      "Iteration 8623, loss = 1.51099450\n",
      "Iteration 8624, loss = 1.51098251\n",
      "Iteration 8625, loss = 1.51097051\n",
      "Iteration 8626, loss = 1.51095852\n",
      "Iteration 8627, loss = 1.51094654\n",
      "Iteration 8628, loss = 1.51093455\n",
      "Iteration 8629, loss = 1.51092257\n",
      "Iteration 8630, loss = 1.51091059\n",
      "Iteration 8631, loss = 1.51089862\n",
      "Iteration 8632, loss = 1.51088664\n",
      "Iteration 8633, loss = 1.51087467\n",
      "Iteration 8634, loss = 1.51086270\n",
      "Iteration 8635, loss = 1.51085074\n",
      "Iteration 8636, loss = 1.51083878\n",
      "Iteration 8637, loss = 1.51082682\n",
      "Iteration 8638, loss = 1.51081486\n",
      "Iteration 8639, loss = 1.51080291\n",
      "Iteration 8640, loss = 1.51079095\n",
      "Iteration 8641, loss = 1.51077901\n",
      "Iteration 8642, loss = 1.51076706\n",
      "Iteration 8643, loss = 1.51075512\n",
      "Iteration 8644, loss = 1.51074318\n",
      "Iteration 8645, loss = 1.51073124\n",
      "Iteration 8646, loss = 1.51071930\n",
      "Iteration 8647, loss = 1.51070737\n",
      "Iteration 8648, loss = 1.51069544\n",
      "Iteration 8649, loss = 1.51068351\n",
      "Iteration 8650, loss = 1.51067159\n",
      "Iteration 8651, loss = 1.51065967\n",
      "Iteration 8652, loss = 1.51064775\n",
      "Iteration 8653, loss = 1.51063583\n",
      "Iteration 8654, loss = 1.51062392\n",
      "Iteration 8655, loss = 1.51061201\n",
      "Iteration 8656, loss = 1.51060010\n",
      "Iteration 8657, loss = 1.51058820\n",
      "Iteration 8658, loss = 1.51057629\n",
      "Iteration 8659, loss = 1.51056439\n",
      "Iteration 8660, loss = 1.51055250\n",
      "Iteration 8661, loss = 1.51054060\n",
      "Iteration 8662, loss = 1.51052871\n",
      "Iteration 8663, loss = 1.51051682\n",
      "Iteration 8664, loss = 1.51050494\n",
      "Iteration 8665, loss = 1.51049305\n",
      "Iteration 8666, loss = 1.51048117\n",
      "Iteration 8667, loss = 1.51046929\n",
      "Iteration 8668, loss = 1.51045742\n",
      "Iteration 8669, loss = 1.51044555\n",
      "Iteration 8670, loss = 1.51043368\n",
      "Iteration 8671, loss = 1.51042181\n",
      "Iteration 8672, loss = 1.51040994\n",
      "Iteration 8673, loss = 1.51039808\n",
      "Iteration 8674, loss = 1.51038622\n",
      "Iteration 8675, loss = 1.51037437\n",
      "Iteration 8676, loss = 1.51036251\n",
      "Iteration 8677, loss = 1.51035066\n",
      "Iteration 8678, loss = 1.51033881\n",
      "Iteration 8679, loss = 1.51032697\n",
      "Iteration 8680, loss = 1.51031513\n",
      "Iteration 8681, loss = 1.51030329\n",
      "Iteration 8682, loss = 1.51029145\n",
      "Iteration 8683, loss = 1.51027961\n",
      "Iteration 8684, loss = 1.51026778\n",
      "Iteration 8685, loss = 1.51025595\n",
      "Iteration 8686, loss = 1.51024412\n",
      "Iteration 8687, loss = 1.51023230\n",
      "Iteration 8688, loss = 1.51022048\n",
      "Iteration 8689, loss = 1.51020866\n",
      "Iteration 8690, loss = 1.51019684\n",
      "Iteration 8691, loss = 1.51018503\n",
      "Iteration 8692, loss = 1.51017322\n",
      "Iteration 8693, loss = 1.51016141\n",
      "Iteration 8694, loss = 1.51014961\n",
      "Iteration 8695, loss = 1.51013781\n",
      "Iteration 8696, loss = 1.51012601\n",
      "Iteration 8697, loss = 1.51011421\n",
      "Iteration 8698, loss = 1.51010241\n",
      "Iteration 8699, loss = 1.51009062\n",
      "Iteration 8700, loss = 1.51007883\n",
      "Iteration 8701, loss = 1.51006705\n",
      "Iteration 8702, loss = 1.51005526\n",
      "Iteration 8703, loss = 1.51004348\n",
      "Iteration 8704, loss = 1.51003170\n",
      "Iteration 8705, loss = 1.51001993\n",
      "Iteration 8706, loss = 1.51000816\n",
      "Iteration 8707, loss = 1.50999638\n",
      "Iteration 8708, loss = 1.50998462\n",
      "Iteration 8709, loss = 1.50997285\n",
      "Iteration 8710, loss = 1.50996109\n",
      "Iteration 8711, loss = 1.50994933\n",
      "Iteration 8712, loss = 1.50993757\n",
      "Iteration 8713, loss = 1.50992582\n",
      "Iteration 8714, loss = 1.50991407\n",
      "Iteration 8715, loss = 1.50990232\n",
      "Iteration 8716, loss = 1.50989057\n",
      "Iteration 8717, loss = 1.50987883\n",
      "Iteration 8718, loss = 1.50986709\n",
      "Iteration 8719, loss = 1.50985535\n",
      "Iteration 8720, loss = 1.50984361\n",
      "Iteration 8721, loss = 1.50983188\n",
      "Iteration 8722, loss = 1.50982015\n",
      "Iteration 8723, loss = 1.50980842\n",
      "Iteration 8724, loss = 1.50979670\n",
      "Iteration 8725, loss = 1.50978498\n",
      "Iteration 8726, loss = 1.50977326\n",
      "Iteration 8727, loss = 1.50976154\n",
      "Iteration 8728, loss = 1.50974983\n",
      "Iteration 8729, loss = 1.50973811\n",
      "Iteration 8730, loss = 1.50972640\n",
      "Iteration 8731, loss = 1.50971470\n",
      "Iteration 8732, loss = 1.50970299\n",
      "Iteration 8733, loss = 1.50969129\n",
      "Iteration 8734, loss = 1.50967960\n",
      "Iteration 8735, loss = 1.50966790\n",
      "Iteration 8736, loss = 1.50965621\n",
      "Iteration 8737, loss = 1.50964452\n",
      "Iteration 8738, loss = 1.50963283\n",
      "Iteration 8739, loss = 1.50962114\n",
      "Iteration 8740, loss = 1.50960946\n",
      "Iteration 8741, loss = 1.50959778\n",
      "Iteration 8742, loss = 1.50958610\n",
      "Iteration 8743, loss = 1.50957443\n",
      "Iteration 8744, loss = 1.50956276\n",
      "Iteration 8745, loss = 1.50955109\n",
      "Iteration 8746, loss = 1.50953942\n",
      "Iteration 8747, loss = 1.50952776\n",
      "Iteration 8748, loss = 1.50951610\n",
      "Iteration 8749, loss = 1.50950444\n",
      "Iteration 8750, loss = 1.50949278\n",
      "Iteration 8751, loss = 1.50948113\n",
      "Iteration 8752, loss = 1.50946948\n",
      "Iteration 8753, loss = 1.50945783\n",
      "Iteration 8754, loss = 1.50944619\n",
      "Iteration 8755, loss = 1.50943454\n",
      "Iteration 8756, loss = 1.50942290\n",
      "Iteration 8757, loss = 1.50941127\n",
      "Iteration 8758, loss = 1.50939963\n",
      "Iteration 8759, loss = 1.50938800\n",
      "Iteration 8760, loss = 1.50937637\n",
      "Iteration 8761, loss = 1.50936474\n",
      "Iteration 8762, loss = 1.50935312\n",
      "Iteration 8763, loss = 1.50934150\n",
      "Iteration 8764, loss = 1.50932988\n",
      "Iteration 8765, loss = 1.50931826\n",
      "Iteration 8766, loss = 1.50930665\n",
      "Iteration 8767, loss = 1.50929504\n",
      "Iteration 8768, loss = 1.50928343\n",
      "Iteration 8769, loss = 1.50927182\n",
      "Iteration 8770, loss = 1.50926022\n",
      "Iteration 8771, loss = 1.50924862\n",
      "Iteration 8772, loss = 1.50923702\n",
      "Iteration 8773, loss = 1.50922542\n",
      "Iteration 8774, loss = 1.50921383\n",
      "Iteration 8775, loss = 1.50920224\n",
      "Iteration 8776, loss = 1.50919065\n",
      "Iteration 8777, loss = 1.50917907\n",
      "Iteration 8778, loss = 1.50916749\n",
      "Iteration 8779, loss = 1.50915591\n",
      "Iteration 8780, loss = 1.50914433\n",
      "Iteration 8781, loss = 1.50913275\n",
      "Iteration 8782, loss = 1.50912118\n",
      "Iteration 8783, loss = 1.50910961\n",
      "Iteration 8784, loss = 1.50909805\n",
      "Iteration 8785, loss = 1.50908648\n",
      "Iteration 8786, loss = 1.50907492\n",
      "Iteration 8787, loss = 1.50906336\n",
      "Iteration 8788, loss = 1.50905181\n",
      "Iteration 8789, loss = 1.50904025\n",
      "Iteration 8790, loss = 1.50902870\n",
      "Iteration 8791, loss = 1.50901715\n",
      "Iteration 8792, loss = 1.50900561\n",
      "Iteration 8793, loss = 1.50899407\n",
      "Iteration 8794, loss = 1.50898253\n",
      "Iteration 8795, loss = 1.50897099\n",
      "Iteration 8796, loss = 1.50895945\n",
      "Iteration 8797, loss = 1.50894792\n",
      "Iteration 8798, loss = 1.50893639\n",
      "Iteration 8799, loss = 1.50892486\n",
      "Iteration 8800, loss = 1.50891334\n",
      "Iteration 8801, loss = 1.50890181\n",
      "Iteration 8802, loss = 1.50889030\n",
      "Iteration 8803, loss = 1.50887878\n",
      "Iteration 8804, loss = 1.50886726\n",
      "Iteration 8805, loss = 1.50885575\n",
      "Iteration 8806, loss = 1.50884424\n",
      "Iteration 8807, loss = 1.50883274\n",
      "Iteration 8808, loss = 1.50882123\n",
      "Iteration 8809, loss = 1.50880973\n",
      "Iteration 8810, loss = 1.50879823\n",
      "Iteration 8811, loss = 1.50878674\n",
      "Iteration 8812, loss = 1.50877524\n",
      "Iteration 8813, loss = 1.50876375\n",
      "Iteration 8814, loss = 1.50875226\n",
      "Iteration 8815, loss = 1.50874078\n",
      "Iteration 8816, loss = 1.50872929\n",
      "Iteration 8817, loss = 1.50871781\n",
      "Iteration 8818, loss = 1.50870633\n",
      "Iteration 8819, loss = 1.50869486\n",
      "Iteration 8820, loss = 1.50868339\n",
      "Iteration 8821, loss = 1.50867192\n",
      "Iteration 8822, loss = 1.50866045\n",
      "Iteration 8823, loss = 1.50864898\n",
      "Iteration 8824, loss = 1.50863752\n",
      "Iteration 8825, loss = 1.50862606\n",
      "Iteration 8826, loss = 1.50861460\n",
      "Iteration 8827, loss = 1.50860315\n",
      "Iteration 8828, loss = 1.50859170\n",
      "Iteration 8829, loss = 1.50858025\n",
      "Iteration 8830, loss = 1.50856880\n",
      "Iteration 8831, loss = 1.50855735\n",
      "Iteration 8832, loss = 1.50854591\n",
      "Iteration 8833, loss = 1.50853447\n",
      "Iteration 8834, loss = 1.50852304\n",
      "Iteration 8835, loss = 1.50851160\n",
      "Iteration 8836, loss = 1.50850017\n",
      "Iteration 8837, loss = 1.50848874\n",
      "Iteration 8838, loss = 1.50847731\n",
      "Iteration 8839, loss = 1.50846589\n",
      "Iteration 8840, loss = 1.50845447\n",
      "Iteration 8841, loss = 1.50844305\n",
      "Iteration 8842, loss = 1.50843163\n",
      "Iteration 8843, loss = 1.50842022\n",
      "Iteration 8844, loss = 1.50840881\n",
      "Iteration 8845, loss = 1.50839740\n",
      "Iteration 8846, loss = 1.50838600\n",
      "Iteration 8847, loss = 1.50837459\n",
      "Iteration 8848, loss = 1.50836319\n",
      "Iteration 8849, loss = 1.50835179\n",
      "Iteration 8850, loss = 1.50834040\n",
      "Iteration 8851, loss = 1.50832900\n",
      "Iteration 8852, loss = 1.50831761\n",
      "Iteration 8853, loss = 1.50830623\n",
      "Iteration 8854, loss = 1.50829484\n",
      "Iteration 8855, loss = 1.50828346\n",
      "Iteration 8856, loss = 1.50827208\n",
      "Iteration 8857, loss = 1.50826070\n",
      "Iteration 8858, loss = 1.50824932\n",
      "Iteration 8859, loss = 1.50823795\n",
      "Iteration 8860, loss = 1.50822658\n",
      "Iteration 8861, loss = 1.50821521\n",
      "Iteration 8862, loss = 1.50820385\n",
      "Iteration 8863, loss = 1.50819249\n",
      "Iteration 8864, loss = 1.50818113\n",
      "Iteration 8865, loss = 1.50816977\n",
      "Iteration 8866, loss = 1.50815841\n",
      "Iteration 8867, loss = 1.50814706\n",
      "Iteration 8868, loss = 1.50813571\n",
      "Iteration 8869, loss = 1.50812437\n",
      "Iteration 8870, loss = 1.50811302\n",
      "Iteration 8871, loss = 1.50810168\n",
      "Iteration 8872, loss = 1.50809034\n",
      "Iteration 8873, loss = 1.50807900\n",
      "Iteration 8874, loss = 1.50806767\n",
      "Iteration 8875, loss = 1.50805634\n",
      "Iteration 8876, loss = 1.50804501\n",
      "Iteration 8877, loss = 1.50803368\n",
      "Iteration 8878, loss = 1.50802236\n",
      "Iteration 8879, loss = 1.50801103\n",
      "Iteration 8880, loss = 1.50799972\n",
      "Iteration 8881, loss = 1.50798840\n",
      "Iteration 8882, loss = 1.50797708\n",
      "Iteration 8883, loss = 1.50796577\n",
      "Iteration 8884, loss = 1.50795446\n",
      "Iteration 8885, loss = 1.50794316\n",
      "Iteration 8886, loss = 1.50793185\n",
      "Iteration 8887, loss = 1.50792055\n",
      "Iteration 8888, loss = 1.50790925\n",
      "Iteration 8889, loss = 1.50789796\n",
      "Iteration 8890, loss = 1.50788666\n",
      "Iteration 8891, loss = 1.50787537\n",
      "Iteration 8892, loss = 1.50786408\n",
      "Iteration 8893, loss = 1.50785280\n",
      "Iteration 8894, loss = 1.50784151\n",
      "Iteration 8895, loss = 1.50783023\n",
      "Iteration 8896, loss = 1.50781895\n",
      "Iteration 8897, loss = 1.50780768\n",
      "Iteration 8898, loss = 1.50779640\n",
      "Iteration 8899, loss = 1.50778513\n",
      "Iteration 8900, loss = 1.50777387\n",
      "Iteration 8901, loss = 1.50776260\n",
      "Iteration 8902, loss = 1.50775134\n",
      "Iteration 8903, loss = 1.50774008\n",
      "Iteration 8904, loss = 1.50772882\n",
      "Iteration 8905, loss = 1.50771756\n",
      "Iteration 8906, loss = 1.50770631\n",
      "Iteration 8907, loss = 1.50769506\n",
      "Iteration 8908, loss = 1.50768381\n",
      "Iteration 8909, loss = 1.50767256\n",
      "Iteration 8910, loss = 1.50766132\n",
      "Iteration 8911, loss = 1.50765008\n",
      "Iteration 8912, loss = 1.50763884\n",
      "Iteration 8913, loss = 1.50762760\n",
      "Iteration 8914, loss = 1.50761637\n",
      "Iteration 8915, loss = 1.50760514\n",
      "Iteration 8916, loss = 1.50759391\n",
      "Iteration 8917, loss = 1.50758269\n",
      "Iteration 8918, loss = 1.50757146\n",
      "Iteration 8919, loss = 1.50756024\n",
      "Iteration 8920, loss = 1.50754903\n",
      "Iteration 8921, loss = 1.50753781\n",
      "Iteration 8922, loss = 1.50752660\n",
      "Iteration 8923, loss = 1.50751539\n",
      "Iteration 8924, loss = 1.50750418\n",
      "Iteration 8925, loss = 1.50749297\n",
      "Iteration 8926, loss = 1.50748177\n",
      "Iteration 8927, loss = 1.50747057\n",
      "Iteration 8928, loss = 1.50745937\n",
      "Iteration 8929, loss = 1.50744818\n",
      "Iteration 8930, loss = 1.50743698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8931, loss = 1.50742579\n",
      "Iteration 8932, loss = 1.50741460\n",
      "Iteration 8933, loss = 1.50740342\n",
      "Iteration 8934, loss = 1.50739224\n",
      "Iteration 8935, loss = 1.50738105\n",
      "Iteration 8936, loss = 1.50736988\n",
      "Iteration 8937, loss = 1.50735870\n",
      "Iteration 8938, loss = 1.50734753\n",
      "Iteration 8939, loss = 1.50733636\n",
      "Iteration 8940, loss = 1.50732519\n",
      "Iteration 8941, loss = 1.50731402\n",
      "Iteration 8942, loss = 1.50730286\n",
      "Iteration 8943, loss = 1.50729170\n",
      "Iteration 8944, loss = 1.50728054\n",
      "Iteration 8945, loss = 1.50726939\n",
      "Iteration 8946, loss = 1.50725823\n",
      "Iteration 8947, loss = 1.50724708\n",
      "Iteration 8948, loss = 1.50723593\n",
      "Iteration 8949, loss = 1.50722479\n",
      "Iteration 8950, loss = 1.50721365\n",
      "Iteration 8951, loss = 1.50720251\n",
      "Iteration 8952, loss = 1.50719137\n",
      "Iteration 8953, loss = 1.50718023\n",
      "Iteration 8954, loss = 1.50716910\n",
      "Iteration 8955, loss = 1.50715797\n",
      "Iteration 8956, loss = 1.50714684\n",
      "Iteration 8957, loss = 1.50713571\n",
      "Iteration 8958, loss = 1.50712459\n",
      "Iteration 8959, loss = 1.50711347\n",
      "Iteration 8960, loss = 1.50710235\n",
      "Iteration 8961, loss = 1.50709124\n",
      "Iteration 8962, loss = 1.50708012\n",
      "Iteration 8963, loss = 1.50706901\n",
      "Iteration 8964, loss = 1.50705790\n",
      "Iteration 8965, loss = 1.50704680\n",
      "Iteration 8966, loss = 1.50703569\n",
      "Iteration 8967, loss = 1.50702459\n",
      "Iteration 8968, loss = 1.50701349\n",
      "Iteration 8969, loss = 1.50700240\n",
      "Iteration 8970, loss = 1.50699130\n",
      "Iteration 8971, loss = 1.50698021\n",
      "Iteration 8972, loss = 1.50696913\n",
      "Iteration 8973, loss = 1.50695804\n",
      "Iteration 8974, loss = 1.50694696\n",
      "Iteration 8975, loss = 1.50693587\n",
      "Iteration 8976, loss = 1.50692480\n",
      "Iteration 8977, loss = 1.50691372\n",
      "Iteration 8978, loss = 1.50690265\n",
      "Iteration 8979, loss = 1.50689157\n",
      "Iteration 8980, loss = 1.50688051\n",
      "Iteration 8981, loss = 1.50686944\n",
      "Iteration 8982, loss = 1.50685838\n",
      "Iteration 8983, loss = 1.50684731\n",
      "Iteration 8984, loss = 1.50683625\n",
      "Iteration 8985, loss = 1.50682520\n",
      "Iteration 8986, loss = 1.50681414\n",
      "Iteration 8987, loss = 1.50680309\n",
      "Iteration 8988, loss = 1.50679204\n",
      "Iteration 8989, loss = 1.50678100\n",
      "Iteration 8990, loss = 1.50676995\n",
      "Iteration 8991, loss = 1.50675891\n",
      "Iteration 8992, loss = 1.50674787\n",
      "Iteration 8993, loss = 1.50673683\n",
      "Iteration 8994, loss = 1.50672580\n",
      "Iteration 8995, loss = 1.50671477\n",
      "Iteration 8996, loss = 1.50670374\n",
      "Iteration 8997, loss = 1.50669271\n",
      "Iteration 8998, loss = 1.50668169\n",
      "Iteration 8999, loss = 1.50667066\n",
      "Iteration 9000, loss = 1.50665965\n",
      "Iteration 9001, loss = 1.50664863\n",
      "Iteration 9002, loss = 1.50663761\n",
      "Iteration 9003, loss = 1.50662660\n",
      "Iteration 9004, loss = 1.50661559\n",
      "Iteration 9005, loss = 1.50660458\n",
      "Iteration 9006, loss = 1.50659358\n",
      "Iteration 9007, loss = 1.50658258\n",
      "Iteration 9008, loss = 1.50657158\n",
      "Iteration 9009, loss = 1.50656058\n",
      "Iteration 9010, loss = 1.50654958\n",
      "Iteration 9011, loss = 1.50653859\n",
      "Iteration 9012, loss = 1.50652760\n",
      "Iteration 9013, loss = 1.50651661\n",
      "Iteration 9014, loss = 1.50650563\n",
      "Iteration 9015, loss = 1.50649464\n",
      "Iteration 9016, loss = 1.50648366\n",
      "Iteration 9017, loss = 1.50647269\n",
      "Iteration 9018, loss = 1.50646171\n",
      "Iteration 9019, loss = 1.50645074\n",
      "Iteration 9020, loss = 1.50643977\n",
      "Iteration 9021, loss = 1.50642880\n",
      "Iteration 9022, loss = 1.50641783\n",
      "Iteration 9023, loss = 1.50640687\n",
      "Iteration 9024, loss = 1.50639591\n",
      "Iteration 9025, loss = 1.50638495\n",
      "Iteration 9026, loss = 1.50637399\n",
      "Iteration 9027, loss = 1.50636304\n",
      "Iteration 9028, loss = 1.50635209\n",
      "Iteration 9029, loss = 1.50634114\n",
      "Iteration 9030, loss = 1.50633019\n",
      "Iteration 9031, loss = 1.50631925\n",
      "Iteration 9032, loss = 1.50630831\n",
      "Iteration 9033, loss = 1.50629737\n",
      "Iteration 9034, loss = 1.50628643\n",
      "Iteration 9035, loss = 1.50627550\n",
      "Iteration 9036, loss = 1.50626456\n",
      "Iteration 9037, loss = 1.50625363\n",
      "Iteration 9038, loss = 1.50624271\n",
      "Iteration 9039, loss = 1.50623178\n",
      "Iteration 9040, loss = 1.50622086\n",
      "Iteration 9041, loss = 1.50620994\n",
      "Iteration 9042, loss = 1.50619902\n",
      "Iteration 9043, loss = 1.50618811\n",
      "Iteration 9044, loss = 1.50617720\n",
      "Iteration 9045, loss = 1.50616629\n",
      "Iteration 9046, loss = 1.50615538\n",
      "Iteration 9047, loss = 1.50614447\n",
      "Iteration 9048, loss = 1.50613357\n",
      "Iteration 9049, loss = 1.50612267\n",
      "Iteration 9050, loss = 1.50611177\n",
      "Iteration 9051, loss = 1.50610088\n",
      "Iteration 9052, loss = 1.50608998\n",
      "Iteration 9053, loss = 1.50607909\n",
      "Iteration 9054, loss = 1.50606820\n",
      "Iteration 9055, loss = 1.50605732\n",
      "Iteration 9056, loss = 1.50604643\n",
      "Iteration 9057, loss = 1.50603555\n",
      "Iteration 9058, loss = 1.50602467\n",
      "Iteration 9059, loss = 1.50601380\n",
      "Iteration 9060, loss = 1.50600292\n",
      "Iteration 9061, loss = 1.50599205\n",
      "Iteration 9062, loss = 1.50598118\n",
      "Iteration 9063, loss = 1.50597032\n",
      "Iteration 9064, loss = 1.50595945\n",
      "Iteration 9065, loss = 1.50594859\n",
      "Iteration 9066, loss = 1.50593773\n",
      "Iteration 9067, loss = 1.50592687\n",
      "Iteration 9068, loss = 1.50591602\n",
      "Iteration 9069, loss = 1.50590517\n",
      "Iteration 9070, loss = 1.50589432\n",
      "Iteration 9071, loss = 1.50588347\n",
      "Iteration 9072, loss = 1.50587262\n",
      "Iteration 9073, loss = 1.50586178\n",
      "Iteration 9074, loss = 1.50585094\n",
      "Iteration 9075, loss = 1.50584010\n",
      "Iteration 9076, loss = 1.50582927\n",
      "Iteration 9077, loss = 1.50581843\n",
      "Iteration 9078, loss = 1.50580760\n",
      "Iteration 9079, loss = 1.50579677\n",
      "Iteration 9080, loss = 1.50578595\n",
      "Iteration 9081, loss = 1.50577513\n",
      "Iteration 9082, loss = 1.50576430\n",
      "Iteration 9083, loss = 1.50575349\n",
      "Iteration 9084, loss = 1.50574267\n",
      "Iteration 9085, loss = 1.50573186\n",
      "Iteration 9086, loss = 1.50572104\n",
      "Iteration 9087, loss = 1.50571023\n",
      "Iteration 9088, loss = 1.50569943\n",
      "Iteration 9089, loss = 1.50568862\n",
      "Iteration 9090, loss = 1.50567782\n",
      "Iteration 9091, loss = 1.50566702\n",
      "Iteration 9092, loss = 1.50565622\n",
      "Iteration 9093, loss = 1.50564543\n",
      "Iteration 9094, loss = 1.50563464\n",
      "Iteration 9095, loss = 1.50562385\n",
      "Iteration 9096, loss = 1.50561306\n",
      "Iteration 9097, loss = 1.50560227\n",
      "Iteration 9098, loss = 1.50559149\n",
      "Iteration 9099, loss = 1.50558071\n",
      "Iteration 9100, loss = 1.50556993\n",
      "Iteration 9101, loss = 1.50555916\n",
      "Iteration 9102, loss = 1.50554838\n",
      "Iteration 9103, loss = 1.50553761\n",
      "Iteration 9104, loss = 1.50552684\n",
      "Iteration 9105, loss = 1.50551608\n",
      "Iteration 9106, loss = 1.50550531\n",
      "Iteration 9107, loss = 1.50549455\n",
      "Iteration 9108, loss = 1.50548379\n",
      "Iteration 9109, loss = 1.50547303\n",
      "Iteration 9110, loss = 1.50546228\n",
      "Iteration 9111, loss = 1.50545153\n",
      "Iteration 9112, loss = 1.50544078\n",
      "Iteration 9113, loss = 1.50543003\n",
      "Iteration 9114, loss = 1.50541929\n",
      "Iteration 9115, loss = 1.50540854\n",
      "Iteration 9116, loss = 1.50539780\n",
      "Iteration 9117, loss = 1.50538707\n",
      "Iteration 9118, loss = 1.50537633\n",
      "Iteration 9119, loss = 1.50536560\n",
      "Iteration 9120, loss = 1.50535487\n",
      "Iteration 9121, loss = 1.50534414\n",
      "Iteration 9122, loss = 1.50533341\n",
      "Iteration 9123, loss = 1.50532269\n",
      "Iteration 9124, loss = 1.50531197\n",
      "Iteration 9125, loss = 1.50530125\n",
      "Iteration 9126, loss = 1.50529053\n",
      "Iteration 9127, loss = 1.50527982\n",
      "Iteration 9128, loss = 1.50526910\n",
      "Iteration 9129, loss = 1.50525840\n",
      "Iteration 9130, loss = 1.50524769\n",
      "Iteration 9131, loss = 1.50523698\n",
      "Iteration 9132, loss = 1.50522628\n",
      "Iteration 9133, loss = 1.50521558\n",
      "Iteration 9134, loss = 1.50520488\n",
      "Iteration 9135, loss = 1.50519419\n",
      "Iteration 9136, loss = 1.50518349\n",
      "Iteration 9137, loss = 1.50517280\n",
      "Iteration 9138, loss = 1.50516212\n",
      "Iteration 9139, loss = 1.50515143\n",
      "Iteration 9140, loss = 1.50514075\n",
      "Iteration 9141, loss = 1.50513006\n",
      "Iteration 9142, loss = 1.50511939\n",
      "Iteration 9143, loss = 1.50510871\n",
      "Iteration 9144, loss = 1.50509804\n",
      "Iteration 9145, loss = 1.50508736\n",
      "Iteration 9146, loss = 1.50507669\n",
      "Iteration 9147, loss = 1.50506603\n",
      "Iteration 9148, loss = 1.50505536\n",
      "Iteration 9149, loss = 1.50504470\n",
      "Iteration 9150, loss = 1.50503404\n",
      "Iteration 9151, loss = 1.50502338\n",
      "Iteration 9152, loss = 1.50501273\n",
      "Iteration 9153, loss = 1.50500207\n",
      "Iteration 9154, loss = 1.50499142\n",
      "Iteration 9155, loss = 1.50498077\n",
      "Iteration 9156, loss = 1.50497013\n",
      "Iteration 9157, loss = 1.50495948\n",
      "Iteration 9158, loss = 1.50494884\n",
      "Iteration 9159, loss = 1.50493820\n",
      "Iteration 9160, loss = 1.50492757\n",
      "Iteration 9161, loss = 1.50491693\n",
      "Iteration 9162, loss = 1.50490630\n",
      "Iteration 9163, loss = 1.50489567\n",
      "Iteration 9164, loss = 1.50488504\n",
      "Iteration 9165, loss = 1.50487442\n",
      "Iteration 9166, loss = 1.50486380\n",
      "Iteration 9167, loss = 1.50485317\n",
      "Iteration 9168, loss = 1.50484256\n",
      "Iteration 9169, loss = 1.50483194\n",
      "Iteration 9170, loss = 1.50482133\n",
      "Iteration 9171, loss = 1.50481072\n",
      "Iteration 9172, loss = 1.50480011\n",
      "Iteration 9173, loss = 1.50478950\n",
      "Iteration 9174, loss = 1.50477890\n",
      "Iteration 9175, loss = 1.50476830\n",
      "Iteration 9176, loss = 1.50475770\n",
      "Iteration 9177, loss = 1.50474710\n",
      "Iteration 9178, loss = 1.50473650\n",
      "Iteration 9179, loss = 1.50472591\n",
      "Iteration 9180, loss = 1.50471532\n",
      "Iteration 9181, loss = 1.50470473\n",
      "Iteration 9182, loss = 1.50469415\n",
      "Iteration 9183, loss = 1.50468357\n",
      "Iteration 9184, loss = 1.50467298\n",
      "Iteration 9185, loss = 1.50466241\n",
      "Iteration 9186, loss = 1.50465183\n",
      "Iteration 9187, loss = 1.50464126\n",
      "Iteration 9188, loss = 1.50463068\n",
      "Iteration 9189, loss = 1.50462011\n",
      "Iteration 9190, loss = 1.50460955\n",
      "Iteration 9191, loss = 1.50459898\n",
      "Iteration 9192, loss = 1.50458842\n",
      "Iteration 9193, loss = 1.50457786\n",
      "Iteration 9194, loss = 1.50456730\n",
      "Iteration 9195, loss = 1.50455675\n",
      "Iteration 9196, loss = 1.50454619\n",
      "Iteration 9197, loss = 1.50453564\n",
      "Iteration 9198, loss = 1.50452510\n",
      "Iteration 9199, loss = 1.50451455\n",
      "Iteration 9200, loss = 1.50450401\n",
      "Iteration 9201, loss = 1.50449346\n",
      "Iteration 9202, loss = 1.50448292\n",
      "Iteration 9203, loss = 1.50447239\n",
      "Iteration 9204, loss = 1.50446185\n",
      "Iteration 9205, loss = 1.50445132\n",
      "Iteration 9206, loss = 1.50444079\n",
      "Iteration 9207, loss = 1.50443026\n",
      "Iteration 9208, loss = 1.50441974\n",
      "Iteration 9209, loss = 1.50440922\n",
      "Iteration 9210, loss = 1.50439869\n",
      "Iteration 9211, loss = 1.50438818\n",
      "Iteration 9212, loss = 1.50437766\n",
      "Iteration 9213, loss = 1.50436715\n",
      "Iteration 9214, loss = 1.50435663\n",
      "Iteration 9215, loss = 1.50434613\n",
      "Iteration 9216, loss = 1.50433562\n",
      "Iteration 9217, loss = 1.50432511\n",
      "Iteration 9218, loss = 1.50431461\n",
      "Iteration 9219, loss = 1.50430411\n",
      "Iteration 9220, loss = 1.50429361\n",
      "Iteration 9221, loss = 1.50428312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9222, loss = 1.50427262\n",
      "Iteration 9223, loss = 1.50426213\n",
      "Iteration 9224, loss = 1.50425165\n",
      "Iteration 9225, loss = 1.50424116\n",
      "Iteration 9226, loss = 1.50423068\n",
      "Iteration 9227, loss = 1.50422019\n",
      "Iteration 9228, loss = 1.50420971\n",
      "Iteration 9229, loss = 1.50419924\n",
      "Iteration 9230, loss = 1.50418876\n",
      "Iteration 9231, loss = 1.50417829\n",
      "Iteration 9232, loss = 1.50416782\n",
      "Iteration 9233, loss = 1.50415735\n",
      "Iteration 9234, loss = 1.50414689\n",
      "Iteration 9235, loss = 1.50413642\n",
      "Iteration 9236, loss = 1.50412596\n",
      "Iteration 9237, loss = 1.50411550\n",
      "Iteration 9238, loss = 1.50410505\n",
      "Iteration 9239, loss = 1.50409459\n",
      "Iteration 9240, loss = 1.50408414\n",
      "Iteration 9241, loss = 1.50407369\n",
      "Iteration 9242, loss = 1.50406324\n",
      "Iteration 9243, loss = 1.50405280\n",
      "Iteration 9244, loss = 1.50404235\n",
      "Iteration 9245, loss = 1.50403191\n",
      "Iteration 9246, loss = 1.50402148\n",
      "Iteration 9247, loss = 1.50401104\n",
      "Iteration 9248, loss = 1.50400061\n",
      "Iteration 9249, loss = 1.50399017\n",
      "Iteration 9250, loss = 1.50397975\n",
      "Iteration 9251, loss = 1.50396932\n",
      "Iteration 9252, loss = 1.50395889\n",
      "Iteration 9253, loss = 1.50394847\n",
      "Iteration 9254, loss = 1.50393805\n",
      "Iteration 9255, loss = 1.50392763\n",
      "Iteration 9256, loss = 1.50391722\n",
      "Iteration 9257, loss = 1.50390680\n",
      "Iteration 9258, loss = 1.50389639\n",
      "Iteration 9259, loss = 1.50388598\n",
      "Iteration 9260, loss = 1.50387558\n",
      "Iteration 9261, loss = 1.50386517\n",
      "Iteration 9262, loss = 1.50385477\n",
      "Iteration 9263, loss = 1.50384437\n",
      "Iteration 9264, loss = 1.50383398\n",
      "Iteration 9265, loss = 1.50382358\n",
      "Iteration 9266, loss = 1.50381319\n",
      "Iteration 9267, loss = 1.50380280\n",
      "Iteration 9268, loss = 1.50379241\n",
      "Iteration 9269, loss = 1.50378202\n",
      "Iteration 9270, loss = 1.50377164\n",
      "Iteration 9271, loss = 1.50376126\n",
      "Iteration 9272, loss = 1.50375088\n",
      "Iteration 9273, loss = 1.50374050\n",
      "Iteration 9274, loss = 1.50373013\n",
      "Iteration 9275, loss = 1.50371975\n",
      "Iteration 9276, loss = 1.50370938\n",
      "Iteration 9277, loss = 1.50369902\n",
      "Iteration 9278, loss = 1.50368865\n",
      "Iteration 9279, loss = 1.50367829\n",
      "Iteration 9280, loss = 1.50366792\n",
      "Iteration 9281, loss = 1.50365757\n",
      "Iteration 9282, loss = 1.50364721\n",
      "Iteration 9283, loss = 1.50363685\n",
      "Iteration 9284, loss = 1.50362650\n",
      "Iteration 9285, loss = 1.50361615\n",
      "Iteration 9286, loss = 1.50360580\n",
      "Iteration 9287, loss = 1.50359546\n",
      "Iteration 9288, loss = 1.50358512\n",
      "Iteration 9289, loss = 1.50357478\n",
      "Iteration 9290, loss = 1.50356444\n",
      "Iteration 9291, loss = 1.50355410\n",
      "Iteration 9292, loss = 1.50354377\n",
      "Iteration 9293, loss = 1.50353343\n",
      "Iteration 9294, loss = 1.50352310\n",
      "Iteration 9295, loss = 1.50351278\n",
      "Iteration 9296, loss = 1.50350245\n",
      "Iteration 9297, loss = 1.50349213\n",
      "Iteration 9298, loss = 1.50348181\n",
      "Iteration 9299, loss = 1.50347149\n",
      "Iteration 9300, loss = 1.50346117\n",
      "Iteration 9301, loss = 1.50345086\n",
      "Iteration 9302, loss = 1.50344055\n",
      "Iteration 9303, loss = 1.50343024\n",
      "Iteration 9304, loss = 1.50341993\n",
      "Iteration 9305, loss = 1.50340963\n",
      "Iteration 9306, loss = 1.50339932\n",
      "Iteration 9307, loss = 1.50338902\n",
      "Iteration 9308, loss = 1.50337872\n",
      "Iteration 9309, loss = 1.50336843\n",
      "Iteration 9310, loss = 1.50335813\n",
      "Iteration 9311, loss = 1.50334784\n",
      "Iteration 9312, loss = 1.50333755\n",
      "Iteration 9313, loss = 1.50332727\n",
      "Iteration 9314, loss = 1.50331698\n",
      "Iteration 9315, loss = 1.50330670\n",
      "Iteration 9316, loss = 1.50329642\n",
      "Iteration 9317, loss = 1.50328614\n",
      "Iteration 9318, loss = 1.50327586\n",
      "Iteration 9319, loss = 1.50326559\n",
      "Iteration 9320, loss = 1.50325532\n",
      "Iteration 9321, loss = 1.50324505\n",
      "Iteration 9322, loss = 1.50323478\n",
      "Iteration 9323, loss = 1.50322452\n",
      "Iteration 9324, loss = 1.50321425\n",
      "Iteration 9325, loss = 1.50320399\n",
      "Iteration 9326, loss = 1.50319374\n",
      "Iteration 9327, loss = 1.50318348\n",
      "Iteration 9328, loss = 1.50317323\n",
      "Iteration 9329, loss = 1.50316298\n",
      "Iteration 9330, loss = 1.50315273\n",
      "Iteration 9331, loss = 1.50314248\n",
      "Iteration 9332, loss = 1.50313223\n",
      "Iteration 9333, loss = 1.50312199\n",
      "Iteration 9334, loss = 1.50311175\n",
      "Iteration 9335, loss = 1.50310151\n",
      "Iteration 9336, loss = 1.50309128\n",
      "Iteration 9337, loss = 1.50308104\n",
      "Iteration 9338, loss = 1.50307081\n",
      "Iteration 9339, loss = 1.50306058\n",
      "Iteration 9340, loss = 1.50305036\n",
      "Iteration 9341, loss = 1.50304013\n",
      "Iteration 9342, loss = 1.50302991\n",
      "Iteration 9343, loss = 1.50301969\n",
      "Iteration 9344, loss = 1.50300947\n",
      "Iteration 9345, loss = 1.50299925\n",
      "Iteration 9346, loss = 1.50298904\n",
      "Iteration 9347, loss = 1.50297883\n",
      "Iteration 9348, loss = 1.50296862\n",
      "Iteration 9349, loss = 1.50295841\n",
      "Iteration 9350, loss = 1.50294821\n",
      "Iteration 9351, loss = 1.50293800\n",
      "Iteration 9352, loss = 1.50292780\n",
      "Iteration 9353, loss = 1.50291761\n",
      "Iteration 9354, loss = 1.50290741\n",
      "Iteration 9355, loss = 1.50289722\n",
      "Iteration 9356, loss = 1.50288702\n",
      "Iteration 9357, loss = 1.50287683\n",
      "Iteration 9358, loss = 1.50286665\n",
      "Iteration 9359, loss = 1.50285646\n",
      "Iteration 9360, loss = 1.50284628\n",
      "Iteration 9361, loss = 1.50283610\n",
      "Iteration 9362, loss = 1.50282592\n",
      "Iteration 9363, loss = 1.50281574\n",
      "Iteration 9364, loss = 1.50280557\n",
      "Iteration 9365, loss = 1.50279540\n",
      "Iteration 9366, loss = 1.50278523\n",
      "Iteration 9367, loss = 1.50277506\n",
      "Iteration 9368, loss = 1.50276490\n",
      "Iteration 9369, loss = 1.50275473\n",
      "Iteration 9370, loss = 1.50274457\n",
      "Iteration 9371, loss = 1.50273441\n",
      "Iteration 9372, loss = 1.50272426\n",
      "Iteration 9373, loss = 1.50271410\n",
      "Iteration 9374, loss = 1.50270395\n",
      "Iteration 9375, loss = 1.50269380\n",
      "Iteration 9376, loss = 1.50268365\n",
      "Iteration 9377, loss = 1.50267351\n",
      "Iteration 9378, loss = 1.50266336\n",
      "Iteration 9379, loss = 1.50265322\n",
      "Iteration 9380, loss = 1.50264308\n",
      "Iteration 9381, loss = 1.50263295\n",
      "Iteration 9382, loss = 1.50262281\n",
      "Iteration 9383, loss = 1.50261268\n",
      "Iteration 9384, loss = 1.50260255\n",
      "Iteration 9385, loss = 1.50259242\n",
      "Iteration 9386, loss = 1.50258230\n",
      "Iteration 9387, loss = 1.50257217\n",
      "Iteration 9388, loss = 1.50256205\n",
      "Iteration 9389, loss = 1.50255193\n",
      "Iteration 9390, loss = 1.50254182\n",
      "Iteration 9391, loss = 1.50253170\n",
      "Iteration 9392, loss = 1.50252159\n",
      "Iteration 9393, loss = 1.50251148\n",
      "Iteration 9394, loss = 1.50250137\n",
      "Iteration 9395, loss = 1.50249126\n",
      "Iteration 9396, loss = 1.50248116\n",
      "Iteration 9397, loss = 1.50247106\n",
      "Iteration 9398, loss = 1.50246096\n",
      "Iteration 9399, loss = 1.50245086\n",
      "Iteration 9400, loss = 1.50244076\n",
      "Iteration 9401, loss = 1.50243067\n",
      "Iteration 9402, loss = 1.50242058\n",
      "Iteration 9403, loss = 1.50241049\n",
      "Iteration 9404, loss = 1.50240041\n",
      "Iteration 9405, loss = 1.50239032\n",
      "Iteration 9406, loss = 1.50238024\n",
      "Iteration 9407, loss = 1.50237016\n",
      "Iteration 9408, loss = 1.50236008\n",
      "Iteration 9409, loss = 1.50235001\n",
      "Iteration 9410, loss = 1.50233993\n",
      "Iteration 9411, loss = 1.50232986\n",
      "Iteration 9412, loss = 1.50231979\n",
      "Iteration 9413, loss = 1.50230972\n",
      "Iteration 9414, loss = 1.50229966\n",
      "Iteration 9415, loss = 1.50228960\n",
      "Iteration 9416, loss = 1.50227954\n",
      "Iteration 9417, loss = 1.50226948\n",
      "Iteration 9418, loss = 1.50225942\n",
      "Iteration 9419, loss = 1.50224937\n",
      "Iteration 9420, loss = 1.50223932\n",
      "Iteration 9421, loss = 1.50222927\n",
      "Iteration 9422, loss = 1.50221922\n",
      "Iteration 9423, loss = 1.50220917\n",
      "Iteration 9424, loss = 1.50219913\n",
      "Iteration 9425, loss = 1.50218909\n",
      "Iteration 9426, loss = 1.50217905\n",
      "Iteration 9427, loss = 1.50216901\n",
      "Iteration 9428, loss = 1.50215898\n",
      "Iteration 9429, loss = 1.50214895\n",
      "Iteration 9430, loss = 1.50213892\n",
      "Iteration 9431, loss = 1.50212889\n",
      "Iteration 9432, loss = 1.50211886\n",
      "Iteration 9433, loss = 1.50210884\n",
      "Iteration 9434, loss = 1.50209882\n",
      "Iteration 9435, loss = 1.50208880\n",
      "Iteration 9436, loss = 1.50207878\n",
      "Iteration 9437, loss = 1.50206877\n",
      "Iteration 9438, loss = 1.50205875\n",
      "Iteration 9439, loss = 1.50204874\n",
      "Iteration 9440, loss = 1.50203873\n",
      "Iteration 9441, loss = 1.50202873\n",
      "Iteration 9442, loss = 1.50201872\n",
      "Iteration 9443, loss = 1.50200872\n",
      "Iteration 9444, loss = 1.50199872\n",
      "Iteration 9445, loss = 1.50198872\n",
      "Iteration 9446, loss = 1.50197873\n",
      "Iteration 9447, loss = 1.50196873\n",
      "Iteration 9448, loss = 1.50195874\n",
      "Iteration 9449, loss = 1.50194875\n",
      "Iteration 9450, loss = 1.50193877\n",
      "Iteration 9451, loss = 1.50192878\n",
      "Iteration 9452, loss = 1.50191880\n",
      "Iteration 9453, loss = 1.50190882\n",
      "Iteration 9454, loss = 1.50189884\n",
      "Iteration 9455, loss = 1.50188886\n",
      "Iteration 9456, loss = 1.50187889\n",
      "Iteration 9457, loss = 1.50186892\n",
      "Iteration 9458, loss = 1.50185895\n",
      "Iteration 9459, loss = 1.50184898\n",
      "Iteration 9460, loss = 1.50183901\n",
      "Iteration 9461, loss = 1.50182905\n",
      "Iteration 9462, loss = 1.50181909\n",
      "Iteration 9463, loss = 1.50180913\n",
      "Iteration 9464, loss = 1.50179917\n",
      "Iteration 9465, loss = 1.50178922\n",
      "Iteration 9466, loss = 1.50177926\n",
      "Iteration 9467, loss = 1.50176931\n",
      "Iteration 9468, loss = 1.50175936\n",
      "Iteration 9469, loss = 1.50174942\n",
      "Iteration 9470, loss = 1.50173947\n",
      "Iteration 9471, loss = 1.50172953\n",
      "Iteration 9472, loss = 1.50171959\n",
      "Iteration 9473, loss = 1.50170965\n",
      "Iteration 9474, loss = 1.50169972\n",
      "Iteration 9475, loss = 1.50168978\n",
      "Iteration 9476, loss = 1.50167985\n",
      "Iteration 9477, loss = 1.50166992\n",
      "Iteration 9478, loss = 1.50166000\n",
      "Iteration 9479, loss = 1.50165007\n",
      "Iteration 9480, loss = 1.50164015\n",
      "Iteration 9481, loss = 1.50163023\n",
      "Iteration 9482, loss = 1.50162031\n",
      "Iteration 9483, loss = 1.50161039\n",
      "Iteration 9484, loss = 1.50160048\n",
      "Iteration 9485, loss = 1.50159056\n",
      "Iteration 9486, loss = 1.50158065\n",
      "Iteration 9487, loss = 1.50157075\n",
      "Iteration 9488, loss = 1.50156084\n",
      "Iteration 9489, loss = 1.50155094\n",
      "Iteration 9490, loss = 1.50154103\n",
      "Iteration 9491, loss = 1.50153114\n",
      "Iteration 9492, loss = 1.50152124\n",
      "Iteration 9493, loss = 1.50151134\n",
      "Iteration 9494, loss = 1.50150145\n",
      "Iteration 9495, loss = 1.50149156\n",
      "Iteration 9496, loss = 1.50148167\n",
      "Iteration 9497, loss = 1.50147178\n",
      "Iteration 9498, loss = 1.50146190\n",
      "Iteration 9499, loss = 1.50145201\n",
      "Iteration 9500, loss = 1.50144213\n",
      "Iteration 9501, loss = 1.50143226\n",
      "Iteration 9502, loss = 1.50142238\n",
      "Iteration 9503, loss = 1.50141250\n",
      "Iteration 9504, loss = 1.50140263\n",
      "Iteration 9505, loss = 1.50139276\n",
      "Iteration 9506, loss = 1.50138289\n",
      "Iteration 9507, loss = 1.50137303\n",
      "Iteration 9508, loss = 1.50136317\n",
      "Iteration 9509, loss = 1.50135330\n",
      "Iteration 9510, loss = 1.50134344\n",
      "Iteration 9511, loss = 1.50133359\n",
      "Iteration 9512, loss = 1.50132373\n",
      "Iteration 9513, loss = 1.50131388\n",
      "Iteration 9514, loss = 1.50130403\n",
      "Iteration 9515, loss = 1.50129418\n",
      "Iteration 9516, loss = 1.50128433\n",
      "Iteration 9517, loss = 1.50127449\n",
      "Iteration 9518, loss = 1.50126464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9519, loss = 1.50125480\n",
      "Iteration 9520, loss = 1.50124497\n",
      "Iteration 9521, loss = 1.50123513\n",
      "Iteration 9522, loss = 1.50122530\n",
      "Iteration 9523, loss = 1.50121546\n",
      "Iteration 9524, loss = 1.50120563\n",
      "Iteration 9525, loss = 1.50119581\n",
      "Iteration 9526, loss = 1.50118598\n",
      "Iteration 9527, loss = 1.50117616\n",
      "Iteration 9528, loss = 1.50116633\n",
      "Iteration 9529, loss = 1.50115651\n",
      "Iteration 9530, loss = 1.50114670\n",
      "Iteration 9531, loss = 1.50113688\n",
      "Iteration 9532, loss = 1.50112707\n",
      "Iteration 9533, loss = 1.50111726\n",
      "Iteration 9534, loss = 1.50110745\n",
      "Iteration 9535, loss = 1.50109764\n",
      "Iteration 9536, loss = 1.50108784\n",
      "Iteration 9537, loss = 1.50107803\n",
      "Iteration 9538, loss = 1.50106823\n",
      "Iteration 9539, loss = 1.50105844\n",
      "Iteration 9540, loss = 1.50104864\n",
      "Iteration 9541, loss = 1.50103884\n",
      "Iteration 9542, loss = 1.50102905\n",
      "Iteration 9543, loss = 1.50101926\n",
      "Iteration 9544, loss = 1.50100947\n",
      "Iteration 9545, loss = 1.50099969\n",
      "Iteration 9546, loss = 1.50098990\n",
      "Iteration 9547, loss = 1.50098012\n",
      "Iteration 9548, loss = 1.50097034\n",
      "Iteration 9549, loss = 1.50096056\n",
      "Iteration 9550, loss = 1.50095079\n",
      "Iteration 9551, loss = 1.50094102\n",
      "Iteration 9552, loss = 1.50093124\n",
      "Iteration 9553, loss = 1.50092147\n",
      "Iteration 9554, loss = 1.50091171\n",
      "Iteration 9555, loss = 1.50090194\n",
      "Iteration 9556, loss = 1.50089218\n",
      "Iteration 9557, loss = 1.50088242\n",
      "Iteration 9558, loss = 1.50087266\n",
      "Iteration 9559, loss = 1.50086290\n",
      "Iteration 9560, loss = 1.50085315\n",
      "Iteration 9561, loss = 1.50084340\n",
      "Iteration 9562, loss = 1.50083364\n",
      "Iteration 9563, loss = 1.50082390\n",
      "Iteration 9564, loss = 1.50081415\n",
      "Iteration 9565, loss = 1.50080441\n",
      "Iteration 9566, loss = 1.50079466\n",
      "Iteration 9567, loss = 1.50078492\n",
      "Iteration 9568, loss = 1.50077518\n",
      "Iteration 9569, loss = 1.50076545\n",
      "Iteration 9570, loss = 1.50075571\n",
      "Iteration 9571, loss = 1.50074598\n",
      "Iteration 9572, loss = 1.50073625\n",
      "Iteration 9573, loss = 1.50072653\n",
      "Iteration 9574, loss = 1.50071680\n",
      "Iteration 9575, loss = 1.50070708\n",
      "Iteration 9576, loss = 1.50069735\n",
      "Iteration 9577, loss = 1.50068763\n",
      "Iteration 9578, loss = 1.50067792\n",
      "Iteration 9579, loss = 1.50066820\n",
      "Iteration 9580, loss = 1.50065849\n",
      "Iteration 9581, loss = 1.50064878\n",
      "Iteration 9582, loss = 1.50063907\n",
      "Iteration 9583, loss = 1.50062936\n",
      "Iteration 9584, loss = 1.50061966\n",
      "Iteration 9585, loss = 1.50060995\n",
      "Iteration 9586, loss = 1.50060025\n",
      "Iteration 9587, loss = 1.50059055\n",
      "Iteration 9588, loss = 1.50058086\n",
      "Iteration 9589, loss = 1.50057116\n",
      "Iteration 9590, loss = 1.50056147\n",
      "Iteration 9591, loss = 1.50055178\n",
      "Iteration 9592, loss = 1.50054209\n",
      "Iteration 9593, loss = 1.50053240\n",
      "Iteration 9594, loss = 1.50052272\n",
      "Iteration 9595, loss = 1.50051304\n",
      "Iteration 9596, loss = 1.50050336\n",
      "Iteration 9597, loss = 1.50049368\n",
      "Iteration 9598, loss = 1.50048400\n",
      "Iteration 9599, loss = 1.50047433\n",
      "Iteration 9600, loss = 1.50046466\n",
      "Iteration 9601, loss = 1.50045499\n",
      "Iteration 9602, loss = 1.50044532\n",
      "Iteration 9603, loss = 1.50043565\n",
      "Iteration 9604, loss = 1.50042599\n",
      "Iteration 9605, loss = 1.50041633\n",
      "Iteration 9606, loss = 1.50040667\n",
      "Iteration 9607, loss = 1.50039701\n",
      "Iteration 9608, loss = 1.50038735\n",
      "Iteration 9609, loss = 1.50037770\n",
      "Iteration 9610, loss = 1.50036805\n",
      "Iteration 9611, loss = 1.50035840\n",
      "Iteration 9612, loss = 1.50034875\n",
      "Iteration 9613, loss = 1.50033911\n",
      "Iteration 9614, loss = 1.50032946\n",
      "Iteration 9615, loss = 1.50031982\n",
      "Iteration 9616, loss = 1.50031018\n",
      "Iteration 9617, loss = 1.50030055\n",
      "Iteration 9618, loss = 1.50029091\n",
      "Iteration 9619, loss = 1.50028128\n",
      "Iteration 9620, loss = 1.50027165\n",
      "Iteration 9621, loss = 1.50026202\n",
      "Iteration 9622, loss = 1.50025239\n",
      "Iteration 9623, loss = 1.50024277\n",
      "Iteration 9624, loss = 1.50023314\n",
      "Iteration 9625, loss = 1.50022352\n",
      "Iteration 9626, loss = 1.50021390\n",
      "Iteration 9627, loss = 1.50020429\n",
      "Iteration 9628, loss = 1.50019467\n",
      "Iteration 9629, loss = 1.50018506\n",
      "Iteration 9630, loss = 1.50017545\n",
      "Iteration 9631, loss = 1.50016584\n",
      "Iteration 9632, loss = 1.50015623\n",
      "Iteration 9633, loss = 1.50014663\n",
      "Iteration 9634, loss = 1.50013703\n",
      "Iteration 9635, loss = 1.50012742\n",
      "Iteration 9636, loss = 1.50011783\n",
      "Iteration 9637, loss = 1.50010823\n",
      "Iteration 9638, loss = 1.50009864\n",
      "Iteration 9639, loss = 1.50008904\n",
      "Iteration 9640, loss = 1.50007945\n",
      "Iteration 9641, loss = 1.50006986\n",
      "Iteration 9642, loss = 1.50006028\n",
      "Iteration 9643, loss = 1.50005069\n",
      "Iteration 9644, loss = 1.50004111\n",
      "Iteration 9645, loss = 1.50003153\n",
      "Iteration 9646, loss = 1.50002195\n",
      "Iteration 9647, loss = 1.50001238\n",
      "Iteration 9648, loss = 1.50000280\n",
      "Iteration 9649, loss = 1.49999323\n",
      "Iteration 9650, loss = 1.49998366\n",
      "Iteration 9651, loss = 1.49997409\n",
      "Iteration 9652, loss = 1.49996453\n",
      "Iteration 9653, loss = 1.49995496\n",
      "Iteration 9654, loss = 1.49994540\n",
      "Iteration 9655, loss = 1.49993584\n",
      "Iteration 9656, loss = 1.49992628\n",
      "Iteration 9657, loss = 1.49991673\n",
      "Iteration 9658, loss = 1.49990717\n",
      "Iteration 9659, loss = 1.49989762\n",
      "Iteration 9660, loss = 1.49988807\n",
      "Iteration 9661, loss = 1.49987852\n",
      "Iteration 9662, loss = 1.49986898\n",
      "Iteration 9663, loss = 1.49985944\n",
      "Iteration 9664, loss = 1.49984989\n",
      "Iteration 9665, loss = 1.49984035\n",
      "Iteration 9666, loss = 1.49983082\n",
      "Iteration 9667, loss = 1.49982128\n",
      "Iteration 9668, loss = 1.49981175\n",
      "Iteration 9669, loss = 1.49980222\n",
      "Iteration 9670, loss = 1.49979269\n",
      "Iteration 9671, loss = 1.49978316\n",
      "Iteration 9672, loss = 1.49977363\n",
      "Iteration 9673, loss = 1.49976411\n",
      "Iteration 9674, loss = 1.49975459\n",
      "Iteration 9675, loss = 1.49974507\n",
      "Iteration 9676, loss = 1.49973555\n",
      "Iteration 9677, loss = 1.49972603\n",
      "Iteration 9678, loss = 1.49971652\n",
      "Iteration 9679, loss = 1.49970701\n",
      "Iteration 9680, loss = 1.49969750\n",
      "Iteration 9681, loss = 1.49968799\n",
      "Iteration 9682, loss = 1.49967849\n",
      "Iteration 9683, loss = 1.49966898\n",
      "Iteration 9684, loss = 1.49965948\n",
      "Iteration 9685, loss = 1.49964998\n",
      "Iteration 9686, loss = 1.49964048\n",
      "Iteration 9687, loss = 1.49963099\n",
      "Iteration 9688, loss = 1.49962150\n",
      "Iteration 9689, loss = 1.49961200\n",
      "Iteration 9690, loss = 1.49960252\n",
      "Iteration 9691, loss = 1.49959303\n",
      "Iteration 9692, loss = 1.49958354\n",
      "Iteration 9693, loss = 1.49957406\n",
      "Iteration 9694, loss = 1.49956458\n",
      "Iteration 9695, loss = 1.49955510\n",
      "Iteration 9696, loss = 1.49954562\n",
      "Iteration 9697, loss = 1.49953614\n",
      "Iteration 9698, loss = 1.49952667\n",
      "Iteration 9699, loss = 1.49951720\n",
      "Iteration 9700, loss = 1.49950773\n",
      "Iteration 9701, loss = 1.49949826\n",
      "Iteration 9702, loss = 1.49948880\n",
      "Iteration 9703, loss = 1.49947933\n",
      "Iteration 9704, loss = 1.49946987\n",
      "Iteration 9705, loss = 1.49946041\n",
      "Iteration 9706, loss = 1.49945096\n",
      "Iteration 9707, loss = 1.49944150\n",
      "Iteration 9708, loss = 1.49943205\n",
      "Iteration 9709, loss = 1.49942260\n",
      "Iteration 9710, loss = 1.49941315\n",
      "Iteration 9711, loss = 1.49940370\n",
      "Iteration 9712, loss = 1.49939425\n",
      "Iteration 9713, loss = 1.49938481\n",
      "Iteration 9714, loss = 1.49937537\n",
      "Iteration 9715, loss = 1.49936593\n",
      "Iteration 9716, loss = 1.49935649\n",
      "Iteration 9717, loss = 1.49934705\n",
      "Iteration 9718, loss = 1.49933762\n",
      "Iteration 9719, loss = 1.49932819\n",
      "Iteration 9720, loss = 1.49931876\n",
      "Iteration 9721, loss = 1.49930933\n",
      "Iteration 9722, loss = 1.49929991\n",
      "Iteration 9723, loss = 1.49929048\n",
      "Iteration 9724, loss = 1.49928106\n",
      "Iteration 9725, loss = 1.49927164\n",
      "Iteration 9726, loss = 1.49926222\n",
      "Iteration 9727, loss = 1.49925281\n",
      "Iteration 9728, loss = 1.49924339\n",
      "Iteration 9729, loss = 1.49923398\n",
      "Iteration 9730, loss = 1.49922457\n",
      "Iteration 9731, loss = 1.49921516\n",
      "Iteration 9732, loss = 1.49920576\n",
      "Iteration 9733, loss = 1.49919636\n",
      "Iteration 9734, loss = 1.49918695\n",
      "Iteration 9735, loss = 1.49917755\n",
      "Iteration 9736, loss = 1.49916816\n",
      "Iteration 9737, loss = 1.49915876\n",
      "Iteration 9738, loss = 1.49914937\n",
      "Iteration 9739, loss = 1.49913997\n",
      "Iteration 9740, loss = 1.49913058\n",
      "Iteration 9741, loss = 1.49912120\n",
      "Iteration 9742, loss = 1.49911181\n",
      "Iteration 9743, loss = 1.49910243\n",
      "Iteration 9744, loss = 1.49909304\n",
      "Iteration 9745, loss = 1.49908366\n",
      "Iteration 9746, loss = 1.49907428\n",
      "Iteration 9747, loss = 1.49906491\n",
      "Iteration 9748, loss = 1.49905553\n",
      "Iteration 9749, loss = 1.49904616\n",
      "Iteration 9750, loss = 1.49903679\n",
      "Iteration 9751, loss = 1.49902742\n",
      "Iteration 9752, loss = 1.49901806\n",
      "Iteration 9753, loss = 1.49900869\n",
      "Iteration 9754, loss = 1.49899933\n",
      "Iteration 9755, loss = 1.49898997\n",
      "Iteration 9756, loss = 1.49898061\n",
      "Iteration 9757, loss = 1.49897125\n",
      "Iteration 9758, loss = 1.49896190\n",
      "Iteration 9759, loss = 1.49895255\n",
      "Iteration 9760, loss = 1.49894320\n",
      "Iteration 9761, loss = 1.49893385\n",
      "Iteration 9762, loss = 1.49892450\n",
      "Iteration 9763, loss = 1.49891516\n",
      "Iteration 9764, loss = 1.49890581\n",
      "Iteration 9765, loss = 1.49889647\n",
      "Iteration 9766, loss = 1.49888713\n",
      "Iteration 9767, loss = 1.49887780\n",
      "Iteration 9768, loss = 1.49886846\n",
      "Iteration 9769, loss = 1.49885913\n",
      "Iteration 9770, loss = 1.49884980\n",
      "Iteration 9771, loss = 1.49884047\n",
      "Iteration 9772, loss = 1.49883114\n",
      "Iteration 9773, loss = 1.49882182\n",
      "Iteration 9774, loss = 1.49881249\n",
      "Iteration 9775, loss = 1.49880317\n",
      "Iteration 9776, loss = 1.49879385\n",
      "Iteration 9777, loss = 1.49878454\n",
      "Iteration 9778, loss = 1.49877522\n",
      "Iteration 9779, loss = 1.49876591\n",
      "Iteration 9780, loss = 1.49875659\n",
      "Iteration 9781, loss = 1.49874729\n",
      "Iteration 9782, loss = 1.49873798\n",
      "Iteration 9783, loss = 1.49872867\n",
      "Iteration 9784, loss = 1.49871937\n",
      "Iteration 9785, loss = 1.49871007\n",
      "Iteration 9786, loss = 1.49870077\n",
      "Iteration 9787, loss = 1.49869147\n",
      "Iteration 9788, loss = 1.49868217\n",
      "Iteration 9789, loss = 1.49867288\n",
      "Iteration 9790, loss = 1.49866359\n",
      "Iteration 9791, loss = 1.49865430\n",
      "Iteration 9792, loss = 1.49864501\n",
      "Iteration 9793, loss = 1.49863572\n",
      "Iteration 9794, loss = 1.49862644\n",
      "Iteration 9795, loss = 1.49861716\n",
      "Iteration 9796, loss = 1.49860788\n",
      "Iteration 9797, loss = 1.49859860\n",
      "Iteration 9798, loss = 1.49858932\n",
      "Iteration 9799, loss = 1.49858005\n",
      "Iteration 9800, loss = 1.49857077\n",
      "Iteration 9801, loss = 1.49856150\n",
      "Iteration 9802, loss = 1.49855223\n",
      "Iteration 9803, loss = 1.49854297\n",
      "Iteration 9804, loss = 1.49853370\n",
      "Iteration 9805, loss = 1.49852444\n",
      "Iteration 9806, loss = 1.49851518\n",
      "Iteration 9807, loss = 1.49850592\n",
      "Iteration 9808, loss = 1.49849666\n",
      "Iteration 9809, loss = 1.49848741\n",
      "Iteration 9810, loss = 1.49847815\n",
      "Iteration 9811, loss = 1.49846890\n",
      "Iteration 9812, loss = 1.49845965\n",
      "Iteration 9813, loss = 1.49845041\n",
      "Iteration 9814, loss = 1.49844116\n",
      "Iteration 9815, loss = 1.49843192\n",
      "Iteration 9816, loss = 1.49842268\n",
      "Iteration 9817, loss = 1.49841344\n",
      "Iteration 9818, loss = 1.49840420\n",
      "Iteration 9819, loss = 1.49839496\n",
      "Iteration 9820, loss = 1.49838573\n",
      "Iteration 9821, loss = 1.49837650\n",
      "Iteration 9822, loss = 1.49836727\n",
      "Iteration 9823, loss = 1.49835804\n",
      "Iteration 9824, loss = 1.49834881\n",
      "Iteration 9825, loss = 1.49833959\n",
      "Iteration 9826, loss = 1.49833037\n",
      "Iteration 9827, loss = 1.49832115\n",
      "Iteration 9828, loss = 1.49831193\n",
      "Iteration 9829, loss = 1.49830271\n",
      "Iteration 9830, loss = 1.49829350\n",
      "Iteration 9831, loss = 1.49828428\n",
      "Iteration 9832, loss = 1.49827507\n",
      "Iteration 9833, loss = 1.49826586\n",
      "Iteration 9834, loss = 1.49825666\n",
      "Iteration 9835, loss = 1.49824745\n",
      "Iteration 9836, loss = 1.49823825\n",
      "Iteration 9837, loss = 1.49822905\n",
      "Iteration 9838, loss = 1.49821985\n",
      "Iteration 9839, loss = 1.49821065\n",
      "Iteration 9840, loss = 1.49820146\n",
      "Iteration 9841, loss = 1.49819226\n",
      "Iteration 9842, loss = 1.49818307\n",
      "Iteration 9843, loss = 1.49817388\n",
      "Iteration 9844, loss = 1.49816469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9845, loss = 1.49815551\n",
      "Iteration 9846, loss = 1.49814632\n",
      "Iteration 9847, loss = 1.49813714\n",
      "Iteration 9848, loss = 1.49812796\n",
      "Iteration 9849, loss = 1.49811878\n",
      "Iteration 9850, loss = 1.49810961\n",
      "Iteration 9851, loss = 1.49810043\n",
      "Iteration 9852, loss = 1.49809126\n",
      "Iteration 9853, loss = 1.49808209\n",
      "Iteration 9854, loss = 1.49807292\n",
      "Iteration 9855, loss = 1.49806376\n",
      "Iteration 9856, loss = 1.49805459\n",
      "Iteration 9857, loss = 1.49804543\n",
      "Iteration 9858, loss = 1.49803627\n",
      "Iteration 9859, loss = 1.49802711\n",
      "Iteration 9860, loss = 1.49801795\n",
      "Iteration 9861, loss = 1.49800880\n",
      "Iteration 9862, loss = 1.49799964\n",
      "Iteration 9863, loss = 1.49799049\n",
      "Iteration 9864, loss = 1.49798134\n",
      "Iteration 9865, loss = 1.49797219\n",
      "Iteration 9866, loss = 1.49796305\n",
      "Iteration 9867, loss = 1.49795390\n",
      "Iteration 9868, loss = 1.49794476\n",
      "Iteration 9869, loss = 1.49793562\n",
      "Iteration 9870, loss = 1.49792648\n",
      "Iteration 9871, loss = 1.49791735\n",
      "Iteration 9872, loss = 1.49790821\n",
      "Iteration 9873, loss = 1.49789908\n",
      "Iteration 9874, loss = 1.49788995\n",
      "Iteration 9875, loss = 1.49788082\n",
      "Iteration 9876, loss = 1.49787170\n",
      "Iteration 9877, loss = 1.49786257\n",
      "Iteration 9878, loss = 1.49785345\n",
      "Iteration 9879, loss = 1.49784433\n",
      "Iteration 9880, loss = 1.49783521\n",
      "Iteration 9881, loss = 1.49782609\n",
      "Iteration 9882, loss = 1.49781698\n",
      "Iteration 9883, loss = 1.49780786\n",
      "Iteration 9884, loss = 1.49779875\n",
      "Iteration 9885, loss = 1.49778964\n",
      "Iteration 9886, loss = 1.49778053\n",
      "Iteration 9887, loss = 1.49777143\n",
      "Iteration 9888, loss = 1.49776232\n",
      "Iteration 9889, loss = 1.49775322\n",
      "Iteration 9890, loss = 1.49774412\n",
      "Iteration 9891, loss = 1.49773502\n",
      "Iteration 9892, loss = 1.49772593\n",
      "Iteration 9893, loss = 1.49771683\n",
      "Iteration 9894, loss = 1.49770774\n",
      "Iteration 9895, loss = 1.49769865\n",
      "Iteration 9896, loss = 1.49768956\n",
      "Iteration 9897, loss = 1.49768047\n",
      "Iteration 9898, loss = 1.49767139\n",
      "Iteration 9899, loss = 1.49766231\n",
      "Iteration 9900, loss = 1.49765322\n",
      "Iteration 9901, loss = 1.49764415\n",
      "Iteration 9902, loss = 1.49763507\n",
      "Iteration 9903, loss = 1.49762599\n",
      "Iteration 9904, loss = 1.49761692\n",
      "Iteration 9905, loss = 1.49760785\n",
      "Iteration 9906, loss = 1.49759878\n",
      "Iteration 9907, loss = 1.49758971\n",
      "Iteration 9908, loss = 1.49758064\n",
      "Iteration 9909, loss = 1.49757158\n",
      "Iteration 9910, loss = 1.49756252\n",
      "Iteration 9911, loss = 1.49755346\n",
      "Iteration 9912, loss = 1.49754440\n",
      "Iteration 9913, loss = 1.49753534\n",
      "Iteration 9914, loss = 1.49752629\n",
      "Iteration 9915, loss = 1.49751723\n",
      "Iteration 9916, loss = 1.49750818\n",
      "Iteration 9917, loss = 1.49749913\n",
      "Iteration 9918, loss = 1.49749008\n",
      "Iteration 9919, loss = 1.49748104\n",
      "Iteration 9920, loss = 1.49747200\n",
      "Iteration 9921, loss = 1.49746295\n",
      "Iteration 9922, loss = 1.49745391\n",
      "Iteration 9923, loss = 1.49744488\n",
      "Iteration 9924, loss = 1.49743584\n",
      "Iteration 9925, loss = 1.49742681\n",
      "Iteration 9926, loss = 1.49741777\n",
      "Iteration 9927, loss = 1.49740874\n",
      "Iteration 9928, loss = 1.49739971\n",
      "Iteration 9929, loss = 1.49739069\n",
      "Iteration 9930, loss = 1.49738166\n",
      "Iteration 9931, loss = 1.49737264\n",
      "Iteration 9932, loss = 1.49736362\n",
      "Iteration 9933, loss = 1.49735460\n",
      "Iteration 9934, loss = 1.49734558\n",
      "Iteration 9935, loss = 1.49733657\n",
      "Iteration 9936, loss = 1.49732755\n",
      "Iteration 9937, loss = 1.49731854\n",
      "Iteration 9938, loss = 1.49730953\n",
      "Iteration 9939, loss = 1.49730052\n",
      "Iteration 9940, loss = 1.49729152\n",
      "Iteration 9941, loss = 1.49728251\n",
      "Iteration 9942, loss = 1.49727351\n",
      "Iteration 9943, loss = 1.49726451\n",
      "Iteration 9944, loss = 1.49725551\n",
      "Iteration 9945, loss = 1.49724651\n",
      "Iteration 9946, loss = 1.49723752\n",
      "Iteration 9947, loss = 1.49722853\n",
      "Iteration 9948, loss = 1.49721953\n",
      "Iteration 9949, loss = 1.49721054\n",
      "Iteration 9950, loss = 1.49720156\n",
      "Iteration 9951, loss = 1.49719257\n",
      "Iteration 9952, loss = 1.49718359\n",
      "Iteration 9953, loss = 1.49717461\n",
      "Iteration 9954, loss = 1.49716563\n",
      "Iteration 9955, loss = 1.49715665\n",
      "Iteration 9956, loss = 1.49714767\n",
      "Iteration 9957, loss = 1.49713870\n",
      "Iteration 9958, loss = 1.49712972\n",
      "Iteration 9959, loss = 1.49712075\n",
      "Iteration 9960, loss = 1.49711178\n",
      "Iteration 9961, loss = 1.49710282\n",
      "Iteration 9962, loss = 1.49709385\n",
      "Iteration 9963, loss = 1.49708489\n",
      "Iteration 9964, loss = 1.49707593\n",
      "Iteration 9965, loss = 1.49706697\n",
      "Iteration 9966, loss = 1.49705801\n",
      "Iteration 9967, loss = 1.49704905\n",
      "Iteration 9968, loss = 1.49704010\n",
      "Iteration 9969, loss = 1.49703115\n",
      "Iteration 9970, loss = 1.49702220\n",
      "Iteration 9971, loss = 1.49701325\n",
      "Iteration 9972, loss = 1.49700430\n",
      "Iteration 9973, loss = 1.49699536\n",
      "Iteration 9974, loss = 1.49698641\n",
      "Iteration 9975, loss = 1.49697747\n",
      "Iteration 9976, loss = 1.49696853\n",
      "Iteration 9977, loss = 1.49695959\n",
      "Iteration 9978, loss = 1.49695066\n",
      "Iteration 9979, loss = 1.49694173\n",
      "Iteration 9980, loss = 1.49693279\n",
      "Iteration 9981, loss = 1.49692386\n",
      "Iteration 9982, loss = 1.49691493\n",
      "Iteration 9983, loss = 1.49690601\n",
      "Iteration 9984, loss = 1.49689708\n",
      "Iteration 9985, loss = 1.49688816\n",
      "Iteration 9986, loss = 1.49687924\n",
      "Iteration 9987, loss = 1.49687032\n",
      "Iteration 9988, loss = 1.49686140\n",
      "Iteration 9989, loss = 1.49685249\n",
      "Iteration 9990, loss = 1.49684357\n",
      "Iteration 9991, loss = 1.49683466\n",
      "Iteration 9992, loss = 1.49682575\n",
      "Iteration 9993, loss = 1.49681684\n",
      "Iteration 9994, loss = 1.49680794\n",
      "Iteration 9995, loss = 1.49679903\n",
      "Iteration 9996, loss = 1.49679013\n",
      "Iteration 9997, loss = 1.49678123\n",
      "Iteration 9998, loss = 1.49677233\n",
      "Iteration 9999, loss = 1.49676344\n",
      "Iteration 10000, loss = 1.49675454\n",
      "Iteration 10001, loss = 1.49674565\n",
      "Iteration 10002, loss = 1.49673676\n",
      "Iteration 10003, loss = 1.49672787\n",
      "Iteration 10004, loss = 1.49671898\n",
      "Iteration 10005, loss = 1.49671009\n",
      "Iteration 10006, loss = 1.49670121\n",
      "Iteration 10007, loss = 1.49669232\n",
      "Iteration 10008, loss = 1.49668344\n",
      "Iteration 10009, loss = 1.49667457\n",
      "Iteration 10010, loss = 1.49666569\n",
      "Iteration 10011, loss = 1.49665681\n",
      "Iteration 10012, loss = 1.49664794\n",
      "Iteration 10013, loss = 1.49663907\n",
      "Iteration 10014, loss = 1.49663020\n",
      "Iteration 10015, loss = 1.49662133\n",
      "Iteration 10016, loss = 1.49661247\n",
      "Iteration 10017, loss = 1.49660360\n",
      "Iteration 10018, loss = 1.49659474\n",
      "Iteration 10019, loss = 1.49658588\n",
      "Iteration 10020, loss = 1.49657702\n",
      "Iteration 10021, loss = 1.49656816\n",
      "Iteration 10022, loss = 1.49655931\n",
      "Iteration 10023, loss = 1.49655046\n",
      "Iteration 10024, loss = 1.49654160\n",
      "Iteration 10025, loss = 1.49653275\n",
      "Iteration 10026, loss = 1.49652391\n",
      "Iteration 10027, loss = 1.49651506\n",
      "Iteration 10028, loss = 1.49650622\n",
      "Iteration 10029, loss = 1.49649737\n",
      "Iteration 10030, loss = 1.49648853\n",
      "Iteration 10031, loss = 1.49647970\n",
      "Iteration 10032, loss = 1.49647086\n",
      "Iteration 10033, loss = 1.49646202\n",
      "Iteration 10034, loss = 1.49645319\n",
      "Iteration 10035, loss = 1.49644436\n",
      "Iteration 10036, loss = 1.49643553\n",
      "Iteration 10037, loss = 1.49642670\n",
      "Iteration 10038, loss = 1.49641788\n",
      "Iteration 10039, loss = 1.49640905\n",
      "Iteration 10040, loss = 1.49640023\n",
      "Iteration 10041, loss = 1.49639141\n",
      "Iteration 10042, loss = 1.49638259\n",
      "Iteration 10043, loss = 1.49637377\n",
      "Iteration 10044, loss = 1.49636496\n",
      "Iteration 10045, loss = 1.49635615\n",
      "Iteration 10046, loss = 1.49634733\n",
      "Iteration 10047, loss = 1.49633853\n",
      "Iteration 10048, loss = 1.49632972\n",
      "Iteration 10049, loss = 1.49632091\n",
      "Iteration 10050, loss = 1.49631211\n",
      "Iteration 10051, loss = 1.49630331\n",
      "Iteration 10052, loss = 1.49629450\n",
      "Iteration 10053, loss = 1.49628571\n",
      "Iteration 10054, loss = 1.49627691\n",
      "Iteration 10055, loss = 1.49626811\n",
      "Iteration 10056, loss = 1.49625932\n",
      "Iteration 10057, loss = 1.49625053\n",
      "Iteration 10058, loss = 1.49624174\n",
      "Iteration 10059, loss = 1.49623295\n",
      "Iteration 10060, loss = 1.49622417\n",
      "Iteration 10061, loss = 1.49621538\n",
      "Iteration 10062, loss = 1.49620660\n",
      "Iteration 10063, loss = 1.49619782\n",
      "Iteration 10064, loss = 1.49618904\n",
      "Iteration 10065, loss = 1.49618026\n",
      "Iteration 10066, loss = 1.49617149\n",
      "Iteration 10067, loss = 1.49616271\n",
      "Iteration 10068, loss = 1.49615394\n",
      "Iteration 10069, loss = 1.49614517\n",
      "Iteration 10070, loss = 1.49613640\n",
      "Iteration 10071, loss = 1.49612764\n",
      "Iteration 10072, loss = 1.49611887\n",
      "Iteration 10073, loss = 1.49611011\n",
      "Iteration 10074, loss = 1.49610135\n",
      "Iteration 10075, loss = 1.49609259\n",
      "Iteration 10076, loss = 1.49608383\n",
      "Iteration 10077, loss = 1.49607508\n",
      "Iteration 10078, loss = 1.49606632\n",
      "Iteration 10079, loss = 1.49605757\n",
      "Iteration 10080, loss = 1.49604882\n",
      "Iteration 10081, loss = 1.49604007\n",
      "Iteration 10082, loss = 1.49603133\n",
      "Iteration 10083, loss = 1.49602258\n",
      "Iteration 10084, loss = 1.49601384\n",
      "Iteration 10085, loss = 1.49600510\n",
      "Iteration 10086, loss = 1.49599636\n",
      "Iteration 10087, loss = 1.49598762\n",
      "Iteration 10088, loss = 1.49597889\n",
      "Iteration 10089, loss = 1.49597015\n",
      "Iteration 10090, loss = 1.49596142\n",
      "Iteration 10091, loss = 1.49595269\n",
      "Iteration 10092, loss = 1.49594396\n",
      "Iteration 10093, loss = 1.49593523\n",
      "Iteration 10094, loss = 1.49592651\n",
      "Iteration 10095, loss = 1.49591779\n",
      "Iteration 10096, loss = 1.49590906\n",
      "Iteration 10097, loss = 1.49590034\n",
      "Iteration 10098, loss = 1.49589163\n",
      "Iteration 10099, loss = 1.49588291\n",
      "Iteration 10100, loss = 1.49587420\n",
      "Iteration 10101, loss = 1.49586548\n",
      "Iteration 10102, loss = 1.49585677\n",
      "Iteration 10103, loss = 1.49584806\n",
      "Iteration 10104, loss = 1.49583936\n",
      "Iteration 10105, loss = 1.49583065\n",
      "Iteration 10106, loss = 1.49582195\n",
      "Iteration 10107, loss = 1.49581325\n",
      "Iteration 10108, loss = 1.49580455\n",
      "Iteration 10109, loss = 1.49579585\n",
      "Iteration 10110, loss = 1.49578715\n",
      "Iteration 10111, loss = 1.49577846\n",
      "Iteration 10112, loss = 1.49576976\n",
      "Iteration 10113, loss = 1.49576107\n",
      "Iteration 10114, loss = 1.49575238\n",
      "Iteration 10115, loss = 1.49574369\n",
      "Iteration 10116, loss = 1.49573501\n",
      "Iteration 10117, loss = 1.49572632\n",
      "Iteration 10118, loss = 1.49571764\n",
      "Iteration 10119, loss = 1.49570896\n",
      "Iteration 10120, loss = 1.49570028\n",
      "Iteration 10121, loss = 1.49569161\n",
      "Iteration 10122, loss = 1.49568293\n",
      "Iteration 10123, loss = 1.49567426\n",
      "Iteration 10124, loss = 1.49566559\n",
      "Iteration 10125, loss = 1.49565692\n",
      "Iteration 10126, loss = 1.49564825\n",
      "Iteration 10127, loss = 1.49563958\n",
      "Iteration 10128, loss = 1.49563092\n",
      "Iteration 10129, loss = 1.49562225\n",
      "Iteration 10130, loss = 1.49561359\n",
      "Iteration 10131, loss = 1.49560493\n",
      "Iteration 10132, loss = 1.49559628\n",
      "Iteration 10133, loss = 1.49558762\n",
      "Iteration 10134, loss = 1.49557897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10135, loss = 1.49557031\n",
      "Iteration 10136, loss = 1.49556166\n",
      "Iteration 10137, loss = 1.49555301\n",
      "Iteration 10138, loss = 1.49554437\n",
      "Iteration 10139, loss = 1.49553572\n",
      "Iteration 10140, loss = 1.49552708\n",
      "Iteration 10141, loss = 1.49551844\n",
      "Iteration 10142, loss = 1.49550980\n",
      "Iteration 10143, loss = 1.49550116\n",
      "Iteration 10144, loss = 1.49549252\n",
      "Iteration 10145, loss = 1.49548389\n",
      "Iteration 10146, loss = 1.49547526\n",
      "Iteration 10147, loss = 1.49546662\n",
      "Iteration 10148, loss = 1.49545800\n",
      "Iteration 10149, loss = 1.49544937\n",
      "Iteration 10150, loss = 1.49544074\n",
      "Iteration 10151, loss = 1.49543212\n",
      "Iteration 10152, loss = 1.49542350\n",
      "Iteration 10153, loss = 1.49541488\n",
      "Iteration 10154, loss = 1.49540626\n",
      "Iteration 10155, loss = 1.49539764\n",
      "Iteration 10156, loss = 1.49538902\n",
      "Iteration 10157, loss = 1.49538041\n",
      "Iteration 10158, loss = 1.49537180\n",
      "Iteration 10159, loss = 1.49536319\n",
      "Iteration 10160, loss = 1.49535458\n",
      "Iteration 10161, loss = 1.49534597\n",
      "Iteration 10162, loss = 1.49533737\n",
      "Iteration 10163, loss = 1.49532877\n",
      "Iteration 10164, loss = 1.49532017\n",
      "Iteration 10165, loss = 1.49531157\n",
      "Iteration 10166, loss = 1.49530297\n",
      "Iteration 10167, loss = 1.49529437\n",
      "Iteration 10168, loss = 1.49528578\n",
      "Iteration 10169, loss = 1.49527719\n",
      "Iteration 10170, loss = 1.49526860\n",
      "Iteration 10171, loss = 1.49526001\n",
      "Iteration 10172, loss = 1.49525142\n",
      "Iteration 10173, loss = 1.49524283\n",
      "Iteration 10174, loss = 1.49523425\n",
      "Iteration 10175, loss = 1.49522567\n",
      "Iteration 10176, loss = 1.49521709\n",
      "Iteration 10177, loss = 1.49520851\n",
      "Iteration 10178, loss = 1.49519993\n",
      "Iteration 10179, loss = 1.49519136\n",
      "Iteration 10180, loss = 1.49518278\n",
      "Iteration 10181, loss = 1.49517421\n",
      "Iteration 10182, loss = 1.49516564\n",
      "Iteration 10183, loss = 1.49515708\n",
      "Iteration 10184, loss = 1.49514851\n",
      "Iteration 10185, loss = 1.49513994\n",
      "Iteration 10186, loss = 1.49513138\n",
      "Iteration 10187, loss = 1.49512282\n",
      "Iteration 10188, loss = 1.49511426\n",
      "Iteration 10189, loss = 1.49510570\n",
      "Iteration 10190, loss = 1.49509715\n",
      "Iteration 10191, loss = 1.49508859\n",
      "Iteration 10192, loss = 1.49508004\n",
      "Iteration 10193, loss = 1.49507149\n",
      "Iteration 10194, loss = 1.49506294\n",
      "Iteration 10195, loss = 1.49505439\n",
      "Iteration 10196, loss = 1.49504585\n",
      "Iteration 10197, loss = 1.49503731\n",
      "Iteration 10198, loss = 1.49502876\n",
      "Iteration 10199, loss = 1.49502022\n",
      "Iteration 10200, loss = 1.49501169\n",
      "Iteration 10201, loss = 1.49500315\n",
      "Iteration 10202, loss = 1.49499461\n",
      "Iteration 10203, loss = 1.49498608\n",
      "Iteration 10204, loss = 1.49497755\n",
      "Iteration 10205, loss = 1.49496902\n",
      "Iteration 10206, loss = 1.49496049\n",
      "Iteration 10207, loss = 1.49495196\n",
      "Iteration 10208, loss = 1.49494344\n",
      "Iteration 10209, loss = 1.49493492\n",
      "Iteration 10210, loss = 1.49492640\n",
      "Iteration 10211, loss = 1.49491788\n",
      "Iteration 10212, loss = 1.49490936\n",
      "Iteration 10213, loss = 1.49490084\n",
      "Iteration 10214, loss = 1.49489233\n",
      "Iteration 10215, loss = 1.49488382\n",
      "Iteration 10216, loss = 1.49487531\n",
      "Iteration 10217, loss = 1.49486680\n",
      "Iteration 10218, loss = 1.49485829\n",
      "Iteration 10219, loss = 1.49484978\n",
      "Iteration 10220, loss = 1.49484128\n",
      "Iteration 10221, loss = 1.49483278\n",
      "Iteration 10222, loss = 1.49482428\n",
      "Iteration 10223, loss = 1.49481578\n",
      "Iteration 10224, loss = 1.49480728\n",
      "Iteration 10225, loss = 1.49479879\n",
      "Iteration 10226, loss = 1.49479029\n",
      "Iteration 10227, loss = 1.49478180\n",
      "Iteration 10228, loss = 1.49477331\n",
      "Iteration 10229, loss = 1.49476482\n",
      "Iteration 10230, loss = 1.49475634\n",
      "Iteration 10231, loss = 1.49474785\n",
      "Iteration 10232, loss = 1.49473937\n",
      "Iteration 10233, loss = 1.49473089\n",
      "Iteration 10234, loss = 1.49472241\n",
      "Iteration 10235, loss = 1.49471393\n",
      "Iteration 10236, loss = 1.49470545\n",
      "Iteration 10237, loss = 1.49469698\n",
      "Iteration 10238, loss = 1.49468850\n",
      "Iteration 10239, loss = 1.49468003\n",
      "Iteration 10240, loss = 1.49467156\n",
      "Iteration 10241, loss = 1.49466310\n",
      "Iteration 10242, loss = 1.49465463\n",
      "Iteration 10243, loss = 1.49464617\n",
      "Iteration 10244, loss = 1.49463770\n",
      "Iteration 10245, loss = 1.49462924\n",
      "Iteration 10246, loss = 1.49462078\n",
      "Iteration 10247, loss = 1.49461233\n",
      "Iteration 10248, loss = 1.49460387\n",
      "Iteration 10249, loss = 1.49459542\n",
      "Iteration 10250, loss = 1.49458696\n",
      "Iteration 10251, loss = 1.49457851\n",
      "Iteration 10252, loss = 1.49457006\n",
      "Iteration 10253, loss = 1.49456162\n",
      "Iteration 10254, loss = 1.49455317\n",
      "Iteration 10255, loss = 1.49454473\n",
      "Iteration 10256, loss = 1.49453629\n",
      "Iteration 10257, loss = 1.49452785\n",
      "Iteration 10258, loss = 1.49451941\n",
      "Iteration 10259, loss = 1.49451097\n",
      "Iteration 10260, loss = 1.49450253\n",
      "Iteration 10261, loss = 1.49449410\n",
      "Iteration 10262, loss = 1.49448567\n",
      "Iteration 10263, loss = 1.49447724\n",
      "Iteration 10264, loss = 1.49446881\n",
      "Iteration 10265, loss = 1.49446038\n",
      "Iteration 10266, loss = 1.49445196\n",
      "Iteration 10267, loss = 1.49444354\n",
      "Iteration 10268, loss = 1.49443511\n",
      "Iteration 10269, loss = 1.49442669\n",
      "Iteration 10270, loss = 1.49441828\n",
      "Iteration 10271, loss = 1.49440986\n",
      "Iteration 10272, loss = 1.49440144\n",
      "Iteration 10273, loss = 1.49439303\n",
      "Iteration 10274, loss = 1.49438462\n",
      "Iteration 10275, loss = 1.49437621\n",
      "Iteration 10276, loss = 1.49436780\n",
      "Iteration 10277, loss = 1.49435940\n",
      "Iteration 10278, loss = 1.49435099\n",
      "Iteration 10279, loss = 1.49434259\n",
      "Iteration 10280, loss = 1.49433419\n",
      "Iteration 10281, loss = 1.49432579\n",
      "Iteration 10282, loss = 1.49431739\n",
      "Iteration 10283, loss = 1.49430899\n",
      "Iteration 10284, loss = 1.49430060\n",
      "Iteration 10285, loss = 1.49429221\n",
      "Iteration 10286, loss = 1.49428382\n",
      "Iteration 10287, loss = 1.49427543\n",
      "Iteration 10288, loss = 1.49426704\n",
      "Iteration 10289, loss = 1.49425865\n",
      "Iteration 10290, loss = 1.49425027\n",
      "Iteration 10291, loss = 1.49424189\n",
      "Iteration 10292, loss = 1.49423350\n",
      "Iteration 10293, loss = 1.49422513\n",
      "Iteration 10294, loss = 1.49421675\n",
      "Iteration 10295, loss = 1.49420837\n",
      "Iteration 10296, loss = 1.49420000\n",
      "Iteration 10297, loss = 1.49419163\n",
      "Iteration 10298, loss = 1.49418325\n",
      "Iteration 10299, loss = 1.49417489\n",
      "Iteration 10300, loss = 1.49416652\n",
      "Iteration 10301, loss = 1.49415815\n",
      "Iteration 10302, loss = 1.49414979\n",
      "Iteration 10303, loss = 1.49414143\n",
      "Iteration 10304, loss = 1.49413307\n",
      "Iteration 10305, loss = 1.49412471\n",
      "Iteration 10306, loss = 1.49411635\n",
      "Iteration 10307, loss = 1.49410799\n",
      "Iteration 10308, loss = 1.49409964\n",
      "Iteration 10309, loss = 1.49409129\n",
      "Iteration 10310, loss = 1.49408294\n",
      "Iteration 10311, loss = 1.49407459\n",
      "Iteration 10312, loss = 1.49406624\n",
      "Iteration 10313, loss = 1.49405790\n",
      "Iteration 10314, loss = 1.49404955\n",
      "Iteration 10315, loss = 1.49404121\n",
      "Iteration 10316, loss = 1.49403287\n",
      "Iteration 10317, loss = 1.49402453\n",
      "Iteration 10318, loss = 1.49401619\n",
      "Iteration 10319, loss = 1.49400786\n",
      "Iteration 10320, loss = 1.49399952\n",
      "Iteration 10321, loss = 1.49399119\n",
      "Iteration 10322, loss = 1.49398286\n",
      "Iteration 10323, loss = 1.49397453\n",
      "Iteration 10324, loss = 1.49396621\n",
      "Iteration 10325, loss = 1.49395788\n",
      "Iteration 10326, loss = 1.49394956\n",
      "Iteration 10327, loss = 1.49394124\n",
      "Iteration 10328, loss = 1.49393292\n",
      "Iteration 10329, loss = 1.49392460\n",
      "Iteration 10330, loss = 1.49391628\n",
      "Iteration 10331, loss = 1.49390796\n",
      "Iteration 10332, loss = 1.49389965\n",
      "Iteration 10333, loss = 1.49389134\n",
      "Iteration 10334, loss = 1.49388303\n",
      "Iteration 10335, loss = 1.49387472\n",
      "Iteration 10336, loss = 1.49386641\n",
      "Iteration 10337, loss = 1.49385811\n",
      "Iteration 10338, loss = 1.49384980\n",
      "Iteration 10339, loss = 1.49384150\n",
      "Iteration 10340, loss = 1.49383320\n",
      "Iteration 10341, loss = 1.49382490\n",
      "Iteration 10342, loss = 1.49381661\n",
      "Iteration 10343, loss = 1.49380831\n",
      "Iteration 10344, loss = 1.49380002\n",
      "Iteration 10345, loss = 1.49379173\n",
      "Iteration 10346, loss = 1.49378344\n",
      "Iteration 10347, loss = 1.49377515\n",
      "Iteration 10348, loss = 1.49376686\n",
      "Iteration 10349, loss = 1.49375857\n",
      "Iteration 10350, loss = 1.49375029\n",
      "Iteration 10351, loss = 1.49374201\n",
      "Iteration 10352, loss = 1.49373373\n",
      "Iteration 10353, loss = 1.49372545\n",
      "Iteration 10354, loss = 1.49371717\n",
      "Iteration 10355, loss = 1.49370890\n",
      "Iteration 10356, loss = 1.49370062\n",
      "Iteration 10357, loss = 1.49369235\n",
      "Iteration 10358, loss = 1.49368408\n",
      "Iteration 10359, loss = 1.49367581\n",
      "Iteration 10360, loss = 1.49366755\n",
      "Iteration 10361, loss = 1.49365928\n",
      "Iteration 10362, loss = 1.49365102\n",
      "Iteration 10363, loss = 1.49364276\n",
      "Iteration 10364, loss = 1.49363449\n",
      "Iteration 10365, loss = 1.49362624\n",
      "Iteration 10366, loss = 1.49361798\n",
      "Iteration 10367, loss = 1.49360972\n",
      "Iteration 10368, loss = 1.49360147\n",
      "Iteration 10369, loss = 1.49359322\n",
      "Iteration 10370, loss = 1.49358497\n",
      "Iteration 10371, loss = 1.49357672\n",
      "Iteration 10372, loss = 1.49356847\n",
      "Iteration 10373, loss = 1.49356023\n",
      "Iteration 10374, loss = 1.49355198\n",
      "Iteration 10375, loss = 1.49354374\n",
      "Iteration 10376, loss = 1.49353550\n",
      "Iteration 10377, loss = 1.49352726\n",
      "Iteration 10378, loss = 1.49351902\n",
      "Iteration 10379, loss = 1.49351079\n",
      "Iteration 10380, loss = 1.49350255\n",
      "Iteration 10381, loss = 1.49349432\n",
      "Iteration 10382, loss = 1.49348609\n",
      "Iteration 10383, loss = 1.49347786\n",
      "Iteration 10384, loss = 1.49346964\n",
      "Iteration 10385, loss = 1.49346141\n",
      "Iteration 10386, loss = 1.49345319\n",
      "Iteration 10387, loss = 1.49344496\n",
      "Iteration 10388, loss = 1.49343674\n",
      "Iteration 10389, loss = 1.49342852\n",
      "Iteration 10390, loss = 1.49342031\n",
      "Iteration 10391, loss = 1.49341209\n",
      "Iteration 10392, loss = 1.49340388\n",
      "Iteration 10393, loss = 1.49339566\n",
      "Iteration 10394, loss = 1.49338745\n",
      "Iteration 10395, loss = 1.49337924\n",
      "Iteration 10396, loss = 1.49337104\n",
      "Iteration 10397, loss = 1.49336283\n",
      "Iteration 10398, loss = 1.49335463\n",
      "Iteration 10399, loss = 1.49334642\n",
      "Iteration 10400, loss = 1.49333822\n",
      "Iteration 10401, loss = 1.49333002\n",
      "Iteration 10402, loss = 1.49332183\n",
      "Iteration 10403, loss = 1.49331363\n",
      "Iteration 10404, loss = 1.49330544\n",
      "Iteration 10405, loss = 1.49329724\n",
      "Iteration 10406, loss = 1.49328905\n",
      "Iteration 10407, loss = 1.49328086\n",
      "Iteration 10408, loss = 1.49327267\n",
      "Iteration 10409, loss = 1.49326449\n",
      "Iteration 10410, loss = 1.49325630\n",
      "Iteration 10411, loss = 1.49324812\n",
      "Iteration 10412, loss = 1.49323994\n",
      "Iteration 10413, loss = 1.49323176\n",
      "Iteration 10414, loss = 1.49322358\n",
      "Iteration 10415, loss = 1.49321540\n",
      "Iteration 10416, loss = 1.49320723\n",
      "Iteration 10417, loss = 1.49319906\n",
      "Iteration 10418, loss = 1.49319089\n",
      "Iteration 10419, loss = 1.49318272\n",
      "Iteration 10420, loss = 1.49317455\n",
      "Iteration 10421, loss = 1.49316638\n",
      "Iteration 10422, loss = 1.49315822\n",
      "Iteration 10423, loss = 1.49315005\n",
      "Iteration 10424, loss = 1.49314189\n",
      "Iteration 10425, loss = 1.49313373\n",
      "Iteration 10426, loss = 1.49312557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10427, loss = 1.49311742\n",
      "Iteration 10428, loss = 1.49310926\n",
      "Iteration 10429, loss = 1.49310111\n",
      "Iteration 10430, loss = 1.49309295\n",
      "Iteration 10431, loss = 1.49308480\n",
      "Iteration 10432, loss = 1.49307666\n",
      "Iteration 10433, loss = 1.49306851\n",
      "Iteration 10434, loss = 1.49306036\n",
      "Iteration 10435, loss = 1.49305222\n",
      "Iteration 10436, loss = 1.49304408\n",
      "Iteration 10437, loss = 1.49303594\n",
      "Iteration 10438, loss = 1.49302780\n",
      "Iteration 10439, loss = 1.49301966\n",
      "Iteration 10440, loss = 1.49301152\n",
      "Iteration 10441, loss = 1.49300339\n",
      "Iteration 10442, loss = 1.49299526\n",
      "Iteration 10443, loss = 1.49298713\n",
      "Iteration 10444, loss = 1.49297900\n",
      "Iteration 10445, loss = 1.49297087\n",
      "Iteration 10446, loss = 1.49296275\n",
      "Iteration 10447, loss = 1.49295462\n",
      "Iteration 10448, loss = 1.49294650\n",
      "Iteration 10449, loss = 1.49293838\n",
      "Iteration 10450, loss = 1.49293026\n",
      "Iteration 10451, loss = 1.49292214\n",
      "Iteration 10452, loss = 1.49291402\n",
      "Iteration 10453, loss = 1.49290591\n",
      "Iteration 10454, loss = 1.49289780\n",
      "Iteration 10455, loss = 1.49288968\n",
      "Iteration 10456, loss = 1.49288157\n",
      "Iteration 10457, loss = 1.49287347\n",
      "Iteration 10458, loss = 1.49286536\n",
      "Iteration 10459, loss = 1.49285726\n",
      "Iteration 10460, loss = 1.49284915\n",
      "Iteration 10461, loss = 1.49284105\n",
      "Iteration 10462, loss = 1.49283295\n",
      "Iteration 10463, loss = 1.49282485\n",
      "Iteration 10464, loss = 1.49281675\n",
      "Iteration 10465, loss = 1.49280866\n",
      "Iteration 10466, loss = 1.49280057\n",
      "Iteration 10467, loss = 1.49279247\n",
      "Iteration 10468, loss = 1.49278438\n",
      "Iteration 10469, loss = 1.49277629\n",
      "Iteration 10470, loss = 1.49276821\n",
      "Iteration 10471, loss = 1.49276012\n",
      "Iteration 10472, loss = 1.49275204\n",
      "Iteration 10473, loss = 1.49274396\n",
      "Iteration 10474, loss = 1.49273588\n",
      "Iteration 10475, loss = 1.49272780\n",
      "Iteration 10476, loss = 1.49271972\n",
      "Iteration 10477, loss = 1.49271164\n",
      "Iteration 10478, loss = 1.49270357\n",
      "Iteration 10479, loss = 1.49269550\n",
      "Iteration 10480, loss = 1.49268742\n",
      "Iteration 10481, loss = 1.49267936\n",
      "Iteration 10482, loss = 1.49267129\n",
      "Iteration 10483, loss = 1.49266322\n",
      "Iteration 10484, loss = 1.49265516\n",
      "Iteration 10485, loss = 1.49264709\n",
      "Iteration 10486, loss = 1.49263903\n",
      "Iteration 10487, loss = 1.49263097\n",
      "Iteration 10488, loss = 1.49262291\n",
      "Iteration 10489, loss = 1.49261486\n",
      "Iteration 10490, loss = 1.49260680\n",
      "Iteration 10491, loss = 1.49259875\n",
      "Iteration 10492, loss = 1.49259070\n",
      "Iteration 10493, loss = 1.49258265\n",
      "Iteration 10494, loss = 1.49257460\n",
      "Iteration 10495, loss = 1.49256655\n",
      "Iteration 10496, loss = 1.49255851\n",
      "Iteration 10497, loss = 1.49255046\n",
      "Iteration 10498, loss = 1.49254242\n",
      "Iteration 10499, loss = 1.49253438\n",
      "Iteration 10500, loss = 1.49252634\n",
      "Iteration 10501, loss = 1.49251830\n",
      "Iteration 10502, loss = 1.49251027\n",
      "Iteration 10503, loss = 1.49250223\n",
      "Iteration 10504, loss = 1.49249420\n",
      "Iteration 10505, loss = 1.49248617\n",
      "Iteration 10506, loss = 1.49247814\n",
      "Iteration 10507, loss = 1.49247011\n",
      "Iteration 10508, loss = 1.49246208\n",
      "Iteration 10509, loss = 1.49245406\n",
      "Iteration 10510, loss = 1.49244604\n",
      "Iteration 10511, loss = 1.49243802\n",
      "Iteration 10512, loss = 1.49243000\n",
      "Iteration 10513, loss = 1.49242198\n",
      "Iteration 10514, loss = 1.49241396\n",
      "Iteration 10515, loss = 1.49240595\n",
      "Iteration 10516, loss = 1.49239793\n",
      "Iteration 10517, loss = 1.49238992\n",
      "Iteration 10518, loss = 1.49238191\n",
      "Iteration 10519, loss = 1.49237390\n",
      "Iteration 10520, loss = 1.49236589\n",
      "Iteration 10521, loss = 1.49235789\n",
      "Iteration 10522, loss = 1.49234988\n",
      "Iteration 10523, loss = 1.49234188\n",
      "Iteration 10524, loss = 1.49233388\n",
      "Iteration 10525, loss = 1.49232588\n",
      "Iteration 10526, loss = 1.49231788\n",
      "Iteration 10527, loss = 1.49230989\n",
      "Iteration 10528, loss = 1.49230189\n",
      "Iteration 10529, loss = 1.49229390\n",
      "Iteration 10530, loss = 1.49228591\n",
      "Iteration 10531, loss = 1.49227792\n",
      "Iteration 10532, loss = 1.49226993\n",
      "Iteration 10533, loss = 1.49226194\n",
      "Iteration 10534, loss = 1.49225396\n",
      "Iteration 10535, loss = 1.49224598\n",
      "Iteration 10536, loss = 1.49223799\n",
      "Iteration 10537, loss = 1.49223001\n",
      "Iteration 10538, loss = 1.49222204\n",
      "Iteration 10539, loss = 1.49221406\n",
      "Iteration 10540, loss = 1.49220608\n",
      "Iteration 10541, loss = 1.49219811\n",
      "Iteration 10542, loss = 1.49219014\n",
      "Iteration 10543, loss = 1.49218217\n",
      "Iteration 10544, loss = 1.49217420\n",
      "Iteration 10545, loss = 1.49216623\n",
      "Iteration 10546, loss = 1.49215826\n",
      "Iteration 10547, loss = 1.49215030\n",
      "Iteration 10548, loss = 1.49214234\n",
      "Iteration 10549, loss = 1.49213437\n",
      "Iteration 10550, loss = 1.49212641\n",
      "Iteration 10551, loss = 1.49211846\n",
      "Iteration 10552, loss = 1.49211050\n",
      "Iteration 10553, loss = 1.49210254\n",
      "Iteration 10554, loss = 1.49209459\n",
      "Iteration 10555, loss = 1.49208664\n",
      "Iteration 10556, loss = 1.49207869\n",
      "Iteration 10557, loss = 1.49207074\n",
      "Iteration 10558, loss = 1.49206279\n",
      "Iteration 10559, loss = 1.49205485\n",
      "Iteration 10560, loss = 1.49204690\n",
      "Iteration 10561, loss = 1.49203896\n",
      "Iteration 10562, loss = 1.49203102\n",
      "Iteration 10563, loss = 1.49202308\n",
      "Iteration 10564, loss = 1.49201514\n",
      "Iteration 10565, loss = 1.49200721\n",
      "Iteration 10566, loss = 1.49199927\n",
      "Iteration 10567, loss = 1.49199134\n",
      "Iteration 10568, loss = 1.49198341\n",
      "Iteration 10569, loss = 1.49197548\n",
      "Iteration 10570, loss = 1.49196755\n",
      "Iteration 10571, loss = 1.49195962\n",
      "Iteration 10572, loss = 1.49195170\n",
      "Iteration 10573, loss = 1.49194377\n",
      "Iteration 10574, loss = 1.49193585\n",
      "Iteration 10575, loss = 1.49192793\n",
      "Iteration 10576, loss = 1.49192001\n",
      "Iteration 10577, loss = 1.49191210\n",
      "Iteration 10578, loss = 1.49190418\n",
      "Iteration 10579, loss = 1.49189627\n",
      "Iteration 10580, loss = 1.49188835\n",
      "Iteration 10581, loss = 1.49188044\n",
      "Iteration 10582, loss = 1.49187253\n",
      "Iteration 10583, loss = 1.49186462\n",
      "Iteration 10584, loss = 1.49185672\n",
      "Iteration 10585, loss = 1.49184881\n",
      "Iteration 10586, loss = 1.49184091\n",
      "Iteration 10587, loss = 1.49183301\n",
      "Iteration 10588, loss = 1.49182511\n",
      "Iteration 10589, loss = 1.49181721\n",
      "Iteration 10590, loss = 1.49180931\n",
      "Iteration 10591, loss = 1.49180142\n",
      "Iteration 10592, loss = 1.49179352\n",
      "Iteration 10593, loss = 1.49178563\n",
      "Iteration 10594, loss = 1.49177774\n",
      "Iteration 10595, loss = 1.49176985\n",
      "Iteration 10596, loss = 1.49176196\n",
      "Iteration 10597, loss = 1.49175408\n",
      "Iteration 10598, loss = 1.49174619\n",
      "Iteration 10599, loss = 1.49173831\n",
      "Iteration 10600, loss = 1.49173043\n",
      "Iteration 10601, loss = 1.49172255\n",
      "Iteration 10602, loss = 1.49171467\n",
      "Iteration 10603, loss = 1.49170679\n",
      "Iteration 10604, loss = 1.49169892\n",
      "Iteration 10605, loss = 1.49169104\n",
      "Iteration 10606, loss = 1.49168317\n",
      "Iteration 10607, loss = 1.49167530\n",
      "Iteration 10608, loss = 1.49166743\n",
      "Iteration 10609, loss = 1.49165956\n",
      "Iteration 10610, loss = 1.49165170\n",
      "Iteration 10611, loss = 1.49164383\n",
      "Iteration 10612, loss = 1.49163597\n",
      "Iteration 10613, loss = 1.49162811\n",
      "Iteration 10614, loss = 1.49162025\n",
      "Iteration 10615, loss = 1.49161239\n",
      "Iteration 10616, loss = 1.49160454\n",
      "Iteration 10617, loss = 1.49159668\n",
      "Iteration 10618, loss = 1.49158883\n",
      "Iteration 10619, loss = 1.49158097\n",
      "Iteration 10620, loss = 1.49157312\n",
      "Iteration 10621, loss = 1.49156527\n",
      "Iteration 10622, loss = 1.49155743\n",
      "Iteration 10623, loss = 1.49154958\n",
      "Iteration 10624, loss = 1.49154174\n",
      "Iteration 10625, loss = 1.49153389\n",
      "Iteration 10626, loss = 1.49152605\n",
      "Iteration 10627, loss = 1.49151821\n",
      "Iteration 10628, loss = 1.49151038\n",
      "Iteration 10629, loss = 1.49150254\n",
      "Iteration 10630, loss = 1.49149470\n",
      "Iteration 10631, loss = 1.49148687\n",
      "Iteration 10632, loss = 1.49147904\n",
      "Iteration 10633, loss = 1.49147121\n",
      "Iteration 10634, loss = 1.49146338\n",
      "Iteration 10635, loss = 1.49145555\n",
      "Iteration 10636, loss = 1.49144773\n",
      "Iteration 10637, loss = 1.49143990\n",
      "Iteration 10638, loss = 1.49143208\n",
      "Iteration 10639, loss = 1.49142426\n",
      "Iteration 10640, loss = 1.49141644\n",
      "Iteration 10641, loss = 1.49140862\n",
      "Iteration 10642, loss = 1.49140080\n",
      "Iteration 10643, loss = 1.49139299\n",
      "Iteration 10644, loss = 1.49138518\n",
      "Iteration 10645, loss = 1.49137736\n",
      "Iteration 10646, loss = 1.49136955\n",
      "Iteration 10647, loss = 1.49136174\n",
      "Iteration 10648, loss = 1.49135394\n",
      "Iteration 10649, loss = 1.49134613\n",
      "Iteration 10650, loss = 1.49133833\n",
      "Iteration 10651, loss = 1.49133052\n",
      "Iteration 10652, loss = 1.49132272\n",
      "Iteration 10653, loss = 1.49131492\n",
      "Iteration 10654, loss = 1.49130712\n",
      "Iteration 10655, loss = 1.49129933\n",
      "Iteration 10656, loss = 1.49129153\n",
      "Iteration 10657, loss = 1.49128374\n",
      "Iteration 10658, loss = 1.49127595\n",
      "Iteration 10659, loss = 1.49126816\n",
      "Iteration 10660, loss = 1.49126037\n",
      "Iteration 10661, loss = 1.49125258\n",
      "Iteration 10662, loss = 1.49124479\n",
      "Iteration 10663, loss = 1.49123701\n",
      "Iteration 10664, loss = 1.49122923\n",
      "Iteration 10665, loss = 1.49122145\n",
      "Iteration 10666, loss = 1.49121367\n",
      "Iteration 10667, loss = 1.49120589\n",
      "Iteration 10668, loss = 1.49119811\n",
      "Iteration 10669, loss = 1.49119034\n",
      "Iteration 10670, loss = 1.49118256\n",
      "Iteration 10671, loss = 1.49117479\n",
      "Iteration 10672, loss = 1.49116702\n",
      "Iteration 10673, loss = 1.49115925\n",
      "Iteration 10674, loss = 1.49115148\n",
      "Iteration 10675, loss = 1.49114372\n",
      "Iteration 10676, loss = 1.49113595\n",
      "Iteration 10677, loss = 1.49112819\n",
      "Iteration 10678, loss = 1.49112043\n",
      "Iteration 10679, loss = 1.49111267\n",
      "Iteration 10680, loss = 1.49110491\n",
      "Iteration 10681, loss = 1.49109715\n",
      "Iteration 10682, loss = 1.49108940\n",
      "Iteration 10683, loss = 1.49108164\n",
      "Iteration 10684, loss = 1.49107389\n",
      "Iteration 10685, loss = 1.49106614\n",
      "Iteration 10686, loss = 1.49105839\n",
      "Iteration 10687, loss = 1.49105064\n",
      "Iteration 10688, loss = 1.49104290\n",
      "Iteration 10689, loss = 1.49103515\n",
      "Iteration 10690, loss = 1.49102741\n",
      "Iteration 10691, loss = 1.49101967\n",
      "Iteration 10692, loss = 1.49101193\n",
      "Iteration 10693, loss = 1.49100419\n",
      "Iteration 10694, loss = 1.49099645\n",
      "Iteration 10695, loss = 1.49098871\n",
      "Iteration 10696, loss = 1.49098098\n",
      "Iteration 10697, loss = 1.49097325\n",
      "Iteration 10698, loss = 1.49096552\n",
      "Iteration 10699, loss = 1.49095779\n",
      "Iteration 10700, loss = 1.49095006\n",
      "Iteration 10701, loss = 1.49094233\n",
      "Iteration 10702, loss = 1.49093461\n",
      "Iteration 10703, loss = 1.49092688\n",
      "Iteration 10704, loss = 1.49091916\n",
      "Iteration 10705, loss = 1.49091144\n",
      "Iteration 10706, loss = 1.49090372\n",
      "Iteration 10707, loss = 1.49089600\n",
      "Iteration 10708, loss = 1.49088829\n",
      "Iteration 10709, loss = 1.49088057\n",
      "Iteration 10710, loss = 1.49087286\n",
      "Iteration 10711, loss = 1.49086515\n",
      "Iteration 10712, loss = 1.49085744\n",
      "Iteration 10713, loss = 1.49084973\n",
      "Iteration 10714, loss = 1.49084202\n",
      "Iteration 10715, loss = 1.49083432\n",
      "Iteration 10716, loss = 1.49082661\n",
      "Iteration 10717, loss = 1.49081891\n",
      "Iteration 10718, loss = 1.49081121\n",
      "Iteration 10719, loss = 1.49080351\n",
      "Iteration 10720, loss = 1.49079581\n",
      "Iteration 10721, loss = 1.49078812\n",
      "Iteration 10722, loss = 1.49078042\n",
      "Iteration 10723, loss = 1.49077273\n",
      "Iteration 10724, loss = 1.49076504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10725, loss = 1.49075735\n",
      "Iteration 10726, loss = 1.49074966\n",
      "Iteration 10727, loss = 1.49074197\n",
      "Iteration 10728, loss = 1.49073428\n",
      "Iteration 10729, loss = 1.49072660\n",
      "Iteration 10730, loss = 1.49071892\n",
      "Iteration 10731, loss = 1.49071124\n",
      "Iteration 10732, loss = 1.49070356\n",
      "Iteration 10733, loss = 1.49069588\n",
      "Iteration 10734, loss = 1.49068820\n",
      "Iteration 10735, loss = 1.49068052\n",
      "Iteration 10736, loss = 1.49067285\n",
      "Iteration 10737, loss = 1.49066518\n",
      "Iteration 10738, loss = 1.49065751\n",
      "Iteration 10739, loss = 1.49064984\n",
      "Iteration 10740, loss = 1.49064217\n",
      "Iteration 10741, loss = 1.49063450\n",
      "Iteration 10742, loss = 1.49062684\n",
      "Iteration 10743, loss = 1.49061918\n",
      "Iteration 10744, loss = 1.49061151\n",
      "Iteration 10745, loss = 1.49060385\n",
      "Iteration 10746, loss = 1.49059619\n",
      "Iteration 10747, loss = 1.49058854\n",
      "Iteration 10748, loss = 1.49058088\n",
      "Iteration 10749, loss = 1.49057323\n",
      "Iteration 10750, loss = 1.49056557\n",
      "Iteration 10751, loss = 1.49055792\n",
      "Iteration 10752, loss = 1.49055027\n",
      "Iteration 10753, loss = 1.49054262\n",
      "Iteration 10754, loss = 1.49053498\n",
      "Iteration 10755, loss = 1.49052733\n",
      "Iteration 10756, loss = 1.49051969\n",
      "Iteration 10757, loss = 1.49051204\n",
      "Iteration 10758, loss = 1.49050440\n",
      "Iteration 10759, loss = 1.49049676\n",
      "Iteration 10760, loss = 1.49048913\n",
      "Iteration 10761, loss = 1.49048149\n",
      "Iteration 10762, loss = 1.49047385\n",
      "Iteration 10763, loss = 1.49046622\n",
      "Iteration 10764, loss = 1.49045859\n",
      "Iteration 10765, loss = 1.49045096\n",
      "Iteration 10766, loss = 1.49044333\n",
      "Iteration 10767, loss = 1.49043570\n",
      "Iteration 10768, loss = 1.49042807\n",
      "Iteration 10769, loss = 1.49042045\n",
      "Iteration 10770, loss = 1.49041283\n",
      "Iteration 10771, loss = 1.49040520\n",
      "Iteration 10772, loss = 1.49039758\n",
      "Iteration 10773, loss = 1.49038997\n",
      "Iteration 10774, loss = 1.49038235\n",
      "Iteration 10775, loss = 1.49037473\n",
      "Iteration 10776, loss = 1.49036712\n",
      "Iteration 10777, loss = 1.49035951\n",
      "Iteration 10778, loss = 1.49035189\n",
      "Iteration 10779, loss = 1.49034428\n",
      "Iteration 10780, loss = 1.49033668\n",
      "Iteration 10781, loss = 1.49032907\n",
      "Iteration 10782, loss = 1.49032146\n",
      "Iteration 10783, loss = 1.49031386\n",
      "Iteration 10784, loss = 1.49030626\n",
      "Iteration 10785, loss = 1.49029866\n",
      "Iteration 10786, loss = 1.49029106\n",
      "Iteration 10787, loss = 1.49028346\n",
      "Iteration 10788, loss = 1.49027586\n",
      "Iteration 10789, loss = 1.49026827\n",
      "Iteration 10790, loss = 1.49026067\n",
      "Iteration 10791, loss = 1.49025308\n",
      "Iteration 10792, loss = 1.49024549\n",
      "Iteration 10793, loss = 1.49023790\n",
      "Iteration 10794, loss = 1.49023031\n",
      "Iteration 10795, loss = 1.49022273\n",
      "Iteration 10796, loss = 1.49021514\n",
      "Iteration 10797, loss = 1.49020756\n",
      "Iteration 10798, loss = 1.49019998\n",
      "Iteration 10799, loss = 1.49019240\n",
      "Iteration 10800, loss = 1.49018482\n",
      "Iteration 10801, loss = 1.49017724\n",
      "Iteration 10802, loss = 1.49016967\n",
      "Iteration 10803, loss = 1.49016209\n",
      "Iteration 10804, loss = 1.49015452\n",
      "Iteration 10805, loss = 1.49014695\n",
      "Iteration 10806, loss = 1.49013938\n",
      "Iteration 10807, loss = 1.49013181\n",
      "Iteration 10808, loss = 1.49012424\n",
      "Iteration 10809, loss = 1.49011668\n",
      "Iteration 10810, loss = 1.49010911\n",
      "Iteration 10811, loss = 1.49010155\n",
      "Iteration 10812, loss = 1.49009399\n",
      "Iteration 10813, loss = 1.49008643\n",
      "Iteration 10814, loss = 1.49007887\n",
      "Iteration 10815, loss = 1.49007131\n",
      "Iteration 10816, loss = 1.49006376\n",
      "Iteration 10817, loss = 1.49005620\n",
      "Iteration 10818, loss = 1.49004865\n",
      "Iteration 10819, loss = 1.49004110\n",
      "Iteration 10820, loss = 1.49003355\n",
      "Iteration 10821, loss = 1.49002600\n",
      "Iteration 10822, loss = 1.49001846\n",
      "Iteration 10823, loss = 1.49001091\n",
      "Iteration 10824, loss = 1.49000337\n",
      "Iteration 10825, loss = 1.48999583\n",
      "Iteration 10826, loss = 1.48998829\n",
      "Iteration 10827, loss = 1.48998075\n",
      "Iteration 10828, loss = 1.48997321\n",
      "Iteration 10829, loss = 1.48996567\n",
      "Iteration 10830, loss = 1.48995814\n",
      "Iteration 10831, loss = 1.48995060\n",
      "Iteration 10832, loss = 1.48994307\n",
      "Iteration 10833, loss = 1.48993554\n",
      "Iteration 10834, loss = 1.48992801\n",
      "Iteration 10835, loss = 1.48992048\n",
      "Iteration 10836, loss = 1.48991296\n",
      "Iteration 10837, loss = 1.48990543\n",
      "Iteration 10838, loss = 1.48989791\n",
      "Iteration 10839, loss = 1.48989039\n",
      "Iteration 10840, loss = 1.48988287\n",
      "Iteration 10841, loss = 1.48987535\n",
      "Iteration 10842, loss = 1.48986783\n",
      "Iteration 10843, loss = 1.48986032\n",
      "Iteration 10844, loss = 1.48985280\n",
      "Iteration 10845, loss = 1.48984529\n",
      "Iteration 10846, loss = 1.48983778\n",
      "Iteration 10847, loss = 1.48983027\n",
      "Iteration 10848, loss = 1.48982276\n",
      "Iteration 10849, loss = 1.48981525\n",
      "Iteration 10850, loss = 1.48980775\n",
      "Iteration 10851, loss = 1.48980024\n",
      "Iteration 10852, loss = 1.48979274\n",
      "Iteration 10853, loss = 1.48978524\n",
      "Iteration 10854, loss = 1.48977774\n",
      "Iteration 10855, loss = 1.48977024\n",
      "Iteration 10856, loss = 1.48976274\n",
      "Iteration 10857, loss = 1.48975525\n",
      "Iteration 10858, loss = 1.48974775\n",
      "Iteration 10859, loss = 1.48974026\n",
      "Iteration 10860, loss = 1.48973277\n",
      "Iteration 10861, loss = 1.48972528\n",
      "Iteration 10862, loss = 1.48971779\n",
      "Iteration 10863, loss = 1.48971031\n",
      "Iteration 10864, loss = 1.48970282\n",
      "Iteration 10865, loss = 1.48969534\n",
      "Iteration 10866, loss = 1.48968785\n",
      "Iteration 10867, loss = 1.48968037\n",
      "Iteration 10868, loss = 1.48967289\n",
      "Iteration 10869, loss = 1.48966541\n",
      "Iteration 10870, loss = 1.48965794\n",
      "Iteration 10871, loss = 1.48965046\n",
      "Iteration 10872, loss = 1.48964299\n",
      "Iteration 10873, loss = 1.48963552\n",
      "Iteration 10874, loss = 1.48962805\n",
      "Iteration 10875, loss = 1.48962058\n",
      "Iteration 10876, loss = 1.48961311\n",
      "Iteration 10877, loss = 1.48960564\n",
      "Iteration 10878, loss = 1.48959818\n",
      "Iteration 10879, loss = 1.48959071\n",
      "Iteration 10880, loss = 1.48958325\n",
      "Iteration 10881, loss = 1.48957579\n",
      "Iteration 10882, loss = 1.48956833\n",
      "Iteration 10883, loss = 1.48956087\n",
      "Iteration 10884, loss = 1.48955342\n",
      "Iteration 10885, loss = 1.48954596\n",
      "Iteration 10886, loss = 1.48953851\n",
      "Iteration 10887, loss = 1.48953106\n",
      "Iteration 10888, loss = 1.48952360\n",
      "Iteration 10889, loss = 1.48951616\n",
      "Iteration 10890, loss = 1.48950871\n",
      "Iteration 10891, loss = 1.48950126\n",
      "Iteration 10892, loss = 1.48949382\n",
      "Iteration 10893, loss = 1.48948637\n",
      "Iteration 10894, loss = 1.48947893\n",
      "Iteration 10895, loss = 1.48947149\n",
      "Iteration 10896, loss = 1.48946405\n",
      "Iteration 10897, loss = 1.48945661\n",
      "Iteration 10898, loss = 1.48944918\n",
      "Iteration 10899, loss = 1.48944174\n",
      "Iteration 10900, loss = 1.48943431\n",
      "Iteration 10901, loss = 1.48942688\n",
      "Iteration 10902, loss = 1.48941945\n",
      "Iteration 10903, loss = 1.48941202\n",
      "Iteration 10904, loss = 1.48940459\n",
      "Iteration 10905, loss = 1.48939716\n",
      "Iteration 10906, loss = 1.48938974\n",
      "Iteration 10907, loss = 1.48938231\n",
      "Iteration 10908, loss = 1.48937489\n",
      "Iteration 10909, loss = 1.48936747\n",
      "Iteration 10910, loss = 1.48936005\n",
      "Iteration 10911, loss = 1.48935263\n",
      "Iteration 10912, loss = 1.48934522\n",
      "Iteration 10913, loss = 1.48933780\n",
      "Iteration 10914, loss = 1.48933039\n",
      "Iteration 10915, loss = 1.48932298\n",
      "Iteration 10916, loss = 1.48931557\n",
      "Iteration 10917, loss = 1.48930816\n",
      "Iteration 10918, loss = 1.48930075\n",
      "Iteration 10919, loss = 1.48929334\n",
      "Iteration 10920, loss = 1.48928594\n",
      "Iteration 10921, loss = 1.48927854\n",
      "Iteration 10922, loss = 1.48927113\n",
      "Iteration 10923, loss = 1.48926373\n",
      "Iteration 10924, loss = 1.48925633\n",
      "Iteration 10925, loss = 1.48924894\n",
      "Iteration 10926, loss = 1.48924154\n",
      "Iteration 10927, loss = 1.48923415\n",
      "Iteration 10928, loss = 1.48922675\n",
      "Iteration 10929, loss = 1.48921936\n",
      "Iteration 10930, loss = 1.48921197\n",
      "Iteration 10931, loss = 1.48920458\n",
      "Iteration 10932, loss = 1.48919719\n",
      "Iteration 10933, loss = 1.48918981\n",
      "Iteration 10934, loss = 1.48918242\n",
      "Iteration 10935, loss = 1.48917504\n",
      "Iteration 10936, loss = 1.48916766\n",
      "Iteration 10937, loss = 1.48916027\n",
      "Iteration 10938, loss = 1.48915290\n",
      "Iteration 10939, loss = 1.48914552\n",
      "Iteration 10940, loss = 1.48913814\n",
      "Iteration 10941, loss = 1.48913077\n",
      "Iteration 10942, loss = 1.48912339\n",
      "Iteration 10943, loss = 1.48911602\n",
      "Iteration 10944, loss = 1.48910865\n",
      "Iteration 10945, loss = 1.48910128\n",
      "Iteration 10946, loss = 1.48909391\n",
      "Iteration 10947, loss = 1.48908655\n",
      "Iteration 10948, loss = 1.48907918\n",
      "Iteration 10949, loss = 1.48907182\n",
      "Iteration 10950, loss = 1.48906445\n",
      "Iteration 10951, loss = 1.48905709\n",
      "Iteration 10952, loss = 1.48904973\n",
      "Iteration 10953, loss = 1.48904238\n",
      "Iteration 10954, loss = 1.48903502\n",
      "Iteration 10955, loss = 1.48902767\n",
      "Iteration 10956, loss = 1.48902031\n",
      "Iteration 10957, loss = 1.48901296\n",
      "Iteration 10958, loss = 1.48900561\n",
      "Iteration 10959, loss = 1.48899826\n",
      "Iteration 10960, loss = 1.48899091\n",
      "Iteration 10961, loss = 1.48898356\n",
      "Iteration 10962, loss = 1.48897622\n",
      "Iteration 10963, loss = 1.48896887\n",
      "Iteration 10964, loss = 1.48896153\n",
      "Iteration 10965, loss = 1.48895419\n",
      "Iteration 10966, loss = 1.48894685\n",
      "Iteration 10967, loss = 1.48893951\n",
      "Iteration 10968, loss = 1.48893218\n",
      "Iteration 10969, loss = 1.48892484\n",
      "Iteration 10970, loss = 1.48891751\n",
      "Iteration 10971, loss = 1.48891017\n",
      "Iteration 10972, loss = 1.48890284\n",
      "Iteration 10973, loss = 1.48889551\n",
      "Iteration 10974, loss = 1.48888818\n",
      "Iteration 10975, loss = 1.48888086\n",
      "Iteration 10976, loss = 1.48887353\n",
      "Iteration 10977, loss = 1.48886621\n",
      "Iteration 10978, loss = 1.48885889\n",
      "Iteration 10979, loss = 1.48885156\n",
      "Iteration 10980, loss = 1.48884424\n",
      "Iteration 10981, loss = 1.48883693\n",
      "Iteration 10982, loss = 1.48882961\n",
      "Iteration 10983, loss = 1.48882229\n",
      "Iteration 10984, loss = 1.48881498\n",
      "Iteration 10985, loss = 1.48880767\n",
      "Iteration 10986, loss = 1.48880035\n",
      "Iteration 10987, loss = 1.48879304\n",
      "Iteration 10988, loss = 1.48878574\n",
      "Iteration 10989, loss = 1.48877843\n",
      "Iteration 10990, loss = 1.48877112\n",
      "Iteration 10991, loss = 1.48876382\n",
      "Iteration 10992, loss = 1.48875651\n",
      "Iteration 10993, loss = 1.48874921\n",
      "Iteration 10994, loss = 1.48874191\n",
      "Iteration 10995, loss = 1.48873461\n",
      "Iteration 10996, loss = 1.48872732\n",
      "Iteration 10997, loss = 1.48872002\n",
      "Iteration 10998, loss = 1.48871272\n",
      "Iteration 10999, loss = 1.48870543\n",
      "Iteration 11000, loss = 1.48869814\n",
      "Iteration 11001, loss = 1.48869085\n",
      "Iteration 11002, loss = 1.48868356\n",
      "Iteration 11003, loss = 1.48867627\n",
      "Iteration 11004, loss = 1.48866899\n",
      "Iteration 11005, loss = 1.48866170\n",
      "Iteration 11006, loss = 1.48865442\n",
      "Iteration 11007, loss = 1.48864714\n",
      "Iteration 11008, loss = 1.48863985\n",
      "Iteration 11009, loss = 1.48863258\n",
      "Iteration 11010, loss = 1.48862530\n",
      "Iteration 11011, loss = 1.48861802\n",
      "Iteration 11012, loss = 1.48861075\n",
      "Iteration 11013, loss = 1.48860347\n",
      "Iteration 11014, loss = 1.48859620\n",
      "Iteration 11015, loss = 1.48858893\n",
      "Iteration 11016, loss = 1.48858166\n",
      "Iteration 11017, loss = 1.48857439\n",
      "Iteration 11018, loss = 1.48856712\n",
      "Iteration 11019, loss = 1.48855986\n",
      "Iteration 11020, loss = 1.48855259\n",
      "Iteration 11021, loss = 1.48854533\n",
      "Iteration 11022, loss = 1.48853807\n",
      "Iteration 11023, loss = 1.48853081\n",
      "Iteration 11024, loss = 1.48852355\n",
      "Iteration 11025, loss = 1.48851629\n",
      "Iteration 11026, loss = 1.48850904\n",
      "Iteration 11027, loss = 1.48850178\n",
      "Iteration 11028, loss = 1.48849453\n",
      "Iteration 11029, loss = 1.48848728\n",
      "Iteration 11030, loss = 1.48848003\n",
      "Iteration 11031, loss = 1.48847278\n",
      "Iteration 11032, loss = 1.48846553\n",
      "Iteration 11033, loss = 1.48845829\n",
      "Iteration 11034, loss = 1.48845104\n",
      "Iteration 11035, loss = 1.48844380\n",
      "Iteration 11036, loss = 1.48843656\n",
      "Iteration 11037, loss = 1.48842932\n",
      "Iteration 11038, loss = 1.48842208\n",
      "Iteration 11039, loss = 1.48841484\n",
      "Iteration 11040, loss = 1.48840760\n",
      "Iteration 11041, loss = 1.48840037\n",
      "Iteration 11042, loss = 1.48839314\n",
      "Iteration 11043, loss = 1.48838590\n",
      "Iteration 11044, loss = 1.48837867\n",
      "Iteration 11045, loss = 1.48837144\n",
      "Iteration 11046, loss = 1.48836421\n",
      "Iteration 11047, loss = 1.48835699\n",
      "Iteration 11048, loss = 1.48834976\n",
      "Iteration 11049, loss = 1.48834254\n",
      "Iteration 11050, loss = 1.48833532\n",
      "Iteration 11051, loss = 1.48832809\n",
      "Iteration 11052, loss = 1.48832088\n",
      "Iteration 11053, loss = 1.48831366\n",
      "Iteration 11054, loss = 1.48830644\n",
      "Iteration 11055, loss = 1.48829922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11056, loss = 1.48829201\n",
      "Iteration 11057, loss = 1.48828480\n",
      "Iteration 11058, loss = 1.48827759\n",
      "Iteration 11059, loss = 1.48827038\n",
      "Iteration 11060, loss = 1.48826317\n",
      "Iteration 11061, loss = 1.48825596\n",
      "Iteration 11062, loss = 1.48824875\n",
      "Iteration 11063, loss = 1.48824155\n",
      "Iteration 11064, loss = 1.48823434\n",
      "Iteration 11065, loss = 1.48822714\n",
      "Iteration 11066, loss = 1.48821994\n",
      "Iteration 11067, loss = 1.48821274\n",
      "Iteration 11068, loss = 1.48820555\n",
      "Iteration 11069, loss = 1.48819835\n",
      "Iteration 11070, loss = 1.48819115\n",
      "Iteration 11071, loss = 1.48818396\n",
      "Iteration 11072, loss = 1.48817677\n",
      "Iteration 11073, loss = 1.48816958\n",
      "Iteration 11074, loss = 1.48816239\n",
      "Iteration 11075, loss = 1.48815520\n",
      "Iteration 11076, loss = 1.48814801\n",
      "Iteration 11077, loss = 1.48814083\n",
      "Iteration 11078, loss = 1.48813364\n",
      "Iteration 11079, loss = 1.48812646\n",
      "Iteration 11080, loss = 1.48811928\n",
      "Iteration 11081, loss = 1.48811210\n",
      "Iteration 11082, loss = 1.48810492\n",
      "Iteration 11083, loss = 1.48809774\n",
      "Iteration 11084, loss = 1.48809057\n",
      "Iteration 11085, loss = 1.48808339\n",
      "Iteration 11086, loss = 1.48807622\n",
      "Iteration 11087, loss = 1.48806905\n",
      "Iteration 11088, loss = 1.48806188\n",
      "Iteration 11089, loss = 1.48805471\n",
      "Iteration 11090, loss = 1.48804754\n",
      "Iteration 11091, loss = 1.48804037\n",
      "Iteration 11092, loss = 1.48803321\n",
      "Iteration 11093, loss = 1.48802605\n",
      "Iteration 11094, loss = 1.48801888\n",
      "Iteration 11095, loss = 1.48801172\n",
      "Iteration 11096, loss = 1.48800456\n",
      "Iteration 11097, loss = 1.48799740\n",
      "Iteration 11098, loss = 1.48799025\n",
      "Iteration 11099, loss = 1.48798309\n",
      "Iteration 11100, loss = 1.48797594\n",
      "Iteration 11101, loss = 1.48796879\n",
      "Iteration 11102, loss = 1.48796163\n",
      "Iteration 11103, loss = 1.48795448\n",
      "Iteration 11104, loss = 1.48794734\n",
      "Iteration 11105, loss = 1.48794019\n",
      "Iteration 11106, loss = 1.48793304\n",
      "Iteration 11107, loss = 1.48792590\n",
      "Iteration 11108, loss = 1.48791875\n",
      "Iteration 11109, loss = 1.48791161\n",
      "Iteration 11110, loss = 1.48790447\n",
      "Iteration 11111, loss = 1.48789733\n",
      "Iteration 11112, loss = 1.48789020\n",
      "Iteration 11113, loss = 1.48788306\n",
      "Iteration 11114, loss = 1.48787592\n",
      "Iteration 11115, loss = 1.48786879\n",
      "Iteration 11116, loss = 1.48786166\n",
      "Iteration 11117, loss = 1.48785453\n",
      "Iteration 11118, loss = 1.48784740\n",
      "Iteration 11119, loss = 1.48784027\n",
      "Iteration 11120, loss = 1.48783314\n",
      "Iteration 11121, loss = 1.48782602\n",
      "Iteration 11122, loss = 1.48781889\n",
      "Iteration 11123, loss = 1.48781177\n",
      "Iteration 11124, loss = 1.48780465\n",
      "Iteration 11125, loss = 1.48779753\n",
      "Iteration 11126, loss = 1.48779041\n",
      "Iteration 11127, loss = 1.48778329\n",
      "Iteration 11128, loss = 1.48777618\n",
      "Iteration 11129, loss = 1.48776906\n",
      "Iteration 11130, loss = 1.48776195\n",
      "Iteration 11131, loss = 1.48775483\n",
      "Iteration 11132, loss = 1.48774772\n",
      "Iteration 11133, loss = 1.48774061\n",
      "Iteration 11134, loss = 1.48773351\n",
      "Iteration 11135, loss = 1.48772640\n",
      "Iteration 11136, loss = 1.48771929\n",
      "Iteration 11137, loss = 1.48771219\n",
      "Iteration 11138, loss = 1.48770509\n",
      "Iteration 11139, loss = 1.48769799\n",
      "Iteration 11140, loss = 1.48769089\n",
      "Iteration 11141, loss = 1.48768379\n",
      "Iteration 11142, loss = 1.48767669\n",
      "Iteration 11143, loss = 1.48766960\n",
      "Iteration 11144, loss = 1.48766250\n",
      "Iteration 11145, loss = 1.48765541\n",
      "Iteration 11146, loss = 1.48764832\n",
      "Iteration 11147, loss = 1.48764123\n",
      "Iteration 11148, loss = 1.48763414\n",
      "Iteration 11149, loss = 1.48762705\n",
      "Iteration 11150, loss = 1.48761996\n",
      "Iteration 11151, loss = 1.48761288\n",
      "Iteration 11152, loss = 1.48760579\n",
      "Iteration 11153, loss = 1.48759871\n",
      "Iteration 11154, loss = 1.48759163\n",
      "Iteration 11155, loss = 1.48758455\n",
      "Iteration 11156, loss = 1.48757747\n",
      "Iteration 11157, loss = 1.48757039\n",
      "Iteration 11158, loss = 1.48756332\n",
      "Iteration 11159, loss = 1.48755624\n",
      "Iteration 11160, loss = 1.48754917\n",
      "Iteration 11161, loss = 1.48754210\n",
      "Iteration 11162, loss = 1.48753503\n",
      "Iteration 11163, loss = 1.48752796\n",
      "Iteration 11164, loss = 1.48752089\n",
      "Iteration 11165, loss = 1.48751383\n",
      "Iteration 11166, loss = 1.48750676\n",
      "Iteration 11167, loss = 1.48749970\n",
      "Iteration 11168, loss = 1.48749263\n",
      "Iteration 11169, loss = 1.48748557\n",
      "Iteration 11170, loss = 1.48747851\n",
      "Iteration 11171, loss = 1.48747145\n",
      "Iteration 11172, loss = 1.48746440\n",
      "Iteration 11173, loss = 1.48745734\n",
      "Iteration 11174, loss = 1.48745029\n",
      "Iteration 11175, loss = 1.48744323\n",
      "Iteration 11176, loss = 1.48743618\n",
      "Iteration 11177, loss = 1.48742913\n",
      "Iteration 11178, loss = 1.48742208\n",
      "Iteration 11179, loss = 1.48741504\n",
      "Iteration 11180, loss = 1.48740799\n",
      "Iteration 11181, loss = 1.48740094\n",
      "Iteration 11182, loss = 1.48739390\n",
      "Iteration 11183, loss = 1.48738686\n",
      "Iteration 11184, loss = 1.48737982\n",
      "Iteration 11185, loss = 1.48737278\n",
      "Iteration 11186, loss = 1.48736574\n",
      "Iteration 11187, loss = 1.48735870\n",
      "Iteration 11188, loss = 1.48735167\n",
      "Iteration 11189, loss = 1.48734463\n",
      "Iteration 11190, loss = 1.48733760\n",
      "Iteration 11191, loss = 1.48733057\n",
      "Iteration 11192, loss = 1.48732354\n",
      "Iteration 11193, loss = 1.48731651\n",
      "Iteration 11194, loss = 1.48730948\n",
      "Iteration 11195, loss = 1.48730245\n",
      "Iteration 11196, loss = 1.48729543\n",
      "Iteration 11197, loss = 1.48728840\n",
      "Iteration 11198, loss = 1.48728138\n",
      "Iteration 11199, loss = 1.48727436\n",
      "Iteration 11200, loss = 1.48726734\n",
      "Iteration 11201, loss = 1.48726032\n",
      "Iteration 11202, loss = 1.48725331\n",
      "Iteration 11203, loss = 1.48724629\n",
      "Iteration 11204, loss = 1.48723928\n",
      "Iteration 11205, loss = 1.48723226\n",
      "Iteration 11206, loss = 1.48722525\n",
      "Iteration 11207, loss = 1.48721824\n",
      "Iteration 11208, loss = 1.48721123\n",
      "Iteration 11209, loss = 1.48720422\n",
      "Iteration 11210, loss = 1.48719722\n",
      "Iteration 11211, loss = 1.48719021\n",
      "Iteration 11212, loss = 1.48718321\n",
      "Iteration 11213, loss = 1.48717620\n",
      "Iteration 11214, loss = 1.48716920\n",
      "Iteration 11215, loss = 1.48716220\n",
      "Iteration 11216, loss = 1.48715520\n",
      "Iteration 11217, loss = 1.48714821\n",
      "Iteration 11218, loss = 1.48714121\n",
      "Iteration 11219, loss = 1.48713422\n",
      "Iteration 11220, loss = 1.48712722\n",
      "Iteration 11221, loss = 1.48712023\n",
      "Iteration 11222, loss = 1.48711324\n",
      "Iteration 11223, loss = 1.48710625\n",
      "Iteration 11224, loss = 1.48709926\n",
      "Iteration 11225, loss = 1.48709228\n",
      "Iteration 11226, loss = 1.48708529\n",
      "Iteration 11227, loss = 1.48707831\n",
      "Iteration 11228, loss = 1.48707132\n",
      "Iteration 11229, loss = 1.48706434\n",
      "Iteration 11230, loss = 1.48705736\n",
      "Iteration 11231, loss = 1.48705038\n",
      "Iteration 11232, loss = 1.48704341\n",
      "Iteration 11233, loss = 1.48703643\n",
      "Iteration 11234, loss = 1.48702946\n",
      "Iteration 11235, loss = 1.48702248\n",
      "Iteration 11236, loss = 1.48701551\n",
      "Iteration 11237, loss = 1.48700854\n",
      "Iteration 11238, loss = 1.48700157\n",
      "Iteration 11239, loss = 1.48699460\n",
      "Iteration 11240, loss = 1.48698763\n",
      "Iteration 11241, loss = 1.48698067\n",
      "Iteration 11242, loss = 1.48697370\n",
      "Iteration 11243, loss = 1.48696674\n",
      "Iteration 11244, loss = 1.48695978\n",
      "Iteration 11245, loss = 1.48695282\n",
      "Iteration 11246, loss = 1.48694586\n",
      "Iteration 11247, loss = 1.48693890\n",
      "Iteration 11248, loss = 1.48693194\n",
      "Iteration 11249, loss = 1.48692499\n",
      "Iteration 11250, loss = 1.48691803\n",
      "Iteration 11251, loss = 1.48691108\n",
      "Iteration 11252, loss = 1.48690413\n",
      "Iteration 11253, loss = 1.48689718\n",
      "Iteration 11254, loss = 1.48689023\n",
      "Iteration 11255, loss = 1.48688328\n",
      "Iteration 11256, loss = 1.48687634\n",
      "Iteration 11257, loss = 1.48686939\n",
      "Iteration 11258, loss = 1.48686245\n",
      "Iteration 11259, loss = 1.48685551\n",
      "Iteration 11260, loss = 1.48684857\n",
      "Iteration 11261, loss = 1.48684163\n",
      "Iteration 11262, loss = 1.48683469\n",
      "Iteration 11263, loss = 1.48682775\n",
      "Iteration 11264, loss = 1.48682082\n",
      "Iteration 11265, loss = 1.48681388\n",
      "Iteration 11266, loss = 1.48680695\n",
      "Iteration 11267, loss = 1.48680002\n",
      "Iteration 11268, loss = 1.48679309\n",
      "Iteration 11269, loss = 1.48678616\n",
      "Iteration 11270, loss = 1.48677923\n",
      "Iteration 11271, loss = 1.48677230\n",
      "Iteration 11272, loss = 1.48676538\n",
      "Iteration 11273, loss = 1.48675845\n",
      "Iteration 11274, loss = 1.48675153\n",
      "Iteration 11275, loss = 1.48674461\n",
      "Iteration 11276, loss = 1.48673769\n",
      "Iteration 11277, loss = 1.48673077\n",
      "Iteration 11278, loss = 1.48672385\n",
      "Iteration 11279, loss = 1.48671694\n",
      "Iteration 11280, loss = 1.48671002\n",
      "Iteration 11281, loss = 1.48670311\n",
      "Iteration 11282, loss = 1.48669620\n",
      "Iteration 11283, loss = 1.48668929\n",
      "Iteration 11284, loss = 1.48668238\n",
      "Iteration 11285, loss = 1.48667547\n",
      "Iteration 11286, loss = 1.48666856\n",
      "Iteration 11287, loss = 1.48666166\n",
      "Iteration 11288, loss = 1.48665475\n",
      "Iteration 11289, loss = 1.48664785\n",
      "Iteration 11290, loss = 1.48664095\n",
      "Iteration 11291, loss = 1.48663405\n",
      "Iteration 11292, loss = 1.48662715\n",
      "Iteration 11293, loss = 1.48662025\n",
      "Iteration 11294, loss = 1.48661335\n",
      "Iteration 11295, loss = 1.48660646\n",
      "Iteration 11296, loss = 1.48659956\n",
      "Iteration 11297, loss = 1.48659267\n",
      "Iteration 11298, loss = 1.48658578\n",
      "Iteration 11299, loss = 1.48657889\n",
      "Iteration 11300, loss = 1.48657200\n",
      "Iteration 11301, loss = 1.48656511\n",
      "Iteration 11302, loss = 1.48655822\n",
      "Iteration 11303, loss = 1.48655134\n",
      "Iteration 11304, loss = 1.48654446\n",
      "Iteration 11305, loss = 1.48653757\n",
      "Iteration 11306, loss = 1.48653069\n",
      "Iteration 11307, loss = 1.48652381\n",
      "Iteration 11308, loss = 1.48651693\n",
      "Iteration 11309, loss = 1.48651006\n",
      "Iteration 11310, loss = 1.48650318\n",
      "Iteration 11311, loss = 1.48649631\n",
      "Iteration 11312, loss = 1.48648943\n",
      "Iteration 11313, loss = 1.48648256\n",
      "Iteration 11314, loss = 1.48647569\n",
      "Iteration 11315, loss = 1.48646882\n",
      "Iteration 11316, loss = 1.48646195\n",
      "Iteration 11317, loss = 1.48645508\n",
      "Iteration 11318, loss = 1.48644822\n",
      "Iteration 11319, loss = 1.48644135\n",
      "Iteration 11320, loss = 1.48643449\n",
      "Iteration 11321, loss = 1.48642763\n",
      "Iteration 11322, loss = 1.48642077\n",
      "Iteration 11323, loss = 1.48641391\n",
      "Iteration 11324, loss = 1.48640705\n",
      "Iteration 11325, loss = 1.48640019\n",
      "Iteration 11326, loss = 1.48639334\n",
      "Iteration 11327, loss = 1.48638648\n",
      "Iteration 11328, loss = 1.48637963\n",
      "Iteration 11329, loss = 1.48637278\n",
      "Iteration 11330, loss = 1.48636593\n",
      "Iteration 11331, loss = 1.48635908\n",
      "Iteration 11332, loss = 1.48635223\n",
      "Iteration 11333, loss = 1.48634539\n",
      "Iteration 11334, loss = 1.48633854\n",
      "Iteration 11335, loss = 1.48633170\n",
      "Iteration 11336, loss = 1.48632485\n",
      "Iteration 11337, loss = 1.48631801\n",
      "Iteration 11338, loss = 1.48631117\n",
      "Iteration 11339, loss = 1.48630433\n",
      "Iteration 11340, loss = 1.48629750\n",
      "Iteration 11341, loss = 1.48629066\n",
      "Iteration 11342, loss = 1.48628382\n",
      "Iteration 11343, loss = 1.48627699\n",
      "Iteration 11344, loss = 1.48627016\n",
      "Iteration 11345, loss = 1.48626333\n",
      "Iteration 11346, loss = 1.48625650\n",
      "Iteration 11347, loss = 1.48624967\n",
      "Iteration 11348, loss = 1.48624284\n",
      "Iteration 11349, loss = 1.48623602\n",
      "Iteration 11350, loss = 1.48622919\n",
      "Iteration 11351, loss = 1.48622237\n",
      "Iteration 11352, loss = 1.48621554\n",
      "Iteration 11353, loss = 1.48620872\n",
      "Iteration 11354, loss = 1.48620190\n",
      "Iteration 11355, loss = 1.48619509\n",
      "Iteration 11356, loss = 1.48618827\n",
      "Iteration 11357, loss = 1.48618145\n",
      "Iteration 11358, loss = 1.48617464\n",
      "Iteration 11359, loss = 1.48616782\n",
      "Iteration 11360, loss = 1.48616101\n",
      "Iteration 11361, loss = 1.48615420\n",
      "Iteration 11362, loss = 1.48614739\n",
      "Iteration 11363, loss = 1.48614058\n",
      "Iteration 11364, loss = 1.48613378\n",
      "Iteration 11365, loss = 1.48612697\n",
      "Iteration 11366, loss = 1.48612017\n",
      "Iteration 11367, loss = 1.48611336\n",
      "Iteration 11368, loss = 1.48610656\n",
      "Iteration 11369, loss = 1.48609976\n",
      "Iteration 11370, loss = 1.48609296\n",
      "Iteration 11371, loss = 1.48608616\n",
      "Iteration 11372, loss = 1.48607937\n",
      "Iteration 11373, loss = 1.48607257\n",
      "Iteration 11374, loss = 1.48606578\n",
      "Iteration 11375, loss = 1.48605898\n",
      "Iteration 11376, loss = 1.48605219\n",
      "Iteration 11377, loss = 1.48604540\n",
      "Iteration 11378, loss = 1.48603861\n",
      "Iteration 11379, loss = 1.48603182\n",
      "Iteration 11380, loss = 1.48602504\n",
      "Iteration 11381, loss = 1.48601825\n",
      "Iteration 11382, loss = 1.48601147\n",
      "Iteration 11383, loss = 1.48600469\n",
      "Iteration 11384, loss = 1.48599790\n",
      "Iteration 11385, loss = 1.48599112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11386, loss = 1.48598434\n",
      "Iteration 11387, loss = 1.48597757\n",
      "Iteration 11388, loss = 1.48597079\n",
      "Iteration 11389, loss = 1.48596401\n",
      "Iteration 11390, loss = 1.48595724\n",
      "Iteration 11391, loss = 1.48595047\n",
      "Iteration 11392, loss = 1.48594370\n",
      "Iteration 11393, loss = 1.48593693\n",
      "Iteration 11394, loss = 1.48593016\n",
      "Iteration 11395, loss = 1.48592339\n",
      "Iteration 11396, loss = 1.48591662\n",
      "Iteration 11397, loss = 1.48590986\n",
      "Iteration 11398, loss = 1.48590309\n",
      "Iteration 11399, loss = 1.48589633\n",
      "Iteration 11400, loss = 1.48588957\n",
      "Iteration 11401, loss = 1.48588281\n",
      "Iteration 11402, loss = 1.48587605\n",
      "Iteration 11403, loss = 1.48586929\n",
      "Iteration 11404, loss = 1.48586254\n",
      "Iteration 11405, loss = 1.48585578\n",
      "Iteration 11406, loss = 1.48584903\n",
      "Iteration 11407, loss = 1.48584227\n",
      "Iteration 11408, loss = 1.48583552\n",
      "Iteration 11409, loss = 1.48582877\n",
      "Iteration 11410, loss = 1.48582202\n",
      "Iteration 11411, loss = 1.48581528\n",
      "Iteration 11412, loss = 1.48580853\n",
      "Iteration 11413, loss = 1.48580179\n",
      "Iteration 11414, loss = 1.48579504\n",
      "Iteration 11415, loss = 1.48578830\n",
      "Iteration 11416, loss = 1.48578156\n",
      "Iteration 11417, loss = 1.48577482\n",
      "Iteration 11418, loss = 1.48576808\n",
      "Iteration 11419, loss = 1.48576134\n",
      "Iteration 11420, loss = 1.48575460\n",
      "Iteration 11421, loss = 1.48574787\n",
      "Iteration 11422, loss = 1.48574114\n",
      "Iteration 11423, loss = 1.48573440\n",
      "Iteration 11424, loss = 1.48572767\n",
      "Iteration 11425, loss = 1.48572094\n",
      "Iteration 11426, loss = 1.48571421\n",
      "Iteration 11427, loss = 1.48570749\n",
      "Iteration 11428, loss = 1.48570076\n",
      "Iteration 11429, loss = 1.48569403\n",
      "Iteration 11430, loss = 1.48568731\n",
      "Iteration 11431, loss = 1.48568059\n",
      "Iteration 11432, loss = 1.48567387\n",
      "Iteration 11433, loss = 1.48566715\n",
      "Iteration 11434, loss = 1.48566043\n",
      "Iteration 11435, loss = 1.48565371\n",
      "Iteration 11436, loss = 1.48564699\n",
      "Iteration 11437, loss = 1.48564028\n",
      "Iteration 11438, loss = 1.48563356\n",
      "Iteration 11439, loss = 1.48562685\n",
      "Iteration 11440, loss = 1.48562014\n",
      "Iteration 11441, loss = 1.48561343\n",
      "Iteration 11442, loss = 1.48560672\n",
      "Iteration 11443, loss = 1.48560001\n",
      "Iteration 11444, loss = 1.48559331\n",
      "Iteration 11445, loss = 1.48558660\n",
      "Iteration 11446, loss = 1.48557990\n",
      "Iteration 11447, loss = 1.48557320\n",
      "Iteration 11448, loss = 1.48556649\n",
      "Iteration 11449, loss = 1.48555979\n",
      "Iteration 11450, loss = 1.48555309\n",
      "Iteration 11451, loss = 1.48554640\n",
      "Iteration 11452, loss = 1.48553970\n",
      "Iteration 11453, loss = 1.48553301\n",
      "Iteration 11454, loss = 1.48552631\n",
      "Iteration 11455, loss = 1.48551962\n",
      "Iteration 11456, loss = 1.48551293\n",
      "Iteration 11457, loss = 1.48550624\n",
      "Iteration 11458, loss = 1.48549955\n",
      "Iteration 11459, loss = 1.48549286\n",
      "Iteration 11460, loss = 1.48548617\n",
      "Iteration 11461, loss = 1.48547949\n",
      "Iteration 11462, loss = 1.48547280\n",
      "Iteration 11463, loss = 1.48546612\n",
      "Iteration 11464, loss = 1.48545944\n",
      "Iteration 11465, loss = 1.48545276\n",
      "Iteration 11466, loss = 1.48544608\n",
      "Iteration 11467, loss = 1.48543940\n",
      "Iteration 11468, loss = 1.48543273\n",
      "Iteration 11469, loss = 1.48542605\n",
      "Iteration 11470, loss = 1.48541938\n",
      "Iteration 11471, loss = 1.48541270\n",
      "Iteration 11472, loss = 1.48540603\n",
      "Iteration 11473, loss = 1.48539936\n",
      "Iteration 11474, loss = 1.48539269\n",
      "Iteration 11475, loss = 1.48538602\n",
      "Iteration 11476, loss = 1.48537936\n",
      "Iteration 11477, loss = 1.48537269\n",
      "Iteration 11478, loss = 1.48536603\n",
      "Iteration 11479, loss = 1.48535937\n",
      "Iteration 11480, loss = 1.48535270\n",
      "Iteration 11481, loss = 1.48534604\n",
      "Iteration 11482, loss = 1.48533938\n",
      "Iteration 11483, loss = 1.48533273\n",
      "Iteration 11484, loss = 1.48532607\n",
      "Iteration 11485, loss = 1.48531941\n",
      "Iteration 11486, loss = 1.48531276\n",
      "Iteration 11487, loss = 1.48530611\n",
      "Iteration 11488, loss = 1.48529945\n",
      "Iteration 11489, loss = 1.48529280\n",
      "Iteration 11490, loss = 1.48528615\n",
      "Iteration 11491, loss = 1.48527950\n",
      "Iteration 11492, loss = 1.48527286\n",
      "Iteration 11493, loss = 1.48526621\n",
      "Iteration 11494, loss = 1.48525957\n",
      "Iteration 11495, loss = 1.48525292\n",
      "Iteration 11496, loss = 1.48524628\n",
      "Iteration 11497, loss = 1.48523964\n",
      "Iteration 11498, loss = 1.48523300\n",
      "Iteration 11499, loss = 1.48522636\n",
      "Iteration 11500, loss = 1.48521973\n",
      "Iteration 11501, loss = 1.48521309\n",
      "Iteration 11502, loss = 1.48520646\n",
      "Iteration 11503, loss = 1.48519982\n",
      "Iteration 11504, loss = 1.48519319\n",
      "Iteration 11505, loss = 1.48518656\n",
      "Iteration 11506, loss = 1.48517993\n",
      "Iteration 11507, loss = 1.48517330\n",
      "Iteration 11508, loss = 1.48516667\n",
      "Iteration 11509, loss = 1.48516005\n",
      "Iteration 11510, loss = 1.48515342\n",
      "Iteration 11511, loss = 1.48514680\n",
      "Iteration 11512, loss = 1.48514018\n",
      "Iteration 11513, loss = 1.48513355\n",
      "Iteration 11514, loss = 1.48512693\n",
      "Iteration 11515, loss = 1.48512031\n",
      "Iteration 11516, loss = 1.48511370\n",
      "Iteration 11517, loss = 1.48510708\n",
      "Iteration 11518, loss = 1.48510047\n",
      "Iteration 11519, loss = 1.48509385\n",
      "Iteration 11520, loss = 1.48508724\n",
      "Iteration 11521, loss = 1.48508063\n",
      "Iteration 11522, loss = 1.48507402\n",
      "Iteration 11523, loss = 1.48506741\n",
      "Iteration 11524, loss = 1.48506080\n",
      "Iteration 11525, loss = 1.48505419\n",
      "Iteration 11526, loss = 1.48504759\n",
      "Iteration 11527, loss = 1.48504098\n",
      "Iteration 11528, loss = 1.48503438\n",
      "Iteration 11529, loss = 1.48502778\n",
      "Iteration 11530, loss = 1.48502118\n",
      "Iteration 11531, loss = 1.48501458\n",
      "Iteration 11532, loss = 1.48500798\n",
      "Iteration 11533, loss = 1.48500138\n",
      "Iteration 11534, loss = 1.48499479\n",
      "Iteration 11535, loss = 1.48498819\n",
      "Iteration 11536, loss = 1.48498160\n",
      "Iteration 11537, loss = 1.48497501\n",
      "Iteration 11538, loss = 1.48496842\n",
      "Iteration 11539, loss = 1.48496183\n",
      "Iteration 11540, loss = 1.48495524\n",
      "Iteration 11541, loss = 1.48494865\n",
      "Iteration 11542, loss = 1.48494207\n",
      "Iteration 11543, loss = 1.48493548\n",
      "Iteration 11544, loss = 1.48492890\n",
      "Iteration 11545, loss = 1.48492232\n",
      "Iteration 11546, loss = 1.48491574\n",
      "Iteration 11547, loss = 1.48490916\n",
      "Iteration 11548, loss = 1.48490258\n",
      "Iteration 11549, loss = 1.48489600\n",
      "Iteration 11550, loss = 1.48488942\n",
      "Iteration 11551, loss = 1.48488285\n",
      "Iteration 11552, loss = 1.48487627\n",
      "Iteration 11553, loss = 1.48486970\n",
      "Iteration 11554, loss = 1.48486313\n",
      "Iteration 11555, loss = 1.48485656\n",
      "Iteration 11556, loss = 1.48484999\n",
      "Iteration 11557, loss = 1.48484342\n",
      "Iteration 11558, loss = 1.48483686\n",
      "Iteration 11559, loss = 1.48483029\n",
      "Iteration 11560, loss = 1.48482373\n",
      "Iteration 11561, loss = 1.48481716\n",
      "Iteration 11562, loss = 1.48481060\n",
      "Iteration 11563, loss = 1.48480404\n",
      "Iteration 11564, loss = 1.48479748\n",
      "Iteration 11565, loss = 1.48479092\n",
      "Iteration 11566, loss = 1.48478437\n",
      "Iteration 11567, loss = 1.48477781\n",
      "Iteration 11568, loss = 1.48477126\n",
      "Iteration 11569, loss = 1.48476470\n",
      "Iteration 11570, loss = 1.48475815\n",
      "Iteration 11571, loss = 1.48475160\n",
      "Iteration 11572, loss = 1.48474505\n",
      "Iteration 11573, loss = 1.48473850\n",
      "Iteration 11574, loss = 1.48473196\n",
      "Iteration 11575, loss = 1.48472541\n",
      "Iteration 11576, loss = 1.48471887\n",
      "Iteration 11577, loss = 1.48471232\n",
      "Iteration 11578, loss = 1.48470578\n",
      "Iteration 11579, loss = 1.48469924\n",
      "Iteration 11580, loss = 1.48469270\n",
      "Iteration 11581, loss = 1.48468616\n",
      "Iteration 11582, loss = 1.48467962\n",
      "Iteration 11583, loss = 1.48467308\n",
      "Iteration 11584, loss = 1.48466655\n",
      "Iteration 11585, loss = 1.48466002\n",
      "Iteration 11586, loss = 1.48465348\n",
      "Iteration 11587, loss = 1.48464695\n",
      "Iteration 11588, loss = 1.48464042\n",
      "Iteration 11589, loss = 1.48463389\n",
      "Iteration 11590, loss = 1.48462736\n",
      "Iteration 11591, loss = 1.48462084\n",
      "Iteration 11592, loss = 1.48461431\n",
      "Iteration 11593, loss = 1.48460779\n",
      "Iteration 11594, loss = 1.48460126\n",
      "Iteration 11595, loss = 1.48459474\n",
      "Iteration 11596, loss = 1.48458822\n",
      "Iteration 11597, loss = 1.48458170\n",
      "Iteration 11598, loss = 1.48457518\n",
      "Iteration 11599, loss = 1.48456866\n",
      "Iteration 11600, loss = 1.48456215\n",
      "Iteration 11601, loss = 1.48455563\n",
      "Iteration 11602, loss = 1.48454912\n",
      "Iteration 11603, loss = 1.48454261\n",
      "Iteration 11604, loss = 1.48453610\n",
      "Iteration 11605, loss = 1.48452959\n",
      "Iteration 11606, loss = 1.48452308\n",
      "Iteration 11607, loss = 1.48451657\n",
      "Iteration 11608, loss = 1.48451006\n",
      "Iteration 11609, loss = 1.48450356\n",
      "Iteration 11610, loss = 1.48449705\n",
      "Iteration 11611, loss = 1.48449055\n",
      "Iteration 11612, loss = 1.48448405\n",
      "Iteration 11613, loss = 1.48447755\n",
      "Iteration 11614, loss = 1.48447105\n",
      "Iteration 11615, loss = 1.48446455\n",
      "Iteration 11616, loss = 1.48445805\n",
      "Iteration 11617, loss = 1.48445156\n",
      "Iteration 11618, loss = 1.48444506\n",
      "Iteration 11619, loss = 1.48443857\n",
      "Iteration 11620, loss = 1.48443208\n",
      "Iteration 11621, loss = 1.48442559\n",
      "Iteration 11622, loss = 1.48441910\n",
      "Iteration 11623, loss = 1.48441261\n",
      "Iteration 11624, loss = 1.48440612\n",
      "Iteration 11625, loss = 1.48439963\n",
      "Iteration 11626, loss = 1.48439315\n",
      "Iteration 11627, loss = 1.48438667\n",
      "Iteration 11628, loss = 1.48438018\n",
      "Iteration 11629, loss = 1.48437370\n",
      "Iteration 11630, loss = 1.48436722\n",
      "Iteration 11631, loss = 1.48436074\n",
      "Iteration 11632, loss = 1.48435426\n",
      "Iteration 11633, loss = 1.48434779\n",
      "Iteration 11634, loss = 1.48434131\n",
      "Iteration 11635, loss = 1.48433484\n",
      "Iteration 11636, loss = 1.48432836\n",
      "Iteration 11637, loss = 1.48432189\n",
      "Iteration 11638, loss = 1.48431542\n",
      "Iteration 11639, loss = 1.48430895\n",
      "Iteration 11640, loss = 1.48430248\n",
      "Iteration 11641, loss = 1.48429602\n",
      "Iteration 11642, loss = 1.48428955\n",
      "Iteration 11643, loss = 1.48428308\n",
      "Iteration 11644, loss = 1.48427662\n",
      "Iteration 11645, loss = 1.48427016\n",
      "Iteration 11646, loss = 1.48426370\n",
      "Iteration 11647, loss = 1.48425724\n",
      "Iteration 11648, loss = 1.48425078\n",
      "Iteration 11649, loss = 1.48424432\n",
      "Iteration 11650, loss = 1.48423786\n",
      "Iteration 11651, loss = 1.48423141\n",
      "Iteration 11652, loss = 1.48422495\n",
      "Iteration 11653, loss = 1.48421850\n",
      "Iteration 11654, loss = 1.48421205\n",
      "Iteration 11655, loss = 1.48420560\n",
      "Iteration 11656, loss = 1.48419915\n",
      "Iteration 11657, loss = 1.48419270\n",
      "Iteration 11658, loss = 1.48418625\n",
      "Iteration 11659, loss = 1.48417981\n",
      "Iteration 11660, loss = 1.48417336\n",
      "Iteration 11661, loss = 1.48416692\n",
      "Iteration 11662, loss = 1.48416048\n",
      "Iteration 11663, loss = 1.48415403\n",
      "Iteration 11664, loss = 1.48414759\n",
      "Iteration 11665, loss = 1.48414115\n",
      "Iteration 11666, loss = 1.48413472\n",
      "Iteration 11667, loss = 1.48412828\n",
      "Iteration 11668, loss = 1.48412184\n",
      "Iteration 11669, loss = 1.48411541\n",
      "Iteration 11670, loss = 1.48410898\n",
      "Iteration 11671, loss = 1.48410255\n",
      "Iteration 11672, loss = 1.48409611\n",
      "Iteration 11673, loss = 1.48408969\n",
      "Iteration 11674, loss = 1.48408326\n",
      "Iteration 11675, loss = 1.48407683\n",
      "Iteration 11676, loss = 1.48407040\n",
      "Iteration 11677, loss = 1.48406398\n",
      "Iteration 11678, loss = 1.48405755\n",
      "Iteration 11679, loss = 1.48405113\n",
      "Iteration 11680, loss = 1.48404471\n",
      "Iteration 11681, loss = 1.48403829\n",
      "Iteration 11682, loss = 1.48403187\n",
      "Iteration 11683, loss = 1.48402545\n",
      "Iteration 11684, loss = 1.48401904\n",
      "Iteration 11685, loss = 1.48401262\n",
      "Iteration 11686, loss = 1.48400621\n",
      "Iteration 11687, loss = 1.48399979\n",
      "Iteration 11688, loss = 1.48399338\n",
      "Iteration 11689, loss = 1.48398697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11690, loss = 1.48398056\n",
      "Iteration 11691, loss = 1.48397415\n",
      "Iteration 11692, loss = 1.48396775\n",
      "Iteration 11693, loss = 1.48396134\n",
      "Iteration 11694, loss = 1.48395493\n",
      "Iteration 11695, loss = 1.48394853\n",
      "Iteration 11696, loss = 1.48394213\n",
      "Iteration 11697, loss = 1.48393573\n",
      "Iteration 11698, loss = 1.48392933\n",
      "Iteration 11699, loss = 1.48392293\n",
      "Iteration 11700, loss = 1.48391653\n",
      "Iteration 11701, loss = 1.48391013\n",
      "Iteration 11702, loss = 1.48390374\n",
      "Iteration 11703, loss = 1.48389734\n",
      "Iteration 11704, loss = 1.48389095\n",
      "Iteration 11705, loss = 1.48388456\n",
      "Iteration 11706, loss = 1.48387817\n",
      "Iteration 11707, loss = 1.48387178\n",
      "Iteration 11708, loss = 1.48386539\n",
      "Iteration 11709, loss = 1.48385900\n",
      "Iteration 11710, loss = 1.48385261\n",
      "Iteration 11711, loss = 1.48384623\n",
      "Iteration 11712, loss = 1.48383984\n",
      "Iteration 11713, loss = 1.48383346\n",
      "Iteration 11714, loss = 1.48382708\n",
      "Iteration 11715, loss = 1.48382070\n",
      "Iteration 11716, loss = 1.48381432\n",
      "Iteration 11717, loss = 1.48380794\n",
      "Iteration 11718, loss = 1.48380156\n",
      "Iteration 11719, loss = 1.48379519\n",
      "Iteration 11720, loss = 1.48378881\n",
      "Iteration 11721, loss = 1.48378244\n",
      "Iteration 11722, loss = 1.48377607\n",
      "Iteration 11723, loss = 1.48376970\n",
      "Iteration 11724, loss = 1.48376333\n",
      "Iteration 11725, loss = 1.48375696\n",
      "Iteration 11726, loss = 1.48375059\n",
      "Iteration 11727, loss = 1.48374422\n",
      "Iteration 11728, loss = 1.48373786\n",
      "Iteration 11729, loss = 1.48373149\n",
      "Iteration 11730, loss = 1.48372513\n",
      "Iteration 11731, loss = 1.48371877\n",
      "Iteration 11732, loss = 1.48371241\n",
      "Iteration 11733, loss = 1.48370605\n",
      "Iteration 11734, loss = 1.48369969\n",
      "Iteration 11735, loss = 1.48369333\n",
      "Iteration 11736, loss = 1.48368698\n",
      "Iteration 11737, loss = 1.48368062\n",
      "Iteration 11738, loss = 1.48367427\n",
      "Iteration 11739, loss = 1.48366791\n",
      "Iteration 11740, loss = 1.48366156\n",
      "Iteration 11741, loss = 1.48365521\n",
      "Iteration 11742, loss = 1.48364886\n",
      "Iteration 11743, loss = 1.48364251\n",
      "Iteration 11744, loss = 1.48363617\n",
      "Iteration 11745, loss = 1.48362982\n",
      "Iteration 11746, loss = 1.48362348\n",
      "Iteration 11747, loss = 1.48361713\n",
      "Iteration 11748, loss = 1.48361079\n",
      "Iteration 11749, loss = 1.48360445\n",
      "Iteration 11750, loss = 1.48359811\n",
      "Iteration 11751, loss = 1.48359177\n",
      "Iteration 11752, loss = 1.48358543\n",
      "Iteration 11753, loss = 1.48357910\n",
      "Iteration 11754, loss = 1.48357276\n",
      "Iteration 11755, loss = 1.48356643\n",
      "Iteration 11756, loss = 1.48356009\n",
      "Iteration 11757, loss = 1.48355376\n",
      "Iteration 11758, loss = 1.48354743\n",
      "Iteration 11759, loss = 1.48354110\n",
      "Iteration 11760, loss = 1.48353477\n",
      "Iteration 11761, loss = 1.48352845\n",
      "Iteration 11762, loss = 1.48352212\n",
      "Iteration 11763, loss = 1.48351579\n",
      "Iteration 11764, loss = 1.48350947\n",
      "Iteration 11765, loss = 1.48350315\n",
      "Iteration 11766, loss = 1.48349683\n",
      "Iteration 11767, loss = 1.48349051\n",
      "Iteration 11768, loss = 1.48348419\n",
      "Iteration 11769, loss = 1.48347787\n",
      "Iteration 11770, loss = 1.48347155\n",
      "Iteration 11771, loss = 1.48346523\n",
      "Iteration 11772, loss = 1.48345892\n",
      "Iteration 11773, loss = 1.48345261\n",
      "Iteration 11774, loss = 1.48344629\n",
      "Iteration 11775, loss = 1.48343998\n",
      "Iteration 11776, loss = 1.48343367\n",
      "Iteration 11777, loss = 1.48342736\n",
      "Iteration 11778, loss = 1.48342106\n",
      "Iteration 11779, loss = 1.48341475\n",
      "Iteration 11780, loss = 1.48340844\n",
      "Iteration 11781, loss = 1.48340214\n",
      "Iteration 11782, loss = 1.48339583\n",
      "Iteration 11783, loss = 1.48338953\n",
      "Iteration 11784, loss = 1.48338323\n",
      "Iteration 11785, loss = 1.48337693\n",
      "Iteration 11786, loss = 1.48337063\n",
      "Iteration 11787, loss = 1.48336434\n",
      "Iteration 11788, loss = 1.48335804\n",
      "Iteration 11789, loss = 1.48335174\n",
      "Iteration 11790, loss = 1.48334545\n",
      "Iteration 11791, loss = 1.48333916\n",
      "Iteration 11792, loss = 1.48333286\n",
      "Iteration 11793, loss = 1.48332657\n",
      "Iteration 11794, loss = 1.48332028\n",
      "Iteration 11795, loss = 1.48331399\n",
      "Iteration 11796, loss = 1.48330771\n",
      "Iteration 11797, loss = 1.48330142\n",
      "Iteration 11798, loss = 1.48329514\n",
      "Iteration 11799, loss = 1.48328885\n",
      "Iteration 11800, loss = 1.48328257\n",
      "Iteration 11801, loss = 1.48327629\n",
      "Iteration 11802, loss = 1.48327001\n",
      "Iteration 11803, loss = 1.48326373\n",
      "Iteration 11804, loss = 1.48325745\n",
      "Iteration 11805, loss = 1.48325117\n",
      "Iteration 11806, loss = 1.48324490\n",
      "Iteration 11807, loss = 1.48323862\n",
      "Iteration 11808, loss = 1.48323235\n",
      "Iteration 11809, loss = 1.48322607\n",
      "Iteration 11810, loss = 1.48321980\n",
      "Iteration 11811, loss = 1.48321353\n",
      "Iteration 11812, loss = 1.48320726\n",
      "Iteration 11813, loss = 1.48320100\n",
      "Iteration 11814, loss = 1.48319473\n",
      "Iteration 11815, loss = 1.48318846\n",
      "Iteration 11816, loss = 1.48318220\n",
      "Iteration 11817, loss = 1.48317593\n",
      "Iteration 11818, loss = 1.48316967\n",
      "Iteration 11819, loss = 1.48316341\n",
      "Iteration 11820, loss = 1.48315715\n",
      "Iteration 11821, loss = 1.48315089\n",
      "Iteration 11822, loss = 1.48314463\n",
      "Iteration 11823, loss = 1.48313838\n",
      "Iteration 11824, loss = 1.48313212\n",
      "Iteration 11825, loss = 1.48312587\n",
      "Iteration 11826, loss = 1.48311961\n",
      "Iteration 11827, loss = 1.48311336\n",
      "Iteration 11828, loss = 1.48310711\n",
      "Iteration 11829, loss = 1.48310086\n",
      "Iteration 11830, loss = 1.48309461\n",
      "Iteration 11831, loss = 1.48308836\n",
      "Iteration 11832, loss = 1.48308212\n",
      "Iteration 11833, loss = 1.48307587\n",
      "Iteration 11834, loss = 1.48306963\n",
      "Iteration 11835, loss = 1.48306338\n",
      "Iteration 11836, loss = 1.48305714\n",
      "Iteration 11837, loss = 1.48305090\n",
      "Iteration 11838, loss = 1.48304466\n",
      "Iteration 11839, loss = 1.48303842\n",
      "Iteration 11840, loss = 1.48303218\n",
      "Iteration 11841, loss = 1.48302595\n",
      "Iteration 11842, loss = 1.48301971\n",
      "Iteration 11843, loss = 1.48301348\n",
      "Iteration 11844, loss = 1.48300725\n",
      "Iteration 11845, loss = 1.48300101\n",
      "Iteration 11846, loss = 1.48299478\n",
      "Iteration 11847, loss = 1.48298855\n",
      "Iteration 11848, loss = 1.48298232\n",
      "Iteration 11849, loss = 1.48297610\n",
      "Iteration 11850, loss = 1.48296987\n",
      "Iteration 11851, loss = 1.48296365\n",
      "Iteration 11852, loss = 1.48295742\n",
      "Iteration 11853, loss = 1.48295120\n",
      "Iteration 11854, loss = 1.48294498\n",
      "Iteration 11855, loss = 1.48293876\n",
      "Iteration 11856, loss = 1.48293254\n",
      "Iteration 11857, loss = 1.48292632\n",
      "Iteration 11858, loss = 1.48292010\n",
      "Iteration 11859, loss = 1.48291388\n",
      "Iteration 11860, loss = 1.48290767\n",
      "Iteration 11861, loss = 1.48290145\n",
      "Iteration 11862, loss = 1.48289524\n",
      "Iteration 11863, loss = 1.48288903\n",
      "Iteration 11864, loss = 1.48288282\n",
      "Iteration 11865, loss = 1.48287661\n",
      "Iteration 11866, loss = 1.48287040\n",
      "Iteration 11867, loss = 1.48286419\n",
      "Iteration 11868, loss = 1.48285799\n",
      "Iteration 11869, loss = 1.48285178\n",
      "Iteration 11870, loss = 1.48284558\n",
      "Iteration 11871, loss = 1.48283938\n",
      "Iteration 11872, loss = 1.48283317\n",
      "Iteration 11873, loss = 1.48282697\n",
      "Iteration 11874, loss = 1.48282077\n",
      "Iteration 11875, loss = 1.48281458\n",
      "Iteration 11876, loss = 1.48280838\n",
      "Iteration 11877, loss = 1.48280218\n",
      "Iteration 11878, loss = 1.48279599\n",
      "Iteration 11879, loss = 1.48278979\n",
      "Iteration 11880, loss = 1.48278360\n",
      "Iteration 11881, loss = 1.48277741\n",
      "Iteration 11882, loss = 1.48277122\n",
      "Iteration 11883, loss = 1.48276503\n",
      "Iteration 11884, loss = 1.48275884\n",
      "Iteration 11885, loss = 1.48275265\n",
      "Iteration 11886, loss = 1.48274647\n",
      "Iteration 11887, loss = 1.48274028\n",
      "Iteration 11888, loss = 1.48273410\n",
      "Iteration 11889, loss = 1.48272792\n",
      "Iteration 11890, loss = 1.48272173\n",
      "Iteration 11891, loss = 1.48271555\n",
      "Iteration 11892, loss = 1.48270937\n",
      "Iteration 11893, loss = 1.48270320\n",
      "Iteration 11894, loss = 1.48269702\n",
      "Iteration 11895, loss = 1.48269084\n",
      "Iteration 11896, loss = 1.48268467\n",
      "Iteration 11897, loss = 1.48267849\n",
      "Iteration 11898, loss = 1.48267232\n",
      "Iteration 11899, loss = 1.48266615\n",
      "Iteration 11900, loss = 1.48265998\n",
      "Iteration 11901, loss = 1.48265381\n",
      "Iteration 11902, loss = 1.48264764\n",
      "Iteration 11903, loss = 1.48264147\n",
      "Iteration 11904, loss = 1.48263531\n",
      "Iteration 11905, loss = 1.48262914\n",
      "Iteration 11906, loss = 1.48262298\n",
      "Iteration 11907, loss = 1.48261682\n",
      "Iteration 11908, loss = 1.48261065\n",
      "Iteration 11909, loss = 1.48260449\n",
      "Iteration 11910, loss = 1.48259833\n",
      "Iteration 11911, loss = 1.48259218\n",
      "Iteration 11912, loss = 1.48258602\n",
      "Iteration 11913, loss = 1.48257986\n",
      "Iteration 11914, loss = 1.48257371\n",
      "Iteration 11915, loss = 1.48256755\n",
      "Iteration 11916, loss = 1.48256140\n",
      "Iteration 11917, loss = 1.48255525\n",
      "Iteration 11918, loss = 1.48254910\n",
      "Iteration 11919, loss = 1.48254295\n",
      "Iteration 11920, loss = 1.48253680\n",
      "Iteration 11921, loss = 1.48253065\n",
      "Iteration 11922, loss = 1.48252451\n",
      "Iteration 11923, loss = 1.48251836\n",
      "Iteration 11924, loss = 1.48251222\n",
      "Iteration 11925, loss = 1.48250607\n",
      "Iteration 11926, loss = 1.48249993\n",
      "Iteration 11927, loss = 1.48249379\n",
      "Iteration 11928, loss = 1.48248765\n",
      "Iteration 11929, loss = 1.48248151\n",
      "Iteration 11930, loss = 1.48247538\n",
      "Iteration 11931, loss = 1.48246924\n",
      "Iteration 11932, loss = 1.48246311\n",
      "Iteration 11933, loss = 1.48245697\n",
      "Iteration 11934, loss = 1.48245084\n",
      "Iteration 11935, loss = 1.48244471\n",
      "Iteration 11936, loss = 1.48243858\n",
      "Iteration 11937, loss = 1.48243245\n",
      "Iteration 11938, loss = 1.48242632\n",
      "Iteration 11939, loss = 1.48242019\n",
      "Iteration 11940, loss = 1.48241406\n",
      "Iteration 11941, loss = 1.48240794\n",
      "Iteration 11942, loss = 1.48240181\n",
      "Iteration 11943, loss = 1.48239569\n",
      "Iteration 11944, loss = 1.48238957\n",
      "Iteration 11945, loss = 1.48238345\n",
      "Iteration 11946, loss = 1.48237733\n",
      "Iteration 11947, loss = 1.48237121\n",
      "Iteration 11948, loss = 1.48236509\n",
      "Iteration 11949, loss = 1.48235897\n",
      "Iteration 11950, loss = 1.48235286\n",
      "Iteration 11951, loss = 1.48234674\n",
      "Iteration 11952, loss = 1.48234063\n",
      "Iteration 11953, loss = 1.48233452\n",
      "Iteration 11954, loss = 1.48232841\n",
      "Iteration 11955, loss = 1.48232230\n",
      "Iteration 11956, loss = 1.48231619\n",
      "Iteration 11957, loss = 1.48231008\n",
      "Iteration 11958, loss = 1.48230398\n",
      "Iteration 11959, loss = 1.48229787\n",
      "Iteration 11960, loss = 1.48229176\n",
      "Iteration 11961, loss = 1.48228566\n",
      "Iteration 11962, loss = 1.48227956\n",
      "Iteration 11963, loss = 1.48227346\n",
      "Iteration 11964, loss = 1.48226736\n",
      "Iteration 11965, loss = 1.48226126\n",
      "Iteration 11966, loss = 1.48225516\n",
      "Iteration 11967, loss = 1.48224906\n",
      "Iteration 11968, loss = 1.48224297\n",
      "Iteration 11969, loss = 1.48223687\n",
      "Iteration 11970, loss = 1.48223078\n",
      "Iteration 11971, loss = 1.48222469\n",
      "Iteration 11972, loss = 1.48221860\n",
      "Iteration 11973, loss = 1.48221251\n",
      "Iteration 11974, loss = 1.48220642\n",
      "Iteration 11975, loss = 1.48220033\n",
      "Iteration 11976, loss = 1.48219424\n",
      "Iteration 11977, loss = 1.48218816\n",
      "Iteration 11978, loss = 1.48218207\n",
      "Iteration 11979, loss = 1.48217599\n",
      "Iteration 11980, loss = 1.48216990\n",
      "Iteration 11981, loss = 1.48216382\n",
      "Iteration 11982, loss = 1.48215774\n",
      "Iteration 11983, loss = 1.48215166\n",
      "Iteration 11984, loss = 1.48214558\n",
      "Iteration 11985, loss = 1.48213951\n",
      "Iteration 11986, loss = 1.48213343\n",
      "Iteration 11987, loss = 1.48212736\n",
      "Iteration 11988, loss = 1.48212128\n",
      "Iteration 11989, loss = 1.48211521\n",
      "Iteration 11990, loss = 1.48210914\n",
      "Iteration 11991, loss = 1.48210307\n",
      "Iteration 11992, loss = 1.48209700\n",
      "Iteration 11993, loss = 1.48209093\n",
      "Iteration 11994, loss = 1.48208486\n",
      "Iteration 11995, loss = 1.48207879\n",
      "Iteration 11996, loss = 1.48207273\n",
      "Iteration 11997, loss = 1.48206666\n",
      "Iteration 11998, loss = 1.48206060\n",
      "Iteration 11999, loss = 1.48205454\n",
      "Iteration 12000, loss = 1.48204848\n",
      "Iteration 12001, loss = 1.48204242\n",
      "Iteration 12002, loss = 1.48203636\n",
      "Iteration 12003, loss = 1.48203030\n",
      "Iteration 12004, loss = 1.48202424\n",
      "Iteration 12005, loss = 1.48201819\n",
      "Iteration 12006, loss = 1.48201213\n",
      "Iteration 12007, loss = 1.48200608\n",
      "Iteration 12008, loss = 1.48200003\n",
      "Iteration 12009, loss = 1.48199398\n",
      "Iteration 12010, loss = 1.48198793\n",
      "Iteration 12011, loss = 1.48198188\n",
      "Iteration 12012, loss = 1.48197583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12013, loss = 1.48196978\n",
      "Iteration 12014, loss = 1.48196374\n",
      "Iteration 12015, loss = 1.48195769\n",
      "Iteration 12016, loss = 1.48195165\n",
      "Iteration 12017, loss = 1.48194561\n",
      "Iteration 12018, loss = 1.48193956\n",
      "Iteration 12019, loss = 1.48193352\n",
      "Iteration 12020, loss = 1.48192748\n",
      "Iteration 12021, loss = 1.48192145\n",
      "Iteration 12022, loss = 1.48191541\n",
      "Iteration 12023, loss = 1.48190937\n",
      "Iteration 12024, loss = 1.48190334\n",
      "Iteration 12025, loss = 1.48189730\n",
      "Iteration 12026, loss = 1.48189127\n",
      "Iteration 12027, loss = 1.48188524\n",
      "Iteration 12028, loss = 1.48187921\n",
      "Iteration 12029, loss = 1.48187318\n",
      "Iteration 12030, loss = 1.48186715\n",
      "Iteration 12031, loss = 1.48186112\n",
      "Iteration 12032, loss = 1.48185509\n",
      "Iteration 12033, loss = 1.48184907\n",
      "Iteration 12034, loss = 1.48184304\n",
      "Iteration 12035, loss = 1.48183702\n",
      "Iteration 12036, loss = 1.48183100\n",
      "Iteration 12037, loss = 1.48182498\n",
      "Iteration 12038, loss = 1.48181896\n",
      "Iteration 12039, loss = 1.48181294\n",
      "Iteration 12040, loss = 1.48180692\n",
      "Iteration 12041, loss = 1.48180090\n",
      "Iteration 12042, loss = 1.48179489\n",
      "Iteration 12043, loss = 1.48178887\n",
      "Iteration 12044, loss = 1.48178286\n",
      "Iteration 12045, loss = 1.48177685\n",
      "Iteration 12046, loss = 1.48177084\n",
      "Iteration 12047, loss = 1.48176482\n",
      "Iteration 12048, loss = 1.48175882\n",
      "Iteration 12049, loss = 1.48175281\n",
      "Iteration 12050, loss = 1.48174680\n",
      "Iteration 12051, loss = 1.48174079\n",
      "Iteration 12052, loss = 1.48173479\n",
      "Iteration 12053, loss = 1.48172878\n",
      "Iteration 12054, loss = 1.48172278\n",
      "Iteration 12055, loss = 1.48171678\n",
      "Iteration 12056, loss = 1.48171078\n",
      "Iteration 12057, loss = 1.48170478\n",
      "Iteration 12058, loss = 1.48169878\n",
      "Iteration 12059, loss = 1.48169278\n",
      "Iteration 12060, loss = 1.48168679\n",
      "Iteration 12061, loss = 1.48168079\n",
      "Iteration 12062, loss = 1.48167480\n",
      "Iteration 12063, loss = 1.48166880\n",
      "Iteration 12064, loss = 1.48166281\n",
      "Iteration 12065, loss = 1.48165682\n",
      "Iteration 12066, loss = 1.48165083\n",
      "Iteration 12067, loss = 1.48164484\n",
      "Iteration 12068, loss = 1.48163885\n",
      "Iteration 12069, loss = 1.48163286\n",
      "Iteration 12070, loss = 1.48162688\n",
      "Iteration 12071, loss = 1.48162089\n",
      "Iteration 12072, loss = 1.48161491\n",
      "Iteration 12073, loss = 1.48160893\n",
      "Iteration 12074, loss = 1.48160295\n",
      "Iteration 12075, loss = 1.48159696\n",
      "Iteration 12076, loss = 1.48159099\n",
      "Iteration 12077, loss = 1.48158501\n",
      "Iteration 12078, loss = 1.48157903\n",
      "Iteration 12079, loss = 1.48157305\n",
      "Iteration 12080, loss = 1.48156708\n",
      "Iteration 12081, loss = 1.48156110\n",
      "Iteration 12082, loss = 1.48155513\n",
      "Iteration 12083, loss = 1.48154916\n",
      "Iteration 12084, loss = 1.48154319\n",
      "Iteration 12085, loss = 1.48153722\n",
      "Iteration 12086, loss = 1.48153125\n",
      "Iteration 12087, loss = 1.48152528\n",
      "Iteration 12088, loss = 1.48151931\n",
      "Iteration 12089, loss = 1.48151335\n",
      "Iteration 12090, loss = 1.48150738\n",
      "Iteration 12091, loss = 1.48150142\n",
      "Iteration 12092, loss = 1.48149546\n",
      "Iteration 12093, loss = 1.48148949\n",
      "Iteration 12094, loss = 1.48148353\n",
      "Iteration 12095, loss = 1.48147757\n",
      "Iteration 12096, loss = 1.48147162\n",
      "Iteration 12097, loss = 1.48146566\n",
      "Iteration 12098, loss = 1.48145970\n",
      "Iteration 12099, loss = 1.48145375\n",
      "Iteration 12100, loss = 1.48144779\n",
      "Iteration 12101, loss = 1.48144184\n",
      "Iteration 12102, loss = 1.48143589\n",
      "Iteration 12103, loss = 1.48142994\n",
      "Iteration 12104, loss = 1.48142399\n",
      "Iteration 12105, loss = 1.48141804\n",
      "Iteration 12106, loss = 1.48141209\n",
      "Iteration 12107, loss = 1.48140614\n",
      "Iteration 12108, loss = 1.48140020\n",
      "Iteration 12109, loss = 1.48139425\n",
      "Iteration 12110, loss = 1.48138831\n",
      "Iteration 12111, loss = 1.48138237\n",
      "Iteration 12112, loss = 1.48137643\n",
      "Iteration 12113, loss = 1.48137048\n",
      "Iteration 12114, loss = 1.48136455\n",
      "Iteration 12115, loss = 1.48135861\n",
      "Iteration 12116, loss = 1.48135267\n",
      "Iteration 12117, loss = 1.48134673\n",
      "Iteration 12118, loss = 1.48134080\n",
      "Iteration 12119, loss = 1.48133486\n",
      "Iteration 12120, loss = 1.48132893\n",
      "Iteration 12121, loss = 1.48132300\n",
      "Iteration 12122, loss = 1.48131707\n",
      "Iteration 12123, loss = 1.48131114\n",
      "Iteration 12124, loss = 1.48130521\n",
      "Iteration 12125, loss = 1.48129928\n",
      "Iteration 12126, loss = 1.48129335\n",
      "Iteration 12127, loss = 1.48128743\n",
      "Iteration 12128, loss = 1.48128150\n",
      "Iteration 12129, loss = 1.48127558\n",
      "Iteration 12130, loss = 1.48126966\n",
      "Iteration 12131, loss = 1.48126374\n",
      "Iteration 12132, loss = 1.48125782\n",
      "Iteration 12133, loss = 1.48125190\n",
      "Iteration 12134, loss = 1.48124598\n",
      "Iteration 12135, loss = 1.48124006\n",
      "Iteration 12136, loss = 1.48123414\n",
      "Iteration 12137, loss = 1.48122823\n",
      "Iteration 12138, loss = 1.48122231\n",
      "Iteration 12139, loss = 1.48121640\n",
      "Iteration 12140, loss = 1.48121049\n",
      "Iteration 12141, loss = 1.48120458\n",
      "Iteration 12142, loss = 1.48119867\n",
      "Iteration 12143, loss = 1.48119276\n",
      "Iteration 12144, loss = 1.48118685\n",
      "Iteration 12145, loss = 1.48118094\n",
      "Iteration 12146, loss = 1.48117504\n",
      "Iteration 12147, loss = 1.48116913\n",
      "Iteration 12148, loss = 1.48116323\n",
      "Iteration 12149, loss = 1.48115733\n",
      "Iteration 12150, loss = 1.48115142\n",
      "Iteration 12151, loss = 1.48114552\n",
      "Iteration 12152, loss = 1.48113962\n",
      "Iteration 12153, loss = 1.48113373\n",
      "Iteration 12154, loss = 1.48112783\n",
      "Iteration 12155, loss = 1.48112193\n",
      "Iteration 12156, loss = 1.48111604\n",
      "Iteration 12157, loss = 1.48111014\n",
      "Iteration 12158, loss = 1.48110425\n",
      "Iteration 12159, loss = 1.48109836\n",
      "Iteration 12160, loss = 1.48109246\n",
      "Iteration 12161, loss = 1.48108657\n",
      "Iteration 12162, loss = 1.48108069\n",
      "Iteration 12163, loss = 1.48107480\n",
      "Iteration 12164, loss = 1.48106891\n",
      "Iteration 12165, loss = 1.48106302\n",
      "Iteration 12166, loss = 1.48105714\n",
      "Iteration 12167, loss = 1.48105125\n",
      "Iteration 12168, loss = 1.48104537\n",
      "Iteration 12169, loss = 1.48103949\n",
      "Iteration 12170, loss = 1.48103361\n",
      "Iteration 12171, loss = 1.48102773\n",
      "Iteration 12172, loss = 1.48102185\n",
      "Iteration 12173, loss = 1.48101597\n",
      "Iteration 12174, loss = 1.48101010\n",
      "Iteration 12175, loss = 1.48100422\n",
      "Iteration 12176, loss = 1.48099835\n",
      "Iteration 12177, loss = 1.48099247\n",
      "Iteration 12178, loss = 1.48098660\n",
      "Iteration 12179, loss = 1.48098073\n",
      "Iteration 12180, loss = 1.48097486\n",
      "Iteration 12181, loss = 1.48096899\n",
      "Iteration 12182, loss = 1.48096312\n",
      "Iteration 12183, loss = 1.48095725\n",
      "Iteration 12184, loss = 1.48095138\n",
      "Iteration 12185, loss = 1.48094552\n",
      "Iteration 12186, loss = 1.48093965\n",
      "Iteration 12187, loss = 1.48093379\n",
      "Iteration 12188, loss = 1.48092793\n",
      "Iteration 12189, loss = 1.48092207\n",
      "Iteration 12190, loss = 1.48091621\n",
      "Iteration 12191, loss = 1.48091035\n",
      "Iteration 12192, loss = 1.48090449\n",
      "Iteration 12193, loss = 1.48089863\n",
      "Iteration 12194, loss = 1.48089278\n",
      "Iteration 12195, loss = 1.48088692\n",
      "Iteration 12196, loss = 1.48088107\n",
      "Iteration 12197, loss = 1.48087522\n",
      "Iteration 12198, loss = 1.48086936\n",
      "Iteration 12199, loss = 1.48086351\n",
      "Iteration 12200, loss = 1.48085766\n",
      "Iteration 12201, loss = 1.48085181\n",
      "Iteration 12202, loss = 1.48084597\n",
      "Iteration 12203, loss = 1.48084012\n",
      "Iteration 12204, loss = 1.48083427\n",
      "Iteration 12205, loss = 1.48082843\n",
      "Iteration 12206, loss = 1.48082258\n",
      "Iteration 12207, loss = 1.48081674\n",
      "Iteration 12208, loss = 1.48081090\n",
      "Iteration 12209, loss = 1.48080506\n",
      "Iteration 12210, loss = 1.48079922\n",
      "Iteration 12211, loss = 1.48079338\n",
      "Iteration 12212, loss = 1.48078754\n",
      "Iteration 12213, loss = 1.48078171\n",
      "Iteration 12214, loss = 1.48077587\n",
      "Iteration 12215, loss = 1.48077004\n",
      "Iteration 12216, loss = 1.48076420\n",
      "Iteration 12217, loss = 1.48075837\n",
      "Iteration 12218, loss = 1.48075254\n",
      "Iteration 12219, loss = 1.48074671\n",
      "Iteration 12220, loss = 1.48074088\n",
      "Iteration 12221, loss = 1.48073505\n",
      "Iteration 12222, loss = 1.48072922\n",
      "Iteration 12223, loss = 1.48072340\n",
      "Iteration 12224, loss = 1.48071757\n",
      "Iteration 12225, loss = 1.48071175\n",
      "Iteration 12226, loss = 1.48070593\n",
      "Iteration 12227, loss = 1.48070010\n",
      "Iteration 12228, loss = 1.48069428\n",
      "Iteration 12229, loss = 1.48068846\n",
      "Iteration 12230, loss = 1.48068264\n",
      "Iteration 12231, loss = 1.48067682\n",
      "Iteration 12232, loss = 1.48067101\n",
      "Iteration 12233, loss = 1.48066519\n",
      "Iteration 12234, loss = 1.48065938\n",
      "Iteration 12235, loss = 1.48065356\n",
      "Iteration 12236, loss = 1.48064775\n",
      "Iteration 12237, loss = 1.48064194\n",
      "Iteration 12238, loss = 1.48063613\n",
      "Iteration 12239, loss = 1.48063032\n",
      "Iteration 12240, loss = 1.48062451\n",
      "Iteration 12241, loss = 1.48061870\n",
      "Iteration 12242, loss = 1.48061289\n",
      "Iteration 12243, loss = 1.48060709\n",
      "Iteration 12244, loss = 1.48060128\n",
      "Iteration 12245, loss = 1.48059548\n",
      "Iteration 12246, loss = 1.48058968\n",
      "Iteration 12247, loss = 1.48058387\n",
      "Iteration 12248, loss = 1.48057807\n",
      "Iteration 12249, loss = 1.48057227\n",
      "Iteration 12250, loss = 1.48056647\n",
      "Iteration 12251, loss = 1.48056068\n",
      "Iteration 12252, loss = 1.48055488\n",
      "Iteration 12253, loss = 1.48054908\n",
      "Iteration 12254, loss = 1.48054329\n",
      "Iteration 12255, loss = 1.48053750\n",
      "Iteration 12256, loss = 1.48053170\n",
      "Iteration 12257, loss = 1.48052591\n",
      "Iteration 12258, loss = 1.48052012\n",
      "Iteration 12259, loss = 1.48051433\n",
      "Iteration 12260, loss = 1.48050854\n",
      "Iteration 12261, loss = 1.48050276\n",
      "Iteration 12262, loss = 1.48049697\n",
      "Iteration 12263, loss = 1.48049118\n",
      "Iteration 12264, loss = 1.48048540\n",
      "Iteration 12265, loss = 1.48047962\n",
      "Iteration 12266, loss = 1.48047383\n",
      "Iteration 12267, loss = 1.48046805\n",
      "Iteration 12268, loss = 1.48046227\n",
      "Iteration 12269, loss = 1.48045649\n",
      "Iteration 12270, loss = 1.48045071\n",
      "Iteration 12271, loss = 1.48044494\n",
      "Iteration 12272, loss = 1.48043916\n",
      "Iteration 12273, loss = 1.48043338\n",
      "Iteration 12274, loss = 1.48042761\n",
      "Iteration 12275, loss = 1.48042184\n",
      "Iteration 12276, loss = 1.48041606\n",
      "Iteration 12277, loss = 1.48041029\n",
      "Iteration 12278, loss = 1.48040452\n",
      "Iteration 12279, loss = 1.48039875\n",
      "Iteration 12280, loss = 1.48039298\n",
      "Iteration 12281, loss = 1.48038722\n",
      "Iteration 12282, loss = 1.48038145\n",
      "Iteration 12283, loss = 1.48037569\n",
      "Iteration 12284, loss = 1.48036992\n",
      "Iteration 12285, loss = 1.48036416\n",
      "Iteration 12286, loss = 1.48035840\n",
      "Iteration 12287, loss = 1.48035263\n",
      "Iteration 12288, loss = 1.48034687\n",
      "Iteration 12289, loss = 1.48034111\n",
      "Iteration 12290, loss = 1.48033536\n",
      "Iteration 12291, loss = 1.48032960\n",
      "Iteration 12292, loss = 1.48032384\n",
      "Iteration 12293, loss = 1.48031809\n",
      "Iteration 12294, loss = 1.48031233\n",
      "Iteration 12295, loss = 1.48030658\n",
      "Iteration 12296, loss = 1.48030083\n",
      "Iteration 12297, loss = 1.48029508\n",
      "Iteration 12298, loss = 1.48028933\n",
      "Iteration 12299, loss = 1.48028358\n",
      "Iteration 12300, loss = 1.48027783\n",
      "Iteration 12301, loss = 1.48027208\n",
      "Iteration 12302, loss = 1.48026634\n",
      "Iteration 12303, loss = 1.48026059\n",
      "Iteration 12304, loss = 1.48025485\n",
      "Iteration 12305, loss = 1.48024910\n",
      "Iteration 12306, loss = 1.48024336\n",
      "Iteration 12307, loss = 1.48023762\n",
      "Iteration 12308, loss = 1.48023188\n",
      "Iteration 12309, loss = 1.48022614\n",
      "Iteration 12310, loss = 1.48022040\n",
      "Iteration 12311, loss = 1.48021467\n",
      "Iteration 12312, loss = 1.48020893\n",
      "Iteration 12313, loss = 1.48020319\n",
      "Iteration 12314, loss = 1.48019746\n",
      "Iteration 12315, loss = 1.48019173\n",
      "Iteration 12316, loss = 1.48018600\n",
      "Iteration 12317, loss = 1.48018026\n",
      "Iteration 12318, loss = 1.48017453\n",
      "Iteration 12319, loss = 1.48016881\n",
      "Iteration 12320, loss = 1.48016308\n",
      "Iteration 12321, loss = 1.48015735\n",
      "Iteration 12322, loss = 1.48015162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12323, loss = 1.48014590\n",
      "Iteration 12324, loss = 1.48014017\n",
      "Iteration 12325, loss = 1.48013445\n",
      "Iteration 12326, loss = 1.48012873\n",
      "Iteration 12327, loss = 1.48012301\n",
      "Iteration 12328, loss = 1.48011729\n",
      "Iteration 12329, loss = 1.48011157\n",
      "Iteration 12330, loss = 1.48010585\n",
      "Iteration 12331, loss = 1.48010013\n",
      "Iteration 12332, loss = 1.48009442\n",
      "Iteration 12333, loss = 1.48008870\n",
      "Iteration 12334, loss = 1.48008299\n",
      "Iteration 12335, loss = 1.48007728\n",
      "Iteration 12336, loss = 1.48007156\n",
      "Iteration 12337, loss = 1.48006585\n",
      "Iteration 12338, loss = 1.48006014\n",
      "Iteration 12339, loss = 1.48005443\n",
      "Iteration 12340, loss = 1.48004872\n",
      "Iteration 12341, loss = 1.48004302\n",
      "Iteration 12342, loss = 1.48003731\n",
      "Iteration 12343, loss = 1.48003161\n",
      "Iteration 12344, loss = 1.48002590\n",
      "Iteration 12345, loss = 1.48002020\n",
      "Iteration 12346, loss = 1.48001450\n",
      "Iteration 12347, loss = 1.48000879\n",
      "Iteration 12348, loss = 1.48000309\n",
      "Iteration 12349, loss = 1.47999740\n",
      "Iteration 12350, loss = 1.47999170\n",
      "Iteration 12351, loss = 1.47998600\n",
      "Iteration 12352, loss = 1.47998030\n",
      "Iteration 12353, loss = 1.47997461\n",
      "Iteration 12354, loss = 1.47996891\n",
      "Iteration 12355, loss = 1.47996322\n",
      "Iteration 12356, loss = 1.47995753\n",
      "Iteration 12357, loss = 1.47995184\n",
      "Iteration 12358, loss = 1.47994615\n",
      "Iteration 12359, loss = 1.47994046\n",
      "Iteration 12360, loss = 1.47993477\n",
      "Iteration 12361, loss = 1.47992908\n",
      "Iteration 12362, loss = 1.47992340\n",
      "Iteration 12363, loss = 1.47991771\n",
      "Iteration 12364, loss = 1.47991203\n",
      "Iteration 12365, loss = 1.47990634\n",
      "Iteration 12366, loss = 1.47990066\n",
      "Iteration 12367, loss = 1.47989498\n",
      "Iteration 12368, loss = 1.47988930\n",
      "Iteration 12369, loss = 1.47988362\n",
      "Iteration 12370, loss = 1.47987794\n",
      "Iteration 12371, loss = 1.47987226\n",
      "Iteration 12372, loss = 1.47986659\n",
      "Iteration 12373, loss = 1.47986091\n",
      "Iteration 12374, loss = 1.47985524\n",
      "Iteration 12375, loss = 1.47984956\n",
      "Iteration 12376, loss = 1.47984389\n",
      "Iteration 12377, loss = 1.47983822\n",
      "Iteration 12378, loss = 1.47983255\n",
      "Iteration 12379, loss = 1.47982688\n",
      "Iteration 12380, loss = 1.47982121\n",
      "Iteration 12381, loss = 1.47981554\n",
      "Iteration 12382, loss = 1.47980988\n",
      "Iteration 12383, loss = 1.47980421\n",
      "Iteration 12384, loss = 1.47979855\n",
      "Iteration 12385, loss = 1.47979288\n",
      "Iteration 12386, loss = 1.47978722\n",
      "Iteration 12387, loss = 1.47978156\n",
      "Iteration 12388, loss = 1.47977590\n",
      "Iteration 12389, loss = 1.47977024\n",
      "Iteration 12390, loss = 1.47976458\n",
      "Iteration 12391, loss = 1.47975892\n",
      "Iteration 12392, loss = 1.47975327\n",
      "Iteration 12393, loss = 1.47974761\n",
      "Iteration 12394, loss = 1.47974196\n",
      "Iteration 12395, loss = 1.47973630\n",
      "Iteration 12396, loss = 1.47973065\n",
      "Iteration 12397, loss = 1.47972500\n",
      "Iteration 12398, loss = 1.47971935\n",
      "Iteration 12399, loss = 1.47971370\n",
      "Iteration 12400, loss = 1.47970805\n",
      "Iteration 12401, loss = 1.47970240\n",
      "Iteration 12402, loss = 1.47969675\n",
      "Iteration 12403, loss = 1.47969111\n",
      "Iteration 12404, loss = 1.47968546\n",
      "Iteration 12405, loss = 1.47967982\n",
      "Iteration 12406, loss = 1.47967417\n",
      "Iteration 12407, loss = 1.47966853\n",
      "Iteration 12408, loss = 1.47966289\n",
      "Iteration 12409, loss = 1.47965725\n",
      "Iteration 12410, loss = 1.47965161\n",
      "Iteration 12411, loss = 1.47964597\n",
      "Iteration 12412, loss = 1.47964034\n",
      "Iteration 12413, loss = 1.47963470\n",
      "Iteration 12414, loss = 1.47962907\n",
      "Iteration 12415, loss = 1.47962343\n",
      "Iteration 12416, loss = 1.47961780\n",
      "Iteration 12417, loss = 1.47961217\n",
      "Iteration 12418, loss = 1.47960653\n",
      "Iteration 12419, loss = 1.47960090\n",
      "Iteration 12420, loss = 1.47959528\n",
      "Iteration 12421, loss = 1.47958965\n",
      "Iteration 12422, loss = 1.47958402\n",
      "Iteration 12423, loss = 1.47957839\n",
      "Iteration 12424, loss = 1.47957277\n",
      "Iteration 12425, loss = 1.47956714\n",
      "Iteration 12426, loss = 1.47956152\n",
      "Iteration 12427, loss = 1.47955590\n",
      "Iteration 12428, loss = 1.47955028\n",
      "Iteration 12429, loss = 1.47954466\n",
      "Iteration 12430, loss = 1.47953904\n",
      "Iteration 12431, loss = 1.47953342\n",
      "Iteration 12432, loss = 1.47952780\n",
      "Iteration 12433, loss = 1.47952218\n",
      "Iteration 12434, loss = 1.47951657\n",
      "Iteration 12435, loss = 1.47951095\n",
      "Iteration 12436, loss = 1.47950534\n",
      "Iteration 12437, loss = 1.47949973\n",
      "Iteration 12438, loss = 1.47949411\n",
      "Iteration 12439, loss = 1.47948850\n",
      "Iteration 12440, loss = 1.47948289\n",
      "Iteration 12441, loss = 1.47947729\n",
      "Iteration 12442, loss = 1.47947168\n",
      "Iteration 12443, loss = 1.47946607\n",
      "Iteration 12444, loss = 1.47946046\n",
      "Iteration 12445, loss = 1.47945486\n",
      "Iteration 12446, loss = 1.47944926\n",
      "Iteration 12447, loss = 1.47944365\n",
      "Iteration 12448, loss = 1.47943805\n",
      "Iteration 12449, loss = 1.47943245\n",
      "Iteration 12450, loss = 1.47942685\n",
      "Iteration 12451, loss = 1.47942125\n",
      "Iteration 12452, loss = 1.47941565\n",
      "Iteration 12453, loss = 1.47941005\n",
      "Iteration 12454, loss = 1.47940446\n",
      "Iteration 12455, loss = 1.47939886\n",
      "Iteration 12456, loss = 1.47939327\n",
      "Iteration 12457, loss = 1.47938768\n",
      "Iteration 12458, loss = 1.47938208\n",
      "Iteration 12459, loss = 1.47937649\n",
      "Iteration 12460, loss = 1.47937090\n",
      "Iteration 12461, loss = 1.47936531\n",
      "Iteration 12462, loss = 1.47935972\n",
      "Iteration 12463, loss = 1.47935414\n",
      "Iteration 12464, loss = 1.47934855\n",
      "Iteration 12465, loss = 1.47934296\n",
      "Iteration 12466, loss = 1.47933738\n",
      "Iteration 12467, loss = 1.47933180\n",
      "Iteration 12468, loss = 1.47932621\n",
      "Iteration 12469, loss = 1.47932063\n",
      "Iteration 12470, loss = 1.47931505\n",
      "Iteration 12471, loss = 1.47930947\n",
      "Iteration 12472, loss = 1.47930389\n",
      "Iteration 12473, loss = 1.47929831\n",
      "Iteration 12474, loss = 1.47929274\n",
      "Iteration 12475, loss = 1.47928716\n",
      "Iteration 12476, loss = 1.47928159\n",
      "Iteration 12477, loss = 1.47927601\n",
      "Iteration 12478, loss = 1.47927044\n",
      "Iteration 12479, loss = 1.47926487\n",
      "Iteration 12480, loss = 1.47925930\n",
      "Iteration 12481, loss = 1.47925373\n",
      "Iteration 12482, loss = 1.47924816\n",
      "Iteration 12483, loss = 1.47924259\n",
      "Iteration 12484, loss = 1.47923702\n",
      "Iteration 12485, loss = 1.47923145\n",
      "Iteration 12486, loss = 1.47922589\n",
      "Iteration 12487, loss = 1.47922032\n",
      "Iteration 12488, loss = 1.47921476\n",
      "Iteration 12489, loss = 1.47920920\n",
      "Iteration 12490, loss = 1.47920364\n",
      "Iteration 12491, loss = 1.47919808\n",
      "Iteration 12492, loss = 1.47919252\n",
      "Iteration 12493, loss = 1.47918696\n",
      "Iteration 12494, loss = 1.47918140\n",
      "Iteration 12495, loss = 1.47917584\n",
      "Iteration 12496, loss = 1.47917029\n",
      "Iteration 12497, loss = 1.47916473\n",
      "Iteration 12498, loss = 1.47915918\n",
      "Iteration 12499, loss = 1.47915363\n",
      "Iteration 12500, loss = 1.47914807\n",
      "Iteration 12501, loss = 1.47914252\n",
      "Iteration 12502, loss = 1.47913697\n",
      "Iteration 12503, loss = 1.47913142\n",
      "Iteration 12504, loss = 1.47912588\n",
      "Iteration 12505, loss = 1.47912033\n",
      "Iteration 12506, loss = 1.47911478\n",
      "Iteration 12507, loss = 1.47910924\n",
      "Iteration 12508, loss = 1.47910369\n",
      "Iteration 12509, loss = 1.47909815\n",
      "Iteration 12510, loss = 1.47909261\n",
      "Iteration 12511, loss = 1.47908707\n",
      "Iteration 12512, loss = 1.47908153\n",
      "Iteration 12513, loss = 1.47907599\n",
      "Iteration 12514, loss = 1.47907045\n",
      "Iteration 12515, loss = 1.47906491\n",
      "Iteration 12516, loss = 1.47905937\n",
      "Iteration 12517, loss = 1.47905384\n",
      "Iteration 12518, loss = 1.47904830\n",
      "Iteration 12519, loss = 1.47904277\n",
      "Iteration 12520, loss = 1.47903724\n",
      "Iteration 12521, loss = 1.47903170\n",
      "Iteration 12522, loss = 1.47902617\n",
      "Iteration 12523, loss = 1.47902064\n",
      "Iteration 12524, loss = 1.47901511\n",
      "Iteration 12525, loss = 1.47900959\n",
      "Iteration 12526, loss = 1.47900406\n",
      "Iteration 12527, loss = 1.47899853\n",
      "Iteration 12528, loss = 1.47899301\n",
      "Iteration 12529, loss = 1.47898748\n",
      "Iteration 12530, loss = 1.47898196\n",
      "Iteration 12531, loss = 1.47897644\n",
      "Iteration 12532, loss = 1.47897092\n",
      "Iteration 12533, loss = 1.47896540\n",
      "Iteration 12534, loss = 1.47895988\n",
      "Iteration 12535, loss = 1.47895436\n",
      "Iteration 12536, loss = 1.47894884\n",
      "Iteration 12537, loss = 1.47894332\n",
      "Iteration 12538, loss = 1.47893781\n",
      "Iteration 12539, loss = 1.47893229\n",
      "Iteration 12540, loss = 1.47892678\n",
      "Iteration 12541, loss = 1.47892127\n",
      "Iteration 12542, loss = 1.47891576\n",
      "Iteration 12543, loss = 1.47891024\n",
      "Iteration 12544, loss = 1.47890473\n",
      "Iteration 12545, loss = 1.47889923\n",
      "Iteration 12546, loss = 1.47889372\n",
      "Iteration 12547, loss = 1.47888821\n",
      "Iteration 12548, loss = 1.47888270\n",
      "Iteration 12549, loss = 1.47887720\n",
      "Iteration 12550, loss = 1.47887169\n",
      "Iteration 12551, loss = 1.47886619\n",
      "Iteration 12552, loss = 1.47886069\n",
      "Iteration 12553, loss = 1.47885519\n",
      "Iteration 12554, loss = 1.47884969\n",
      "Iteration 12555, loss = 1.47884419\n",
      "Iteration 12556, loss = 1.47883869\n",
      "Iteration 12557, loss = 1.47883319\n",
      "Iteration 12558, loss = 1.47882769\n",
      "Iteration 12559, loss = 1.47882220\n",
      "Iteration 12560, loss = 1.47881670\n",
      "Iteration 12561, loss = 1.47881121\n",
      "Iteration 12562, loss = 1.47880572\n",
      "Iteration 12563, loss = 1.47880022\n",
      "Iteration 12564, loss = 1.47879473\n",
      "Iteration 12565, loss = 1.47878924\n",
      "Iteration 12566, loss = 1.47878375\n",
      "Iteration 12567, loss = 1.47877826\n",
      "Iteration 12568, loss = 1.47877278\n",
      "Iteration 12569, loss = 1.47876729\n",
      "Iteration 12570, loss = 1.47876181\n",
      "Iteration 12571, loss = 1.47875632\n",
      "Iteration 12572, loss = 1.47875084\n",
      "Iteration 12573, loss = 1.47874535\n",
      "Iteration 12574, loss = 1.47873987\n",
      "Iteration 12575, loss = 1.47873439\n",
      "Iteration 12576, loss = 1.47872891\n",
      "Iteration 12577, loss = 1.47872343\n",
      "Iteration 12578, loss = 1.47871796\n",
      "Iteration 12579, loss = 1.47871248\n",
      "Iteration 12580, loss = 1.47870700\n",
      "Iteration 12581, loss = 1.47870153\n",
      "Iteration 12582, loss = 1.47869605\n",
      "Iteration 12583, loss = 1.47869058\n",
      "Iteration 12584, loss = 1.47868511\n",
      "Iteration 12585, loss = 1.47867964\n",
      "Iteration 12586, loss = 1.47867417\n",
      "Iteration 12587, loss = 1.47866870\n",
      "Iteration 12588, loss = 1.47866323\n",
      "Iteration 12589, loss = 1.47865776\n",
      "Iteration 12590, loss = 1.47865229\n",
      "Iteration 12591, loss = 1.47864683\n",
      "Iteration 12592, loss = 1.47864136\n",
      "Iteration 12593, loss = 1.47863590\n",
      "Iteration 12594, loss = 1.47863044\n",
      "Iteration 12595, loss = 1.47862497\n",
      "Iteration 12596, loss = 1.47861951\n",
      "Iteration 12597, loss = 1.47861405\n",
      "Iteration 12598, loss = 1.47860859\n",
      "Iteration 12599, loss = 1.47860314\n",
      "Iteration 12600, loss = 1.47859768\n",
      "Iteration 12601, loss = 1.47859222\n",
      "Iteration 12602, loss = 1.47858677\n",
      "Iteration 12603, loss = 1.47858131\n",
      "Iteration 12604, loss = 1.47857586\n",
      "Iteration 12605, loss = 1.47857041\n",
      "Iteration 12606, loss = 1.47856495\n",
      "Iteration 12607, loss = 1.47855950\n",
      "Iteration 12608, loss = 1.47855405\n",
      "Iteration 12609, loss = 1.47854861\n",
      "Iteration 12610, loss = 1.47854316\n",
      "Iteration 12611, loss = 1.47853771\n",
      "Iteration 12612, loss = 1.47853226\n",
      "Iteration 12613, loss = 1.47852682\n",
      "Iteration 12614, loss = 1.47852137\n",
      "Iteration 12615, loss = 1.47851593\n",
      "Iteration 12616, loss = 1.47851049\n",
      "Iteration 12617, loss = 1.47850505\n",
      "Iteration 12618, loss = 1.47849961\n",
      "Iteration 12619, loss = 1.47849417\n",
      "Iteration 12620, loss = 1.47848873\n",
      "Iteration 12621, loss = 1.47848329\n",
      "Iteration 12622, loss = 1.47847785\n",
      "Iteration 12623, loss = 1.47847242\n",
      "Iteration 12624, loss = 1.47846698\n",
      "Iteration 12625, loss = 1.47846155\n",
      "Iteration 12626, loss = 1.47845612\n",
      "Iteration 12627, loss = 1.47845068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12628, loss = 1.47844525\n",
      "Iteration 12629, loss = 1.47843982\n",
      "Iteration 12630, loss = 1.47843439\n",
      "Iteration 12631, loss = 1.47842897\n",
      "Iteration 12632, loss = 1.47842354\n",
      "Iteration 12633, loss = 1.47841811\n",
      "Iteration 12634, loss = 1.47841269\n",
      "Iteration 12635, loss = 1.47840726\n",
      "Iteration 12636, loss = 1.47840184\n",
      "Iteration 12637, loss = 1.47839641\n",
      "Iteration 12638, loss = 1.47839099\n",
      "Iteration 12639, loss = 1.47838557\n",
      "Iteration 12640, loss = 1.47838015\n",
      "Iteration 12641, loss = 1.47837473\n",
      "Iteration 12642, loss = 1.47836931\n",
      "Iteration 12643, loss = 1.47836390\n",
      "Iteration 12644, loss = 1.47835848\n",
      "Iteration 12645, loss = 1.47835306\n",
      "Iteration 12646, loss = 1.47834765\n",
      "Iteration 12647, loss = 1.47834224\n",
      "Iteration 12648, loss = 1.47833682\n",
      "Iteration 12649, loss = 1.47833141\n",
      "Iteration 12650, loss = 1.47832600\n",
      "Iteration 12651, loss = 1.47832059\n",
      "Iteration 12652, loss = 1.47831518\n",
      "Iteration 12653, loss = 1.47830977\n",
      "Iteration 12654, loss = 1.47830437\n",
      "Iteration 12655, loss = 1.47829896\n",
      "Iteration 12656, loss = 1.47829356\n",
      "Iteration 12657, loss = 1.47828815\n",
      "Iteration 12658, loss = 1.47828275\n",
      "Iteration 12659, loss = 1.47827735\n",
      "Iteration 12660, loss = 1.47827194\n",
      "Iteration 12661, loss = 1.47826654\n",
      "Iteration 12662, loss = 1.47826114\n",
      "Iteration 12663, loss = 1.47825574\n",
      "Iteration 12664, loss = 1.47825035\n",
      "Iteration 12665, loss = 1.47824495\n",
      "Iteration 12666, loss = 1.47823955\n",
      "Iteration 12667, loss = 1.47823416\n",
      "Iteration 12668, loss = 1.47822876\n",
      "Iteration 12669, loss = 1.47822337\n",
      "Iteration 12670, loss = 1.47821798\n",
      "Iteration 12671, loss = 1.47821259\n",
      "Iteration 12672, loss = 1.47820720\n",
      "Iteration 12673, loss = 1.47820181\n",
      "Iteration 12674, loss = 1.47819642\n",
      "Iteration 12675, loss = 1.47819103\n",
      "Iteration 12676, loss = 1.47818564\n",
      "Iteration 12677, loss = 1.47818026\n",
      "Iteration 12678, loss = 1.47817487\n",
      "Iteration 12679, loss = 1.47816949\n",
      "Iteration 12680, loss = 1.47816411\n",
      "Iteration 12681, loss = 1.47815872\n",
      "Iteration 12682, loss = 1.47815334\n",
      "Iteration 12683, loss = 1.47814796\n",
      "Iteration 12684, loss = 1.47814258\n",
      "Iteration 12685, loss = 1.47813720\n",
      "Iteration 12686, loss = 1.47813183\n",
      "Iteration 12687, loss = 1.47812645\n",
      "Iteration 12688, loss = 1.47812107\n",
      "Iteration 12689, loss = 1.47811570\n",
      "Iteration 12690, loss = 1.47811032\n",
      "Iteration 12691, loss = 1.47810495\n",
      "Iteration 12692, loss = 1.47809958\n",
      "Iteration 12693, loss = 1.47809421\n",
      "Iteration 12694, loss = 1.47808884\n",
      "Iteration 12695, loss = 1.47808347\n",
      "Iteration 12696, loss = 1.47807810\n",
      "Iteration 12697, loss = 1.47807273\n",
      "Iteration 12698, loss = 1.47806736\n",
      "Iteration 12699, loss = 1.47806200\n",
      "Iteration 12700, loss = 1.47805663\n",
      "Iteration 12701, loss = 1.47805127\n",
      "Iteration 12702, loss = 1.47804591\n",
      "Iteration 12703, loss = 1.47804054\n",
      "Iteration 12704, loss = 1.47803518\n",
      "Iteration 12705, loss = 1.47802982\n",
      "Iteration 12706, loss = 1.47802446\n",
      "Iteration 12707, loss = 1.47801911\n",
      "Iteration 12708, loss = 1.47801375\n",
      "Iteration 12709, loss = 1.47800839\n",
      "Iteration 12710, loss = 1.47800304\n",
      "Iteration 12711, loss = 1.47799768\n",
      "Iteration 12712, loss = 1.47799233\n",
      "Iteration 12713, loss = 1.47798697\n",
      "Iteration 12714, loss = 1.47798162\n",
      "Iteration 12715, loss = 1.47797627\n",
      "Iteration 12716, loss = 1.47797092\n",
      "Iteration 12717, loss = 1.47796557\n",
      "Iteration 12718, loss = 1.47796022\n",
      "Iteration 12719, loss = 1.47795487\n",
      "Iteration 12720, loss = 1.47794953\n",
      "Iteration 12721, loss = 1.47794418\n",
      "Iteration 12722, loss = 1.47793884\n",
      "Iteration 12723, loss = 1.47793349\n",
      "Iteration 12724, loss = 1.47792815\n",
      "Iteration 12725, loss = 1.47792281\n",
      "Iteration 12726, loss = 1.47791747\n",
      "Iteration 12727, loss = 1.47791213\n",
      "Iteration 12728, loss = 1.47790679\n",
      "Iteration 12729, loss = 1.47790145\n",
      "Iteration 12730, loss = 1.47789611\n",
      "Iteration 12731, loss = 1.47789077\n",
      "Iteration 12732, loss = 1.47788544\n",
      "Iteration 12733, loss = 1.47788010\n",
      "Iteration 12734, loss = 1.47787477\n",
      "Iteration 12735, loss = 1.47786944\n",
      "Iteration 12736, loss = 1.47786410\n",
      "Iteration 12737, loss = 1.47785877\n",
      "Iteration 12738, loss = 1.47785344\n",
      "Iteration 12739, loss = 1.47784811\n",
      "Iteration 12740, loss = 1.47784279\n",
      "Iteration 12741, loss = 1.47783746\n",
      "Iteration 12742, loss = 1.47783213\n",
      "Iteration 12743, loss = 1.47782680\n",
      "Iteration 12744, loss = 1.47782148\n",
      "Iteration 12745, loss = 1.47781616\n",
      "Iteration 12746, loss = 1.47781083\n",
      "Iteration 12747, loss = 1.47780551\n",
      "Iteration 12748, loss = 1.47780019\n",
      "Iteration 12749, loss = 1.47779487\n",
      "Iteration 12750, loss = 1.47778955\n",
      "Iteration 12751, loss = 1.47778423\n",
      "Iteration 12752, loss = 1.47777891\n",
      "Iteration 12753, loss = 1.47777360\n",
      "Iteration 12754, loss = 1.47776828\n",
      "Iteration 12755, loss = 1.47776297\n",
      "Iteration 12756, loss = 1.47775765\n",
      "Iteration 12757, loss = 1.47775234\n",
      "Iteration 12758, loss = 1.47774703\n",
      "Iteration 12759, loss = 1.47774171\n",
      "Iteration 12760, loss = 1.47773640\n",
      "Iteration 12761, loss = 1.47773109\n",
      "Iteration 12762, loss = 1.47772579\n",
      "Iteration 12763, loss = 1.47772048\n",
      "Iteration 12764, loss = 1.47771517\n",
      "Iteration 12765, loss = 1.47770987\n",
      "Iteration 12766, loss = 1.47770456\n",
      "Iteration 12767, loss = 1.47769926\n",
      "Iteration 12768, loss = 1.47769395\n",
      "Iteration 12769, loss = 1.47768865\n",
      "Iteration 12770, loss = 1.47768335\n",
      "Iteration 12771, loss = 1.47767805\n",
      "Iteration 12772, loss = 1.47767275\n",
      "Iteration 12773, loss = 1.47766745\n",
      "Iteration 12774, loss = 1.47766215\n",
      "Iteration 12775, loss = 1.47765685\n",
      "Iteration 12776, loss = 1.47765156\n",
      "Iteration 12777, loss = 1.47764626\n",
      "Iteration 12778, loss = 1.47764097\n",
      "Iteration 12779, loss = 1.47763568\n",
      "Iteration 12780, loss = 1.47763038\n",
      "Iteration 12781, loss = 1.47762509\n",
      "Iteration 12782, loss = 1.47761980\n",
      "Iteration 12783, loss = 1.47761451\n",
      "Iteration 12784, loss = 1.47760922\n",
      "Iteration 12785, loss = 1.47760393\n",
      "Iteration 12786, loss = 1.47759865\n",
      "Iteration 12787, loss = 1.47759336\n",
      "Iteration 12788, loss = 1.47758808\n",
      "Iteration 12789, loss = 1.47758279\n",
      "Iteration 12790, loss = 1.47757751\n",
      "Iteration 12791, loss = 1.47757222\n",
      "Iteration 12792, loss = 1.47756694\n",
      "Iteration 12793, loss = 1.47756166\n",
      "Iteration 12794, loss = 1.47755638\n",
      "Iteration 12795, loss = 1.47755110\n",
      "Iteration 12796, loss = 1.47754582\n",
      "Iteration 12797, loss = 1.47754055\n",
      "Iteration 12798, loss = 1.47753527\n",
      "Iteration 12799, loss = 1.47753000\n",
      "Iteration 12800, loss = 1.47752472\n",
      "Iteration 12801, loss = 1.47751945\n",
      "Iteration 12802, loss = 1.47751417\n",
      "Iteration 12803, loss = 1.47750890\n",
      "Iteration 12804, loss = 1.47750363\n",
      "Iteration 12805, loss = 1.47749836\n",
      "Iteration 12806, loss = 1.47749309\n",
      "Iteration 12807, loss = 1.47748782\n",
      "Iteration 12808, loss = 1.47748256\n",
      "Iteration 12809, loss = 1.47747729\n",
      "Iteration 12810, loss = 1.47747202\n",
      "Iteration 12811, loss = 1.47746676\n",
      "Iteration 12812, loss = 1.47746149\n",
      "Iteration 12813, loss = 1.47745623\n",
      "Iteration 12814, loss = 1.47745097\n",
      "Iteration 12815, loss = 1.47744571\n",
      "Iteration 12816, loss = 1.47744045\n",
      "Iteration 12817, loss = 1.47743519\n",
      "Iteration 12818, loss = 1.47742993\n",
      "Iteration 12819, loss = 1.47742467\n",
      "Iteration 12820, loss = 1.47741942\n",
      "Iteration 12821, loss = 1.47741416\n",
      "Iteration 12822, loss = 1.47740890\n",
      "Iteration 12823, loss = 1.47740365\n",
      "Iteration 12824, loss = 1.47739840\n",
      "Iteration 12825, loss = 1.47739314\n",
      "Iteration 12826, loss = 1.47738789\n",
      "Iteration 12827, loss = 1.47738264\n",
      "Iteration 12828, loss = 1.47737739\n",
      "Iteration 12829, loss = 1.47737214\n",
      "Iteration 12830, loss = 1.47736690\n",
      "Iteration 12831, loss = 1.47736165\n",
      "Iteration 12832, loss = 1.47735640\n",
      "Iteration 12833, loss = 1.47735116\n",
      "Iteration 12834, loss = 1.47734591\n",
      "Iteration 12835, loss = 1.47734067\n",
      "Iteration 12836, loss = 1.47733543\n",
      "Iteration 12837, loss = 1.47733018\n",
      "Iteration 12838, loss = 1.47732494\n",
      "Iteration 12839, loss = 1.47731970\n",
      "Iteration 12840, loss = 1.47731446\n",
      "Iteration 12841, loss = 1.47730923\n",
      "Iteration 12842, loss = 1.47730399\n",
      "Iteration 12843, loss = 1.47729875\n",
      "Iteration 12844, loss = 1.47729352\n",
      "Iteration 12845, loss = 1.47728828\n",
      "Iteration 12846, loss = 1.47728305\n",
      "Iteration 12847, loss = 1.47727782\n",
      "Iteration 12848, loss = 1.47727258\n",
      "Iteration 12849, loss = 1.47726735\n",
      "Iteration 12850, loss = 1.47726212\n",
      "Iteration 12851, loss = 1.47725689\n",
      "Iteration 12852, loss = 1.47725166\n",
      "Iteration 12853, loss = 1.47724644\n",
      "Iteration 12854, loss = 1.47724121\n",
      "Iteration 12855, loss = 1.47723598\n",
      "Iteration 12856, loss = 1.47723076\n",
      "Iteration 12857, loss = 1.47722553\n",
      "Iteration 12858, loss = 1.47722031\n",
      "Iteration 12859, loss = 1.47721509\n",
      "Iteration 12860, loss = 1.47720987\n",
      "Iteration 12861, loss = 1.47720465\n",
      "Iteration 12862, loss = 1.47719943\n",
      "Iteration 12863, loss = 1.47719421\n",
      "Iteration 12864, loss = 1.47718899\n",
      "Iteration 12865, loss = 1.47718377\n",
      "Iteration 12866, loss = 1.47717856\n",
      "Iteration 12867, loss = 1.47717334\n",
      "Iteration 12868, loss = 1.47716813\n",
      "Iteration 12869, loss = 1.47716291\n",
      "Iteration 12870, loss = 1.47715770\n",
      "Iteration 12871, loss = 1.47715249\n",
      "Iteration 12872, loss = 1.47714728\n",
      "Iteration 12873, loss = 1.47714207\n",
      "Iteration 12874, loss = 1.47713686\n",
      "Iteration 12875, loss = 1.47713165\n",
      "Iteration 12876, loss = 1.47712644\n",
      "Iteration 12877, loss = 1.47712124\n",
      "Iteration 12878, loss = 1.47711603\n",
      "Iteration 12879, loss = 1.47711083\n",
      "Iteration 12880, loss = 1.47710562\n",
      "Iteration 12881, loss = 1.47710042\n",
      "Iteration 12882, loss = 1.47709522\n",
      "Iteration 12883, loss = 1.47709001\n",
      "Iteration 12884, loss = 1.47708481\n",
      "Iteration 12885, loss = 1.47707961\n",
      "Iteration 12886, loss = 1.47707442\n",
      "Iteration 12887, loss = 1.47706922\n",
      "Iteration 12888, loss = 1.47706402\n",
      "Iteration 12889, loss = 1.47705883\n",
      "Iteration 12890, loss = 1.47705363\n",
      "Iteration 12891, loss = 1.47704844\n",
      "Iteration 12892, loss = 1.47704324\n",
      "Iteration 12893, loss = 1.47703805\n",
      "Iteration 12894, loss = 1.47703286\n",
      "Iteration 12895, loss = 1.47702767\n",
      "Iteration 12896, loss = 1.47702248\n",
      "Iteration 12897, loss = 1.47701729\n",
      "Iteration 12898, loss = 1.47701210\n",
      "Iteration 12899, loss = 1.47700691\n",
      "Iteration 12900, loss = 1.47700173\n",
      "Iteration 12901, loss = 1.47699654\n",
      "Iteration 12902, loss = 1.47699136\n",
      "Iteration 12903, loss = 1.47698617\n",
      "Iteration 12904, loss = 1.47698099\n",
      "Iteration 12905, loss = 1.47697581\n",
      "Iteration 12906, loss = 1.47697063\n",
      "Iteration 12907, loss = 1.47696545\n",
      "Iteration 12908, loss = 1.47696027\n",
      "Iteration 12909, loss = 1.47695509\n",
      "Iteration 12910, loss = 1.47694991\n",
      "Iteration 12911, loss = 1.47694473\n",
      "Iteration 12912, loss = 1.47693956\n",
      "Iteration 12913, loss = 1.47693438\n",
      "Iteration 12914, loss = 1.47692921\n",
      "Iteration 12915, loss = 1.47692403\n",
      "Iteration 12916, loss = 1.47691886\n",
      "Iteration 12917, loss = 1.47691369\n",
      "Iteration 12918, loss = 1.47690852\n",
      "Iteration 12919, loss = 1.47690335\n",
      "Iteration 12920, loss = 1.47689818\n",
      "Iteration 12921, loss = 1.47689301\n",
      "Iteration 12922, loss = 1.47688784\n",
      "Iteration 12923, loss = 1.47688268\n",
      "Iteration 12924, loss = 1.47687751\n",
      "Iteration 12925, loss = 1.47687235\n",
      "Iteration 12926, loss = 1.47686718\n",
      "Iteration 12927, loss = 1.47686202\n",
      "Iteration 12928, loss = 1.47685686\n",
      "Iteration 12929, loss = 1.47685170\n",
      "Iteration 12930, loss = 1.47684654\n",
      "Iteration 12931, loss = 1.47684138\n",
      "Iteration 12932, loss = 1.47683622\n",
      "Iteration 12933, loss = 1.47683106\n",
      "Iteration 12934, loss = 1.47682590\n",
      "Iteration 12935, loss = 1.47682075\n",
      "Iteration 12936, loss = 1.47681559\n",
      "Iteration 12937, loss = 1.47681044\n",
      "Iteration 12938, loss = 1.47680528\n",
      "Iteration 12939, loss = 1.47680013\n",
      "Iteration 12940, loss = 1.47679498\n",
      "Iteration 12941, loss = 1.47678983\n",
      "Iteration 12942, loss = 1.47678468\n",
      "Iteration 12943, loss = 1.47677953\n",
      "Iteration 12944, loss = 1.47677438\n",
      "Iteration 12945, loss = 1.47676923\n",
      "Iteration 12946, loss = 1.47676409\n",
      "Iteration 12947, loss = 1.47675894\n",
      "Iteration 12948, loss = 1.47675380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12949, loss = 1.47674865\n",
      "Iteration 12950, loss = 1.47674351\n",
      "Iteration 12951, loss = 1.47673837\n",
      "Iteration 12952, loss = 1.47673322\n",
      "Iteration 12953, loss = 1.47672808\n",
      "Iteration 12954, loss = 1.47672294\n",
      "Iteration 12955, loss = 1.47671781\n",
      "Iteration 12956, loss = 1.47671267\n",
      "Iteration 12957, loss = 1.47670753\n",
      "Iteration 12958, loss = 1.47670239\n",
      "Iteration 12959, loss = 1.47669726\n",
      "Iteration 12960, loss = 1.47669212\n",
      "Iteration 12961, loss = 1.47668699\n",
      "Iteration 12962, loss = 1.47668186\n",
      "Iteration 12963, loss = 1.47667673\n",
      "Iteration 12964, loss = 1.47667159\n",
      "Iteration 12965, loss = 1.47666646\n",
      "Iteration 12966, loss = 1.47666133\n",
      "Iteration 12967, loss = 1.47665621\n",
      "Iteration 12968, loss = 1.47665108\n",
      "Iteration 12969, loss = 1.47664595\n",
      "Iteration 12970, loss = 1.47664083\n",
      "Iteration 12971, loss = 1.47663570\n",
      "Iteration 12972, loss = 1.47663058\n",
      "Iteration 12973, loss = 1.47662545\n",
      "Iteration 12974, loss = 1.47662033\n",
      "Iteration 12975, loss = 1.47661521\n",
      "Iteration 12976, loss = 1.47661009\n",
      "Iteration 12977, loss = 1.47660497\n",
      "Iteration 12978, loss = 1.47659985\n",
      "Iteration 12979, loss = 1.47659473\n",
      "Iteration 12980, loss = 1.47658961\n",
      "Iteration 12981, loss = 1.47658450\n",
      "Iteration 12982, loss = 1.47657938\n",
      "Iteration 12983, loss = 1.47657427\n",
      "Iteration 12984, loss = 1.47656915\n",
      "Iteration 12985, loss = 1.47656404\n",
      "Iteration 12986, loss = 1.47655893\n",
      "Iteration 12987, loss = 1.47655381\n",
      "Iteration 12988, loss = 1.47654870\n",
      "Iteration 12989, loss = 1.47654359\n",
      "Iteration 12990, loss = 1.47653849\n",
      "Iteration 12991, loss = 1.47653338\n",
      "Iteration 12992, loss = 1.47652827\n",
      "Iteration 12993, loss = 1.47652316\n",
      "Iteration 12994, loss = 1.47651806\n",
      "Iteration 12995, loss = 1.47651295\n",
      "Iteration 12996, loss = 1.47650785\n",
      "Iteration 12997, loss = 1.47650275\n",
      "Iteration 12998, loss = 1.47649765\n",
      "Iteration 12999, loss = 1.47649254\n",
      "Iteration 13000, loss = 1.47648744\n",
      "Iteration 13001, loss = 1.47648234\n",
      "Iteration 13002, loss = 1.47647725\n",
      "Iteration 13003, loss = 1.47647215\n",
      "Iteration 13004, loss = 1.47646705\n",
      "Iteration 13005, loss = 1.47646195\n",
      "Iteration 13006, loss = 1.47645686\n",
      "Iteration 13007, loss = 1.47645176\n",
      "Iteration 13008, loss = 1.47644667\n",
      "Iteration 13009, loss = 1.47644158\n",
      "Iteration 13010, loss = 1.47643649\n",
      "Iteration 13011, loss = 1.47643140\n",
      "Iteration 13012, loss = 1.47642631\n",
      "Iteration 13013, loss = 1.47642122\n",
      "Iteration 13014, loss = 1.47641613\n",
      "Iteration 13015, loss = 1.47641104\n",
      "Iteration 13016, loss = 1.47640595\n",
      "Iteration 13017, loss = 1.47640087\n",
      "Iteration 13018, loss = 1.47639578\n",
      "Iteration 13019, loss = 1.47639070\n",
      "Iteration 13020, loss = 1.47638561\n",
      "Iteration 13021, loss = 1.47638053\n",
      "Iteration 13022, loss = 1.47637545\n",
      "Iteration 13023, loss = 1.47637037\n",
      "Iteration 13024, loss = 1.47636529\n",
      "Iteration 13025, loss = 1.47636021\n",
      "Iteration 13026, loss = 1.47635513\n",
      "Iteration 13027, loss = 1.47635005\n",
      "Iteration 13028, loss = 1.47634498\n",
      "Iteration 13029, loss = 1.47633990\n",
      "Iteration 13030, loss = 1.47633483\n",
      "Iteration 13031, loss = 1.47632975\n",
      "Iteration 13032, loss = 1.47632468\n",
      "Iteration 13033, loss = 1.47631961\n",
      "Iteration 13034, loss = 1.47631454\n",
      "Iteration 13035, loss = 1.47630947\n",
      "Iteration 13036, loss = 1.47630440\n",
      "Iteration 13037, loss = 1.47629933\n",
      "Iteration 13038, loss = 1.47629426\n",
      "Iteration 13039, loss = 1.47628919\n",
      "Iteration 13040, loss = 1.47628412\n",
      "Iteration 13041, loss = 1.47627906\n",
      "Iteration 13042, loss = 1.47627399\n",
      "Iteration 13043, loss = 1.47626893\n",
      "Iteration 13044, loss = 1.47626387\n",
      "Iteration 13045, loss = 1.47625880\n",
      "Iteration 13046, loss = 1.47625374\n",
      "Iteration 13047, loss = 1.47624868\n",
      "Iteration 13048, loss = 1.47624362\n",
      "Iteration 13049, loss = 1.47623856\n",
      "Iteration 13050, loss = 1.47623351\n",
      "Iteration 13051, loss = 1.47622845\n",
      "Iteration 13052, loss = 1.47622339\n",
      "Iteration 13053, loss = 1.47621834\n",
      "Iteration 13054, loss = 1.47621328\n",
      "Iteration 13055, loss = 1.47620823\n",
      "Iteration 13056, loss = 1.47620318\n",
      "Iteration 13057, loss = 1.47619812\n",
      "Iteration 13058, loss = 1.47619307\n",
      "Iteration 13059, loss = 1.47618802\n",
      "Iteration 13060, loss = 1.47618297\n",
      "Iteration 13061, loss = 1.47617792\n",
      "Iteration 13062, loss = 1.47617288\n",
      "Iteration 13063, loss = 1.47616783\n",
      "Iteration 13064, loss = 1.47616278\n",
      "Iteration 13065, loss = 1.47615774\n",
      "Iteration 13066, loss = 1.47615269\n",
      "Iteration 13067, loss = 1.47614765\n",
      "Iteration 13068, loss = 1.47614260\n",
      "Iteration 13069, loss = 1.47613756\n",
      "Iteration 13070, loss = 1.47613252\n",
      "Iteration 13071, loss = 1.47612748\n",
      "Iteration 13072, loss = 1.47612244\n",
      "Iteration 13073, loss = 1.47611740\n",
      "Iteration 13074, loss = 1.47611236\n",
      "Iteration 13075, loss = 1.47610733\n",
      "Iteration 13076, loss = 1.47610229\n",
      "Iteration 13077, loss = 1.47609726\n",
      "Iteration 13078, loss = 1.47609222\n",
      "Iteration 13079, loss = 1.47608719\n",
      "Iteration 13080, loss = 1.47608215\n",
      "Iteration 13081, loss = 1.47607712\n",
      "Iteration 13082, loss = 1.47607209\n",
      "Iteration 13083, loss = 1.47606706\n",
      "Iteration 13084, loss = 1.47606203\n",
      "Iteration 13085, loss = 1.47605700\n",
      "Iteration 13086, loss = 1.47605198\n",
      "Iteration 13087, loss = 1.47604695\n",
      "Iteration 13088, loss = 1.47604192\n",
      "Iteration 13089, loss = 1.47603690\n",
      "Iteration 13090, loss = 1.47603187\n",
      "Iteration 13091, loss = 1.47602685\n",
      "Iteration 13092, loss = 1.47602183\n",
      "Iteration 13093, loss = 1.47601680\n",
      "Iteration 13094, loss = 1.47601178\n",
      "Iteration 13095, loss = 1.47600676\n",
      "Iteration 13096, loss = 1.47600174\n",
      "Iteration 13097, loss = 1.47599672\n",
      "Iteration 13098, loss = 1.47599171\n",
      "Iteration 13099, loss = 1.47598669\n",
      "Iteration 13100, loss = 1.47598167\n",
      "Iteration 13101, loss = 1.47597666\n",
      "Iteration 13102, loss = 1.47597164\n",
      "Iteration 13103, loss = 1.47596663\n",
      "Iteration 13104, loss = 1.47596162\n",
      "Iteration 13105, loss = 1.47595660\n",
      "Iteration 13106, loss = 1.47595159\n",
      "Iteration 13107, loss = 1.47594658\n",
      "Iteration 13108, loss = 1.47594157\n",
      "Iteration 13109, loss = 1.47593656\n",
      "Iteration 13110, loss = 1.47593156\n",
      "Iteration 13111, loss = 1.47592655\n",
      "Iteration 13112, loss = 1.47592154\n",
      "Iteration 13113, loss = 1.47591654\n",
      "Iteration 13114, loss = 1.47591153\n",
      "Iteration 13115, loss = 1.47590653\n",
      "Iteration 13116, loss = 1.47590153\n",
      "Iteration 13117, loss = 1.47589652\n",
      "Iteration 13118, loss = 1.47589152\n",
      "Iteration 13119, loss = 1.47588652\n",
      "Iteration 13120, loss = 1.47588152\n",
      "Iteration 13121, loss = 1.47587652\n",
      "Iteration 13122, loss = 1.47587153\n",
      "Iteration 13123, loss = 1.47586653\n",
      "Iteration 13124, loss = 1.47586153\n",
      "Iteration 13125, loss = 1.47585654\n",
      "Iteration 13126, loss = 1.47585154\n",
      "Iteration 13127, loss = 1.47584655\n",
      "Iteration 13128, loss = 1.47584155\n",
      "Iteration 13129, loss = 1.47583656\n",
      "Iteration 13130, loss = 1.47583157\n",
      "Iteration 13131, loss = 1.47582658\n",
      "Iteration 13132, loss = 1.47582159\n",
      "Iteration 13133, loss = 1.47581660\n",
      "Iteration 13134, loss = 1.47581161\n",
      "Iteration 13135, loss = 1.47580663\n",
      "Iteration 13136, loss = 1.47580164\n",
      "Iteration 13137, loss = 1.47579665\n",
      "Iteration 13138, loss = 1.47579167\n",
      "Iteration 13139, loss = 1.47578669\n",
      "Iteration 13140, loss = 1.47578170\n",
      "Iteration 13141, loss = 1.47577672\n",
      "Iteration 13142, loss = 1.47577174\n",
      "Iteration 13143, loss = 1.47576676\n",
      "Iteration 13144, loss = 1.47576178\n",
      "Iteration 13145, loss = 1.47575680\n",
      "Iteration 13146, loss = 1.47575182\n",
      "Iteration 13147, loss = 1.47574684\n",
      "Iteration 13148, loss = 1.47574187\n",
      "Iteration 13149, loss = 1.47573689\n",
      "Iteration 13150, loss = 1.47573192\n",
      "Iteration 13151, loss = 1.47572694\n",
      "Iteration 13152, loss = 1.47572197\n",
      "Iteration 13153, loss = 1.47571700\n",
      "Iteration 13154, loss = 1.47571203\n",
      "Iteration 13155, loss = 1.47570706\n",
      "Iteration 13156, loss = 1.47570209\n",
      "Iteration 13157, loss = 1.47569712\n",
      "Iteration 13158, loss = 1.47569215\n",
      "Iteration 13159, loss = 1.47568718\n",
      "Iteration 13160, loss = 1.47568221\n",
      "Iteration 13161, loss = 1.47567725\n",
      "Iteration 13162, loss = 1.47567228\n",
      "Iteration 13163, loss = 1.47566732\n",
      "Iteration 13164, loss = 1.47566236\n",
      "Iteration 13165, loss = 1.47565739\n",
      "Iteration 13166, loss = 1.47565243\n",
      "Iteration 13167, loss = 1.47564747\n",
      "Iteration 13168, loss = 1.47564251\n",
      "Iteration 13169, loss = 1.47563755\n",
      "Iteration 13170, loss = 1.47563259\n",
      "Iteration 13171, loss = 1.47562763\n",
      "Iteration 13172, loss = 1.47562268\n",
      "Iteration 13173, loss = 1.47561772\n",
      "Iteration 13174, loss = 1.47561277\n",
      "Iteration 13175, loss = 1.47560781\n",
      "Iteration 13176, loss = 1.47560286\n",
      "Iteration 13177, loss = 1.47559791\n",
      "Iteration 13178, loss = 1.47559295\n",
      "Iteration 13179, loss = 1.47558800\n",
      "Iteration 13180, loss = 1.47558305\n",
      "Iteration 13181, loss = 1.47557810\n",
      "Iteration 13182, loss = 1.47557315\n",
      "Iteration 13183, loss = 1.47556821\n",
      "Iteration 13184, loss = 1.47556326\n",
      "Iteration 13185, loss = 1.47555831\n",
      "Iteration 13186, loss = 1.47555337\n",
      "Iteration 13187, loss = 1.47554842\n",
      "Iteration 13188, loss = 1.47554348\n",
      "Iteration 13189, loss = 1.47553854\n",
      "Iteration 13190, loss = 1.47553359\n",
      "Iteration 13191, loss = 1.47552865\n",
      "Iteration 13192, loss = 1.47552371\n",
      "Iteration 13193, loss = 1.47551877\n",
      "Iteration 13194, loss = 1.47551383\n",
      "Iteration 13195, loss = 1.47550890\n",
      "Iteration 13196, loss = 1.47550396\n",
      "Iteration 13197, loss = 1.47549902\n",
      "Iteration 13198, loss = 1.47549409\n",
      "Iteration 13199, loss = 1.47548915\n",
      "Iteration 13200, loss = 1.47548422\n",
      "Iteration 13201, loss = 1.47547928\n",
      "Iteration 13202, loss = 1.47547435\n",
      "Iteration 13203, loss = 1.47546942\n",
      "Iteration 13204, loss = 1.47546449\n",
      "Iteration 13205, loss = 1.47545956\n",
      "Iteration 13206, loss = 1.47545463\n",
      "Iteration 13207, loss = 1.47544970\n",
      "Iteration 13208, loss = 1.47544478\n",
      "Iteration 13209, loss = 1.47543985\n",
      "Iteration 13210, loss = 1.47543492\n",
      "Iteration 13211, loss = 1.47543000\n",
      "Iteration 13212, loss = 1.47542507\n",
      "Iteration 13213, loss = 1.47542015\n",
      "Iteration 13214, loss = 1.47541523\n",
      "Iteration 13215, loss = 1.47541031\n",
      "Iteration 13216, loss = 1.47540538\n",
      "Iteration 13217, loss = 1.47540046\n",
      "Iteration 13218, loss = 1.47539554\n",
      "Iteration 13219, loss = 1.47539063\n",
      "Iteration 13220, loss = 1.47538571\n",
      "Iteration 13221, loss = 1.47538079\n",
      "Iteration 13222, loss = 1.47537588\n",
      "Iteration 13223, loss = 1.47537096\n",
      "Iteration 13224, loss = 1.47536605\n",
      "Iteration 13225, loss = 1.47536113\n",
      "Iteration 13226, loss = 1.47535622\n",
      "Iteration 13227, loss = 1.47535131\n",
      "Iteration 13228, loss = 1.47534640\n",
      "Iteration 13229, loss = 1.47534148\n",
      "Iteration 13230, loss = 1.47533657\n",
      "Iteration 13231, loss = 1.47533167\n",
      "Iteration 13232, loss = 1.47532676\n",
      "Iteration 13233, loss = 1.47532185\n",
      "Iteration 13234, loss = 1.47531694\n",
      "Iteration 13235, loss = 1.47531204\n",
      "Iteration 13236, loss = 1.47530713\n",
      "Iteration 13237, loss = 1.47530223\n",
      "Iteration 13238, loss = 1.47529733\n",
      "Iteration 13239, loss = 1.47529242\n",
      "Iteration 13240, loss = 1.47528752\n",
      "Iteration 13241, loss = 1.47528262\n",
      "Iteration 13242, loss = 1.47527772\n",
      "Iteration 13243, loss = 1.47527282\n",
      "Iteration 13244, loss = 1.47526792\n",
      "Iteration 13245, loss = 1.47526303\n",
      "Iteration 13246, loss = 1.47525813\n",
      "Iteration 13247, loss = 1.47525323\n",
      "Iteration 13248, loss = 1.47524834\n",
      "Iteration 13249, loss = 1.47524344\n",
      "Iteration 13250, loss = 1.47523855\n",
      "Iteration 13251, loss = 1.47523366\n",
      "Iteration 13252, loss = 1.47522877\n",
      "Iteration 13253, loss = 1.47522387\n",
      "Iteration 13254, loss = 1.47521898\n",
      "Iteration 13255, loss = 1.47521409\n",
      "Iteration 13256, loss = 1.47520921\n",
      "Iteration 13257, loss = 1.47520432\n",
      "Iteration 13258, loss = 1.47519943\n",
      "Iteration 13259, loss = 1.47519454\n",
      "Iteration 13260, loss = 1.47518966\n",
      "Iteration 13261, loss = 1.47518477\n",
      "Iteration 13262, loss = 1.47517989\n",
      "Iteration 13263, loss = 1.47517501\n",
      "Iteration 13264, loss = 1.47517012\n",
      "Iteration 13265, loss = 1.47516524\n",
      "Iteration 13266, loss = 1.47516036\n",
      "Iteration 13267, loss = 1.47515548\n",
      "Iteration 13268, loss = 1.47515060\n",
      "Iteration 13269, loss = 1.47514572\n",
      "Iteration 13270, loss = 1.47514085\n",
      "Iteration 13271, loss = 1.47513597\n",
      "Iteration 13272, loss = 1.47513109\n",
      "Iteration 13273, loss = 1.47512622\n",
      "Iteration 13274, loss = 1.47512134\n",
      "Iteration 13275, loss = 1.47511647\n",
      "Iteration 13276, loss = 1.47511160\n",
      "Iteration 13277, loss = 1.47510673\n",
      "Iteration 13278, loss = 1.47510186\n",
      "Iteration 13279, loss = 1.47509698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13280, loss = 1.47509212\n",
      "Iteration 13281, loss = 1.47508725\n",
      "Iteration 13282, loss = 1.47508238\n",
      "Iteration 13283, loss = 1.47507751\n",
      "Iteration 13284, loss = 1.47507264\n",
      "Iteration 13285, loss = 1.47506778\n",
      "Iteration 13286, loss = 1.47506291\n",
      "Iteration 13287, loss = 1.47505805\n",
      "Iteration 13288, loss = 1.47505319\n",
      "Iteration 13289, loss = 1.47504833\n",
      "Iteration 13290, loss = 1.47504346\n",
      "Iteration 13291, loss = 1.47503860\n",
      "Iteration 13292, loss = 1.47503374\n",
      "Iteration 13293, loss = 1.47502888\n",
      "Iteration 13294, loss = 1.47502403\n",
      "Iteration 13295, loss = 1.47501917\n",
      "Iteration 13296, loss = 1.47501431\n",
      "Iteration 13297, loss = 1.47500945\n",
      "Iteration 13298, loss = 1.47500460\n",
      "Iteration 13299, loss = 1.47499974\n",
      "Iteration 13300, loss = 1.47499489\n",
      "Iteration 13301, loss = 1.47499004\n",
      "Iteration 13302, loss = 1.47498519\n",
      "Iteration 13303, loss = 1.47498034\n",
      "Iteration 13304, loss = 1.47497548\n",
      "Iteration 13305, loss = 1.47497063\n",
      "Iteration 13306, loss = 1.47496579\n",
      "Iteration 13307, loss = 1.47496094\n",
      "Iteration 13308, loss = 1.47495609\n",
      "Iteration 13309, loss = 1.47495124\n",
      "Iteration 13310, loss = 1.47494640\n",
      "Iteration 13311, loss = 1.47494155\n",
      "Iteration 13312, loss = 1.47493671\n",
      "Iteration 13313, loss = 1.47493187\n",
      "Iteration 13314, loss = 1.47492702\n",
      "Iteration 13315, loss = 1.47492218\n",
      "Iteration 13316, loss = 1.47491734\n",
      "Iteration 13317, loss = 1.47491250\n",
      "Iteration 13318, loss = 1.47490766\n",
      "Iteration 13319, loss = 1.47490282\n",
      "Iteration 13320, loss = 1.47489799\n",
      "Iteration 13321, loss = 1.47489315\n",
      "Iteration 13322, loss = 1.47488831\n",
      "Iteration 13323, loss = 1.47488348\n",
      "Iteration 13324, loss = 1.47487864\n",
      "Iteration 13325, loss = 1.47487381\n",
      "Iteration 13326, loss = 1.47486897\n",
      "Iteration 13327, loss = 1.47486414\n",
      "Iteration 13328, loss = 1.47485931\n",
      "Iteration 13329, loss = 1.47485448\n",
      "Iteration 13330, loss = 1.47484965\n",
      "Iteration 13331, loss = 1.47484482\n",
      "Iteration 13332, loss = 1.47483999\n",
      "Iteration 13333, loss = 1.47483517\n",
      "Iteration 13334, loss = 1.47483034\n",
      "Iteration 13335, loss = 1.47482551\n",
      "Iteration 13336, loss = 1.47482069\n",
      "Iteration 13337, loss = 1.47481586\n",
      "Iteration 13338, loss = 1.47481104\n",
      "Iteration 13339, loss = 1.47480622\n",
      "Iteration 13340, loss = 1.47480140\n",
      "Iteration 13341, loss = 1.47479657\n",
      "Iteration 13342, loss = 1.47479175\n",
      "Iteration 13343, loss = 1.47478693\n",
      "Iteration 13344, loss = 1.47478211\n",
      "Iteration 13345, loss = 1.47477730\n",
      "Iteration 13346, loss = 1.47477248\n",
      "Iteration 13347, loss = 1.47476766\n",
      "Iteration 13348, loss = 1.47476285\n",
      "Iteration 13349, loss = 1.47475803\n",
      "Iteration 13350, loss = 1.47475322\n",
      "Iteration 13351, loss = 1.47474840\n",
      "Iteration 13352, loss = 1.47474359\n",
      "Iteration 13353, loss = 1.47473878\n",
      "Iteration 13354, loss = 1.47473397\n",
      "Iteration 13355, loss = 1.47472916\n",
      "Iteration 13356, loss = 1.47472435\n",
      "Iteration 13357, loss = 1.47471954\n",
      "Iteration 13358, loss = 1.47471473\n",
      "Iteration 13359, loss = 1.47470993\n",
      "Iteration 13360, loss = 1.47470512\n",
      "Iteration 13361, loss = 1.47470031\n",
      "Iteration 13362, loss = 1.47469551\n",
      "Iteration 13363, loss = 1.47469071\n",
      "Iteration 13364, loss = 1.47468590\n",
      "Iteration 13365, loss = 1.47468110\n",
      "Iteration 13366, loss = 1.47467630\n",
      "Iteration 13367, loss = 1.47467150\n",
      "Iteration 13368, loss = 1.47466670\n",
      "Iteration 13369, loss = 1.47466190\n",
      "Iteration 13370, loss = 1.47465710\n",
      "Iteration 13371, loss = 1.47465230\n",
      "Iteration 13372, loss = 1.47464751\n",
      "Iteration 13373, loss = 1.47464271\n",
      "Iteration 13374, loss = 1.47463791\n",
      "Iteration 13375, loss = 1.47463312\n",
      "Iteration 13376, loss = 1.47462833\n",
      "Iteration 13377, loss = 1.47462353\n",
      "Iteration 13378, loss = 1.47461874\n",
      "Iteration 13379, loss = 1.47461395\n",
      "Iteration 13380, loss = 1.47460916\n",
      "Iteration 13381, loss = 1.47460437\n",
      "Iteration 13382, loss = 1.47459958\n",
      "Iteration 13383, loss = 1.47459479\n",
      "Iteration 13384, loss = 1.47459000\n",
      "Iteration 13385, loss = 1.47458522\n",
      "Iteration 13386, loss = 1.47458043\n",
      "Iteration 13387, loss = 1.47457565\n",
      "Iteration 13388, loss = 1.47457086\n",
      "Iteration 13389, loss = 1.47456608\n",
      "Iteration 13390, loss = 1.47456129\n",
      "Iteration 13391, loss = 1.47455651\n",
      "Iteration 13392, loss = 1.47455173\n",
      "Iteration 13393, loss = 1.47454695\n",
      "Iteration 13394, loss = 1.47454217\n",
      "Iteration 13395, loss = 1.47453739\n",
      "Iteration 13396, loss = 1.47453261\n",
      "Iteration 13397, loss = 1.47452784\n",
      "Iteration 13398, loss = 1.47452306\n",
      "Iteration 13399, loss = 1.47451828\n",
      "Iteration 13400, loss = 1.47451351\n",
      "Iteration 13401, loss = 1.47450873\n",
      "Iteration 13402, loss = 1.47450396\n",
      "Iteration 13403, loss = 1.47449919\n",
      "Iteration 13404, loss = 1.47449442\n",
      "Iteration 13405, loss = 1.47448965\n",
      "Iteration 13406, loss = 1.47448487\n",
      "Iteration 13407, loss = 1.47448010\n",
      "Iteration 13408, loss = 1.47447534\n",
      "Iteration 13409, loss = 1.47447057\n",
      "Iteration 13410, loss = 1.47446580\n",
      "Iteration 13411, loss = 1.47446103\n",
      "Iteration 13412, loss = 1.47445627\n",
      "Iteration 13413, loss = 1.47445150\n",
      "Iteration 13414, loss = 1.47444674\n",
      "Iteration 13415, loss = 1.47444198\n",
      "Iteration 13416, loss = 1.47443721\n",
      "Iteration 13417, loss = 1.47443245\n",
      "Iteration 13418, loss = 1.47442769\n",
      "Iteration 13419, loss = 1.47442293\n",
      "Iteration 13420, loss = 1.47441817\n",
      "Iteration 13421, loss = 1.47441341\n",
      "Iteration 13422, loss = 1.47440865\n",
      "Iteration 13423, loss = 1.47440390\n",
      "Iteration 13424, loss = 1.47439914\n",
      "Iteration 13425, loss = 1.47439438\n",
      "Iteration 13426, loss = 1.47438963\n",
      "Iteration 13427, loss = 1.47438487\n",
      "Iteration 13428, loss = 1.47438012\n",
      "Iteration 13429, loss = 1.47437537\n",
      "Iteration 13430, loss = 1.47437062\n",
      "Iteration 13431, loss = 1.47436587\n",
      "Iteration 13432, loss = 1.47436112\n",
      "Iteration 13433, loss = 1.47435637\n",
      "Iteration 13434, loss = 1.47435162\n",
      "Iteration 13435, loss = 1.47434687\n",
      "Iteration 13436, loss = 1.47434212\n",
      "Iteration 13437, loss = 1.47433737\n",
      "Iteration 13438, loss = 1.47433263\n",
      "Iteration 13439, loss = 1.47432788\n",
      "Iteration 13440, loss = 1.47432314\n",
      "Iteration 13441, loss = 1.47431840\n",
      "Iteration 13442, loss = 1.47431365\n",
      "Iteration 13443, loss = 1.47430891\n",
      "Iteration 13444, loss = 1.47430417\n",
      "Iteration 13445, loss = 1.47429943\n",
      "Iteration 13446, loss = 1.47429469\n",
      "Iteration 13447, loss = 1.47428995\n",
      "Iteration 13448, loss = 1.47428521\n",
      "Iteration 13449, loss = 1.47428048\n",
      "Iteration 13450, loss = 1.47427574\n",
      "Iteration 13451, loss = 1.47427101\n",
      "Iteration 13452, loss = 1.47426627\n",
      "Iteration 13453, loss = 1.47426154\n",
      "Iteration 13454, loss = 1.47425680\n",
      "Iteration 13455, loss = 1.47425207\n",
      "Iteration 13456, loss = 1.47424734\n",
      "Iteration 13457, loss = 1.47424261\n",
      "Iteration 13458, loss = 1.47423788\n",
      "Iteration 13459, loss = 1.47423315\n",
      "Iteration 13460, loss = 1.47422842\n",
      "Iteration 13461, loss = 1.47422369\n",
      "Iteration 13462, loss = 1.47421896\n",
      "Iteration 13463, loss = 1.47421424\n",
      "Iteration 13464, loss = 1.47420951\n",
      "Iteration 13465, loss = 1.47420479\n",
      "Iteration 13466, loss = 1.47420006\n",
      "Iteration 13467, loss = 1.47419534\n",
      "Iteration 13468, loss = 1.47419062\n",
      "Iteration 13469, loss = 1.47418589\n",
      "Iteration 13470, loss = 1.47418117\n",
      "Iteration 13471, loss = 1.47417645\n",
      "Iteration 13472, loss = 1.47417173\n",
      "Iteration 13473, loss = 1.47416701\n",
      "Iteration 13474, loss = 1.47416230\n",
      "Iteration 13475, loss = 1.47415758\n",
      "Iteration 13476, loss = 1.47415286\n",
      "Iteration 13477, loss = 1.47414815\n",
      "Iteration 13478, loss = 1.47414343\n",
      "Iteration 13479, loss = 1.47413872\n",
      "Iteration 13480, loss = 1.47413400\n",
      "Iteration 13481, loss = 1.47412929\n",
      "Iteration 13482, loss = 1.47412458\n",
      "Iteration 13483, loss = 1.47411987\n",
      "Iteration 13484, loss = 1.47411516\n",
      "Iteration 13485, loss = 1.47411045\n",
      "Iteration 13486, loss = 1.47410574\n",
      "Iteration 13487, loss = 1.47410103\n",
      "Iteration 13488, loss = 1.47409632\n",
      "Iteration 13489, loss = 1.47409162\n",
      "Iteration 13490, loss = 1.47408691\n",
      "Iteration 13491, loss = 1.47408220\n",
      "Iteration 13492, loss = 1.47407750\n",
      "Iteration 13493, loss = 1.47407280\n",
      "Iteration 13494, loss = 1.47406809\n",
      "Iteration 13495, loss = 1.47406339\n",
      "Iteration 13496, loss = 1.47405869\n",
      "Iteration 13497, loss = 1.47405399\n",
      "Iteration 13498, loss = 1.47404929\n",
      "Iteration 13499, loss = 1.47404459\n",
      "Iteration 13500, loss = 1.47403989\n",
      "Iteration 13501, loss = 1.47403519\n",
      "Iteration 13502, loss = 1.47403050\n",
      "Iteration 13503, loss = 1.47402580\n",
      "Iteration 13504, loss = 1.47402111\n",
      "Iteration 13505, loss = 1.47401641\n",
      "Iteration 13506, loss = 1.47401172\n",
      "Iteration 13507, loss = 1.47400703\n",
      "Iteration 13508, loss = 1.47400233\n",
      "Iteration 13509, loss = 1.47399764\n",
      "Iteration 13510, loss = 1.47399295\n",
      "Iteration 13511, loss = 1.47398826\n",
      "Iteration 13512, loss = 1.47398357\n",
      "Iteration 13513, loss = 1.47397888\n",
      "Iteration 13514, loss = 1.47397420\n",
      "Iteration 13515, loss = 1.47396951\n",
      "Iteration 13516, loss = 1.47396482\n",
      "Iteration 13517, loss = 1.47396014\n",
      "Iteration 13518, loss = 1.47395545\n",
      "Iteration 13519, loss = 1.47395077\n",
      "Iteration 13520, loss = 1.47394609\n",
      "Iteration 13521, loss = 1.47394140\n",
      "Iteration 13522, loss = 1.47393672\n",
      "Iteration 13523, loss = 1.47393204\n",
      "Iteration 13524, loss = 1.47392736\n",
      "Iteration 13525, loss = 1.47392268\n",
      "Iteration 13526, loss = 1.47391800\n",
      "Iteration 13527, loss = 1.47391332\n",
      "Iteration 13528, loss = 1.47390865\n",
      "Iteration 13529, loss = 1.47390397\n",
      "Iteration 13530, loss = 1.47389930\n",
      "Iteration 13531, loss = 1.47389462\n",
      "Iteration 13532, loss = 1.47388995\n",
      "Iteration 13533, loss = 1.47388527\n",
      "Iteration 13534, loss = 1.47388060\n",
      "Iteration 13535, loss = 1.47387593\n",
      "Iteration 13536, loss = 1.47387126\n",
      "Iteration 13537, loss = 1.47386659\n",
      "Iteration 13538, loss = 1.47386192\n",
      "Iteration 13539, loss = 1.47385725\n",
      "Iteration 13540, loss = 1.47385258\n",
      "Iteration 13541, loss = 1.47384791\n",
      "Iteration 13542, loss = 1.47384325\n",
      "Iteration 13543, loss = 1.47383858\n",
      "Iteration 13544, loss = 1.47383392\n",
      "Iteration 13545, loss = 1.47382925\n",
      "Iteration 13546, loss = 1.47382459\n",
      "Iteration 13547, loss = 1.47381993\n",
      "Iteration 13548, loss = 1.47381526\n",
      "Iteration 13549, loss = 1.47381060\n",
      "Iteration 13550, loss = 1.47380594\n",
      "Iteration 13551, loss = 1.47380128\n",
      "Iteration 13552, loss = 1.47379662\n",
      "Iteration 13553, loss = 1.47379196\n",
      "Iteration 13554, loss = 1.47378731\n",
      "Iteration 13555, loss = 1.47378265\n",
      "Iteration 13556, loss = 1.47377799\n",
      "Iteration 13557, loss = 1.47377334\n",
      "Iteration 13558, loss = 1.47376868\n",
      "Iteration 13559, loss = 1.47376403\n",
      "Iteration 13560, loss = 1.47375938\n",
      "Iteration 13561, loss = 1.47375473\n",
      "Iteration 13562, loss = 1.47375007\n",
      "Iteration 13563, loss = 1.47374542\n",
      "Iteration 13564, loss = 1.47374077\n",
      "Iteration 13565, loss = 1.47373612\n",
      "Iteration 13566, loss = 1.47373148\n",
      "Iteration 13567, loss = 1.47372683\n",
      "Iteration 13568, loss = 1.47372218\n",
      "Iteration 13569, loss = 1.47371753\n",
      "Iteration 13570, loss = 1.47371289\n",
      "Iteration 13571, loss = 1.47370824\n",
      "Iteration 13572, loss = 1.47370360\n",
      "Iteration 13573, loss = 1.47369896\n",
      "Iteration 13574, loss = 1.47369431\n",
      "Iteration 13575, loss = 1.47368967\n",
      "Iteration 13576, loss = 1.47368503\n",
      "Iteration 13577, loss = 1.47368039\n",
      "Iteration 13578, loss = 1.47367575\n",
      "Iteration 13579, loss = 1.47367111\n",
      "Iteration 13580, loss = 1.47366647\n",
      "Iteration 13581, loss = 1.47366184\n",
      "Iteration 13582, loss = 1.47365720\n",
      "Iteration 13583, loss = 1.47365256\n",
      "Iteration 13584, loss = 1.47364793\n",
      "Iteration 13585, loss = 1.47364329\n",
      "Iteration 13586, loss = 1.47363866\n",
      "Iteration 13587, loss = 1.47363403\n",
      "Iteration 13588, loss = 1.47362940\n",
      "Iteration 13589, loss = 1.47362476\n",
      "Iteration 13590, loss = 1.47362013\n",
      "Iteration 13591, loss = 1.47361550\n",
      "Iteration 13592, loss = 1.47361087\n",
      "Iteration 13593, loss = 1.47360625\n",
      "Iteration 13594, loss = 1.47360162\n",
      "Iteration 13595, loss = 1.47359699\n",
      "Iteration 13596, loss = 1.47359237\n",
      "Iteration 13597, loss = 1.47358774\n",
      "Iteration 13598, loss = 1.47358312\n",
      "Iteration 13599, loss = 1.47357849\n",
      "Iteration 13600, loss = 1.47357387\n",
      "Iteration 13601, loss = 1.47356925\n",
      "Iteration 13602, loss = 1.47356462\n",
      "Iteration 13603, loss = 1.47356000\n",
      "Iteration 13604, loss = 1.47355538\n",
      "Iteration 13605, loss = 1.47355076\n",
      "Iteration 13606, loss = 1.47354614\n",
      "Iteration 13607, loss = 1.47354153\n",
      "Iteration 13608, loss = 1.47353691\n",
      "Iteration 13609, loss = 1.47353229\n",
      "Iteration 13610, loss = 1.47352768\n",
      "Iteration 13611, loss = 1.47352306\n",
      "Iteration 13612, loss = 1.47351845\n",
      "Iteration 13613, loss = 1.47351383\n",
      "Iteration 13614, loss = 1.47350922\n",
      "Iteration 13615, loss = 1.47350461\n",
      "Iteration 13616, loss = 1.47350000\n",
      "Iteration 13617, loss = 1.47349539\n",
      "Iteration 13618, loss = 1.47349078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13619, loss = 1.47348617\n",
      "Iteration 13620, loss = 1.47348156\n",
      "Iteration 13621, loss = 1.47347695\n",
      "Iteration 13622, loss = 1.47347235\n",
      "Iteration 13623, loss = 1.47346774\n",
      "Iteration 13624, loss = 1.47346313\n",
      "Iteration 13625, loss = 1.47345853\n",
      "Iteration 13626, loss = 1.47345392\n",
      "Iteration 13627, loss = 1.47344932\n",
      "Iteration 13628, loss = 1.47344472\n",
      "Iteration 13629, loss = 1.47344012\n",
      "Iteration 13630, loss = 1.47343552\n",
      "Iteration 13631, loss = 1.47343092\n",
      "Iteration 13632, loss = 1.47342632\n",
      "Iteration 13633, loss = 1.47342172\n",
      "Iteration 13634, loss = 1.47341712\n",
      "Iteration 13635, loss = 1.47341252\n",
      "Iteration 13636, loss = 1.47340793\n",
      "Iteration 13637, loss = 1.47340333\n",
      "Iteration 13638, loss = 1.47339873\n",
      "Iteration 13639, loss = 1.47339414\n",
      "Iteration 13640, loss = 1.47338955\n",
      "Iteration 13641, loss = 1.47338495\n",
      "Iteration 13642, loss = 1.47338036\n",
      "Iteration 13643, loss = 1.47337577\n",
      "Iteration 13644, loss = 1.47337118\n",
      "Iteration 13645, loss = 1.47336659\n",
      "Iteration 13646, loss = 1.47336200\n",
      "Iteration 13647, loss = 1.47335741\n",
      "Iteration 13648, loss = 1.47335282\n",
      "Iteration 13649, loss = 1.47334824\n",
      "Iteration 13650, loss = 1.47334365\n",
      "Iteration 13651, loss = 1.47333907\n",
      "Iteration 13652, loss = 1.47333448\n",
      "Iteration 13653, loss = 1.47332990\n",
      "Iteration 13654, loss = 1.47332531\n",
      "Iteration 13655, loss = 1.47332073\n",
      "Iteration 13656, loss = 1.47331615\n",
      "Iteration 13657, loss = 1.47331157\n",
      "Iteration 13658, loss = 1.47330699\n",
      "Iteration 13659, loss = 1.47330241\n",
      "Iteration 13660, loss = 1.47329783\n",
      "Iteration 13661, loss = 1.47329325\n",
      "Iteration 13662, loss = 1.47328867\n",
      "Iteration 13663, loss = 1.47328410\n",
      "Iteration 13664, loss = 1.47327952\n",
      "Iteration 13665, loss = 1.47327494\n",
      "Iteration 13666, loss = 1.47327037\n",
      "Iteration 13667, loss = 1.47326580\n",
      "Iteration 13668, loss = 1.47326122\n",
      "Iteration 13669, loss = 1.47325665\n",
      "Iteration 13670, loss = 1.47325208\n",
      "Iteration 13671, loss = 1.47324751\n",
      "Iteration 13672, loss = 1.47324294\n",
      "Iteration 13673, loss = 1.47323837\n",
      "Iteration 13674, loss = 1.47323380\n",
      "Iteration 13675, loss = 1.47322923\n",
      "Iteration 13676, loss = 1.47322466\n",
      "Iteration 13677, loss = 1.47322010\n",
      "Iteration 13678, loss = 1.47321553\n",
      "Iteration 13679, loss = 1.47321097\n",
      "Iteration 13680, loss = 1.47320640\n",
      "Iteration 13681, loss = 1.47320184\n",
      "Iteration 13682, loss = 1.47319728\n",
      "Iteration 13683, loss = 1.47319271\n",
      "Iteration 13684, loss = 1.47318815\n",
      "Iteration 13685, loss = 1.47318359\n",
      "Iteration 13686, loss = 1.47317903\n",
      "Iteration 13687, loss = 1.47317447\n",
      "Iteration 13688, loss = 1.47316991\n",
      "Iteration 13689, loss = 1.47316536\n",
      "Iteration 13690, loss = 1.47316080\n",
      "Iteration 13691, loss = 1.47315624\n",
      "Iteration 13692, loss = 1.47315169\n",
      "Iteration 13693, loss = 1.47314713\n",
      "Iteration 13694, loss = 1.47314258\n",
      "Iteration 13695, loss = 1.47313802\n",
      "Iteration 13696, loss = 1.47313347\n",
      "Iteration 13697, loss = 1.47312892\n",
      "Iteration 13698, loss = 1.47312437\n",
      "Iteration 13699, loss = 1.47311982\n",
      "Iteration 13700, loss = 1.47311527\n",
      "Iteration 13701, loss = 1.47311072\n",
      "Iteration 13702, loss = 1.47310617\n",
      "Iteration 13703, loss = 1.47310162\n",
      "Iteration 13704, loss = 1.47309708\n",
      "Iteration 13705, loss = 1.47309253\n",
      "Iteration 13706, loss = 1.47308799\n",
      "Iteration 13707, loss = 1.47308344\n",
      "Iteration 13708, loss = 1.47307890\n",
      "Iteration 13709, loss = 1.47307435\n",
      "Iteration 13710, loss = 1.47306981\n",
      "Iteration 13711, loss = 1.47306527\n",
      "Iteration 13712, loss = 1.47306073\n",
      "Iteration 13713, loss = 1.47305619\n",
      "Iteration 13714, loss = 1.47305165\n",
      "Iteration 13715, loss = 1.47304711\n",
      "Iteration 13716, loss = 1.47304257\n",
      "Iteration 13717, loss = 1.47303803\n",
      "Iteration 13718, loss = 1.47303350\n",
      "Iteration 13719, loss = 1.47302896\n",
      "Iteration 13720, loss = 1.47302443\n",
      "Iteration 13721, loss = 1.47301989\n",
      "Iteration 13722, loss = 1.47301536\n",
      "Iteration 13723, loss = 1.47301082\n",
      "Iteration 13724, loss = 1.47300629\n",
      "Iteration 13725, loss = 1.47300176\n",
      "Iteration 13726, loss = 1.47299723\n",
      "Iteration 13727, loss = 1.47299270\n",
      "Iteration 13728, loss = 1.47298817\n",
      "Iteration 13729, loss = 1.47298364\n",
      "Iteration 13730, loss = 1.47297911\n",
      "Iteration 13731, loss = 1.47297458\n",
      "Iteration 13732, loss = 1.47297006\n",
      "Iteration 13733, loss = 1.47296553\n",
      "Iteration 13734, loss = 1.47296101\n",
      "Iteration 13735, loss = 1.47295648\n",
      "Iteration 13736, loss = 1.47295196\n",
      "Iteration 13737, loss = 1.47294744\n",
      "Iteration 13738, loss = 1.47294291\n",
      "Iteration 13739, loss = 1.47293839\n",
      "Iteration 13740, loss = 1.47293387\n",
      "Iteration 13741, loss = 1.47292935\n",
      "Iteration 13742, loss = 1.47292483\n",
      "Iteration 13743, loss = 1.47292031\n",
      "Iteration 13744, loss = 1.47291579\n",
      "Iteration 13745, loss = 1.47291128\n",
      "Iteration 13746, loss = 1.47290676\n",
      "Iteration 13747, loss = 1.47290225\n",
      "Iteration 13748, loss = 1.47289773\n",
      "Iteration 13749, loss = 1.47289322\n",
      "Iteration 13750, loss = 1.47288870\n",
      "Iteration 13751, loss = 1.47288419\n",
      "Iteration 13752, loss = 1.47287968\n",
      "Iteration 13753, loss = 1.47287517\n",
      "Iteration 13754, loss = 1.47287065\n",
      "Iteration 13755, loss = 1.47286614\n",
      "Iteration 13756, loss = 1.47286163\n",
      "Iteration 13757, loss = 1.47285713\n",
      "Iteration 13758, loss = 1.47285262\n",
      "Iteration 13759, loss = 1.47284811\n",
      "Iteration 13760, loss = 1.47284360\n",
      "Iteration 13761, loss = 1.47283910\n",
      "Iteration 13762, loss = 1.47283459\n",
      "Iteration 13763, loss = 1.47283009\n",
      "Iteration 13764, loss = 1.47282559\n",
      "Iteration 13765, loss = 1.47282108\n",
      "Iteration 13766, loss = 1.47281658\n",
      "Iteration 13767, loss = 1.47281208\n",
      "Iteration 13768, loss = 1.47280758\n",
      "Iteration 13769, loss = 1.47280308\n",
      "Iteration 13770, loss = 1.47279858\n",
      "Iteration 13771, loss = 1.47279408\n",
      "Iteration 13772, loss = 1.47278958\n",
      "Iteration 13773, loss = 1.47278509\n",
      "Iteration 13774, loss = 1.47278059\n",
      "Iteration 13775, loss = 1.47277609\n",
      "Iteration 13776, loss = 1.47277160\n",
      "Iteration 13777, loss = 1.47276710\n",
      "Iteration 13778, loss = 1.47276261\n",
      "Iteration 13779, loss = 1.47275812\n",
      "Iteration 13780, loss = 1.47275362\n",
      "Iteration 13781, loss = 1.47274913\n",
      "Iteration 13782, loss = 1.47274464\n",
      "Iteration 13783, loss = 1.47274015\n",
      "Iteration 13784, loss = 1.47273566\n",
      "Iteration 13785, loss = 1.47273117\n",
      "Iteration 13786, loss = 1.47272669\n",
      "Iteration 13787, loss = 1.47272220\n",
      "Iteration 13788, loss = 1.47271771\n",
      "Iteration 13789, loss = 1.47271323\n",
      "Iteration 13790, loss = 1.47270874\n",
      "Iteration 13791, loss = 1.47270426\n",
      "Iteration 13792, loss = 1.47269977\n",
      "Iteration 13793, loss = 1.47269529\n",
      "Iteration 13794, loss = 1.47269081\n",
      "Iteration 13795, loss = 1.47268633\n",
      "Iteration 13796, loss = 1.47268185\n",
      "Iteration 13797, loss = 1.47267737\n",
      "Iteration 13798, loss = 1.47267289\n",
      "Iteration 13799, loss = 1.47266841\n",
      "Iteration 13800, loss = 1.47266393\n",
      "Iteration 13801, loss = 1.47265945\n",
      "Iteration 13802, loss = 1.47265498\n",
      "Iteration 13803, loss = 1.47265050\n",
      "Iteration 13804, loss = 1.47264603\n",
      "Iteration 13805, loss = 1.47264155\n",
      "Iteration 13806, loss = 1.47263708\n",
      "Iteration 13807, loss = 1.47263260\n",
      "Iteration 13808, loss = 1.47262813\n",
      "Iteration 13809, loss = 1.47262366\n",
      "Iteration 13810, loss = 1.47261919\n",
      "Iteration 13811, loss = 1.47261472\n",
      "Iteration 13812, loss = 1.47261025\n",
      "Iteration 13813, loss = 1.47260578\n",
      "Iteration 13814, loss = 1.47260131\n",
      "Iteration 13815, loss = 1.47259685\n",
      "Iteration 13816, loss = 1.47259238\n",
      "Iteration 13817, loss = 1.47258791\n",
      "Iteration 13818, loss = 1.47258345\n",
      "Iteration 13819, loss = 1.47257898\n",
      "Iteration 13820, loss = 1.47257452\n",
      "Iteration 13821, loss = 1.47257006\n",
      "Iteration 13822, loss = 1.47256559\n",
      "Iteration 13823, loss = 1.47256113\n",
      "Iteration 13824, loss = 1.47255667\n",
      "Iteration 13825, loss = 1.47255221\n",
      "Iteration 13826, loss = 1.47254775\n",
      "Iteration 13827, loss = 1.47254329\n",
      "Iteration 13828, loss = 1.47253883\n",
      "Iteration 13829, loss = 1.47253438\n",
      "Iteration 13830, loss = 1.47252992\n",
      "Iteration 13831, loss = 1.47252546\n",
      "Iteration 13832, loss = 1.47252101\n",
      "Iteration 13833, loss = 1.47251655\n",
      "Iteration 13834, loss = 1.47251210\n",
      "Iteration 13835, loss = 1.47250765\n",
      "Iteration 13836, loss = 1.47250319\n",
      "Iteration 13837, loss = 1.47249874\n",
      "Iteration 13838, loss = 1.47249429\n",
      "Iteration 13839, loss = 1.47248984\n",
      "Iteration 13840, loss = 1.47248539\n",
      "Iteration 13841, loss = 1.47248094\n",
      "Iteration 13842, loss = 1.47247649\n",
      "Iteration 13843, loss = 1.47247205\n",
      "Iteration 13844, loss = 1.47246760\n",
      "Iteration 13845, loss = 1.47246315\n",
      "Iteration 13846, loss = 1.47245871\n",
      "Iteration 13847, loss = 1.47245426\n",
      "Iteration 13848, loss = 1.47244982\n",
      "Iteration 13849, loss = 1.47244538\n",
      "Iteration 13850, loss = 1.47244093\n",
      "Iteration 13851, loss = 1.47243649\n",
      "Iteration 13852, loss = 1.47243205\n",
      "Iteration 13853, loss = 1.47242761\n",
      "Iteration 13854, loss = 1.47242317\n",
      "Iteration 13855, loss = 1.47241873\n",
      "Iteration 13856, loss = 1.47241429\n",
      "Iteration 13857, loss = 1.47240985\n",
      "Iteration 13858, loss = 1.47240542\n",
      "Iteration 13859, loss = 1.47240098\n",
      "Iteration 13860, loss = 1.47239655\n",
      "Iteration 13861, loss = 1.47239211\n",
      "Iteration 13862, loss = 1.47238768\n",
      "Iteration 13863, loss = 1.47238324\n",
      "Iteration 13864, loss = 1.47237881\n",
      "Iteration 13865, loss = 1.47237438\n",
      "Iteration 13866, loss = 1.47236995\n",
      "Iteration 13867, loss = 1.47236552\n",
      "Iteration 13868, loss = 1.47236109\n",
      "Iteration 13869, loss = 1.47235666\n",
      "Iteration 13870, loss = 1.47235223\n",
      "Iteration 13871, loss = 1.47234780\n",
      "Iteration 13872, loss = 1.47234337\n",
      "Iteration 13873, loss = 1.47233895\n",
      "Iteration 13874, loss = 1.47233452\n",
      "Iteration 13875, loss = 1.47233009\n",
      "Iteration 13876, loss = 1.47232567\n",
      "Iteration 13877, loss = 1.47232125\n",
      "Iteration 13878, loss = 1.47231682\n",
      "Iteration 13879, loss = 1.47231240\n",
      "Iteration 13880, loss = 1.47230798\n",
      "Iteration 13881, loss = 1.47230356\n",
      "Iteration 13882, loss = 1.47229914\n",
      "Iteration 13883, loss = 1.47229472\n",
      "Iteration 13884, loss = 1.47229030\n",
      "Iteration 13885, loss = 1.47228588\n",
      "Iteration 13886, loss = 1.47228146\n",
      "Iteration 13887, loss = 1.47227705\n",
      "Iteration 13888, loss = 1.47227263\n",
      "Iteration 13889, loss = 1.47226822\n",
      "Iteration 13890, loss = 1.47226380\n",
      "Iteration 13891, loss = 1.47225939\n",
      "Iteration 13892, loss = 1.47225497\n",
      "Iteration 13893, loss = 1.47225056\n",
      "Iteration 13894, loss = 1.47224615\n",
      "Iteration 13895, loss = 1.47224174\n",
      "Iteration 13896, loss = 1.47223733\n",
      "Iteration 13897, loss = 1.47223292\n",
      "Iteration 13898, loss = 1.47222851\n",
      "Iteration 13899, loss = 1.47222410\n",
      "Iteration 13900, loss = 1.47221969\n",
      "Iteration 13901, loss = 1.47221529\n",
      "Iteration 13902, loss = 1.47221088\n",
      "Iteration 13903, loss = 1.47220647\n",
      "Iteration 13904, loss = 1.47220207\n",
      "Iteration 13905, loss = 1.47219766\n",
      "Iteration 13906, loss = 1.47219326\n",
      "Iteration 13907, loss = 1.47218886\n",
      "Iteration 13908, loss = 1.47218446\n",
      "Iteration 13909, loss = 1.47218005\n",
      "Iteration 13910, loss = 1.47217565\n",
      "Iteration 13911, loss = 1.47217125\n",
      "Iteration 13912, loss = 1.47216685\n",
      "Iteration 13913, loss = 1.47216246\n",
      "Iteration 13914, loss = 1.47215806\n",
      "Iteration 13915, loss = 1.47215366\n",
      "Iteration 13916, loss = 1.47214926\n",
      "Iteration 13917, loss = 1.47214487\n",
      "Iteration 13918, loss = 1.47214047\n",
      "Iteration 13919, loss = 1.47213608\n",
      "Iteration 13920, loss = 1.47213168\n",
      "Iteration 13921, loss = 1.47212729\n",
      "Iteration 13922, loss = 1.47212290\n",
      "Iteration 13923, loss = 1.47211851\n",
      "Iteration 13924, loss = 1.47211412\n",
      "Iteration 13925, loss = 1.47210973\n",
      "Iteration 13926, loss = 1.47210534\n",
      "Iteration 13927, loss = 1.47210095\n",
      "Iteration 13928, loss = 1.47209656\n",
      "Iteration 13929, loss = 1.47209217\n",
      "Iteration 13930, loss = 1.47208778\n",
      "Iteration 13931, loss = 1.47208340\n",
      "Iteration 13932, loss = 1.47207901\n",
      "Iteration 13933, loss = 1.47207463\n",
      "Iteration 13934, loss = 1.47207024\n",
      "Iteration 13935, loss = 1.47206586\n",
      "Iteration 13936, loss = 1.47206148\n",
      "Iteration 13937, loss = 1.47205709\n",
      "Iteration 13938, loss = 1.47205271\n",
      "Iteration 13939, loss = 1.47204833\n",
      "Iteration 13940, loss = 1.47204395\n",
      "Iteration 13941, loss = 1.47203957\n",
      "Iteration 13942, loss = 1.47203519\n",
      "Iteration 13943, loss = 1.47203082\n",
      "Iteration 13944, loss = 1.47202644\n",
      "Iteration 13945, loss = 1.47202206\n",
      "Iteration 13946, loss = 1.47201769\n",
      "Iteration 13947, loss = 1.47201331\n",
      "Iteration 13948, loss = 1.47200894\n",
      "Iteration 13949, loss = 1.47200456\n",
      "Iteration 13950, loss = 1.47200019\n",
      "Iteration 13951, loss = 1.47199582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13952, loss = 1.47199144\n",
      "Iteration 13953, loss = 1.47198707\n",
      "Iteration 13954, loss = 1.47198270\n",
      "Iteration 13955, loss = 1.47197833\n",
      "Iteration 13956, loss = 1.47197396\n",
      "Iteration 13957, loss = 1.47196960\n",
      "Iteration 13958, loss = 1.47196523\n",
      "Iteration 13959, loss = 1.47196086\n",
      "Iteration 13960, loss = 1.47195650\n",
      "Iteration 13961, loss = 1.47195213\n",
      "Iteration 13962, loss = 1.47194776\n",
      "Iteration 13963, loss = 1.47194340\n",
      "Iteration 13964, loss = 1.47193904\n",
      "Iteration 13965, loss = 1.47193467\n",
      "Iteration 13966, loss = 1.47193031\n",
      "Iteration 13967, loss = 1.47192595\n",
      "Iteration 13968, loss = 1.47192159\n",
      "Iteration 13969, loss = 1.47191723\n",
      "Iteration 13970, loss = 1.47191287\n",
      "Iteration 13971, loss = 1.47190851\n",
      "Iteration 13972, loss = 1.47190415\n",
      "Iteration 13973, loss = 1.47189980\n",
      "Iteration 13974, loss = 1.47189544\n",
      "Iteration 13975, loss = 1.47189108\n",
      "Iteration 13976, loss = 1.47188673\n",
      "Iteration 13977, loss = 1.47188237\n",
      "Iteration 13978, loss = 1.47187802\n",
      "Iteration 13979, loss = 1.47187367\n",
      "Iteration 13980, loss = 1.47186931\n",
      "Iteration 13981, loss = 1.47186496\n",
      "Iteration 13982, loss = 1.47186061\n",
      "Iteration 13983, loss = 1.47185626\n",
      "Iteration 13984, loss = 1.47185191\n",
      "Iteration 13985, loss = 1.47184756\n",
      "Iteration 13986, loss = 1.47184321\n",
      "Iteration 13987, loss = 1.47183886\n",
      "Iteration 13988, loss = 1.47183452\n",
      "Iteration 13989, loss = 1.47183017\n",
      "Iteration 13990, loss = 1.47182582\n",
      "Iteration 13991, loss = 1.47182148\n",
      "Iteration 13992, loss = 1.47181713\n",
      "Iteration 13993, loss = 1.47181279\n",
      "Iteration 13994, loss = 1.47180845\n",
      "Iteration 13995, loss = 1.47180411\n",
      "Iteration 13996, loss = 1.47179976\n",
      "Iteration 13997, loss = 1.47179542\n",
      "Iteration 13998, loss = 1.47179108\n",
      "Iteration 13999, loss = 1.47178674\n",
      "Iteration 14000, loss = 1.47178240\n",
      "Iteration 14001, loss = 1.47177806\n",
      "Iteration 14002, loss = 1.47177373\n",
      "Iteration 14003, loss = 1.47176939\n",
      "Iteration 14004, loss = 1.47176505\n",
      "Iteration 14005, loss = 1.47176072\n",
      "Iteration 14006, loss = 1.47175638\n",
      "Iteration 14007, loss = 1.47175205\n",
      "Iteration 14008, loss = 1.47174772\n",
      "Iteration 14009, loss = 1.47174338\n",
      "Iteration 14010, loss = 1.47173905\n",
      "Iteration 14011, loss = 1.47173472\n",
      "Iteration 14012, loss = 1.47173039\n",
      "Iteration 14013, loss = 1.47172606\n",
      "Iteration 14014, loss = 1.47172173\n",
      "Iteration 14015, loss = 1.47171740\n",
      "Iteration 14016, loss = 1.47171307\n",
      "Iteration 14017, loss = 1.47170874\n",
      "Iteration 14018, loss = 1.47170442\n",
      "Iteration 14019, loss = 1.47170009\n",
      "Iteration 14020, loss = 1.47169577\n",
      "Iteration 14021, loss = 1.47169144\n",
      "Iteration 14022, loss = 1.47168712\n",
      "Iteration 14023, loss = 1.47168279\n",
      "Iteration 14024, loss = 1.47167847\n",
      "Iteration 14025, loss = 1.47167415\n",
      "Iteration 14026, loss = 1.47166983\n",
      "Iteration 14027, loss = 1.47166551\n",
      "Iteration 14028, loss = 1.47166119\n",
      "Iteration 14029, loss = 1.47165687\n",
      "Iteration 14030, loss = 1.47165255\n",
      "Iteration 14031, loss = 1.47164823\n",
      "Iteration 14032, loss = 1.47164391\n",
      "Iteration 14033, loss = 1.47163960\n",
      "Iteration 14034, loss = 1.47163528\n",
      "Iteration 14035, loss = 1.47163097\n",
      "Iteration 14036, loss = 1.47162665\n",
      "Iteration 14037, loss = 1.47162234\n",
      "Iteration 14038, loss = 1.47161802\n",
      "Iteration 14039, loss = 1.47161371\n",
      "Iteration 14040, loss = 1.47160940\n",
      "Iteration 14041, loss = 1.47160509\n",
      "Iteration 14042, loss = 1.47160078\n",
      "Iteration 14043, loss = 1.47159647\n",
      "Iteration 14044, loss = 1.47159216\n",
      "Iteration 14045, loss = 1.47158785\n",
      "Iteration 14046, loss = 1.47158354\n",
      "Iteration 14047, loss = 1.47157923\n",
      "Iteration 14048, loss = 1.47157493\n",
      "Iteration 14049, loss = 1.47157062\n",
      "Iteration 14050, loss = 1.47156632\n",
      "Iteration 14051, loss = 1.47156201\n",
      "Iteration 14052, loss = 1.47155771\n",
      "Iteration 14053, loss = 1.47155340\n",
      "Iteration 14054, loss = 1.47154910\n",
      "Iteration 14055, loss = 1.47154480\n",
      "Iteration 14056, loss = 1.47154050\n",
      "Iteration 14057, loss = 1.47153620\n",
      "Iteration 14058, loss = 1.47153190\n",
      "Iteration 14059, loss = 1.47152760\n",
      "Iteration 14060, loss = 1.47152330\n",
      "Iteration 14061, loss = 1.47151900\n",
      "Iteration 14062, loss = 1.47151471\n",
      "Iteration 14063, loss = 1.47151041\n",
      "Iteration 14064, loss = 1.47150611\n",
      "Iteration 14065, loss = 1.47150182\n",
      "Iteration 14066, loss = 1.47149752\n",
      "Iteration 14067, loss = 1.47149323\n",
      "Iteration 14068, loss = 1.47148894\n",
      "Iteration 14069, loss = 1.47148465\n",
      "Iteration 14070, loss = 1.47148035\n",
      "Iteration 14071, loss = 1.47147606\n",
      "Iteration 14072, loss = 1.47147177\n",
      "Iteration 14073, loss = 1.47146748\n",
      "Iteration 14074, loss = 1.47146319\n",
      "Iteration 14075, loss = 1.47145890\n",
      "Iteration 14076, loss = 1.47145462\n",
      "Iteration 14077, loss = 1.47145033\n",
      "Iteration 14078, loss = 1.47144604\n",
      "Iteration 14079, loss = 1.47144176\n",
      "Iteration 14080, loss = 1.47143747\n",
      "Iteration 14081, loss = 1.47143319\n",
      "Iteration 14082, loss = 1.47142890\n",
      "Iteration 14083, loss = 1.47142462\n",
      "Iteration 14084, loss = 1.47142034\n",
      "Iteration 14085, loss = 1.47141606\n",
      "Iteration 14086, loss = 1.47141178\n",
      "Iteration 14087, loss = 1.47140750\n",
      "Iteration 14088, loss = 1.47140322\n",
      "Iteration 14089, loss = 1.47139894\n",
      "Iteration 14090, loss = 1.47139466\n",
      "Iteration 14091, loss = 1.47139038\n",
      "Iteration 14092, loss = 1.47138610\n",
      "Iteration 14093, loss = 1.47138183\n",
      "Iteration 14094, loss = 1.47137755\n",
      "Iteration 14095, loss = 1.47137328\n",
      "Iteration 14096, loss = 1.47136900\n",
      "Iteration 14097, loss = 1.47136473\n",
      "Iteration 14098, loss = 1.47136045\n",
      "Iteration 14099, loss = 1.47135618\n",
      "Iteration 14100, loss = 1.47135191\n",
      "Iteration 14101, loss = 1.47134764\n",
      "Iteration 14102, loss = 1.47134337\n",
      "Iteration 14103, loss = 1.47133910\n",
      "Iteration 14104, loss = 1.47133483\n",
      "Iteration 14105, loss = 1.47133056\n",
      "Iteration 14106, loss = 1.47132629\n",
      "Iteration 14107, loss = 1.47132203\n",
      "Iteration 14108, loss = 1.47131776\n",
      "Iteration 14109, loss = 1.47131349\n",
      "Iteration 14110, loss = 1.47130923\n",
      "Iteration 14111, loss = 1.47130497\n",
      "Iteration 14112, loss = 1.47130070\n",
      "Iteration 14113, loss = 1.47129644\n",
      "Iteration 14114, loss = 1.47129218\n",
      "Iteration 14115, loss = 1.47128791\n",
      "Iteration 14116, loss = 1.47128365\n",
      "Iteration 14117, loss = 1.47127939\n",
      "Iteration 14118, loss = 1.47127513\n",
      "Iteration 14119, loss = 1.47127087\n",
      "Iteration 14120, loss = 1.47126661\n",
      "Iteration 14121, loss = 1.47126236\n",
      "Iteration 14122, loss = 1.47125810\n",
      "Iteration 14123, loss = 1.47125384\n",
      "Iteration 14124, loss = 1.47124959\n",
      "Iteration 14125, loss = 1.47124533\n",
      "Iteration 14126, loss = 1.47124108\n",
      "Iteration 14127, loss = 1.47123682\n",
      "Iteration 14128, loss = 1.47123257\n",
      "Iteration 14129, loss = 1.47122832\n",
      "Iteration 14130, loss = 1.47122407\n",
      "Iteration 14131, loss = 1.47121982\n",
      "Iteration 14132, loss = 1.47121556\n",
      "Iteration 14133, loss = 1.47121131\n",
      "Iteration 14134, loss = 1.47120707\n",
      "Iteration 14135, loss = 1.47120282\n",
      "Iteration 14136, loss = 1.47119857\n",
      "Iteration 14137, loss = 1.47119432\n",
      "Iteration 14138, loss = 1.47119008\n",
      "Iteration 14139, loss = 1.47118583\n",
      "Iteration 14140, loss = 1.47118158\n",
      "Iteration 14141, loss = 1.47117734\n",
      "Iteration 14142, loss = 1.47117310\n",
      "Iteration 14143, loss = 1.47116885\n",
      "Iteration 14144, loss = 1.47116461\n",
      "Iteration 14145, loss = 1.47116037\n",
      "Iteration 14146, loss = 1.47115613\n",
      "Iteration 14147, loss = 1.47115189\n",
      "Iteration 14148, loss = 1.47114765\n",
      "Iteration 14149, loss = 1.47114341\n",
      "Iteration 14150, loss = 1.47113917\n",
      "Iteration 14151, loss = 1.47113493\n",
      "Iteration 14152, loss = 1.47113069\n",
      "Iteration 14153, loss = 1.47112646\n",
      "Iteration 14154, loss = 1.47112222\n",
      "Iteration 14155, loss = 1.47111798\n",
      "Iteration 14156, loss = 1.47111375\n",
      "Iteration 14157, loss = 1.47110951\n",
      "Iteration 14158, loss = 1.47110528\n",
      "Iteration 14159, loss = 1.47110105\n",
      "Iteration 14160, loss = 1.47109682\n",
      "Iteration 14161, loss = 1.47109259\n",
      "Iteration 14162, loss = 1.47108835\n",
      "Iteration 14163, loss = 1.47108412\n",
      "Iteration 14164, loss = 1.47107989\n",
      "Iteration 14165, loss = 1.47107567\n",
      "Iteration 14166, loss = 1.47107144\n",
      "Iteration 14167, loss = 1.47106721\n",
      "Iteration 14168, loss = 1.47106298\n",
      "Iteration 14169, loss = 1.47105876\n",
      "Iteration 14170, loss = 1.47105453\n",
      "Iteration 14171, loss = 1.47105031\n",
      "Iteration 14172, loss = 1.47104608\n",
      "Iteration 14173, loss = 1.47104186\n",
      "Iteration 14174, loss = 1.47103764\n",
      "Iteration 14175, loss = 1.47103341\n",
      "Iteration 14176, loss = 1.47102919\n",
      "Iteration 14177, loss = 1.47102497\n",
      "Iteration 14178, loss = 1.47102075\n",
      "Iteration 14179, loss = 1.47101653\n",
      "Iteration 14180, loss = 1.47101231\n",
      "Iteration 14181, loss = 1.47100809\n",
      "Iteration 14182, loss = 1.47100388\n",
      "Iteration 14183, loss = 1.47099966\n",
      "Iteration 14184, loss = 1.47099544\n",
      "Iteration 14185, loss = 1.47099123\n",
      "Iteration 14186, loss = 1.47098701\n",
      "Iteration 14187, loss = 1.47098280\n",
      "Iteration 14188, loss = 1.47097858\n",
      "Iteration 14189, loss = 1.47097437\n",
      "Iteration 14190, loss = 1.47097016\n",
      "Iteration 14191, loss = 1.47096595\n",
      "Iteration 14192, loss = 1.47096173\n",
      "Iteration 14193, loss = 1.47095752\n",
      "Iteration 14194, loss = 1.47095331\n",
      "Iteration 14195, loss = 1.47094911\n",
      "Iteration 14196, loss = 1.47094490\n",
      "Iteration 14197, loss = 1.47094069\n",
      "Iteration 14198, loss = 1.47093648\n",
      "Iteration 14199, loss = 1.47093227\n",
      "Iteration 14200, loss = 1.47092807\n",
      "Iteration 14201, loss = 1.47092386\n",
      "Iteration 14202, loss = 1.47091966\n",
      "Iteration 14203, loss = 1.47091545\n",
      "Iteration 14204, loss = 1.47091125\n",
      "Iteration 14205, loss = 1.47090705\n",
      "Iteration 14206, loss = 1.47090285\n",
      "Iteration 14207, loss = 1.47089865\n",
      "Iteration 14208, loss = 1.47089444\n",
      "Iteration 14209, loss = 1.47089024\n",
      "Iteration 14210, loss = 1.47088605\n",
      "Iteration 14211, loss = 1.47088185\n",
      "Iteration 14212, loss = 1.47087765\n",
      "Iteration 14213, loss = 1.47087345\n",
      "Iteration 14214, loss = 1.47086925\n",
      "Iteration 14215, loss = 1.47086506\n",
      "Iteration 14216, loss = 1.47086086\n",
      "Iteration 14217, loss = 1.47085667\n",
      "Iteration 14218, loss = 1.47085247\n",
      "Iteration 14219, loss = 1.47084828\n",
      "Iteration 14220, loss = 1.47084409\n",
      "Iteration 14221, loss = 1.47083989\n",
      "Iteration 14222, loss = 1.47083570\n",
      "Iteration 14223, loss = 1.47083151\n",
      "Iteration 14224, loss = 1.47082732\n",
      "Iteration 14225, loss = 1.47082313\n",
      "Iteration 14226, loss = 1.47081894\n",
      "Iteration 14227, loss = 1.47081475\n",
      "Iteration 14228, loss = 1.47081057\n",
      "Iteration 14229, loss = 1.47080638\n",
      "Iteration 14230, loss = 1.47080219\n",
      "Iteration 14231, loss = 1.47079801\n",
      "Iteration 14232, loss = 1.47079382\n",
      "Iteration 14233, loss = 1.47078964\n",
      "Iteration 14234, loss = 1.47078545\n",
      "Iteration 14235, loss = 1.47078127\n",
      "Iteration 14236, loss = 1.47077709\n",
      "Iteration 14237, loss = 1.47077290\n",
      "Iteration 14238, loss = 1.47076872\n",
      "Iteration 14239, loss = 1.47076454\n",
      "Iteration 14240, loss = 1.47076036\n",
      "Iteration 14241, loss = 1.47075618\n",
      "Iteration 14242, loss = 1.47075200\n",
      "Iteration 14243, loss = 1.47074783\n",
      "Iteration 14244, loss = 1.47074365\n",
      "Iteration 14245, loss = 1.47073947\n",
      "Iteration 14246, loss = 1.47073530\n",
      "Iteration 14247, loss = 1.47073112\n",
      "Iteration 14248, loss = 1.47072695\n",
      "Iteration 14249, loss = 1.47072277\n",
      "Iteration 14250, loss = 1.47071860\n",
      "Iteration 14251, loss = 1.47071442\n",
      "Iteration 14252, loss = 1.47071025\n",
      "Iteration 14253, loss = 1.47070608\n",
      "Iteration 14254, loss = 1.47070191\n",
      "Iteration 14255, loss = 1.47069774\n",
      "Iteration 14256, loss = 1.47069357\n",
      "Iteration 14257, loss = 1.47068940\n",
      "Iteration 14258, loss = 1.47068523\n",
      "Iteration 14259, loss = 1.47068106\n",
      "Iteration 14260, loss = 1.47067690\n",
      "Iteration 14261, loss = 1.47067273\n",
      "Iteration 14262, loss = 1.47066856\n",
      "Iteration 14263, loss = 1.47066440\n",
      "Iteration 14264, loss = 1.47066023\n",
      "Iteration 14265, loss = 1.47065607\n",
      "Iteration 14266, loss = 1.47065191\n",
      "Iteration 14267, loss = 1.47064775\n",
      "Iteration 14268, loss = 1.47064358\n",
      "Iteration 14269, loss = 1.47063942\n",
      "Iteration 14270, loss = 1.47063526\n",
      "Iteration 14271, loss = 1.47063110\n",
      "Iteration 14272, loss = 1.47062694\n",
      "Iteration 14273, loss = 1.47062278\n",
      "Iteration 14274, loss = 1.47061862\n",
      "Iteration 14275, loss = 1.47061447\n",
      "Iteration 14276, loss = 1.47061031\n",
      "Iteration 14277, loss = 1.47060615\n",
      "Iteration 14278, loss = 1.47060200\n",
      "Iteration 14279, loss = 1.47059784\n",
      "Iteration 14280, loss = 1.47059369\n",
      "Iteration 14281, loss = 1.47058954\n",
      "Iteration 14282, loss = 1.47058538\n",
      "Iteration 14283, loss = 1.47058123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14284, loss = 1.47057708\n",
      "Iteration 14285, loss = 1.47057293\n",
      "Iteration 14286, loss = 1.47056878\n",
      "Iteration 14287, loss = 1.47056463\n",
      "Iteration 14288, loss = 1.47056048\n",
      "Iteration 14289, loss = 1.47055633\n",
      "Iteration 14290, loss = 1.47055218\n",
      "Iteration 14291, loss = 1.47054803\n",
      "Iteration 14292, loss = 1.47054389\n",
      "Iteration 14293, loss = 1.47053974\n",
      "Iteration 14294, loss = 1.47053560\n",
      "Iteration 14295, loss = 1.47053145\n",
      "Iteration 14296, loss = 1.47052731\n",
      "Iteration 14297, loss = 1.47052316\n",
      "Iteration 14298, loss = 1.47051902\n",
      "Iteration 14299, loss = 1.47051488\n",
      "Iteration 14300, loss = 1.47051074\n",
      "Iteration 14301, loss = 1.47050660\n",
      "Iteration 14302, loss = 1.47050246\n",
      "Iteration 14303, loss = 1.47049832\n",
      "Iteration 14304, loss = 1.47049418\n",
      "Iteration 14305, loss = 1.47049004\n",
      "Iteration 14306, loss = 1.47048590\n",
      "Iteration 14307, loss = 1.47048176\n",
      "Iteration 14308, loss = 1.47047763\n",
      "Iteration 14309, loss = 1.47047349\n",
      "Iteration 14310, loss = 1.47046936\n",
      "Iteration 14311, loss = 1.47046522\n",
      "Iteration 14312, loss = 1.47046109\n",
      "Iteration 14313, loss = 1.47045695\n",
      "Iteration 14314, loss = 1.47045282\n",
      "Iteration 14315, loss = 1.47044869\n",
      "Iteration 14316, loss = 1.47044456\n",
      "Iteration 14317, loss = 1.47044043\n",
      "Iteration 14318, loss = 1.47043630\n",
      "Iteration 14319, loss = 1.47043217\n",
      "Iteration 14320, loss = 1.47042804\n",
      "Iteration 14321, loss = 1.47042391\n",
      "Iteration 14322, loss = 1.47041978\n",
      "Iteration 14323, loss = 1.47041566\n",
      "Iteration 14324, loss = 1.47041153\n",
      "Iteration 14325, loss = 1.47040740\n",
      "Iteration 14326, loss = 1.47040328\n",
      "Iteration 14327, loss = 1.47039915\n",
      "Iteration 14328, loss = 1.47039503\n",
      "Iteration 14329, loss = 1.47039091\n",
      "Iteration 14330, loss = 1.47038678\n",
      "Iteration 14331, loss = 1.47038266\n",
      "Iteration 14332, loss = 1.47037854\n",
      "Iteration 14333, loss = 1.47037442\n",
      "Iteration 14334, loss = 1.47037030\n",
      "Iteration 14335, loss = 1.47036618\n",
      "Iteration 14336, loss = 1.47036206\n",
      "Iteration 14337, loss = 1.47035795\n",
      "Iteration 14338, loss = 1.47035383\n",
      "Iteration 14339, loss = 1.47034971\n",
      "Iteration 14340, loss = 1.47034559\n",
      "Iteration 14341, loss = 1.47034148\n",
      "Iteration 14342, loss = 1.47033736\n",
      "Iteration 14343, loss = 1.47033325\n",
      "Iteration 14344, loss = 1.47032914\n",
      "Iteration 14345, loss = 1.47032502\n",
      "Iteration 14346, loss = 1.47032091\n",
      "Iteration 14347, loss = 1.47031680\n",
      "Iteration 14348, loss = 1.47031269\n",
      "Iteration 14349, loss = 1.47030858\n",
      "Iteration 14350, loss = 1.47030447\n",
      "Iteration 14351, loss = 1.47030036\n",
      "Iteration 14352, loss = 1.47029625\n",
      "Iteration 14353, loss = 1.47029214\n",
      "Iteration 14354, loss = 1.47028803\n",
      "Iteration 14355, loss = 1.47028393\n",
      "Iteration 14356, loss = 1.47027982\n",
      "Iteration 14357, loss = 1.47027572\n",
      "Iteration 14358, loss = 1.47027161\n",
      "Iteration 14359, loss = 1.47026751\n",
      "Iteration 14360, loss = 1.47026340\n",
      "Iteration 14361, loss = 1.47025930\n",
      "Iteration 14362, loss = 1.47025520\n",
      "Iteration 14363, loss = 1.47025110\n",
      "Iteration 14364, loss = 1.47024700\n",
      "Iteration 14365, loss = 1.47024289\n",
      "Iteration 14366, loss = 1.47023879\n",
      "Iteration 14367, loss = 1.47023470\n",
      "Iteration 14368, loss = 1.47023060\n",
      "Iteration 14369, loss = 1.47022650\n",
      "Iteration 14370, loss = 1.47022240\n",
      "Iteration 14371, loss = 1.47021831\n",
      "Iteration 14372, loss = 1.47021421\n",
      "Iteration 14373, loss = 1.47021011\n",
      "Iteration 14374, loss = 1.47020602\n",
      "Iteration 14375, loss = 1.47020192\n",
      "Iteration 14376, loss = 1.47019783\n",
      "Iteration 14377, loss = 1.47019374\n",
      "Iteration 14378, loss = 1.47018965\n",
      "Iteration 14379, loss = 1.47018555\n",
      "Iteration 14380, loss = 1.47018146\n",
      "Iteration 14381, loss = 1.47017737\n",
      "Iteration 14382, loss = 1.47017328\n",
      "Iteration 14383, loss = 1.47016919\n",
      "Iteration 14384, loss = 1.47016511\n",
      "Iteration 14385, loss = 1.47016102\n",
      "Iteration 14386, loss = 1.47015693\n",
      "Iteration 14387, loss = 1.47015284\n",
      "Iteration 14388, loss = 1.47014876\n",
      "Iteration 14389, loss = 1.47014467\n",
      "Iteration 14390, loss = 1.47014059\n",
      "Iteration 14391, loss = 1.47013650\n",
      "Iteration 14392, loss = 1.47013242\n",
      "Iteration 14393, loss = 1.47012834\n",
      "Iteration 14394, loss = 1.47012425\n",
      "Iteration 14395, loss = 1.47012017\n",
      "Iteration 14396, loss = 1.47011609\n",
      "Iteration 14397, loss = 1.47011201\n",
      "Iteration 14398, loss = 1.47010793\n",
      "Iteration 14399, loss = 1.47010385\n",
      "Iteration 14400, loss = 1.47009977\n",
      "Iteration 14401, loss = 1.47009570\n",
      "Iteration 14402, loss = 1.47009162\n",
      "Iteration 14403, loss = 1.47008754\n",
      "Iteration 14404, loss = 1.47008347\n",
      "Iteration 14405, loss = 1.47007939\n",
      "Iteration 14406, loss = 1.47007532\n",
      "Iteration 14407, loss = 1.47007124\n",
      "Iteration 14408, loss = 1.47006717\n",
      "Iteration 14409, loss = 1.47006310\n",
      "Iteration 14410, loss = 1.47005902\n",
      "Iteration 14411, loss = 1.47005495\n",
      "Iteration 14412, loss = 1.47005088\n",
      "Iteration 14413, loss = 1.47004681\n",
      "Iteration 14414, loss = 1.47004274\n",
      "Iteration 14415, loss = 1.47003867\n",
      "Iteration 14416, loss = 1.47003460\n",
      "Iteration 14417, loss = 1.47003053\n",
      "Iteration 14418, loss = 1.47002647\n",
      "Iteration 14419, loss = 1.47002240\n",
      "Iteration 14420, loss = 1.47001833\n",
      "Iteration 14421, loss = 1.47001427\n",
      "Iteration 14422, loss = 1.47001020\n",
      "Iteration 14423, loss = 1.47000614\n",
      "Iteration 14424, loss = 1.47000208\n",
      "Iteration 14425, loss = 1.46999801\n",
      "Iteration 14426, loss = 1.46999395\n",
      "Iteration 14427, loss = 1.46998989\n",
      "Iteration 14428, loss = 1.46998583\n",
      "Iteration 14429, loss = 1.46998177\n",
      "Iteration 14430, loss = 1.46997771\n",
      "Iteration 14431, loss = 1.46997365\n",
      "Iteration 14432, loss = 1.46996959\n",
      "Iteration 14433, loss = 1.46996553\n",
      "Iteration 14434, loss = 1.46996147\n",
      "Iteration 14435, loss = 1.46995742\n",
      "Iteration 14436, loss = 1.46995336\n",
      "Iteration 14437, loss = 1.46994931\n",
      "Iteration 14438, loss = 1.46994525\n",
      "Iteration 14439, loss = 1.46994120\n",
      "Iteration 14440, loss = 1.46993714\n",
      "Iteration 14441, loss = 1.46993309\n",
      "Iteration 14442, loss = 1.46992904\n",
      "Iteration 14443, loss = 1.46992499\n",
      "Iteration 14444, loss = 1.46992093\n",
      "Iteration 14445, loss = 1.46991688\n",
      "Iteration 14446, loss = 1.46991283\n",
      "Iteration 14447, loss = 1.46990878\n",
      "Iteration 14448, loss = 1.46990474\n",
      "Iteration 14449, loss = 1.46990069\n",
      "Iteration 14450, loss = 1.46989664\n",
      "Iteration 14451, loss = 1.46989259\n",
      "Iteration 14452, loss = 1.46988855\n",
      "Iteration 14453, loss = 1.46988450\n",
      "Iteration 14454, loss = 1.46988046\n",
      "Iteration 14455, loss = 1.46987641\n",
      "Iteration 14456, loss = 1.46987237\n",
      "Iteration 14457, loss = 1.46986832\n",
      "Iteration 14458, loss = 1.46986428\n",
      "Iteration 14459, loss = 1.46986024\n",
      "Iteration 14460, loss = 1.46985620\n",
      "Iteration 14461, loss = 1.46985216\n",
      "Iteration 14462, loss = 1.46984812\n",
      "Iteration 14463, loss = 1.46984408\n",
      "Iteration 14464, loss = 1.46984004\n",
      "Iteration 14465, loss = 1.46983600\n",
      "Iteration 14466, loss = 1.46983196\n",
      "Iteration 14467, loss = 1.46982793\n",
      "Iteration 14468, loss = 1.46982389\n",
      "Iteration 14469, loss = 1.46981985\n",
      "Iteration 14470, loss = 1.46981582\n",
      "Iteration 14471, loss = 1.46981178\n",
      "Iteration 14472, loss = 1.46980775\n",
      "Iteration 14473, loss = 1.46980372\n",
      "Iteration 14474, loss = 1.46979968\n",
      "Iteration 14475, loss = 1.46979565\n",
      "Iteration 14476, loss = 1.46979162\n",
      "Iteration 14477, loss = 1.46978759\n",
      "Iteration 14478, loss = 1.46978356\n",
      "Iteration 14479, loss = 1.46977953\n",
      "Iteration 14480, loss = 1.46977550\n",
      "Iteration 14481, loss = 1.46977147\n",
      "Iteration 14482, loss = 1.46976745\n",
      "Iteration 14483, loss = 1.46976342\n",
      "Iteration 14484, loss = 1.46975939\n",
      "Iteration 14485, loss = 1.46975537\n",
      "Iteration 14486, loss = 1.46975134\n",
      "Iteration 14487, loss = 1.46974732\n",
      "Iteration 14488, loss = 1.46974329\n",
      "Iteration 14489, loss = 1.46973927\n",
      "Iteration 14490, loss = 1.46973524\n",
      "Iteration 14491, loss = 1.46973122\n",
      "Iteration 14492, loss = 1.46972720\n",
      "Iteration 14493, loss = 1.46972318\n",
      "Iteration 14494, loss = 1.46971916\n",
      "Iteration 14495, loss = 1.46971514\n",
      "Iteration 14496, loss = 1.46971112\n",
      "Iteration 14497, loss = 1.46970710\n",
      "Iteration 14498, loss = 1.46970308\n",
      "Iteration 14499, loss = 1.46969907\n",
      "Iteration 14500, loss = 1.46969505\n",
      "Iteration 14501, loss = 1.46969103\n",
      "Iteration 14502, loss = 1.46968702\n",
      "Iteration 14503, loss = 1.46968300\n",
      "Iteration 14504, loss = 1.46967899\n",
      "Iteration 14505, loss = 1.46967497\n",
      "Iteration 14506, loss = 1.46967096\n",
      "Iteration 14507, loss = 1.46966695\n",
      "Iteration 14508, loss = 1.46966294\n",
      "Iteration 14509, loss = 1.46965892\n",
      "Iteration 14510, loss = 1.46965491\n",
      "Iteration 14511, loss = 1.46965090\n",
      "Iteration 14512, loss = 1.46964689\n",
      "Iteration 14513, loss = 1.46964289\n",
      "Iteration 14514, loss = 1.46963888\n",
      "Iteration 14515, loss = 1.46963487\n",
      "Iteration 14516, loss = 1.46963086\n",
      "Iteration 14517, loss = 1.46962686\n",
      "Iteration 14518, loss = 1.46962285\n",
      "Iteration 14519, loss = 1.46961885\n",
      "Iteration 14520, loss = 1.46961484\n",
      "Iteration 14521, loss = 1.46961084\n",
      "Iteration 14522, loss = 1.46960683\n",
      "Iteration 14523, loss = 1.46960283\n",
      "Iteration 14524, loss = 1.46959883\n",
      "Iteration 14525, loss = 1.46959483\n",
      "Iteration 14526, loss = 1.46959083\n",
      "Iteration 14527, loss = 1.46958683\n",
      "Iteration 14528, loss = 1.46958283\n",
      "Iteration 14529, loss = 1.46957883\n",
      "Iteration 14530, loss = 1.46957483\n",
      "Iteration 14531, loss = 1.46957083\n",
      "Iteration 14532, loss = 1.46956683\n",
      "Iteration 14533, loss = 1.46956284\n",
      "Iteration 14534, loss = 1.46955884\n",
      "Iteration 14535, loss = 1.46955484\n",
      "Iteration 14536, loss = 1.46955085\n",
      "Iteration 14537, loss = 1.46954685\n",
      "Iteration 14538, loss = 1.46954286\n",
      "Iteration 14539, loss = 1.46953887\n",
      "Iteration 14540, loss = 1.46953488\n",
      "Iteration 14541, loss = 1.46953088\n",
      "Iteration 14542, loss = 1.46952689\n",
      "Iteration 14543, loss = 1.46952290\n",
      "Iteration 14544, loss = 1.46951891\n",
      "Iteration 14545, loss = 1.46951492\n",
      "Iteration 14546, loss = 1.46951093\n",
      "Iteration 14547, loss = 1.46950694\n",
      "Iteration 14548, loss = 1.46950296\n",
      "Iteration 14549, loss = 1.46949897\n",
      "Iteration 14550, loss = 1.46949498\n",
      "Iteration 14551, loss = 1.46949100\n",
      "Iteration 14552, loss = 1.46948701\n",
      "Iteration 14553, loss = 1.46948303\n",
      "Iteration 14554, loss = 1.46947904\n",
      "Iteration 14555, loss = 1.46947506\n",
      "Iteration 14556, loss = 1.46947108\n",
      "Iteration 14557, loss = 1.46946710\n",
      "Iteration 14558, loss = 1.46946311\n",
      "Iteration 14559, loss = 1.46945913\n",
      "Iteration 14560, loss = 1.46945515\n",
      "Iteration 14561, loss = 1.46945117\n",
      "Iteration 14562, loss = 1.46944719\n",
      "Iteration 14563, loss = 1.46944321\n",
      "Iteration 14564, loss = 1.46943924\n",
      "Iteration 14565, loss = 1.46943526\n",
      "Iteration 14566, loss = 1.46943128\n",
      "Iteration 14567, loss = 1.46942731\n",
      "Iteration 14568, loss = 1.46942333\n",
      "Iteration 14569, loss = 1.46941936\n",
      "Iteration 14570, loss = 1.46941538\n",
      "Iteration 14571, loss = 1.46941141\n",
      "Iteration 14572, loss = 1.46940743\n",
      "Iteration 14573, loss = 1.46940346\n",
      "Iteration 14574, loss = 1.46939949\n",
      "Iteration 14575, loss = 1.46939552\n",
      "Iteration 14576, loss = 1.46939155\n",
      "Iteration 14577, loss = 1.46938758\n",
      "Iteration 14578, loss = 1.46938361\n",
      "Iteration 14579, loss = 1.46937964\n",
      "Iteration 14580, loss = 1.46937567\n",
      "Iteration 14581, loss = 1.46937170\n",
      "Iteration 14582, loss = 1.46936774\n",
      "Iteration 14583, loss = 1.46936377\n",
      "Iteration 14584, loss = 1.46935980\n",
      "Iteration 14585, loss = 1.46935584\n",
      "Iteration 14586, loss = 1.46935187\n",
      "Iteration 14587, loss = 1.46934791\n",
      "Iteration 14588, loss = 1.46934395\n",
      "Iteration 14589, loss = 1.46933998\n",
      "Iteration 14590, loss = 1.46933602\n",
      "Iteration 14591, loss = 1.46933206\n",
      "Iteration 14592, loss = 1.46932810\n",
      "Iteration 14593, loss = 1.46932414\n",
      "Iteration 14594, loss = 1.46932018\n",
      "Iteration 14595, loss = 1.46931622\n",
      "Iteration 14596, loss = 1.46931226\n",
      "Iteration 14597, loss = 1.46930830\n",
      "Iteration 14598, loss = 1.46930434\n",
      "Iteration 14599, loss = 1.46930039\n",
      "Iteration 14600, loss = 1.46929643\n",
      "Iteration 14601, loss = 1.46929247\n",
      "Iteration 14602, loss = 1.46928852\n",
      "Iteration 14603, loss = 1.46928456\n",
      "Iteration 14604, loss = 1.46928061\n",
      "Iteration 14605, loss = 1.46927666\n",
      "Iteration 14606, loss = 1.46927270\n",
      "Iteration 14607, loss = 1.46926875\n",
      "Iteration 14608, loss = 1.46926480\n",
      "Iteration 14609, loss = 1.46926085\n",
      "Iteration 14610, loss = 1.46925690\n",
      "Iteration 14611, loss = 1.46925295\n",
      "Iteration 14612, loss = 1.46924900\n",
      "Iteration 14613, loss = 1.46924505\n",
      "Iteration 14614, loss = 1.46924110\n",
      "Iteration 14615, loss = 1.46923715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14616, loss = 1.46923321\n",
      "Iteration 14617, loss = 1.46922926\n",
      "Iteration 14618, loss = 1.46922532\n",
      "Iteration 14619, loss = 1.46922137\n",
      "Iteration 14620, loss = 1.46921743\n",
      "Iteration 14621, loss = 1.46921348\n",
      "Iteration 14622, loss = 1.46920954\n",
      "Iteration 14623, loss = 1.46920560\n",
      "Iteration 14624, loss = 1.46920165\n",
      "Iteration 14625, loss = 1.46919771\n",
      "Iteration 14626, loss = 1.46919377\n",
      "Iteration 14627, loss = 1.46918983\n",
      "Iteration 14628, loss = 1.46918589\n",
      "Iteration 14629, loss = 1.46918195\n",
      "Iteration 14630, loss = 1.46917801\n",
      "Iteration 14631, loss = 1.46917408\n",
      "Iteration 14632, loss = 1.46917014\n",
      "Iteration 14633, loss = 1.46916620\n",
      "Iteration 14634, loss = 1.46916227\n",
      "Iteration 14635, loss = 1.46915833\n",
      "Iteration 14636, loss = 1.46915440\n",
      "Iteration 14637, loss = 1.46915046\n",
      "Iteration 14638, loss = 1.46914653\n",
      "Iteration 14639, loss = 1.46914259\n",
      "Iteration 14640, loss = 1.46913866\n",
      "Iteration 14641, loss = 1.46913473\n",
      "Iteration 14642, loss = 1.46913080\n",
      "Iteration 14643, loss = 1.46912687\n",
      "Iteration 14644, loss = 1.46912294\n",
      "Iteration 14645, loss = 1.46911901\n",
      "Iteration 14646, loss = 1.46911508\n",
      "Iteration 14647, loss = 1.46911115\n",
      "Iteration 14648, loss = 1.46910722\n",
      "Iteration 14649, loss = 1.46910330\n",
      "Iteration 14650, loss = 1.46909937\n",
      "Iteration 14651, loss = 1.46909544\n",
      "Iteration 14652, loss = 1.46909152\n",
      "Iteration 14653, loss = 1.46908759\n",
      "Iteration 14654, loss = 1.46908367\n",
      "Iteration 14655, loss = 1.46907974\n",
      "Iteration 14656, loss = 1.46907582\n",
      "Iteration 14657, loss = 1.46907190\n",
      "Iteration 14658, loss = 1.46906798\n",
      "Iteration 14659, loss = 1.46906406\n",
      "Iteration 14660, loss = 1.46906013\n",
      "Iteration 14661, loss = 1.46905621\n",
      "Iteration 14662, loss = 1.46905229\n",
      "Iteration 14663, loss = 1.46904838\n",
      "Iteration 14664, loss = 1.46904446\n",
      "Iteration 14665, loss = 1.46904054\n",
      "Iteration 14666, loss = 1.46903662\n",
      "Iteration 14667, loss = 1.46903271\n",
      "Iteration 14668, loss = 1.46902879\n",
      "Iteration 14669, loss = 1.46902487\n",
      "Iteration 14670, loss = 1.46902096\n",
      "Iteration 14671, loss = 1.46901705\n",
      "Iteration 14672, loss = 1.46901313\n",
      "Iteration 14673, loss = 1.46900922\n",
      "Iteration 14674, loss = 1.46900531\n",
      "Iteration 14675, loss = 1.46900139\n",
      "Iteration 14676, loss = 1.46899748\n",
      "Iteration 14677, loss = 1.46899357\n",
      "Iteration 14678, loss = 1.46898966\n",
      "Iteration 14679, loss = 1.46898575\n",
      "Iteration 14680, loss = 1.46898184\n",
      "Iteration 14681, loss = 1.46897793\n",
      "Iteration 14682, loss = 1.46897403\n",
      "Iteration 14683, loss = 1.46897012\n",
      "Iteration 14684, loss = 1.46896621\n",
      "Iteration 14685, loss = 1.46896231\n",
      "Iteration 14686, loss = 1.46895840\n",
      "Iteration 14687, loss = 1.46895450\n",
      "Iteration 14688, loss = 1.46895059\n",
      "Iteration 14689, loss = 1.46894669\n",
      "Iteration 14690, loss = 1.46894279\n",
      "Iteration 14691, loss = 1.46893888\n",
      "Iteration 14692, loss = 1.46893498\n",
      "Iteration 14693, loss = 1.46893108\n",
      "Iteration 14694, loss = 1.46892718\n",
      "Iteration 14695, loss = 1.46892328\n",
      "Iteration 14696, loss = 1.46891938\n",
      "Iteration 14697, loss = 1.46891548\n",
      "Iteration 14698, loss = 1.46891158\n",
      "Iteration 14699, loss = 1.46890768\n",
      "Iteration 14700, loss = 1.46890379\n",
      "Iteration 14701, loss = 1.46889989\n",
      "Iteration 14702, loss = 1.46889599\n",
      "Iteration 14703, loss = 1.46889210\n",
      "Iteration 14704, loss = 1.46888820\n",
      "Iteration 14705, loss = 1.46888431\n",
      "Iteration 14706, loss = 1.46888042\n",
      "Iteration 14707, loss = 1.46887652\n",
      "Iteration 14708, loss = 1.46887263\n",
      "Iteration 14709, loss = 1.46886874\n",
      "Iteration 14710, loss = 1.46886485\n",
      "Iteration 14711, loss = 1.46886096\n",
      "Iteration 14712, loss = 1.46885707\n",
      "Iteration 14713, loss = 1.46885318\n",
      "Iteration 14714, loss = 1.46884929\n",
      "Iteration 14715, loss = 1.46884540\n",
      "Iteration 14716, loss = 1.46884151\n",
      "Iteration 14717, loss = 1.46883762\n",
      "Iteration 14718, loss = 1.46883374\n",
      "Iteration 14719, loss = 1.46882985\n",
      "Iteration 14720, loss = 1.46882597\n",
      "Iteration 14721, loss = 1.46882208\n",
      "Iteration 14722, loss = 1.46881820\n",
      "Iteration 14723, loss = 1.46881431\n",
      "Iteration 14724, loss = 1.46881043\n",
      "Iteration 14725, loss = 1.46880655\n",
      "Iteration 14726, loss = 1.46880266\n",
      "Iteration 14727, loss = 1.46879878\n",
      "Iteration 14728, loss = 1.46879490\n",
      "Iteration 14729, loss = 1.46879102\n",
      "Iteration 14730, loss = 1.46878714\n",
      "Iteration 14731, loss = 1.46878326\n",
      "Iteration 14732, loss = 1.46877938\n",
      "Iteration 14733, loss = 1.46877551\n",
      "Iteration 14734, loss = 1.46877163\n",
      "Iteration 14735, loss = 1.46876775\n",
      "Iteration 14736, loss = 1.46876388\n",
      "Iteration 14737, loss = 1.46876000\n",
      "Iteration 14738, loss = 1.46875613\n",
      "Iteration 14739, loss = 1.46875225\n",
      "Iteration 14740, loss = 1.46874838\n",
      "Iteration 14741, loss = 1.46874450\n",
      "Iteration 14742, loss = 1.46874063\n",
      "Iteration 14743, loss = 1.46873676\n",
      "Iteration 14744, loss = 1.46873289\n",
      "Iteration 14745, loss = 1.46872902\n",
      "Iteration 14746, loss = 1.46872515\n",
      "Iteration 14747, loss = 1.46872128\n",
      "Iteration 14748, loss = 1.46871741\n",
      "Iteration 14749, loss = 1.46871354\n",
      "Iteration 14750, loss = 1.46870967\n",
      "Iteration 14751, loss = 1.46870580\n",
      "Iteration 14752, loss = 1.46870194\n",
      "Iteration 14753, loss = 1.46869807\n",
      "Iteration 14754, loss = 1.46869420\n",
      "Iteration 14755, loss = 1.46869034\n",
      "Iteration 14756, loss = 1.46868647\n",
      "Iteration 14757, loss = 1.46868261\n",
      "Iteration 14758, loss = 1.46867875\n",
      "Iteration 14759, loss = 1.46867488\n",
      "Iteration 14760, loss = 1.46867102\n",
      "Iteration 14761, loss = 1.46866716\n",
      "Iteration 14762, loss = 1.46866330\n",
      "Iteration 14763, loss = 1.46865944\n",
      "Iteration 14764, loss = 1.46865558\n",
      "Iteration 14765, loss = 1.46865172\n",
      "Iteration 14766, loss = 1.46864786\n",
      "Iteration 14767, loss = 1.46864400\n",
      "Iteration 14768, loss = 1.46864014\n",
      "Iteration 14769, loss = 1.46863629\n",
      "Iteration 14770, loss = 1.46863243\n",
      "Iteration 14771, loss = 1.46862857\n",
      "Iteration 14772, loss = 1.46862472\n",
      "Iteration 14773, loss = 1.46862086\n",
      "Iteration 14774, loss = 1.46861701\n",
      "Iteration 14775, loss = 1.46861315\n",
      "Iteration 14776, loss = 1.46860930\n",
      "Iteration 14777, loss = 1.46860545\n",
      "Iteration 14778, loss = 1.46860160\n",
      "Iteration 14779, loss = 1.46859775\n",
      "Iteration 14780, loss = 1.46859390\n",
      "Iteration 14781, loss = 1.46859004\n",
      "Iteration 14782, loss = 1.46858620\n",
      "Iteration 14783, loss = 1.46858235\n",
      "Iteration 14784, loss = 1.46857850\n",
      "Iteration 14785, loss = 1.46857465\n",
      "Iteration 14786, loss = 1.46857080\n",
      "Iteration 14787, loss = 1.46856696\n",
      "Iteration 14788, loss = 1.46856311\n",
      "Iteration 14789, loss = 1.46855926\n",
      "Iteration 14790, loss = 1.46855542\n",
      "Iteration 14791, loss = 1.46855157\n",
      "Iteration 14792, loss = 1.46854773\n",
      "Iteration 14793, loss = 1.46854389\n",
      "Iteration 14794, loss = 1.46854004\n",
      "Iteration 14795, loss = 1.46853620\n",
      "Iteration 14796, loss = 1.46853236\n",
      "Iteration 14797, loss = 1.46852852\n",
      "Iteration 14798, loss = 1.46852468\n",
      "Iteration 14799, loss = 1.46852084\n",
      "Iteration 14800, loss = 1.46851700\n",
      "Iteration 14801, loss = 1.46851316\n",
      "Iteration 14802, loss = 1.46850932\n",
      "Iteration 14803, loss = 1.46850549\n",
      "Iteration 14804, loss = 1.46850165\n",
      "Iteration 14805, loss = 1.46849781\n",
      "Iteration 14806, loss = 1.46849398\n",
      "Iteration 14807, loss = 1.46849014\n",
      "Iteration 14808, loss = 1.46848631\n",
      "Iteration 14809, loss = 1.46848247\n",
      "Iteration 14810, loss = 1.46847864\n",
      "Iteration 14811, loss = 1.46847481\n",
      "Iteration 14812, loss = 1.46847097\n",
      "Iteration 14813, loss = 1.46846714\n",
      "Iteration 14814, loss = 1.46846331\n",
      "Iteration 14815, loss = 1.46845948\n",
      "Iteration 14816, loss = 1.46845565\n",
      "Iteration 14817, loss = 1.46845182\n",
      "Iteration 14818, loss = 1.46844799\n",
      "Iteration 14819, loss = 1.46844416\n",
      "Iteration 14820, loss = 1.46844033\n",
      "Iteration 14821, loss = 1.46843651\n",
      "Iteration 14822, loss = 1.46843268\n",
      "Iteration 14823, loss = 1.46842885\n",
      "Iteration 14824, loss = 1.46842503\n",
      "Iteration 14825, loss = 1.46842120\n",
      "Iteration 14826, loss = 1.46841738\n",
      "Iteration 14827, loss = 1.46841356\n",
      "Iteration 14828, loss = 1.46840973\n",
      "Iteration 14829, loss = 1.46840591\n",
      "Iteration 14830, loss = 1.46840209\n",
      "Iteration 14831, loss = 1.46839827\n",
      "Iteration 14832, loss = 1.46839445\n",
      "Iteration 14833, loss = 1.46839062\n",
      "Iteration 14834, loss = 1.46838680\n",
      "Iteration 14835, loss = 1.46838299\n",
      "Iteration 14836, loss = 1.46837917\n",
      "Iteration 14837, loss = 1.46837535\n",
      "Iteration 14838, loss = 1.46837153\n",
      "Iteration 14839, loss = 1.46836771\n",
      "Iteration 14840, loss = 1.46836390\n",
      "Iteration 14841, loss = 1.46836008\n",
      "Iteration 14842, loss = 1.46835627\n",
      "Iteration 14843, loss = 1.46835245\n",
      "Iteration 14844, loss = 1.46834864\n",
      "Iteration 14845, loss = 1.46834482\n",
      "Iteration 14846, loss = 1.46834101\n",
      "Iteration 14847, loss = 1.46833720\n",
      "Iteration 14848, loss = 1.46833339\n",
      "Iteration 14849, loss = 1.46832958\n",
      "Iteration 14850, loss = 1.46832576\n",
      "Iteration 14851, loss = 1.46832195\n",
      "Iteration 14852, loss = 1.46831814\n",
      "Iteration 14853, loss = 1.46831434\n",
      "Iteration 14854, loss = 1.46831053\n",
      "Iteration 14855, loss = 1.46830672\n",
      "Iteration 14856, loss = 1.46830291\n",
      "Iteration 14857, loss = 1.46829910\n",
      "Iteration 14858, loss = 1.46829530\n",
      "Iteration 14859, loss = 1.46829149\n",
      "Iteration 14860, loss = 1.46828769\n",
      "Iteration 14861, loss = 1.46828388\n",
      "Iteration 14862, loss = 1.46828008\n",
      "Iteration 14863, loss = 1.46827628\n",
      "Iteration 14864, loss = 1.46827247\n",
      "Iteration 14865, loss = 1.46826867\n",
      "Iteration 14866, loss = 1.46826487\n",
      "Iteration 14867, loss = 1.46826107\n",
      "Iteration 14868, loss = 1.46825727\n",
      "Iteration 14869, loss = 1.46825347\n",
      "Iteration 14870, loss = 1.46824967\n",
      "Iteration 14871, loss = 1.46824587\n",
      "Iteration 14872, loss = 1.46824207\n",
      "Iteration 14873, loss = 1.46823827\n",
      "Iteration 14874, loss = 1.46823448\n",
      "Iteration 14875, loss = 1.46823068\n",
      "Iteration 14876, loss = 1.46822688\n",
      "Iteration 14877, loss = 1.46822309\n",
      "Iteration 14878, loss = 1.46821929\n",
      "Iteration 14879, loss = 1.46821550\n",
      "Iteration 14880, loss = 1.46821170\n",
      "Iteration 14881, loss = 1.46820791\n",
      "Iteration 14882, loss = 1.46820412\n",
      "Iteration 14883, loss = 1.46820033\n",
      "Iteration 14884, loss = 1.46819654\n",
      "Iteration 14885, loss = 1.46819274\n",
      "Iteration 14886, loss = 1.46818895\n",
      "Iteration 14887, loss = 1.46818516\n",
      "Iteration 14888, loss = 1.46818137\n",
      "Iteration 14889, loss = 1.46817759\n",
      "Iteration 14890, loss = 1.46817380\n",
      "Iteration 14891, loss = 1.46817001\n",
      "Iteration 14892, loss = 1.46816622\n",
      "Iteration 14893, loss = 1.46816244\n",
      "Iteration 14894, loss = 1.46815865\n",
      "Iteration 14895, loss = 1.46815487\n",
      "Iteration 14896, loss = 1.46815108\n",
      "Iteration 14897, loss = 1.46814730\n",
      "Iteration 14898, loss = 1.46814351\n",
      "Iteration 14899, loss = 1.46813973\n",
      "Iteration 14900, loss = 1.46813595\n",
      "Iteration 14901, loss = 1.46813217\n",
      "Iteration 14902, loss = 1.46812838\n",
      "Iteration 14903, loss = 1.46812460\n",
      "Iteration 14904, loss = 1.46812082\n",
      "Iteration 14905, loss = 1.46811704\n",
      "Iteration 14906, loss = 1.46811326\n",
      "Iteration 14907, loss = 1.46810949\n",
      "Iteration 14908, loss = 1.46810571\n",
      "Iteration 14909, loss = 1.46810193\n",
      "Iteration 14910, loss = 1.46809815\n",
      "Iteration 14911, loss = 1.46809438\n",
      "Iteration 14912, loss = 1.46809060\n",
      "Iteration 14913, loss = 1.46808683\n",
      "Iteration 14914, loss = 1.46808305\n",
      "Iteration 14915, loss = 1.46807928\n",
      "Iteration 14916, loss = 1.46807550\n",
      "Iteration 14917, loss = 1.46807173\n",
      "Iteration 14918, loss = 1.46806796\n",
      "Iteration 14919, loss = 1.46806419\n",
      "Iteration 14920, loss = 1.46806041\n",
      "Iteration 14921, loss = 1.46805664\n",
      "Iteration 14922, loss = 1.46805287\n",
      "Iteration 14923, loss = 1.46804910\n",
      "Iteration 14924, loss = 1.46804533\n",
      "Iteration 14925, loss = 1.46804157\n",
      "Iteration 14926, loss = 1.46803780\n",
      "Iteration 14927, loss = 1.46803403\n",
      "Iteration 14928, loss = 1.46803026\n",
      "Iteration 14929, loss = 1.46802650\n",
      "Iteration 14930, loss = 1.46802273\n",
      "Iteration 14931, loss = 1.46801897\n",
      "Iteration 14932, loss = 1.46801520\n",
      "Iteration 14933, loss = 1.46801144\n",
      "Iteration 14934, loss = 1.46800767\n",
      "Iteration 14935, loss = 1.46800391\n",
      "Iteration 14936, loss = 1.46800015\n",
      "Iteration 14937, loss = 1.46799639\n",
      "Iteration 14938, loss = 1.46799263\n",
      "Iteration 14939, loss = 1.46798887\n",
      "Iteration 14940, loss = 1.46798511\n",
      "Iteration 14941, loss = 1.46798135\n",
      "Iteration 14942, loss = 1.46797759\n",
      "Iteration 14943, loss = 1.46797383\n",
      "Iteration 14944, loss = 1.46797007\n",
      "Iteration 14945, loss = 1.46796631\n",
      "Iteration 14946, loss = 1.46796256\n",
      "Iteration 14947, loss = 1.46795880\n",
      "Iteration 14948, loss = 1.46795504\n",
      "Iteration 14949, loss = 1.46795129\n",
      "Iteration 14950, loss = 1.46794753\n",
      "Iteration 14951, loss = 1.46794378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14952, loss = 1.46794003\n",
      "Iteration 14953, loss = 1.46793627\n",
      "Iteration 14954, loss = 1.46793252\n",
      "Iteration 14955, loss = 1.46792877\n",
      "Iteration 14956, loss = 1.46792502\n",
      "Iteration 14957, loss = 1.46792127\n",
      "Iteration 14958, loss = 1.46791752\n",
      "Iteration 14959, loss = 1.46791377\n",
      "Iteration 14960, loss = 1.46791002\n",
      "Iteration 14961, loss = 1.46790627\n",
      "Iteration 14962, loss = 1.46790252\n",
      "Iteration 14963, loss = 1.46789877\n",
      "Iteration 14964, loss = 1.46789503\n",
      "Iteration 14965, loss = 1.46789128\n",
      "Iteration 14966, loss = 1.46788754\n",
      "Iteration 14967, loss = 1.46788379\n",
      "Iteration 14968, loss = 1.46788005\n",
      "Iteration 14969, loss = 1.46787630\n",
      "Iteration 14970, loss = 1.46787256\n",
      "Iteration 14971, loss = 1.46786882\n",
      "Iteration 14972, loss = 1.46786507\n",
      "Iteration 14973, loss = 1.46786133\n",
      "Iteration 14974, loss = 1.46785759\n",
      "Iteration 14975, loss = 1.46785385\n",
      "Iteration 14976, loss = 1.46785011\n",
      "Iteration 14977, loss = 1.46784637\n",
      "Iteration 14978, loss = 1.46784263\n",
      "Iteration 14979, loss = 1.46783889\n",
      "Iteration 14980, loss = 1.46783515\n",
      "Iteration 14981, loss = 1.46783142\n",
      "Iteration 14982, loss = 1.46782768\n",
      "Iteration 14983, loss = 1.46782394\n",
      "Iteration 14984, loss = 1.46782021\n",
      "Iteration 14985, loss = 1.46781647\n",
      "Iteration 14986, loss = 1.46781274\n",
      "Iteration 14987, loss = 1.46780900\n",
      "Iteration 14988, loss = 1.46780527\n",
      "Iteration 14989, loss = 1.46780154\n",
      "Iteration 14990, loss = 1.46779780\n",
      "Iteration 14991, loss = 1.46779407\n",
      "Iteration 14992, loss = 1.46779034\n",
      "Iteration 14993, loss = 1.46778661\n",
      "Iteration 14994, loss = 1.46778288\n",
      "Iteration 14995, loss = 1.46777915\n",
      "Iteration 14996, loss = 1.46777542\n",
      "Iteration 14997, loss = 1.46777169\n",
      "Iteration 14998, loss = 1.46776797\n",
      "Iteration 14999, loss = 1.46776424\n",
      "Iteration 15000, loss = 1.46776051\n",
      "Iteration 15001, loss = 1.46775678\n",
      "Iteration 15002, loss = 1.46775306\n",
      "Iteration 15003, loss = 1.46774933\n",
      "Iteration 15004, loss = 1.46774561\n",
      "Iteration 15005, loss = 1.46774188\n",
      "Iteration 15006, loss = 1.46773816\n",
      "Iteration 15007, loss = 1.46773444\n",
      "Iteration 15008, loss = 1.46773071\n",
      "Iteration 15009, loss = 1.46772699\n",
      "Iteration 15010, loss = 1.46772327\n",
      "Iteration 15011, loss = 1.46771955\n",
      "Iteration 15012, loss = 1.46771583\n",
      "Iteration 15013, loss = 1.46771211\n",
      "Iteration 15014, loss = 1.46770839\n",
      "Iteration 15015, loss = 1.46770467\n",
      "Iteration 15016, loss = 1.46770095\n",
      "Iteration 15017, loss = 1.46769724\n",
      "Iteration 15018, loss = 1.46769352\n",
      "Iteration 15019, loss = 1.46768980\n",
      "Iteration 15020, loss = 1.46768609\n",
      "Iteration 15021, loss = 1.46768237\n",
      "Iteration 15022, loss = 1.46767866\n",
      "Iteration 15023, loss = 1.46767494\n",
      "Iteration 15024, loss = 1.46767123\n",
      "Iteration 15025, loss = 1.46766752\n",
      "Iteration 15026, loss = 1.46766380\n",
      "Iteration 15027, loss = 1.46766009\n",
      "Iteration 15028, loss = 1.46765638\n",
      "Iteration 15029, loss = 1.46765267\n",
      "Iteration 15030, loss = 1.46764896\n",
      "Iteration 15031, loss = 1.46764525\n",
      "Iteration 15032, loss = 1.46764154\n",
      "Iteration 15033, loss = 1.46763783\n",
      "Iteration 15034, loss = 1.46763412\n",
      "Iteration 15035, loss = 1.46763041\n",
      "Iteration 15036, loss = 1.46762671\n",
      "Iteration 15037, loss = 1.46762300\n",
      "Iteration 15038, loss = 1.46761929\n",
      "Iteration 15039, loss = 1.46761559\n",
      "Iteration 15040, loss = 1.46761188\n",
      "Iteration 15041, loss = 1.46760818\n",
      "Iteration 15042, loss = 1.46760447\n",
      "Iteration 15043, loss = 1.46760077\n",
      "Iteration 15044, loss = 1.46759707\n",
      "Iteration 15045, loss = 1.46759337\n",
      "Iteration 15046, loss = 1.46758966\n",
      "Iteration 15047, loss = 1.46758596\n",
      "Iteration 15048, loss = 1.46758226\n",
      "Iteration 15049, loss = 1.46757856\n",
      "Iteration 15050, loss = 1.46757486\n",
      "Iteration 15051, loss = 1.46757116\n",
      "Iteration 15052, loss = 1.46756747\n",
      "Iteration 15053, loss = 1.46756377\n",
      "Iteration 15054, loss = 1.46756007\n",
      "Iteration 15055, loss = 1.46755637\n",
      "Iteration 15056, loss = 1.46755268\n",
      "Iteration 15057, loss = 1.46754898\n",
      "Iteration 15058, loss = 1.46754529\n",
      "Iteration 15059, loss = 1.46754159\n",
      "Iteration 15060, loss = 1.46753790\n",
      "Iteration 15061, loss = 1.46753420\n",
      "Iteration 15062, loss = 1.46753051\n",
      "Iteration 15063, loss = 1.46752682\n",
      "Iteration 15064, loss = 1.46752313\n",
      "Iteration 15065, loss = 1.46751943\n",
      "Iteration 15066, loss = 1.46751574\n",
      "Iteration 15067, loss = 1.46751205\n",
      "Iteration 15068, loss = 1.46750836\n",
      "Iteration 15069, loss = 1.46750467\n",
      "Iteration 15070, loss = 1.46750099\n",
      "Iteration 15071, loss = 1.46749730\n",
      "Iteration 15072, loss = 1.46749361\n",
      "Iteration 15073, loss = 1.46748992\n",
      "Iteration 15074, loss = 1.46748624\n",
      "Iteration 15075, loss = 1.46748255\n",
      "Iteration 15076, loss = 1.46747886\n",
      "Iteration 15077, loss = 1.46747518\n",
      "Iteration 15078, loss = 1.46747150\n",
      "Iteration 15079, loss = 1.46746781\n",
      "Iteration 15080, loss = 1.46746413\n",
      "Iteration 15081, loss = 1.46746045\n",
      "Iteration 15082, loss = 1.46745676\n",
      "Iteration 15083, loss = 1.46745308\n",
      "Iteration 15084, loss = 1.46744940\n",
      "Iteration 15085, loss = 1.46744572\n",
      "Iteration 15086, loss = 1.46744204\n",
      "Iteration 15087, loss = 1.46743836\n",
      "Iteration 15088, loss = 1.46743468\n",
      "Iteration 15089, loss = 1.46743100\n",
      "Iteration 15090, loss = 1.46742733\n",
      "Iteration 15091, loss = 1.46742365\n",
      "Iteration 15092, loss = 1.46741997\n",
      "Iteration 15093, loss = 1.46741630\n",
      "Iteration 15094, loss = 1.46741262\n",
      "Iteration 15095, loss = 1.46740894\n",
      "Iteration 15096, loss = 1.46740527\n",
      "Iteration 15097, loss = 1.46740160\n",
      "Iteration 15098, loss = 1.46739792\n",
      "Iteration 15099, loss = 1.46739425\n",
      "Iteration 15100, loss = 1.46739058\n",
      "Iteration 15101, loss = 1.46738690\n",
      "Iteration 15102, loss = 1.46738323\n",
      "Iteration 15103, loss = 1.46737956\n",
      "Iteration 15104, loss = 1.46737589\n",
      "Iteration 15105, loss = 1.46737222\n",
      "Iteration 15106, loss = 1.46736855\n",
      "Iteration 15107, loss = 1.46736488\n",
      "Iteration 15108, loss = 1.46736122\n",
      "Iteration 15109, loss = 1.46735755\n",
      "Iteration 15110, loss = 1.46735388\n",
      "Iteration 15111, loss = 1.46735021\n",
      "Iteration 15112, loss = 1.46734655\n",
      "Iteration 15113, loss = 1.46734288\n",
      "Iteration 15114, loss = 1.46733922\n",
      "Iteration 15115, loss = 1.46733555\n",
      "Iteration 15116, loss = 1.46733189\n",
      "Iteration 15117, loss = 1.46732823\n",
      "Iteration 15118, loss = 1.46732456\n",
      "Iteration 15119, loss = 1.46732090\n",
      "Iteration 15120, loss = 1.46731724\n",
      "Iteration 15121, loss = 1.46731358\n",
      "Iteration 15122, loss = 1.46730992\n",
      "Iteration 15123, loss = 1.46730626\n",
      "Iteration 15124, loss = 1.46730260\n",
      "Iteration 15125, loss = 1.46729894\n",
      "Iteration 15126, loss = 1.46729528\n",
      "Iteration 15127, loss = 1.46729162\n",
      "Iteration 15128, loss = 1.46728797\n",
      "Iteration 15129, loss = 1.46728431\n",
      "Iteration 15130, loss = 1.46728065\n",
      "Iteration 15131, loss = 1.46727700\n",
      "Iteration 15132, loss = 1.46727334\n",
      "Iteration 15133, loss = 1.46726969\n",
      "Iteration 15134, loss = 1.46726603\n",
      "Iteration 15135, loss = 1.46726238\n",
      "Iteration 15136, loss = 1.46725873\n",
      "Iteration 15137, loss = 1.46725507\n",
      "Iteration 15138, loss = 1.46725142\n",
      "Iteration 15139, loss = 1.46724777\n",
      "Iteration 15140, loss = 1.46724412\n",
      "Iteration 15141, loss = 1.46724047\n",
      "Iteration 15142, loss = 1.46723682\n",
      "Iteration 15143, loss = 1.46723317\n",
      "Iteration 15144, loss = 1.46722952\n",
      "Iteration 15145, loss = 1.46722587\n",
      "Iteration 15146, loss = 1.46722222\n",
      "Iteration 15147, loss = 1.46721858\n",
      "Iteration 15148, loss = 1.46721493\n",
      "Iteration 15149, loss = 1.46721128\n",
      "Iteration 15150, loss = 1.46720764\n",
      "Iteration 15151, loss = 1.46720399\n",
      "Iteration 15152, loss = 1.46720035\n",
      "Iteration 15153, loss = 1.46719670\n",
      "Iteration 15154, loss = 1.46719306\n",
      "Iteration 15155, loss = 1.46718942\n",
      "Iteration 15156, loss = 1.46718578\n",
      "Iteration 15157, loss = 1.46718213\n",
      "Iteration 15158, loss = 1.46717849\n",
      "Iteration 15159, loss = 1.46717485\n",
      "Iteration 15160, loss = 1.46717121\n",
      "Iteration 15161, loss = 1.46716757\n",
      "Iteration 15162, loss = 1.46716393\n",
      "Iteration 15163, loss = 1.46716029\n",
      "Iteration 15164, loss = 1.46715665\n",
      "Iteration 15165, loss = 1.46715302\n",
      "Iteration 15166, loss = 1.46714938\n",
      "Iteration 15167, loss = 1.46714574\n",
      "Iteration 15168, loss = 1.46714211\n",
      "Iteration 15169, loss = 1.46713847\n",
      "Iteration 15170, loss = 1.46713484\n",
      "Iteration 15171, loss = 1.46713120\n",
      "Iteration 15172, loss = 1.46712757\n",
      "Iteration 15173, loss = 1.46712394\n",
      "Iteration 15174, loss = 1.46712030\n",
      "Iteration 15175, loss = 1.46711667\n",
      "Iteration 15176, loss = 1.46711304\n",
      "Iteration 15177, loss = 1.46710941\n",
      "Iteration 15178, loss = 1.46710578\n",
      "Iteration 15179, loss = 1.46710215\n",
      "Iteration 15180, loss = 1.46709852\n",
      "Iteration 15181, loss = 1.46709489\n",
      "Iteration 15182, loss = 1.46709126\n",
      "Iteration 15183, loss = 1.46708763\n",
      "Iteration 15184, loss = 1.46708400\n",
      "Iteration 15185, loss = 1.46708038\n",
      "Iteration 15186, loss = 1.46707675\n",
      "Iteration 15187, loss = 1.46707312\n",
      "Iteration 15188, loss = 1.46706950\n",
      "Iteration 15189, loss = 1.46706587\n",
      "Iteration 15190, loss = 1.46706225\n",
      "Iteration 15191, loss = 1.46705862\n",
      "Iteration 15192, loss = 1.46705500\n",
      "Iteration 15193, loss = 1.46705138\n",
      "Iteration 15194, loss = 1.46704776\n",
      "Iteration 15195, loss = 1.46704413\n",
      "Iteration 15196, loss = 1.46704051\n",
      "Iteration 15197, loss = 1.46703689\n",
      "Iteration 15198, loss = 1.46703327\n",
      "Iteration 15199, loss = 1.46702965\n",
      "Iteration 15200, loss = 1.46702603\n",
      "Iteration 15201, loss = 1.46702242\n",
      "Iteration 15202, loss = 1.46701880\n",
      "Iteration 15203, loss = 1.46701518\n",
      "Iteration 15204, loss = 1.46701156\n",
      "Iteration 15205, loss = 1.46700795\n",
      "Iteration 15206, loss = 1.46700433\n",
      "Iteration 15207, loss = 1.46700071\n",
      "Iteration 15208, loss = 1.46699710\n",
      "Iteration 15209, loss = 1.46699349\n",
      "Iteration 15210, loss = 1.46698987\n",
      "Iteration 15211, loss = 1.46698626\n",
      "Iteration 15212, loss = 1.46698265\n",
      "Iteration 15213, loss = 1.46697903\n",
      "Iteration 15214, loss = 1.46697542\n",
      "Iteration 15215, loss = 1.46697181\n",
      "Iteration 15216, loss = 1.46696820\n",
      "Iteration 15217, loss = 1.46696459\n",
      "Iteration 15218, loss = 1.46696098\n",
      "Iteration 15219, loss = 1.46695737\n",
      "Iteration 15220, loss = 1.46695376\n",
      "Iteration 15221, loss = 1.46695015\n",
      "Iteration 15222, loss = 1.46694655\n",
      "Iteration 15223, loss = 1.46694294\n",
      "Iteration 15224, loss = 1.46693933\n",
      "Iteration 15225, loss = 1.46693573\n",
      "Iteration 15226, loss = 1.46693212\n",
      "Iteration 15227, loss = 1.46692852\n",
      "Iteration 15228, loss = 1.46692491\n",
      "Iteration 15229, loss = 1.46692131\n",
      "Iteration 15230, loss = 1.46691771\n",
      "Iteration 15231, loss = 1.46691410\n",
      "Iteration 15232, loss = 1.46691050\n",
      "Iteration 15233, loss = 1.46690690\n",
      "Iteration 15234, loss = 1.46690330\n",
      "Iteration 15235, loss = 1.46689970\n",
      "Iteration 15236, loss = 1.46689610\n",
      "Iteration 15237, loss = 1.46689250\n",
      "Iteration 15238, loss = 1.46688890\n",
      "Iteration 15239, loss = 1.46688530\n",
      "Iteration 15240, loss = 1.46688170\n",
      "Iteration 15241, loss = 1.46687810\n",
      "Iteration 15242, loss = 1.46687451\n",
      "Iteration 15243, loss = 1.46687091\n",
      "Iteration 15244, loss = 1.46686731\n",
      "Iteration 15245, loss = 1.46686372\n",
      "Iteration 15246, loss = 1.46686012\n",
      "Iteration 15247, loss = 1.46685653\n",
      "Iteration 15248, loss = 1.46685294\n",
      "Iteration 15249, loss = 1.46684934\n",
      "Iteration 15250, loss = 1.46684575\n",
      "Iteration 15251, loss = 1.46684216\n",
      "Iteration 15252, loss = 1.46683857\n",
      "Iteration 15253, loss = 1.46683497\n",
      "Iteration 15254, loss = 1.46683138\n",
      "Iteration 15255, loss = 1.46682779\n",
      "Iteration 15256, loss = 1.46682420\n",
      "Iteration 15257, loss = 1.46682061\n",
      "Iteration 15258, loss = 1.46681703\n",
      "Iteration 15259, loss = 1.46681344\n",
      "Iteration 15260, loss = 1.46680985\n",
      "Iteration 15261, loss = 1.46680626\n",
      "Iteration 15262, loss = 1.46680268\n",
      "Iteration 15263, loss = 1.46679909\n",
      "Iteration 15264, loss = 1.46679551\n",
      "Iteration 15265, loss = 1.46679192\n",
      "Iteration 15266, loss = 1.46678834\n",
      "Iteration 15267, loss = 1.46678475\n",
      "Iteration 15268, loss = 1.46678117\n",
      "Iteration 15269, loss = 1.46677759\n",
      "Iteration 15270, loss = 1.46677400\n",
      "Iteration 15271, loss = 1.46677042\n",
      "Iteration 15272, loss = 1.46676684\n",
      "Iteration 15273, loss = 1.46676326\n",
      "Iteration 15274, loss = 1.46675968\n",
      "Iteration 15275, loss = 1.46675610\n",
      "Iteration 15276, loss = 1.46675252\n",
      "Iteration 15277, loss = 1.46674894\n",
      "Iteration 15278, loss = 1.46674536\n",
      "Iteration 15279, loss = 1.46674179\n",
      "Iteration 15280, loss = 1.46673821\n",
      "Iteration 15281, loss = 1.46673463\n",
      "Iteration 15282, loss = 1.46673106\n",
      "Iteration 15283, loss = 1.46672748\n",
      "Iteration 15284, loss = 1.46672390\n",
      "Iteration 15285, loss = 1.46672033\n",
      "Iteration 15286, loss = 1.46671676\n",
      "Iteration 15287, loss = 1.46671318\n",
      "Iteration 15288, loss = 1.46670961\n",
      "Iteration 15289, loss = 1.46670604\n",
      "Iteration 15290, loss = 1.46670246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15291, loss = 1.46669889\n",
      "Iteration 15292, loss = 1.46669532\n",
      "Iteration 15293, loss = 1.46669175\n",
      "Iteration 15294, loss = 1.46668818\n",
      "Iteration 15295, loss = 1.46668461\n",
      "Iteration 15296, loss = 1.46668104\n",
      "Iteration 15297, loss = 1.46667748\n",
      "Iteration 15298, loss = 1.46667391\n",
      "Iteration 15299, loss = 1.46667034\n",
      "Iteration 15300, loss = 1.46666677\n",
      "Iteration 15301, loss = 1.46666321\n",
      "Iteration 15302, loss = 1.46665964\n",
      "Iteration 15303, loss = 1.46665608\n",
      "Iteration 15304, loss = 1.46665251\n",
      "Iteration 15305, loss = 1.46664895\n",
      "Iteration 15306, loss = 1.46664538\n",
      "Iteration 15307, loss = 1.46664182\n",
      "Iteration 15308, loss = 1.46663826\n",
      "Iteration 15309, loss = 1.46663470\n",
      "Iteration 15310, loss = 1.46663113\n",
      "Iteration 15311, loss = 1.46662757\n",
      "Iteration 15312, loss = 1.46662401\n",
      "Iteration 15313, loss = 1.46662045\n",
      "Iteration 15314, loss = 1.46661689\n",
      "Iteration 15315, loss = 1.46661333\n",
      "Iteration 15316, loss = 1.46660977\n",
      "Iteration 15317, loss = 1.46660622\n",
      "Iteration 15318, loss = 1.46660266\n",
      "Iteration 15319, loss = 1.46659910\n",
      "Iteration 15320, loss = 1.46659555\n",
      "Iteration 15321, loss = 1.46659199\n",
      "Iteration 15322, loss = 1.46658843\n",
      "Iteration 15323, loss = 1.46658488\n",
      "Iteration 15324, loss = 1.46658133\n",
      "Iteration 15325, loss = 1.46657777\n",
      "Iteration 15326, loss = 1.46657422\n",
      "Iteration 15327, loss = 1.46657067\n",
      "Iteration 15328, loss = 1.46656711\n",
      "Iteration 15329, loss = 1.46656356\n",
      "Iteration 15330, loss = 1.46656001\n",
      "Iteration 15331, loss = 1.46655646\n",
      "Iteration 15332, loss = 1.46655291\n",
      "Iteration 15333, loss = 1.46654936\n",
      "Iteration 15334, loss = 1.46654581\n",
      "Iteration 15335, loss = 1.46654226\n",
      "Iteration 15336, loss = 1.46653871\n",
      "Iteration 15337, loss = 1.46653516\n",
      "Iteration 15338, loss = 1.46653162\n",
      "Iteration 15339, loss = 1.46652807\n",
      "Iteration 15340, loss = 1.46652452\n",
      "Iteration 15341, loss = 1.46652098\n",
      "Iteration 15342, loss = 1.46651743\n",
      "Iteration 15343, loss = 1.46651389\n",
      "Iteration 15344, loss = 1.46651035\n",
      "Iteration 15345, loss = 1.46650680\n",
      "Iteration 15346, loss = 1.46650326\n",
      "Iteration 15347, loss = 1.46649972\n",
      "Iteration 15348, loss = 1.46649617\n",
      "Iteration 15349, loss = 1.46649263\n",
      "Iteration 15350, loss = 1.46648909\n",
      "Iteration 15351, loss = 1.46648555\n",
      "Iteration 15352, loss = 1.46648201\n",
      "Iteration 15353, loss = 1.46647847\n",
      "Iteration 15354, loss = 1.46647493\n",
      "Iteration 15355, loss = 1.46647139\n",
      "Iteration 15356, loss = 1.46646786\n",
      "Iteration 15357, loss = 1.46646432\n",
      "Iteration 15358, loss = 1.46646078\n",
      "Iteration 15359, loss = 1.46645724\n",
      "Iteration 15360, loss = 1.46645371\n",
      "Iteration 15361, loss = 1.46645017\n",
      "Iteration 15362, loss = 1.46644664\n",
      "Iteration 15363, loss = 1.46644310\n",
      "Iteration 15364, loss = 1.46643957\n",
      "Iteration 15365, loss = 1.46643604\n",
      "Iteration 15366, loss = 1.46643250\n",
      "Iteration 15367, loss = 1.46642897\n",
      "Iteration 15368, loss = 1.46642544\n",
      "Iteration 15369, loss = 1.46642191\n",
      "Iteration 15370, loss = 1.46641838\n",
      "Iteration 15371, loss = 1.46641485\n",
      "Iteration 15372, loss = 1.46641132\n",
      "Iteration 15373, loss = 1.46640779\n",
      "Iteration 15374, loss = 1.46640426\n",
      "Iteration 15375, loss = 1.46640073\n",
      "Iteration 15376, loss = 1.46639720\n",
      "Iteration 15377, loss = 1.46639368\n",
      "Iteration 15378, loss = 1.46639015\n",
      "Iteration 15379, loss = 1.46638662\n",
      "Iteration 15380, loss = 1.46638310\n",
      "Iteration 15381, loss = 1.46637957\n",
      "Iteration 15382, loss = 1.46637605\n",
      "Iteration 15383, loss = 1.46637252\n",
      "Iteration 15384, loss = 1.46636900\n",
      "Iteration 15385, loss = 1.46636548\n",
      "Iteration 15386, loss = 1.46636195\n",
      "Iteration 15387, loss = 1.46635843\n",
      "Iteration 15388, loss = 1.46635491\n",
      "Iteration 15389, loss = 1.46635139\n",
      "Iteration 15390, loss = 1.46634787\n",
      "Iteration 15391, loss = 1.46634435\n",
      "Iteration 15392, loss = 1.46634083\n",
      "Iteration 15393, loss = 1.46633731\n",
      "Iteration 15394, loss = 1.46633379\n",
      "Iteration 15395, loss = 1.46633027\n",
      "Iteration 15396, loss = 1.46632676\n",
      "Iteration 15397, loss = 1.46632324\n",
      "Iteration 15398, loss = 1.46631972\n",
      "Iteration 15399, loss = 1.46631621\n",
      "Iteration 15400, loss = 1.46631269\n",
      "Iteration 15401, loss = 1.46630918\n",
      "Iteration 15402, loss = 1.46630566\n",
      "Iteration 15403, loss = 1.46630215\n",
      "Iteration 15404, loss = 1.46629863\n",
      "Iteration 15405, loss = 1.46629512\n",
      "Iteration 15406, loss = 1.46629161\n",
      "Iteration 15407, loss = 1.46628810\n",
      "Iteration 15408, loss = 1.46628458\n",
      "Iteration 15409, loss = 1.46628107\n",
      "Iteration 15410, loss = 1.46627756\n",
      "Iteration 15411, loss = 1.46627405\n",
      "Iteration 15412, loss = 1.46627054\n",
      "Iteration 15413, loss = 1.46626703\n",
      "Iteration 15414, loss = 1.46626353\n",
      "Iteration 15415, loss = 1.46626002\n",
      "Iteration 15416, loss = 1.46625651\n",
      "Iteration 15417, loss = 1.46625300\n",
      "Iteration 15418, loss = 1.46624950\n",
      "Iteration 15419, loss = 1.46624599\n",
      "Iteration 15420, loss = 1.46624249\n",
      "Iteration 15421, loss = 1.46623898\n",
      "Iteration 15422, loss = 1.46623548\n",
      "Iteration 15423, loss = 1.46623197\n",
      "Iteration 15424, loss = 1.46622847\n",
      "Iteration 15425, loss = 1.46622497\n",
      "Iteration 15426, loss = 1.46622146\n",
      "Iteration 15427, loss = 1.46621796\n",
      "Iteration 15428, loss = 1.46621446\n",
      "Iteration 15429, loss = 1.46621096\n",
      "Iteration 15430, loss = 1.46620746\n",
      "Iteration 15431, loss = 1.46620396\n",
      "Iteration 15432, loss = 1.46620046\n",
      "Iteration 15433, loss = 1.46619696\n",
      "Iteration 15434, loss = 1.46619346\n",
      "Iteration 15435, loss = 1.46618997\n",
      "Iteration 15436, loss = 1.46618647\n",
      "Iteration 15437, loss = 1.46618297\n",
      "Iteration 15438, loss = 1.46617947\n",
      "Iteration 15439, loss = 1.46617598\n",
      "Iteration 15440, loss = 1.46617248\n",
      "Iteration 15441, loss = 1.46616899\n",
      "Iteration 15442, loss = 1.46616549\n",
      "Iteration 15443, loss = 1.46616200\n",
      "Iteration 15444, loss = 1.46615851\n",
      "Iteration 15445, loss = 1.46615501\n",
      "Iteration 15446, loss = 1.46615152\n",
      "Iteration 15447, loss = 1.46614803\n",
      "Iteration 15448, loss = 1.46614454\n",
      "Iteration 15449, loss = 1.46614105\n",
      "Iteration 15450, loss = 1.46613756\n",
      "Iteration 15451, loss = 1.46613407\n",
      "Iteration 15452, loss = 1.46613058\n",
      "Iteration 15453, loss = 1.46612709\n",
      "Iteration 15454, loss = 1.46612360\n",
      "Iteration 15455, loss = 1.46612011\n",
      "Iteration 15456, loss = 1.46611663\n",
      "Iteration 15457, loss = 1.46611314\n",
      "Iteration 15458, loss = 1.46610965\n",
      "Iteration 15459, loss = 1.46610617\n",
      "Iteration 15460, loss = 1.46610268\n",
      "Iteration 15461, loss = 1.46609920\n",
      "Iteration 15462, loss = 1.46609571\n",
      "Iteration 15463, loss = 1.46609223\n",
      "Iteration 15464, loss = 1.46608875\n",
      "Iteration 15465, loss = 1.46608526\n",
      "Iteration 15466, loss = 1.46608178\n",
      "Iteration 15467, loss = 1.46607830\n",
      "Iteration 15468, loss = 1.46607482\n",
      "Iteration 15469, loss = 1.46607134\n",
      "Iteration 15470, loss = 1.46606786\n",
      "Iteration 15471, loss = 1.46606438\n",
      "Iteration 15472, loss = 1.46606090\n",
      "Iteration 15473, loss = 1.46605742\n",
      "Iteration 15474, loss = 1.46605394\n",
      "Iteration 15475, loss = 1.46605046\n",
      "Iteration 15476, loss = 1.46604699\n",
      "Iteration 15477, loss = 1.46604351\n",
      "Iteration 15478, loss = 1.46604003\n",
      "Iteration 15479, loss = 1.46603656\n",
      "Iteration 15480, loss = 1.46603308\n",
      "Iteration 15481, loss = 1.46602961\n",
      "Iteration 15482, loss = 1.46602613\n",
      "Iteration 15483, loss = 1.46602266\n",
      "Iteration 15484, loss = 1.46601919\n",
      "Iteration 15485, loss = 1.46601571\n",
      "Iteration 15486, loss = 1.46601224\n",
      "Iteration 15487, loss = 1.46600877\n",
      "Iteration 15488, loss = 1.46600530\n",
      "Iteration 15489, loss = 1.46600183\n",
      "Iteration 15490, loss = 1.46599836\n",
      "Iteration 15491, loss = 1.46599489\n",
      "Iteration 15492, loss = 1.46599142\n",
      "Iteration 15493, loss = 1.46598795\n",
      "Iteration 15494, loss = 1.46598448\n",
      "Iteration 15495, loss = 1.46598101\n",
      "Iteration 15496, loss = 1.46597755\n",
      "Iteration 15497, loss = 1.46597408\n",
      "Iteration 15498, loss = 1.46597061\n",
      "Iteration 15499, loss = 1.46596715\n",
      "Iteration 15500, loss = 1.46596368\n",
      "Iteration 15501, loss = 1.46596022\n",
      "Iteration 15502, loss = 1.46595675\n",
      "Iteration 15503, loss = 1.46595329\n",
      "Iteration 15504, loss = 1.46594983\n",
      "Iteration 15505, loss = 1.46594636\n",
      "Iteration 15506, loss = 1.46594290\n",
      "Iteration 15507, loss = 1.46593944\n",
      "Iteration 15508, loss = 1.46593598\n",
      "Iteration 15509, loss = 1.46593252\n",
      "Iteration 15510, loss = 1.46592906\n",
      "Iteration 15511, loss = 1.46592560\n",
      "Iteration 15512, loss = 1.46592214\n",
      "Iteration 15513, loss = 1.46591868\n",
      "Iteration 15514, loss = 1.46591522\n",
      "Iteration 15515, loss = 1.46591176\n",
      "Iteration 15516, loss = 1.46590830\n",
      "Iteration 15517, loss = 1.46590485\n",
      "Iteration 15518, loss = 1.46590139\n",
      "Iteration 15519, loss = 1.46589794\n",
      "Iteration 15520, loss = 1.46589448\n",
      "Iteration 15521, loss = 1.46589102\n",
      "Iteration 15522, loss = 1.46588757\n",
      "Iteration 15523, loss = 1.46588412\n",
      "Iteration 15524, loss = 1.46588066\n",
      "Iteration 15525, loss = 1.46587721\n",
      "Iteration 15526, loss = 1.46587376\n",
      "Iteration 15527, loss = 1.46587031\n",
      "Iteration 15528, loss = 1.46586685\n",
      "Iteration 15529, loss = 1.46586340\n",
      "Iteration 15530, loss = 1.46585995\n",
      "Iteration 15531, loss = 1.46585650\n",
      "Iteration 15532, loss = 1.46585305\n",
      "Iteration 15533, loss = 1.46584961\n",
      "Iteration 15534, loss = 1.46584616\n",
      "Iteration 15535, loss = 1.46584271\n",
      "Iteration 15536, loss = 1.46583926\n",
      "Iteration 15537, loss = 1.46583581\n",
      "Iteration 15538, loss = 1.46583237\n",
      "Iteration 15539, loss = 1.46582892\n",
      "Iteration 15540, loss = 1.46582548\n",
      "Iteration 15541, loss = 1.46582203\n",
      "Iteration 15542, loss = 1.46581859\n",
      "Iteration 15543, loss = 1.46581514\n",
      "Iteration 15544, loss = 1.46581170\n",
      "Iteration 15545, loss = 1.46580826\n",
      "Iteration 15546, loss = 1.46580481\n",
      "Iteration 15547, loss = 1.46580137\n",
      "Iteration 15548, loss = 1.46579793\n",
      "Iteration 15549, loss = 1.46579449\n",
      "Iteration 15550, loss = 1.46579105\n",
      "Iteration 15551, loss = 1.46578761\n",
      "Iteration 15552, loss = 1.46578417\n",
      "Iteration 15553, loss = 1.46578073\n",
      "Iteration 15554, loss = 1.46577729\n",
      "Iteration 15555, loss = 1.46577385\n",
      "Iteration 15556, loss = 1.46577042\n",
      "Iteration 15557, loss = 1.46576698\n",
      "Iteration 15558, loss = 1.46576354\n",
      "Iteration 15559, loss = 1.46576011\n",
      "Iteration 15560, loss = 1.46575667\n",
      "Iteration 15561, loss = 1.46575324\n",
      "Iteration 15562, loss = 1.46574980\n",
      "Iteration 15563, loss = 1.46574637\n",
      "Iteration 15564, loss = 1.46574293\n",
      "Iteration 15565, loss = 1.46573950\n",
      "Iteration 15566, loss = 1.46573607\n",
      "Iteration 15567, loss = 1.46573263\n",
      "Iteration 15568, loss = 1.46572920\n",
      "Iteration 15569, loss = 1.46572577\n",
      "Iteration 15570, loss = 1.46572234\n",
      "Iteration 15571, loss = 1.46571891\n",
      "Iteration 15572, loss = 1.46571548\n",
      "Iteration 15573, loss = 1.46571205\n",
      "Iteration 15574, loss = 1.46570862\n",
      "Iteration 15575, loss = 1.46570519\n",
      "Iteration 15576, loss = 1.46570177\n",
      "Iteration 15577, loss = 1.46569834\n",
      "Iteration 15578, loss = 1.46569491\n",
      "Iteration 15579, loss = 1.46569149\n",
      "Iteration 15580, loss = 1.46568806\n",
      "Iteration 15581, loss = 1.46568464\n",
      "Iteration 15582, loss = 1.46568121\n",
      "Iteration 15583, loss = 1.46567779\n",
      "Iteration 15584, loss = 1.46567436\n",
      "Iteration 15585, loss = 1.46567094\n",
      "Iteration 15586, loss = 1.46566752\n",
      "Iteration 15587, loss = 1.46566409\n",
      "Iteration 15588, loss = 1.46566067\n",
      "Iteration 15589, loss = 1.46565725\n",
      "Iteration 15590, loss = 1.46565383\n",
      "Iteration 15591, loss = 1.46565041\n",
      "Iteration 15592, loss = 1.46564699\n",
      "Iteration 15593, loss = 1.46564357\n",
      "Iteration 15594, loss = 1.46564015\n",
      "Iteration 15595, loss = 1.46563673\n",
      "Iteration 15596, loss = 1.46563331\n",
      "Iteration 15597, loss = 1.46562990\n",
      "Iteration 15598, loss = 1.46562648\n",
      "Iteration 15599, loss = 1.46562306\n",
      "Iteration 15600, loss = 1.46561965\n",
      "Iteration 15601, loss = 1.46561623\n",
      "Iteration 15602, loss = 1.46561282\n",
      "Iteration 15603, loss = 1.46560940\n",
      "Iteration 15604, loss = 1.46560599\n",
      "Iteration 15605, loss = 1.46560257\n",
      "Iteration 15606, loss = 1.46559916\n",
      "Iteration 15607, loss = 1.46559575\n",
      "Iteration 15608, loss = 1.46559234\n",
      "Iteration 15609, loss = 1.46558892\n",
      "Iteration 15610, loss = 1.46558551\n",
      "Iteration 15611, loss = 1.46558210\n",
      "Iteration 15612, loss = 1.46557869\n",
      "Iteration 15613, loss = 1.46557528\n",
      "Iteration 15614, loss = 1.46557187\n",
      "Iteration 15615, loss = 1.46556846\n",
      "Iteration 15616, loss = 1.46556506\n",
      "Iteration 15617, loss = 1.46556165\n",
      "Iteration 15618, loss = 1.46555824\n",
      "Iteration 15619, loss = 1.46555483\n",
      "Iteration 15620, loss = 1.46555143\n",
      "Iteration 15621, loss = 1.46554802\n",
      "Iteration 15622, loss = 1.46554462\n",
      "Iteration 15623, loss = 1.46554121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15624, loss = 1.46553781\n",
      "Iteration 15625, loss = 1.46553440\n",
      "Iteration 15626, loss = 1.46553100\n",
      "Iteration 15627, loss = 1.46552760\n",
      "Iteration 15628, loss = 1.46552420\n",
      "Iteration 15629, loss = 1.46552079\n",
      "Iteration 15630, loss = 1.46551739\n",
      "Iteration 15631, loss = 1.46551399\n",
      "Iteration 15632, loss = 1.46551059\n",
      "Iteration 15633, loss = 1.46550719\n",
      "Iteration 15634, loss = 1.46550379\n",
      "Iteration 15635, loss = 1.46550039\n",
      "Iteration 15636, loss = 1.46549699\n",
      "Iteration 15637, loss = 1.46549360\n",
      "Iteration 15638, loss = 1.46549020\n",
      "Iteration 15639, loss = 1.46548680\n",
      "Iteration 15640, loss = 1.46548340\n",
      "Iteration 15641, loss = 1.46548001\n",
      "Iteration 15642, loss = 1.46547661\n",
      "Iteration 15643, loss = 1.46547322\n",
      "Iteration 15644, loss = 1.46546982\n",
      "Iteration 15645, loss = 1.46546643\n",
      "Iteration 15646, loss = 1.46546303\n",
      "Iteration 15647, loss = 1.46545964\n",
      "Iteration 15648, loss = 1.46545625\n",
      "Iteration 15649, loss = 1.46545286\n",
      "Iteration 15650, loss = 1.46544946\n",
      "Iteration 15651, loss = 1.46544607\n",
      "Iteration 15652, loss = 1.46544268\n",
      "Iteration 15653, loss = 1.46543929\n",
      "Iteration 15654, loss = 1.46543590\n",
      "Iteration 15655, loss = 1.46543251\n",
      "Iteration 15656, loss = 1.46542912\n",
      "Iteration 15657, loss = 1.46542574\n",
      "Iteration 15658, loss = 1.46542235\n",
      "Iteration 15659, loss = 1.46541896\n",
      "Iteration 15660, loss = 1.46541557\n",
      "Iteration 15661, loss = 1.46541219\n",
      "Iteration 15662, loss = 1.46540880\n",
      "Iteration 15663, loss = 1.46540542\n",
      "Iteration 15664, loss = 1.46540203\n",
      "Iteration 15665, loss = 1.46539865\n",
      "Iteration 15666, loss = 1.46539526\n",
      "Iteration 15667, loss = 1.46539188\n",
      "Iteration 15668, loss = 1.46538849\n",
      "Iteration 15669, loss = 1.46538511\n",
      "Iteration 15670, loss = 1.46538173\n",
      "Iteration 15671, loss = 1.46537835\n",
      "Iteration 15672, loss = 1.46537497\n",
      "Iteration 15673, loss = 1.46537159\n",
      "Iteration 15674, loss = 1.46536821\n",
      "Iteration 15675, loss = 1.46536483\n",
      "Iteration 15676, loss = 1.46536145\n",
      "Iteration 15677, loss = 1.46535807\n",
      "Iteration 15678, loss = 1.46535469\n",
      "Iteration 15679, loss = 1.46535131\n",
      "Iteration 15680, loss = 1.46534793\n",
      "Iteration 15681, loss = 1.46534456\n",
      "Iteration 15682, loss = 1.46534118\n",
      "Iteration 15683, loss = 1.46533781\n",
      "Iteration 15684, loss = 1.46533443\n",
      "Iteration 15685, loss = 1.46533106\n",
      "Iteration 15686, loss = 1.46532768\n",
      "Iteration 15687, loss = 1.46532431\n",
      "Iteration 15688, loss = 1.46532093\n",
      "Iteration 15689, loss = 1.46531756\n",
      "Iteration 15690, loss = 1.46531419\n",
      "Iteration 15691, loss = 1.46531082\n",
      "Iteration 15692, loss = 1.46530744\n",
      "Iteration 15693, loss = 1.46530407\n",
      "Iteration 15694, loss = 1.46530070\n",
      "Iteration 15695, loss = 1.46529733\n",
      "Iteration 15696, loss = 1.46529396\n",
      "Iteration 15697, loss = 1.46529059\n",
      "Iteration 15698, loss = 1.46528722\n",
      "Iteration 15699, loss = 1.46528386\n",
      "Iteration 15700, loss = 1.46528049\n",
      "Iteration 15701, loss = 1.46527712\n",
      "Iteration 15702, loss = 1.46527375\n",
      "Iteration 15703, loss = 1.46527039\n",
      "Iteration 15704, loss = 1.46526702\n",
      "Iteration 15705, loss = 1.46526366\n",
      "Iteration 15706, loss = 1.46526029\n",
      "Iteration 15707, loss = 1.46525693\n",
      "Iteration 15708, loss = 1.46525356\n",
      "Iteration 15709, loss = 1.46525020\n",
      "Iteration 15710, loss = 1.46524684\n",
      "Iteration 15711, loss = 1.46524348\n",
      "Iteration 15712, loss = 1.46524011\n",
      "Iteration 15713, loss = 1.46523675\n",
      "Iteration 15714, loss = 1.46523339\n",
      "Iteration 15715, loss = 1.46523003\n",
      "Iteration 15716, loss = 1.46522667\n",
      "Iteration 15717, loss = 1.46522331\n",
      "Iteration 15718, loss = 1.46521995\n",
      "Iteration 15719, loss = 1.46521659\n",
      "Iteration 15720, loss = 1.46521323\n",
      "Iteration 15721, loss = 1.46520988\n",
      "Iteration 15722, loss = 1.46520652\n",
      "Iteration 15723, loss = 1.46520316\n",
      "Iteration 15724, loss = 1.46519981\n",
      "Iteration 15725, loss = 1.46519645\n",
      "Iteration 15726, loss = 1.46519309\n",
      "Iteration 15727, loss = 1.46518974\n",
      "Iteration 15728, loss = 1.46518639\n",
      "Iteration 15729, loss = 1.46518303\n",
      "Iteration 15730, loss = 1.46517968\n",
      "Iteration 15731, loss = 1.46517633\n",
      "Iteration 15732, loss = 1.46517297\n",
      "Iteration 15733, loss = 1.46516962\n",
      "Iteration 15734, loss = 1.46516627\n",
      "Iteration 15735, loss = 1.46516292\n",
      "Iteration 15736, loss = 1.46515957\n",
      "Iteration 15737, loss = 1.46515622\n",
      "Iteration 15738, loss = 1.46515287\n",
      "Iteration 15739, loss = 1.46514952\n",
      "Iteration 15740, loss = 1.46514617\n",
      "Iteration 15741, loss = 1.46514282\n",
      "Iteration 15742, loss = 1.46513947\n",
      "Iteration 15743, loss = 1.46513613\n",
      "Iteration 15744, loss = 1.46513278\n",
      "Iteration 15745, loss = 1.46512943\n",
      "Iteration 15746, loss = 1.46512609\n",
      "Iteration 15747, loss = 1.46512274\n",
      "Iteration 15748, loss = 1.46511940\n",
      "Iteration 15749, loss = 1.46511605\n",
      "Iteration 15750, loss = 1.46511271\n",
      "Iteration 15751, loss = 1.46510937\n",
      "Iteration 15752, loss = 1.46510602\n",
      "Iteration 15753, loss = 1.46510268\n",
      "Iteration 15754, loss = 1.46509934\n",
      "Iteration 15755, loss = 1.46509600\n",
      "Iteration 15756, loss = 1.46509265\n",
      "Iteration 15757, loss = 1.46508931\n",
      "Iteration 15758, loss = 1.46508597\n",
      "Iteration 15759, loss = 1.46508263\n",
      "Iteration 15760, loss = 1.46507929\n",
      "Iteration 15761, loss = 1.46507596\n",
      "Iteration 15762, loss = 1.46507262\n",
      "Iteration 15763, loss = 1.46506928\n",
      "Iteration 15764, loss = 1.46506594\n",
      "Iteration 15765, loss = 1.46506261\n",
      "Iteration 15766, loss = 1.46505927\n",
      "Iteration 15767, loss = 1.46505593\n",
      "Iteration 15768, loss = 1.46505260\n",
      "Iteration 15769, loss = 1.46504926\n",
      "Iteration 15770, loss = 1.46504593\n",
      "Iteration 15771, loss = 1.46504259\n",
      "Iteration 15772, loss = 1.46503926\n",
      "Iteration 15773, loss = 1.46503593\n",
      "Iteration 15774, loss = 1.46503260\n",
      "Iteration 15775, loss = 1.46502926\n",
      "Iteration 15776, loss = 1.46502593\n",
      "Iteration 15777, loss = 1.46502260\n",
      "Iteration 15778, loss = 1.46501927\n",
      "Iteration 15779, loss = 1.46501594\n",
      "Iteration 15780, loss = 1.46501261\n",
      "Iteration 15781, loss = 1.46500928\n",
      "Iteration 15782, loss = 1.46500595\n",
      "Iteration 15783, loss = 1.46500262\n",
      "Iteration 15784, loss = 1.46499929\n",
      "Iteration 15785, loss = 1.46499597\n",
      "Iteration 15786, loss = 1.46499264\n",
      "Iteration 15787, loss = 1.46498931\n",
      "Iteration 15788, loss = 1.46498599\n",
      "Iteration 15789, loss = 1.46498266\n",
      "Iteration 15790, loss = 1.46497934\n",
      "Iteration 15791, loss = 1.46497601\n",
      "Iteration 15792, loss = 1.46497269\n",
      "Iteration 15793, loss = 1.46496936\n",
      "Iteration 15794, loss = 1.46496604\n",
      "Iteration 15795, loss = 1.46496272\n",
      "Iteration 15796, loss = 1.46495940\n",
      "Iteration 15797, loss = 1.46495607\n",
      "Iteration 15798, loss = 1.46495275\n",
      "Iteration 15799, loss = 1.46494943\n",
      "Iteration 15800, loss = 1.46494611\n",
      "Iteration 15801, loss = 1.46494279\n",
      "Iteration 15802, loss = 1.46493947\n",
      "Iteration 15803, loss = 1.46493615\n",
      "Iteration 15804, loss = 1.46493284\n",
      "Iteration 15805, loss = 1.46492952\n",
      "Iteration 15806, loss = 1.46492620\n",
      "Iteration 15807, loss = 1.46492288\n",
      "Iteration 15808, loss = 1.46491957\n",
      "Iteration 15809, loss = 1.46491625\n",
      "Iteration 15810, loss = 1.46491293\n",
      "Iteration 15811, loss = 1.46490962\n",
      "Iteration 15812, loss = 1.46490630\n",
      "Iteration 15813, loss = 1.46490299\n",
      "Iteration 15814, loss = 1.46489968\n",
      "Iteration 15815, loss = 1.46489636\n",
      "Iteration 15816, loss = 1.46489305\n",
      "Iteration 15817, loss = 1.46488974\n",
      "Iteration 15818, loss = 1.46488642\n",
      "Iteration 15819, loss = 1.46488311\n",
      "Iteration 15820, loss = 1.46487980\n",
      "Iteration 15821, loss = 1.46487649\n",
      "Iteration 15822, loss = 1.46487318\n",
      "Iteration 15823, loss = 1.46486987\n",
      "Iteration 15824, loss = 1.46486656\n",
      "Iteration 15825, loss = 1.46486325\n",
      "Iteration 15826, loss = 1.46485995\n",
      "Iteration 15827, loss = 1.46485664\n",
      "Iteration 15828, loss = 1.46485333\n",
      "Iteration 15829, loss = 1.46485002\n",
      "Iteration 15830, loss = 1.46484672\n",
      "Iteration 15831, loss = 1.46484341\n",
      "Iteration 15832, loss = 1.46484011\n",
      "Iteration 15833, loss = 1.46483680\n",
      "Iteration 15834, loss = 1.46483350\n",
      "Iteration 15835, loss = 1.46483019\n",
      "Iteration 15836, loss = 1.46482689\n",
      "Iteration 15837, loss = 1.46482359\n",
      "Iteration 15838, loss = 1.46482028\n",
      "Iteration 15839, loss = 1.46481698\n",
      "Iteration 15840, loss = 1.46481368\n",
      "Iteration 15841, loss = 1.46481038\n",
      "Iteration 15842, loss = 1.46480708\n",
      "Iteration 15843, loss = 1.46480378\n",
      "Iteration 15844, loss = 1.46480048\n",
      "Iteration 15845, loss = 1.46479718\n",
      "Iteration 15846, loss = 1.46479388\n",
      "Iteration 15847, loss = 1.46479058\n",
      "Iteration 15848, loss = 1.46478728\n",
      "Iteration 15849, loss = 1.46478398\n",
      "Iteration 15850, loss = 1.46478069\n",
      "Iteration 15851, loss = 1.46477739\n",
      "Iteration 15852, loss = 1.46477409\n",
      "Iteration 15853, loss = 1.46477080\n",
      "Iteration 15854, loss = 1.46476750\n",
      "Iteration 15855, loss = 1.46476421\n",
      "Iteration 15856, loss = 1.46476091\n",
      "Iteration 15857, loss = 1.46475762\n",
      "Iteration 15858, loss = 1.46475433\n",
      "Iteration 15859, loss = 1.46475103\n",
      "Iteration 15860, loss = 1.46474774\n",
      "Iteration 15861, loss = 1.46474445\n",
      "Iteration 15862, loss = 1.46474116\n",
      "Iteration 15863, loss = 1.46473787\n",
      "Iteration 15864, loss = 1.46473458\n",
      "Iteration 15865, loss = 1.46473129\n",
      "Iteration 15866, loss = 1.46472800\n",
      "Iteration 15867, loss = 1.46472471\n",
      "Iteration 15868, loss = 1.46472142\n",
      "Iteration 15869, loss = 1.46471813\n",
      "Iteration 15870, loss = 1.46471484\n",
      "Iteration 15871, loss = 1.46471156\n",
      "Iteration 15872, loss = 1.46470827\n",
      "Iteration 15873, loss = 1.46470498\n",
      "Iteration 15874, loss = 1.46470170\n",
      "Iteration 15875, loss = 1.46469841\n",
      "Iteration 15876, loss = 1.46469513\n",
      "Iteration 15877, loss = 1.46469184\n",
      "Iteration 15878, loss = 1.46468856\n",
      "Iteration 15879, loss = 1.46468527\n",
      "Iteration 15880, loss = 1.46468199\n",
      "Iteration 15881, loss = 1.46467871\n",
      "Iteration 15882, loss = 1.46467543\n",
      "Iteration 15883, loss = 1.46467214\n",
      "Iteration 15884, loss = 1.46466886\n",
      "Iteration 15885, loss = 1.46466558\n",
      "Iteration 15886, loss = 1.46466230\n",
      "Iteration 15887, loss = 1.46465902\n",
      "Iteration 15888, loss = 1.46465574\n",
      "Iteration 15889, loss = 1.46465246\n",
      "Iteration 15890, loss = 1.46464918\n",
      "Iteration 15891, loss = 1.46464591\n",
      "Iteration 15892, loss = 1.46464263\n",
      "Iteration 15893, loss = 1.46463935\n",
      "Iteration 15894, loss = 1.46463607\n",
      "Iteration 15895, loss = 1.46463280\n",
      "Iteration 15896, loss = 1.46462952\n",
      "Iteration 15897, loss = 1.46462625\n",
      "Iteration 15898, loss = 1.46462297\n",
      "Iteration 15899, loss = 1.46461970\n",
      "Iteration 15900, loss = 1.46461642\n",
      "Iteration 15901, loss = 1.46461315\n",
      "Iteration 15902, loss = 1.46460988\n",
      "Iteration 15903, loss = 1.46460660\n",
      "Iteration 15904, loss = 1.46460333\n",
      "Iteration 15905, loss = 1.46460006\n",
      "Iteration 15906, loss = 1.46459679\n",
      "Iteration 15907, loss = 1.46459352\n",
      "Iteration 15908, loss = 1.46459025\n",
      "Iteration 15909, loss = 1.46458698\n",
      "Iteration 15910, loss = 1.46458371\n",
      "Iteration 15911, loss = 1.46458044\n",
      "Iteration 15912, loss = 1.46457717\n",
      "Iteration 15913, loss = 1.46457390\n",
      "Iteration 15914, loss = 1.46457064\n",
      "Iteration 15915, loss = 1.46456737\n",
      "Iteration 15916, loss = 1.46456410\n",
      "Iteration 15917, loss = 1.46456084\n",
      "Iteration 15918, loss = 1.46455757\n",
      "Iteration 15919, loss = 1.46455431\n",
      "Iteration 15920, loss = 1.46455104\n",
      "Iteration 15921, loss = 1.46454778\n",
      "Iteration 15922, loss = 1.46454451\n",
      "Iteration 15923, loss = 1.46454125\n",
      "Iteration 15924, loss = 1.46453799\n",
      "Iteration 15925, loss = 1.46453472\n",
      "Iteration 15926, loss = 1.46453146\n",
      "Iteration 15927, loss = 1.46452820\n",
      "Iteration 15928, loss = 1.46452494\n",
      "Iteration 15929, loss = 1.46452168\n",
      "Iteration 15930, loss = 1.46451842\n",
      "Iteration 15931, loss = 1.46451516\n",
      "Iteration 15932, loss = 1.46451190\n",
      "Iteration 15933, loss = 1.46450864\n",
      "Iteration 15934, loss = 1.46450538\n",
      "Iteration 15935, loss = 1.46450212\n",
      "Iteration 15936, loss = 1.46449887\n",
      "Iteration 15937, loss = 1.46449561\n",
      "Iteration 15938, loss = 1.46449235\n",
      "Iteration 15939, loss = 1.46448910\n",
      "Iteration 15940, loss = 1.46448584\n",
      "Iteration 15941, loss = 1.46448259\n",
      "Iteration 15942, loss = 1.46447933\n",
      "Iteration 15943, loss = 1.46447608\n",
      "Iteration 15944, loss = 1.46447282\n",
      "Iteration 15945, loss = 1.46446957\n",
      "Iteration 15946, loss = 1.46446632\n",
      "Iteration 15947, loss = 1.46446306\n",
      "Iteration 15948, loss = 1.46445981\n",
      "Iteration 15949, loss = 1.46445656\n",
      "Iteration 15950, loss = 1.46445331\n",
      "Iteration 15951, loss = 1.46445006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15952, loss = 1.46444681\n",
      "Iteration 15953, loss = 1.46444356\n",
      "Iteration 15954, loss = 1.46444031\n",
      "Iteration 15955, loss = 1.46443706\n",
      "Iteration 15956, loss = 1.46443381\n",
      "Iteration 15957, loss = 1.46443056\n",
      "Iteration 15958, loss = 1.46442732\n",
      "Iteration 15959, loss = 1.46442407\n",
      "Iteration 15960, loss = 1.46442082\n",
      "Iteration 15961, loss = 1.46441758\n",
      "Iteration 15962, loss = 1.46441433\n",
      "Iteration 15963, loss = 1.46441109\n",
      "Iteration 15964, loss = 1.46440784\n",
      "Iteration 15965, loss = 1.46440460\n",
      "Iteration 15966, loss = 1.46440135\n",
      "Iteration 15967, loss = 1.46439811\n",
      "Iteration 15968, loss = 1.46439487\n",
      "Iteration 15969, loss = 1.46439163\n",
      "Iteration 15970, loss = 1.46438838\n",
      "Iteration 15971, loss = 1.46438514\n",
      "Iteration 15972, loss = 1.46438190\n",
      "Iteration 15973, loss = 1.46437866\n",
      "Iteration 15974, loss = 1.46437542\n",
      "Iteration 15975, loss = 1.46437218\n",
      "Iteration 15976, loss = 1.46436894\n",
      "Iteration 15977, loss = 1.46436570\n",
      "Iteration 15978, loss = 1.46436246\n",
      "Iteration 15979, loss = 1.46435923\n",
      "Iteration 15980, loss = 1.46435599\n",
      "Iteration 15981, loss = 1.46435275\n",
      "Iteration 15982, loss = 1.46434951\n",
      "Iteration 15983, loss = 1.46434628\n",
      "Iteration 15984, loss = 1.46434304\n",
      "Iteration 15985, loss = 1.46433981\n",
      "Iteration 15986, loss = 1.46433657\n",
      "Iteration 15987, loss = 1.46433334\n",
      "Iteration 15988, loss = 1.46433010\n",
      "Iteration 15989, loss = 1.46432687\n",
      "Iteration 15990, loss = 1.46432364\n",
      "Iteration 15991, loss = 1.46432041\n",
      "Iteration 15992, loss = 1.46431717\n",
      "Iteration 15993, loss = 1.46431394\n",
      "Iteration 15994, loss = 1.46431071\n",
      "Iteration 15995, loss = 1.46430748\n",
      "Iteration 15996, loss = 1.46430425\n",
      "Iteration 15997, loss = 1.46430102\n",
      "Iteration 15998, loss = 1.46429779\n",
      "Iteration 15999, loss = 1.46429456\n",
      "Iteration 16000, loss = 1.46429133\n",
      "Iteration 16001, loss = 1.46428811\n",
      "Iteration 16002, loss = 1.46428488\n",
      "Iteration 16003, loss = 1.46428165\n",
      "Iteration 16004, loss = 1.46427842\n",
      "Iteration 16005, loss = 1.46427520\n",
      "Iteration 16006, loss = 1.46427197\n",
      "Iteration 16007, loss = 1.46426875\n",
      "Iteration 16008, loss = 1.46426552\n",
      "Iteration 16009, loss = 1.46426230\n",
      "Iteration 16010, loss = 1.46425907\n",
      "Iteration 16011, loss = 1.46425585\n",
      "Iteration 16012, loss = 1.46425263\n",
      "Iteration 16013, loss = 1.46424941\n",
      "Iteration 16014, loss = 1.46424618\n",
      "Iteration 16015, loss = 1.46424296\n",
      "Iteration 16016, loss = 1.46423974\n",
      "Iteration 16017, loss = 1.46423652\n",
      "Iteration 16018, loss = 1.46423330\n",
      "Iteration 16019, loss = 1.46423008\n",
      "Iteration 16020, loss = 1.46422686\n",
      "Iteration 16021, loss = 1.46422364\n",
      "Iteration 16022, loss = 1.46422042\n",
      "Iteration 16023, loss = 1.46421720\n",
      "Iteration 16024, loss = 1.46421399\n",
      "Iteration 16025, loss = 1.46421077\n",
      "Iteration 16026, loss = 1.46420755\n",
      "Iteration 16027, loss = 1.46420434\n",
      "Iteration 16028, loss = 1.46420112\n",
      "Iteration 16029, loss = 1.46419791\n",
      "Iteration 16030, loss = 1.46419469\n",
      "Iteration 16031, loss = 1.46419148\n",
      "Iteration 16032, loss = 1.46418826\n",
      "Iteration 16033, loss = 1.46418505\n",
      "Iteration 16034, loss = 1.46418183\n",
      "Iteration 16035, loss = 1.46417862\n",
      "Iteration 16036, loss = 1.46417541\n",
      "Iteration 16037, loss = 1.46417220\n",
      "Iteration 16038, loss = 1.46416899\n",
      "Iteration 16039, loss = 1.46416578\n",
      "Iteration 16040, loss = 1.46416257\n",
      "Iteration 16041, loss = 1.46415936\n",
      "Iteration 16042, loss = 1.46415615\n",
      "Iteration 16043, loss = 1.46415294\n",
      "Iteration 16044, loss = 1.46414973\n",
      "Iteration 16045, loss = 1.46414652\n",
      "Iteration 16046, loss = 1.46414331\n",
      "Iteration 16047, loss = 1.46414010\n",
      "Iteration 16048, loss = 1.46413690\n",
      "Iteration 16049, loss = 1.46413369\n",
      "Iteration 16050, loss = 1.46413049\n",
      "Iteration 16051, loss = 1.46412728\n",
      "Iteration 16052, loss = 1.46412407\n",
      "Iteration 16053, loss = 1.46412087\n",
      "Iteration 16054, loss = 1.46411767\n",
      "Iteration 16055, loss = 1.46411446\n",
      "Iteration 16056, loss = 1.46411126\n",
      "Iteration 16057, loss = 1.46410806\n",
      "Iteration 16058, loss = 1.46410485\n",
      "Iteration 16059, loss = 1.46410165\n",
      "Iteration 16060, loss = 1.46409845\n",
      "Iteration 16061, loss = 1.46409525\n",
      "Iteration 16062, loss = 1.46409205\n",
      "Iteration 16063, loss = 1.46408885\n",
      "Iteration 16064, loss = 1.46408565\n",
      "Iteration 16065, loss = 1.46408245\n",
      "Iteration 16066, loss = 1.46407925\n",
      "Iteration 16067, loss = 1.46407605\n",
      "Iteration 16068, loss = 1.46407285\n",
      "Iteration 16069, loss = 1.46406966\n",
      "Iteration 16070, loss = 1.46406646\n",
      "Iteration 16071, loss = 1.46406326\n",
      "Iteration 16072, loss = 1.46406007\n",
      "Iteration 16073, loss = 1.46405687\n",
      "Iteration 16074, loss = 1.46405368\n",
      "Iteration 16075, loss = 1.46405048\n",
      "Iteration 16076, loss = 1.46404729\n",
      "Iteration 16077, loss = 1.46404409\n",
      "Iteration 16078, loss = 1.46404090\n",
      "Iteration 16079, loss = 1.46403771\n",
      "Iteration 16080, loss = 1.46403451\n",
      "Iteration 16081, loss = 1.46403132\n",
      "Iteration 16082, loss = 1.46402813\n",
      "Iteration 16083, loss = 1.46402494\n",
      "Iteration 16084, loss = 1.46402175\n",
      "Iteration 16085, loss = 1.46401856\n",
      "Iteration 16086, loss = 1.46401537\n",
      "Iteration 16087, loss = 1.46401218\n",
      "Iteration 16088, loss = 1.46400899\n",
      "Iteration 16089, loss = 1.46400580\n",
      "Iteration 16090, loss = 1.46400261\n",
      "Iteration 16091, loss = 1.46399942\n",
      "Iteration 16092, loss = 1.46399624\n",
      "Iteration 16093, loss = 1.46399305\n",
      "Iteration 16094, loss = 1.46398986\n",
      "Iteration 16095, loss = 1.46398668\n",
      "Iteration 16096, loss = 1.46398349\n",
      "Iteration 16097, loss = 1.46398031\n",
      "Iteration 16098, loss = 1.46397712\n",
      "Iteration 16099, loss = 1.46397394\n",
      "Iteration 16100, loss = 1.46397075\n",
      "Iteration 16101, loss = 1.46396757\n",
      "Iteration 16102, loss = 1.46396439\n",
      "Iteration 16103, loss = 1.46396121\n",
      "Iteration 16104, loss = 1.46395802\n",
      "Iteration 16105, loss = 1.46395484\n",
      "Iteration 16106, loss = 1.46395166\n",
      "Iteration 16107, loss = 1.46394848\n",
      "Iteration 16108, loss = 1.46394530\n",
      "Iteration 16109, loss = 1.46394212\n",
      "Iteration 16110, loss = 1.46393894\n",
      "Iteration 16111, loss = 1.46393576\n",
      "Iteration 16112, loss = 1.46393258\n",
      "Iteration 16113, loss = 1.46392941\n",
      "Iteration 16114, loss = 1.46392623\n",
      "Iteration 16115, loss = 1.46392305\n",
      "Iteration 16116, loss = 1.46391987\n",
      "Iteration 16117, loss = 1.46391670\n",
      "Iteration 16118, loss = 1.46391352\n",
      "Iteration 16119, loss = 1.46391035\n",
      "Iteration 16120, loss = 1.46390717\n",
      "Iteration 16121, loss = 1.46390400\n",
      "Iteration 16122, loss = 1.46390082\n",
      "Iteration 16123, loss = 1.46389765\n",
      "Iteration 16124, loss = 1.46389448\n",
      "Iteration 16125, loss = 1.46389130\n",
      "Iteration 16126, loss = 1.46388813\n",
      "Iteration 16127, loss = 1.46388496\n",
      "Iteration 16128, loss = 1.46388179\n",
      "Iteration 16129, loss = 1.46387862\n",
      "Iteration 16130, loss = 1.46387545\n",
      "Iteration 16131, loss = 1.46387228\n",
      "Iteration 16132, loss = 1.46386911\n",
      "Iteration 16133, loss = 1.46386594\n",
      "Iteration 16134, loss = 1.46386277\n",
      "Iteration 16135, loss = 1.46385960\n",
      "Iteration 16136, loss = 1.46385643\n",
      "Iteration 16137, loss = 1.46385326\n",
      "Iteration 16138, loss = 1.46385010\n",
      "Iteration 16139, loss = 1.46384693\n",
      "Iteration 16140, loss = 1.46384377\n",
      "Iteration 16141, loss = 1.46384060\n",
      "Iteration 16142, loss = 1.46383743\n",
      "Iteration 16143, loss = 1.46383427\n",
      "Iteration 16144, loss = 1.46383110\n",
      "Iteration 16145, loss = 1.46382794\n",
      "Iteration 16146, loss = 1.46382478\n",
      "Iteration 16147, loss = 1.46382161\n",
      "Iteration 16148, loss = 1.46381845\n",
      "Iteration 16149, loss = 1.46381529\n",
      "Iteration 16150, loss = 1.46381213\n",
      "Iteration 16151, loss = 1.46380897\n",
      "Iteration 16152, loss = 1.46380581\n",
      "Iteration 16153, loss = 1.46380265\n",
      "Iteration 16154, loss = 1.46379949\n",
      "Iteration 16155, loss = 1.46379633\n",
      "Iteration 16156, loss = 1.46379317\n",
      "Iteration 16157, loss = 1.46379001\n",
      "Iteration 16158, loss = 1.46378685\n",
      "Iteration 16159, loss = 1.46378369\n",
      "Iteration 16160, loss = 1.46378053\n",
      "Iteration 16161, loss = 1.46377738\n",
      "Iteration 16162, loss = 1.46377422\n",
      "Iteration 16163, loss = 1.46377107\n",
      "Iteration 16164, loss = 1.46376791\n",
      "Iteration 16165, loss = 1.46376475\n",
      "Iteration 16166, loss = 1.46376160\n",
      "Iteration 16167, loss = 1.46375845\n",
      "Iteration 16168, loss = 1.46375529\n",
      "Iteration 16169, loss = 1.46375214\n",
      "Iteration 16170, loss = 1.46374898\n",
      "Iteration 16171, loss = 1.46374583\n",
      "Iteration 16172, loss = 1.46374268\n",
      "Iteration 16173, loss = 1.46373953\n",
      "Iteration 16174, loss = 1.46373638\n",
      "Iteration 16175, loss = 1.46373323\n",
      "Iteration 16176, loss = 1.46373008\n",
      "Iteration 16177, loss = 1.46372693\n",
      "Iteration 16178, loss = 1.46372378\n",
      "Iteration 16179, loss = 1.46372063\n",
      "Iteration 16180, loss = 1.46371748\n",
      "Iteration 16181, loss = 1.46371433\n",
      "Iteration 16182, loss = 1.46371118\n",
      "Iteration 16183, loss = 1.46370804\n",
      "Iteration 16184, loss = 1.46370489\n",
      "Iteration 16185, loss = 1.46370174\n",
      "Iteration 16186, loss = 1.46369860\n",
      "Iteration 16187, loss = 1.46369545\n",
      "Iteration 16188, loss = 1.46369231\n",
      "Iteration 16189, loss = 1.46368916\n",
      "Iteration 16190, loss = 1.46368602\n",
      "Iteration 16191, loss = 1.46368287\n",
      "Iteration 16192, loss = 1.46367973\n",
      "Iteration 16193, loss = 1.46367659\n",
      "Iteration 16194, loss = 1.46367344\n",
      "Iteration 16195, loss = 1.46367030\n",
      "Iteration 16196, loss = 1.46366716\n",
      "Iteration 16197, loss = 1.46366402\n",
      "Iteration 16198, loss = 1.46366088\n",
      "Iteration 16199, loss = 1.46365774\n",
      "Iteration 16200, loss = 1.46365460\n",
      "Iteration 16201, loss = 1.46365146\n",
      "Iteration 16202, loss = 1.46364832\n",
      "Iteration 16203, loss = 1.46364518\n",
      "Iteration 16204, loss = 1.46364204\n",
      "Iteration 16205, loss = 1.46363890\n",
      "Iteration 16206, loss = 1.46363577\n",
      "Iteration 16207, loss = 1.46363263\n",
      "Iteration 16208, loss = 1.46362949\n",
      "Iteration 16209, loss = 1.46362636\n",
      "Iteration 16210, loss = 1.46362322\n",
      "Iteration 16211, loss = 1.46362009\n",
      "Iteration 16212, loss = 1.46361695\n",
      "Iteration 16213, loss = 1.46361382\n",
      "Iteration 16214, loss = 1.46361068\n",
      "Iteration 16215, loss = 1.46360755\n",
      "Iteration 16216, loss = 1.46360442\n",
      "Iteration 16217, loss = 1.46360128\n",
      "Iteration 16218, loss = 1.46359815\n",
      "Iteration 16219, loss = 1.46359502\n",
      "Iteration 16220, loss = 1.46359189\n",
      "Iteration 16221, loss = 1.46358876\n",
      "Iteration 16222, loss = 1.46358563\n",
      "Iteration 16223, loss = 1.46358250\n",
      "Iteration 16224, loss = 1.46357937\n",
      "Iteration 16225, loss = 1.46357624\n",
      "Iteration 16226, loss = 1.46357311\n",
      "Iteration 16227, loss = 1.46356998\n",
      "Iteration 16228, loss = 1.46356686\n",
      "Iteration 16229, loss = 1.46356373\n",
      "Iteration 16230, loss = 1.46356060\n",
      "Iteration 16231, loss = 1.46355747\n",
      "Iteration 16232, loss = 1.46355435\n",
      "Iteration 16233, loss = 1.46355122\n",
      "Iteration 16234, loss = 1.46354810\n",
      "Iteration 16235, loss = 1.46354497\n",
      "Iteration 16236, loss = 1.46354185\n",
      "Iteration 16237, loss = 1.46353872\n",
      "Iteration 16238, loss = 1.46353560\n",
      "Iteration 16239, loss = 1.46353248\n",
      "Iteration 16240, loss = 1.46352935\n",
      "Iteration 16241, loss = 1.46352623\n",
      "Iteration 16242, loss = 1.46352311\n",
      "Iteration 16243, loss = 1.46351999\n",
      "Iteration 16244, loss = 1.46351687\n",
      "Iteration 16245, loss = 1.46351375\n",
      "Iteration 16246, loss = 1.46351063\n",
      "Iteration 16247, loss = 1.46350751\n",
      "Iteration 16248, loss = 1.46350439\n",
      "Iteration 16249, loss = 1.46350127\n",
      "Iteration 16250, loss = 1.46349815\n",
      "Iteration 16251, loss = 1.46349503\n",
      "Iteration 16252, loss = 1.46349192\n",
      "Iteration 16253, loss = 1.46348880\n",
      "Iteration 16254, loss = 1.46348568\n",
      "Iteration 16255, loss = 1.46348257\n",
      "Iteration 16256, loss = 1.46347945\n",
      "Iteration 16257, loss = 1.46347634\n",
      "Iteration 16258, loss = 1.46347322\n",
      "Iteration 16259, loss = 1.46347011\n",
      "Iteration 16260, loss = 1.46346699\n",
      "Iteration 16261, loss = 1.46346388\n",
      "Iteration 16262, loss = 1.46346077\n",
      "Iteration 16263, loss = 1.46345765\n",
      "Iteration 16264, loss = 1.46345454\n",
      "Iteration 16265, loss = 1.46345143\n",
      "Iteration 16266, loss = 1.46344832\n",
      "Iteration 16267, loss = 1.46344521\n",
      "Iteration 16268, loss = 1.46344210\n",
      "Iteration 16269, loss = 1.46343899\n",
      "Iteration 16270, loss = 1.46343588\n",
      "Iteration 16271, loss = 1.46343277\n",
      "Iteration 16272, loss = 1.46342966\n",
      "Iteration 16273, loss = 1.46342655\n",
      "Iteration 16274, loss = 1.46342344\n",
      "Iteration 16275, loss = 1.46342033\n",
      "Iteration 16276, loss = 1.46341723\n",
      "Iteration 16277, loss = 1.46341412\n",
      "Iteration 16278, loss = 1.46341101\n",
      "Iteration 16279, loss = 1.46340791\n",
      "Iteration 16280, loss = 1.46340480\n",
      "Iteration 16281, loss = 1.46340170\n",
      "Iteration 16282, loss = 1.46339859\n",
      "Iteration 16283, loss = 1.46339549\n",
      "Iteration 16284, loss = 1.46339238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16285, loss = 1.46338928\n",
      "Iteration 16286, loss = 1.46338618\n",
      "Iteration 16287, loss = 1.46338308\n",
      "Iteration 16288, loss = 1.46337997\n",
      "Iteration 16289, loss = 1.46337687\n",
      "Iteration 16290, loss = 1.46337377\n",
      "Iteration 16291, loss = 1.46337067\n",
      "Iteration 16292, loss = 1.46336757\n",
      "Iteration 16293, loss = 1.46336447\n",
      "Iteration 16294, loss = 1.46336137\n",
      "Iteration 16295, loss = 1.46335827\n",
      "Iteration 16296, loss = 1.46335517\n",
      "Iteration 16297, loss = 1.46335207\n",
      "Iteration 16298, loss = 1.46334898\n",
      "Iteration 16299, loss = 1.46334588\n",
      "Iteration 16300, loss = 1.46334278\n",
      "Iteration 16301, loss = 1.46333968\n",
      "Iteration 16302, loss = 1.46333659\n",
      "Iteration 16303, loss = 1.46333349\n",
      "Iteration 16304, loss = 1.46333040\n",
      "Iteration 16305, loss = 1.46332730\n",
      "Iteration 16306, loss = 1.46332421\n",
      "Iteration 16307, loss = 1.46332111\n",
      "Iteration 16308, loss = 1.46331802\n",
      "Iteration 16309, loss = 1.46331493\n",
      "Iteration 16310, loss = 1.46331184\n",
      "Iteration 16311, loss = 1.46330874\n",
      "Iteration 16312, loss = 1.46330565\n",
      "Iteration 16313, loss = 1.46330256\n",
      "Iteration 16314, loss = 1.46329947\n",
      "Iteration 16315, loss = 1.46329638\n",
      "Iteration 16316, loss = 1.46329329\n",
      "Iteration 16317, loss = 1.46329020\n",
      "Iteration 16318, loss = 1.46328711\n",
      "Iteration 16319, loss = 1.46328402\n",
      "Iteration 16320, loss = 1.46328093\n",
      "Iteration 16321, loss = 1.46327784\n",
      "Iteration 16322, loss = 1.46327476\n",
      "Iteration 16323, loss = 1.46327167\n",
      "Iteration 16324, loss = 1.46326858\n",
      "Iteration 16325, loss = 1.46326550\n",
      "Iteration 16326, loss = 1.46326241\n",
      "Iteration 16327, loss = 1.46325932\n",
      "Iteration 16328, loss = 1.46325624\n",
      "Iteration 16329, loss = 1.46325315\n",
      "Iteration 16330, loss = 1.46325007\n",
      "Iteration 16331, loss = 1.46324699\n",
      "Iteration 16332, loss = 1.46324390\n",
      "Iteration 16333, loss = 1.46324082\n",
      "Iteration 16334, loss = 1.46323774\n",
      "Iteration 16335, loss = 1.46323466\n",
      "Iteration 16336, loss = 1.46323157\n",
      "Iteration 16337, loss = 1.46322849\n",
      "Iteration 16338, loss = 1.46322541\n",
      "Iteration 16339, loss = 1.46322233\n",
      "Iteration 16340, loss = 1.46321925\n",
      "Iteration 16341, loss = 1.46321617\n",
      "Iteration 16342, loss = 1.46321309\n",
      "Iteration 16343, loss = 1.46321001\n",
      "Iteration 16344, loss = 1.46320694\n",
      "Iteration 16345, loss = 1.46320386\n",
      "Iteration 16346, loss = 1.46320078\n",
      "Iteration 16347, loss = 1.46319770\n",
      "Iteration 16348, loss = 1.46319463\n",
      "Iteration 16349, loss = 1.46319155\n",
      "Iteration 16350, loss = 1.46318848\n",
      "Iteration 16351, loss = 1.46318540\n",
      "Iteration 16352, loss = 1.46318233\n",
      "Iteration 16353, loss = 1.46317925\n",
      "Iteration 16354, loss = 1.46317618\n",
      "Iteration 16355, loss = 1.46317310\n",
      "Iteration 16356, loss = 1.46317003\n",
      "Iteration 16357, loss = 1.46316696\n",
      "Iteration 16358, loss = 1.46316388\n",
      "Iteration 16359, loss = 1.46316081\n",
      "Iteration 16360, loss = 1.46315774\n",
      "Iteration 16361, loss = 1.46315467\n",
      "Iteration 16362, loss = 1.46315160\n",
      "Iteration 16363, loss = 1.46314853\n",
      "Iteration 16364, loss = 1.46314546\n",
      "Iteration 16365, loss = 1.46314239\n",
      "Iteration 16366, loss = 1.46313932\n",
      "Iteration 16367, loss = 1.46313625\n",
      "Iteration 16368, loss = 1.46313318\n",
      "Iteration 16369, loss = 1.46313012\n",
      "Iteration 16370, loss = 1.46312705\n",
      "Iteration 16371, loss = 1.46312398\n",
      "Iteration 16372, loss = 1.46312092\n",
      "Iteration 16373, loss = 1.46311785\n",
      "Iteration 16374, loss = 1.46311478\n",
      "Iteration 16375, loss = 1.46311172\n",
      "Iteration 16376, loss = 1.46310865\n",
      "Iteration 16377, loss = 1.46310559\n",
      "Iteration 16378, loss = 1.46310253\n",
      "Iteration 16379, loss = 1.46309946\n",
      "Iteration 16380, loss = 1.46309640\n",
      "Iteration 16381, loss = 1.46309334\n",
      "Iteration 16382, loss = 1.46309027\n",
      "Iteration 16383, loss = 1.46308721\n",
      "Iteration 16384, loss = 1.46308415\n",
      "Iteration 16385, loss = 1.46308109\n",
      "Iteration 16386, loss = 1.46307803\n",
      "Iteration 16387, loss = 1.46307497\n",
      "Iteration 16388, loss = 1.46307191\n",
      "Iteration 16389, loss = 1.46306885\n",
      "Iteration 16390, loss = 1.46306579\n",
      "Iteration 16391, loss = 1.46306273\n",
      "Iteration 16392, loss = 1.46305967\n",
      "Iteration 16393, loss = 1.46305662\n",
      "Iteration 16394, loss = 1.46305356\n",
      "Iteration 16395, loss = 1.46305050\n",
      "Iteration 16396, loss = 1.46304745\n",
      "Iteration 16397, loss = 1.46304439\n",
      "Iteration 16398, loss = 1.46304133\n",
      "Iteration 16399, loss = 1.46303828\n",
      "Iteration 16400, loss = 1.46303522\n",
      "Iteration 16401, loss = 1.46303217\n",
      "Iteration 16402, loss = 1.46302912\n",
      "Iteration 16403, loss = 1.46302606\n",
      "Iteration 16404, loss = 1.46302301\n",
      "Iteration 16405, loss = 1.46301996\n",
      "Iteration 16406, loss = 1.46301690\n",
      "Iteration 16407, loss = 1.46301385\n",
      "Iteration 16408, loss = 1.46301080\n",
      "Iteration 16409, loss = 1.46300775\n",
      "Iteration 16410, loss = 1.46300470\n",
      "Iteration 16411, loss = 1.46300165\n",
      "Iteration 16412, loss = 1.46299860\n",
      "Iteration 16413, loss = 1.46299555\n",
      "Iteration 16414, loss = 1.46299250\n",
      "Iteration 16415, loss = 1.46298945\n",
      "Iteration 16416, loss = 1.46298640\n",
      "Iteration 16417, loss = 1.46298336\n",
      "Iteration 16418, loss = 1.46298031\n",
      "Iteration 16419, loss = 1.46297726\n",
      "Iteration 16420, loss = 1.46297422\n",
      "Iteration 16421, loss = 1.46297117\n",
      "Iteration 16422, loss = 1.46296812\n",
      "Iteration 16423, loss = 1.46296508\n",
      "Iteration 16424, loss = 1.46296203\n",
      "Iteration 16425, loss = 1.46295899\n",
      "Iteration 16426, loss = 1.46295595\n",
      "Iteration 16427, loss = 1.46295290\n",
      "Iteration 16428, loss = 1.46294986\n",
      "Iteration 16429, loss = 1.46294682\n",
      "Iteration 16430, loss = 1.46294377\n",
      "Iteration 16431, loss = 1.46294073\n",
      "Iteration 16432, loss = 1.46293769\n",
      "Iteration 16433, loss = 1.46293465\n",
      "Iteration 16434, loss = 1.46293161\n",
      "Iteration 16435, loss = 1.46292857\n",
      "Iteration 16436, loss = 1.46292553\n",
      "Iteration 16437, loss = 1.46292249\n",
      "Iteration 16438, loss = 1.46291945\n",
      "Iteration 16439, loss = 1.46291641\n",
      "Iteration 16440, loss = 1.46291338\n",
      "Iteration 16441, loss = 1.46291034\n",
      "Iteration 16442, loss = 1.46290730\n",
      "Iteration 16443, loss = 1.46290426\n",
      "Iteration 16444, loss = 1.46290123\n",
      "Iteration 16445, loss = 1.46289819\n",
      "Iteration 16446, loss = 1.46289516\n",
      "Iteration 16447, loss = 1.46289212\n",
      "Iteration 16448, loss = 1.46288909\n",
      "Iteration 16449, loss = 1.46288605\n",
      "Iteration 16450, loss = 1.46288302\n",
      "Iteration 16451, loss = 1.46287998\n",
      "Iteration 16452, loss = 1.46287695\n",
      "Iteration 16453, loss = 1.46287392\n",
      "Iteration 16454, loss = 1.46287089\n",
      "Iteration 16455, loss = 1.46286785\n",
      "Iteration 16456, loss = 1.46286482\n",
      "Iteration 16457, loss = 1.46286179\n",
      "Iteration 16458, loss = 1.46285876\n",
      "Iteration 16459, loss = 1.46285573\n",
      "Iteration 16460, loss = 1.46285270\n",
      "Iteration 16461, loss = 1.46284967\n",
      "Iteration 16462, loss = 1.46284664\n",
      "Iteration 16463, loss = 1.46284361\n",
      "Iteration 16464, loss = 1.46284058\n",
      "Iteration 16465, loss = 1.46283756\n",
      "Iteration 16466, loss = 1.46283453\n",
      "Iteration 16467, loss = 1.46283150\n",
      "Iteration 16468, loss = 1.46282848\n",
      "Iteration 16469, loss = 1.46282545\n",
      "Iteration 16470, loss = 1.46282242\n",
      "Iteration 16471, loss = 1.46281940\n",
      "Iteration 16472, loss = 1.46281637\n",
      "Iteration 16473, loss = 1.46281335\n",
      "Iteration 16474, loss = 1.46281033\n",
      "Iteration 16475, loss = 1.46280730\n",
      "Iteration 16476, loss = 1.46280428\n",
      "Iteration 16477, loss = 1.46280126\n",
      "Iteration 16478, loss = 1.46279823\n",
      "Iteration 16479, loss = 1.46279521\n",
      "Iteration 16480, loss = 1.46279219\n",
      "Iteration 16481, loss = 1.46278917\n",
      "Iteration 16482, loss = 1.46278615\n",
      "Iteration 16483, loss = 1.46278313\n",
      "Iteration 16484, loss = 1.46278011\n",
      "Iteration 16485, loss = 1.46277709\n",
      "Iteration 16486, loss = 1.46277407\n",
      "Iteration 16487, loss = 1.46277105\n",
      "Iteration 16488, loss = 1.46276803\n",
      "Iteration 16489, loss = 1.46276501\n",
      "Iteration 16490, loss = 1.46276200\n",
      "Iteration 16491, loss = 1.46275898\n",
      "Iteration 16492, loss = 1.46275596\n",
      "Iteration 16493, loss = 1.46275295\n",
      "Iteration 16494, loss = 1.46274993\n",
      "Iteration 16495, loss = 1.46274692\n",
      "Iteration 16496, loss = 1.46274390\n",
      "Iteration 16497, loss = 1.46274089\n",
      "Iteration 16498, loss = 1.46273787\n",
      "Iteration 16499, loss = 1.46273486\n",
      "Iteration 16500, loss = 1.46273184\n",
      "Iteration 16501, loss = 1.46272883\n",
      "Iteration 16502, loss = 1.46272582\n",
      "Iteration 16503, loss = 1.46272281\n",
      "Iteration 16504, loss = 1.46271980\n",
      "Iteration 16505, loss = 1.46271678\n",
      "Iteration 16506, loss = 1.46271377\n",
      "Iteration 16507, loss = 1.46271076\n",
      "Iteration 16508, loss = 1.46270775\n",
      "Iteration 16509, loss = 1.46270474\n",
      "Iteration 16510, loss = 1.46270173\n",
      "Iteration 16511, loss = 1.46269872\n",
      "Iteration 16512, loss = 1.46269572\n",
      "Iteration 16513, loss = 1.46269271\n",
      "Iteration 16514, loss = 1.46268970\n",
      "Iteration 16515, loss = 1.46268669\n",
      "Iteration 16516, loss = 1.46268369\n",
      "Iteration 16517, loss = 1.46268068\n",
      "Iteration 16518, loss = 1.46267767\n",
      "Iteration 16519, loss = 1.46267467\n",
      "Iteration 16520, loss = 1.46267166\n",
      "Iteration 16521, loss = 1.46266866\n",
      "Iteration 16522, loss = 1.46266565\n",
      "Iteration 16523, loss = 1.46266265\n",
      "Iteration 16524, loss = 1.46265965\n",
      "Iteration 16525, loss = 1.46265664\n",
      "Iteration 16526, loss = 1.46265364\n",
      "Iteration 16527, loss = 1.46265064\n",
      "Iteration 16528, loss = 1.46264764\n",
      "Iteration 16529, loss = 1.46264463\n",
      "Iteration 16530, loss = 1.46264163\n",
      "Iteration 16531, loss = 1.46263863\n",
      "Iteration 16532, loss = 1.46263563\n",
      "Iteration 16533, loss = 1.46263263\n",
      "Iteration 16534, loss = 1.46262963\n",
      "Iteration 16535, loss = 1.46262663\n",
      "Iteration 16536, loss = 1.46262364\n",
      "Iteration 16537, loss = 1.46262064\n",
      "Iteration 16538, loss = 1.46261764\n",
      "Iteration 16539, loss = 1.46261464\n",
      "Iteration 16540, loss = 1.46261165\n",
      "Iteration 16541, loss = 1.46260865\n",
      "Iteration 16542, loss = 1.46260565\n",
      "Iteration 16543, loss = 1.46260266\n",
      "Iteration 16544, loss = 1.46259966\n",
      "Iteration 16545, loss = 1.46259667\n",
      "Iteration 16546, loss = 1.46259367\n",
      "Iteration 16547, loss = 1.46259068\n",
      "Iteration 16548, loss = 1.46258768\n",
      "Iteration 16549, loss = 1.46258469\n",
      "Iteration 16550, loss = 1.46258170\n",
      "Iteration 16551, loss = 1.46257870\n",
      "Iteration 16552, loss = 1.46257571\n",
      "Iteration 16553, loss = 1.46257272\n",
      "Iteration 16554, loss = 1.46256973\n",
      "Iteration 16555, loss = 1.46256674\n",
      "Iteration 16556, loss = 1.46256375\n",
      "Iteration 16557, loss = 1.46256076\n",
      "Iteration 16558, loss = 1.46255777\n",
      "Iteration 16559, loss = 1.46255478\n",
      "Iteration 16560, loss = 1.46255179\n",
      "Iteration 16561, loss = 1.46254880\n",
      "Iteration 16562, loss = 1.46254581\n",
      "Iteration 16563, loss = 1.46254283\n",
      "Iteration 16564, loss = 1.46253984\n",
      "Iteration 16565, loss = 1.46253685\n",
      "Iteration 16566, loss = 1.46253387\n",
      "Iteration 16567, loss = 1.46253088\n",
      "Iteration 16568, loss = 1.46252789\n",
      "Iteration 16569, loss = 1.46252491\n",
      "Iteration 16570, loss = 1.46252192\n",
      "Iteration 16571, loss = 1.46251894\n",
      "Iteration 16572, loss = 1.46251595\n",
      "Iteration 16573, loss = 1.46251297\n",
      "Iteration 16574, loss = 1.46250999\n",
      "Iteration 16575, loss = 1.46250701\n",
      "Iteration 16576, loss = 1.46250402\n",
      "Iteration 16577, loss = 1.46250104\n",
      "Iteration 16578, loss = 1.46249806\n",
      "Iteration 16579, loss = 1.46249508\n",
      "Iteration 16580, loss = 1.46249210\n",
      "Iteration 16581, loss = 1.46248912\n",
      "Iteration 16582, loss = 1.46248614\n",
      "Iteration 16583, loss = 1.46248316\n",
      "Iteration 16584, loss = 1.46248018\n",
      "Iteration 16585, loss = 1.46247720\n",
      "Iteration 16586, loss = 1.46247422\n",
      "Iteration 16587, loss = 1.46247124\n",
      "Iteration 16588, loss = 1.46246827\n",
      "Iteration 16589, loss = 1.46246529\n",
      "Iteration 16590, loss = 1.46246231\n",
      "Iteration 16591, loss = 1.46245934\n",
      "Iteration 16592, loss = 1.46245636\n",
      "Iteration 16593, loss = 1.46245338\n",
      "Iteration 16594, loss = 1.46245041\n",
      "Iteration 16595, loss = 1.46244743\n",
      "Iteration 16596, loss = 1.46244446\n",
      "Iteration 16597, loss = 1.46244149\n",
      "Iteration 16598, loss = 1.46243851\n",
      "Iteration 16599, loss = 1.46243554\n",
      "Iteration 16600, loss = 1.46243257\n",
      "Iteration 16601, loss = 1.46242959\n",
      "Iteration 16602, loss = 1.46242662\n",
      "Iteration 16603, loss = 1.46242365\n",
      "Iteration 16604, loss = 1.46242068\n",
      "Iteration 16605, loss = 1.46241771\n",
      "Iteration 16606, loss = 1.46241474\n",
      "Iteration 16607, loss = 1.46241177\n",
      "Iteration 16608, loss = 1.46240880\n",
      "Iteration 16609, loss = 1.46240583\n",
      "Iteration 16610, loss = 1.46240286\n",
      "Iteration 16611, loss = 1.46239989\n",
      "Iteration 16612, loss = 1.46239692\n",
      "Iteration 16613, loss = 1.46239396\n",
      "Iteration 16614, loss = 1.46239099\n",
      "Iteration 16615, loss = 1.46238802\n",
      "Iteration 16616, loss = 1.46238506\n",
      "Iteration 16617, loss = 1.46238209\n",
      "Iteration 16618, loss = 1.46237913\n",
      "Iteration 16619, loss = 1.46237616\n",
      "Iteration 16620, loss = 1.46237320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16621, loss = 1.46237023\n",
      "Iteration 16622, loss = 1.46236727\n",
      "Iteration 16623, loss = 1.46236430\n",
      "Iteration 16624, loss = 1.46236134\n",
      "Iteration 16625, loss = 1.46235838\n",
      "Iteration 16626, loss = 1.46235542\n",
      "Iteration 16627, loss = 1.46235245\n",
      "Iteration 16628, loss = 1.46234949\n",
      "Iteration 16629, loss = 1.46234653\n",
      "Iteration 16630, loss = 1.46234357\n",
      "Iteration 16631, loss = 1.46234061\n",
      "Iteration 16632, loss = 1.46233765\n",
      "Iteration 16633, loss = 1.46233469\n",
      "Iteration 16634, loss = 1.46233173\n",
      "Iteration 16635, loss = 1.46232877\n",
      "Iteration 16636, loss = 1.46232581\n",
      "Iteration 16637, loss = 1.46232286\n",
      "Iteration 16638, loss = 1.46231990\n",
      "Iteration 16639, loss = 1.46231694\n",
      "Iteration 16640, loss = 1.46231399\n",
      "Iteration 16641, loss = 1.46231103\n",
      "Iteration 16642, loss = 1.46230807\n",
      "Iteration 16643, loss = 1.46230512\n",
      "Iteration 16644, loss = 1.46230216\n",
      "Iteration 16645, loss = 1.46229921\n",
      "Iteration 16646, loss = 1.46229625\n",
      "Iteration 16647, loss = 1.46229330\n",
      "Iteration 16648, loss = 1.46229035\n",
      "Iteration 16649, loss = 1.46228739\n",
      "Iteration 16650, loss = 1.46228444\n",
      "Iteration 16651, loss = 1.46228149\n",
      "Iteration 16652, loss = 1.46227854\n",
      "Iteration 16653, loss = 1.46227558\n",
      "Iteration 16654, loss = 1.46227263\n",
      "Iteration 16655, loss = 1.46226968\n",
      "Iteration 16656, loss = 1.46226673\n",
      "Iteration 16657, loss = 1.46226378\n",
      "Iteration 16658, loss = 1.46226083\n",
      "Iteration 16659, loss = 1.46225788\n",
      "Iteration 16660, loss = 1.46225494\n",
      "Iteration 16661, loss = 1.46225199\n",
      "Iteration 16662, loss = 1.46224904\n",
      "Iteration 16663, loss = 1.46224609\n",
      "Iteration 16664, loss = 1.46224314\n",
      "Iteration 16665, loss = 1.46224020\n",
      "Iteration 16666, loss = 1.46223725\n",
      "Iteration 16667, loss = 1.46223431\n",
      "Iteration 16668, loss = 1.46223136\n",
      "Iteration 16669, loss = 1.46222841\n",
      "Iteration 16670, loss = 1.46222547\n",
      "Iteration 16671, loss = 1.46222253\n",
      "Iteration 16672, loss = 1.46221958\n",
      "Iteration 16673, loss = 1.46221664\n",
      "Iteration 16674, loss = 1.46221369\n",
      "Iteration 16675, loss = 1.46221075\n",
      "Iteration 16676, loss = 1.46220781\n",
      "Iteration 16677, loss = 1.46220487\n",
      "Iteration 16678, loss = 1.46220193\n",
      "Iteration 16679, loss = 1.46219899\n",
      "Iteration 16680, loss = 1.46219604\n",
      "Iteration 16681, loss = 1.46219310\n",
      "Iteration 16682, loss = 1.46219016\n",
      "Iteration 16683, loss = 1.46218722\n",
      "Iteration 16684, loss = 1.46218429\n",
      "Iteration 16685, loss = 1.46218135\n",
      "Iteration 16686, loss = 1.46217841\n",
      "Iteration 16687, loss = 1.46217547\n",
      "Iteration 16688, loss = 1.46217253\n",
      "Iteration 16689, loss = 1.46216960\n",
      "Iteration 16690, loss = 1.46216666\n",
      "Iteration 16691, loss = 1.46216372\n",
      "Iteration 16692, loss = 1.46216079\n",
      "Iteration 16693, loss = 1.46215785\n",
      "Iteration 16694, loss = 1.46215492\n",
      "Iteration 16695, loss = 1.46215198\n",
      "Iteration 16696, loss = 1.46214905\n",
      "Iteration 16697, loss = 1.46214611\n",
      "Iteration 16698, loss = 1.46214318\n",
      "Iteration 16699, loss = 1.46214024\n",
      "Iteration 16700, loss = 1.46213731\n",
      "Iteration 16701, loss = 1.46213438\n",
      "Iteration 16702, loss = 1.46213145\n",
      "Iteration 16703, loss = 1.46212852\n",
      "Iteration 16704, loss = 1.46212558\n",
      "Iteration 16705, loss = 1.46212265\n",
      "Iteration 16706, loss = 1.46211972\n",
      "Iteration 16707, loss = 1.46211679\n",
      "Iteration 16708, loss = 1.46211386\n",
      "Iteration 16709, loss = 1.46211093\n",
      "Iteration 16710, loss = 1.46210800\n",
      "Iteration 16711, loss = 1.46210508\n",
      "Iteration 16712, loss = 1.46210215\n",
      "Iteration 16713, loss = 1.46209922\n",
      "Iteration 16714, loss = 1.46209629\n",
      "Iteration 16715, loss = 1.46209337\n",
      "Iteration 16716, loss = 1.46209044\n",
      "Iteration 16717, loss = 1.46208751\n",
      "Iteration 16718, loss = 1.46208459\n",
      "Iteration 16719, loss = 1.46208166\n",
      "Iteration 16720, loss = 1.46207874\n",
      "Iteration 16721, loss = 1.46207581\n",
      "Iteration 16722, loss = 1.46207289\n",
      "Iteration 16723, loss = 1.46206996\n",
      "Iteration 16724, loss = 1.46206704\n",
      "Iteration 16725, loss = 1.46206412\n",
      "Iteration 16726, loss = 1.46206120\n",
      "Iteration 16727, loss = 1.46205827\n",
      "Iteration 16728, loss = 1.46205535\n",
      "Iteration 16729, loss = 1.46205243\n",
      "Iteration 16730, loss = 1.46204951\n",
      "Iteration 16731, loss = 1.46204659\n",
      "Iteration 16732, loss = 1.46204367\n",
      "Iteration 16733, loss = 1.46204075\n",
      "Iteration 16734, loss = 1.46203783\n",
      "Iteration 16735, loss = 1.46203491\n",
      "Iteration 16736, loss = 1.46203199\n",
      "Iteration 16737, loss = 1.46202907\n",
      "Iteration 16738, loss = 1.46202616\n",
      "Iteration 16739, loss = 1.46202324\n",
      "Iteration 16740, loss = 1.46202032\n",
      "Iteration 16741, loss = 1.46201740\n",
      "Iteration 16742, loss = 1.46201449\n",
      "Iteration 16743, loss = 1.46201157\n",
      "Iteration 16744, loss = 1.46200866\n",
      "Iteration 16745, loss = 1.46200574\n",
      "Iteration 16746, loss = 1.46200283\n",
      "Iteration 16747, loss = 1.46199991\n",
      "Iteration 16748, loss = 1.46199700\n",
      "Iteration 16749, loss = 1.46199408\n",
      "Iteration 16750, loss = 1.46199117\n",
      "Iteration 16751, loss = 1.46198826\n",
      "Iteration 16752, loss = 1.46198535\n",
      "Iteration 16753, loss = 1.46198243\n",
      "Iteration 16754, loss = 1.46197952\n",
      "Iteration 16755, loss = 1.46197661\n",
      "Iteration 16756, loss = 1.46197370\n",
      "Iteration 16757, loss = 1.46197079\n",
      "Iteration 16758, loss = 1.46196788\n",
      "Iteration 16759, loss = 1.46196497\n",
      "Iteration 16760, loss = 1.46196206\n",
      "Iteration 16761, loss = 1.46195915\n",
      "Iteration 16762, loss = 1.46195624\n",
      "Iteration 16763, loss = 1.46195333\n",
      "Iteration 16764, loss = 1.46195043\n",
      "Iteration 16765, loss = 1.46194752\n",
      "Iteration 16766, loss = 1.46194461\n",
      "Iteration 16767, loss = 1.46194171\n",
      "Iteration 16768, loss = 1.46193880\n",
      "Iteration 16769, loss = 1.46193589\n",
      "Iteration 16770, loss = 1.46193299\n",
      "Iteration 16771, loss = 1.46193008\n",
      "Iteration 16772, loss = 1.46192718\n",
      "Iteration 16773, loss = 1.46192428\n",
      "Iteration 16774, loss = 1.46192137\n",
      "Iteration 16775, loss = 1.46191847\n",
      "Iteration 16776, loss = 1.46191556\n",
      "Iteration 16777, loss = 1.46191266\n",
      "Iteration 16778, loss = 1.46190976\n",
      "Iteration 16779, loss = 1.46190686\n",
      "Iteration 16780, loss = 1.46190396\n",
      "Iteration 16781, loss = 1.46190106\n",
      "Iteration 16782, loss = 1.46189815\n",
      "Iteration 16783, loss = 1.46189525\n",
      "Iteration 16784, loss = 1.46189235\n",
      "Iteration 16785, loss = 1.46188945\n",
      "Iteration 16786, loss = 1.46188656\n",
      "Iteration 16787, loss = 1.46188366\n",
      "Iteration 16788, loss = 1.46188076\n",
      "Iteration 16789, loss = 1.46187786\n",
      "Iteration 16790, loss = 1.46187496\n",
      "Iteration 16791, loss = 1.46187207\n",
      "Iteration 16792, loss = 1.46186917\n",
      "Iteration 16793, loss = 1.46186627\n",
      "Iteration 16794, loss = 1.46186338\n",
      "Iteration 16795, loss = 1.46186048\n",
      "Iteration 16796, loss = 1.46185759\n",
      "Iteration 16797, loss = 1.46185469\n",
      "Iteration 16798, loss = 1.46185180\n",
      "Iteration 16799, loss = 1.46184890\n",
      "Iteration 16800, loss = 1.46184601\n",
      "Iteration 16801, loss = 1.46184312\n",
      "Iteration 16802, loss = 1.46184022\n",
      "Iteration 16803, loss = 1.46183733\n",
      "Iteration 16804, loss = 1.46183444\n",
      "Iteration 16805, loss = 1.46183155\n",
      "Iteration 16806, loss = 1.46182865\n",
      "Iteration 16807, loss = 1.46182576\n",
      "Iteration 16808, loss = 1.46182287\n",
      "Iteration 16809, loss = 1.46181998\n",
      "Iteration 16810, loss = 1.46181709\n",
      "Iteration 16811, loss = 1.46181420\n",
      "Iteration 16812, loss = 1.46181131\n",
      "Iteration 16813, loss = 1.46180843\n",
      "Iteration 16814, loss = 1.46180554\n",
      "Iteration 16815, loss = 1.46180265\n",
      "Iteration 16816, loss = 1.46179976\n",
      "Iteration 16817, loss = 1.46179687\n",
      "Iteration 16818, loss = 1.46179399\n",
      "Iteration 16819, loss = 1.46179110\n",
      "Iteration 16820, loss = 1.46178822\n",
      "Iteration 16821, loss = 1.46178533\n",
      "Iteration 16822, loss = 1.46178244\n",
      "Iteration 16823, loss = 1.46177956\n",
      "Iteration 16824, loss = 1.46177668\n",
      "Iteration 16825, loss = 1.46177379\n",
      "Iteration 16826, loss = 1.46177091\n",
      "Iteration 16827, loss = 1.46176802\n",
      "Iteration 16828, loss = 1.46176514\n",
      "Iteration 16829, loss = 1.46176226\n",
      "Iteration 16830, loss = 1.46175938\n",
      "Iteration 16831, loss = 1.46175650\n",
      "Iteration 16832, loss = 1.46175361\n",
      "Iteration 16833, loss = 1.46175073\n",
      "Iteration 16834, loss = 1.46174785\n",
      "Iteration 16835, loss = 1.46174497\n",
      "Iteration 16836, loss = 1.46174209\n",
      "Iteration 16837, loss = 1.46173921\n",
      "Iteration 16838, loss = 1.46173633\n",
      "Iteration 16839, loss = 1.46173346\n",
      "Iteration 16840, loss = 1.46173058\n",
      "Iteration 16841, loss = 1.46172770\n",
      "Iteration 16842, loss = 1.46172482\n",
      "Iteration 16843, loss = 1.46172194\n",
      "Iteration 16844, loss = 1.46171907\n",
      "Iteration 16845, loss = 1.46171619\n",
      "Iteration 16846, loss = 1.46171332\n",
      "Iteration 16847, loss = 1.46171044\n",
      "Iteration 16848, loss = 1.46170756\n",
      "Iteration 16849, loss = 1.46170469\n",
      "Iteration 16850, loss = 1.46170182\n",
      "Iteration 16851, loss = 1.46169894\n",
      "Iteration 16852, loss = 1.46169607\n",
      "Iteration 16853, loss = 1.46169319\n",
      "Iteration 16854, loss = 1.46169032\n",
      "Iteration 16855, loss = 1.46168745\n",
      "Iteration 16856, loss = 1.46168458\n",
      "Iteration 16857, loss = 1.46168171\n",
      "Iteration 16858, loss = 1.46167883\n",
      "Iteration 16859, loss = 1.46167596\n",
      "Iteration 16860, loss = 1.46167309\n",
      "Iteration 16861, loss = 1.46167022\n",
      "Iteration 16862, loss = 1.46166735\n",
      "Iteration 16863, loss = 1.46166448\n",
      "Iteration 16864, loss = 1.46166161\n",
      "Iteration 16865, loss = 1.46165875\n",
      "Iteration 16866, loss = 1.46165588\n",
      "Iteration 16867, loss = 1.46165301\n",
      "Iteration 16868, loss = 1.46165014\n",
      "Iteration 16869, loss = 1.46164727\n",
      "Iteration 16870, loss = 1.46164441\n",
      "Iteration 16871, loss = 1.46164154\n",
      "Iteration 16872, loss = 1.46163868\n",
      "Iteration 16873, loss = 1.46163581\n",
      "Iteration 16874, loss = 1.46163294\n",
      "Iteration 16875, loss = 1.46163008\n",
      "Iteration 16876, loss = 1.46162722\n",
      "Iteration 16877, loss = 1.46162435\n",
      "Iteration 16878, loss = 1.46162149\n",
      "Iteration 16879, loss = 1.46161862\n",
      "Iteration 16880, loss = 1.46161576\n",
      "Iteration 16881, loss = 1.46161290\n",
      "Iteration 16882, loss = 1.46161004\n",
      "Iteration 16883, loss = 1.46160717\n",
      "Iteration 16884, loss = 1.46160431\n",
      "Iteration 16885, loss = 1.46160145\n",
      "Iteration 16886, loss = 1.46159859\n",
      "Iteration 16887, loss = 1.46159573\n",
      "Iteration 16888, loss = 1.46159287\n",
      "Iteration 16889, loss = 1.46159001\n",
      "Iteration 16890, loss = 1.46158715\n",
      "Iteration 16891, loss = 1.46158429\n",
      "Iteration 16892, loss = 1.46158144\n",
      "Iteration 16893, loss = 1.46157858\n",
      "Iteration 16894, loss = 1.46157572\n",
      "Iteration 16895, loss = 1.46157286\n",
      "Iteration 16896, loss = 1.46157001\n",
      "Iteration 16897, loss = 1.46156715\n",
      "Iteration 16898, loss = 1.46156429\n",
      "Iteration 16899, loss = 1.46156144\n",
      "Iteration 16900, loss = 1.46155858\n",
      "Iteration 16901, loss = 1.46155573\n",
      "Iteration 16902, loss = 1.46155287\n",
      "Iteration 16903, loss = 1.46155002\n",
      "Iteration 16904, loss = 1.46154717\n",
      "Iteration 16905, loss = 1.46154431\n",
      "Iteration 16906, loss = 1.46154146\n",
      "Iteration 16907, loss = 1.46153861\n",
      "Iteration 16908, loss = 1.46153575\n",
      "Iteration 16909, loss = 1.46153290\n",
      "Iteration 16910, loss = 1.46153005\n",
      "Iteration 16911, loss = 1.46152720\n",
      "Iteration 16912, loss = 1.46152435\n",
      "Iteration 16913, loss = 1.46152150\n",
      "Iteration 16914, loss = 1.46151865\n",
      "Iteration 16915, loss = 1.46151580\n",
      "Iteration 16916, loss = 1.46151295\n",
      "Iteration 16917, loss = 1.46151010\n",
      "Iteration 16918, loss = 1.46150725\n",
      "Iteration 16919, loss = 1.46150440\n",
      "Iteration 16920, loss = 1.46150156\n",
      "Iteration 16921, loss = 1.46149871\n",
      "Iteration 16922, loss = 1.46149586\n",
      "Iteration 16923, loss = 1.46149302\n",
      "Iteration 16924, loss = 1.46149017\n",
      "Iteration 16925, loss = 1.46148732\n",
      "Iteration 16926, loss = 1.46148448\n",
      "Iteration 16927, loss = 1.46148163\n",
      "Iteration 16928, loss = 1.46147879\n",
      "Iteration 16929, loss = 1.46147594\n",
      "Iteration 16930, loss = 1.46147310\n",
      "Iteration 16931, loss = 1.46147026\n",
      "Iteration 16932, loss = 1.46146741\n",
      "Iteration 16933, loss = 1.46146457\n",
      "Iteration 16934, loss = 1.46146173\n",
      "Iteration 16935, loss = 1.46145889\n",
      "Iteration 16936, loss = 1.46145605\n",
      "Iteration 16937, loss = 1.46145320\n",
      "Iteration 16938, loss = 1.46145036\n",
      "Iteration 16939, loss = 1.46144752\n",
      "Iteration 16940, loss = 1.46144468\n",
      "Iteration 16941, loss = 1.46144184\n",
      "Iteration 16942, loss = 1.46143900\n",
      "Iteration 16943, loss = 1.46143616\n",
      "Iteration 16944, loss = 1.46143333\n",
      "Iteration 16945, loss = 1.46143049\n",
      "Iteration 16946, loss = 1.46142765\n",
      "Iteration 16947, loss = 1.46142481\n",
      "Iteration 16948, loss = 1.46142198\n",
      "Iteration 16949, loss = 1.46141914\n",
      "Iteration 16950, loss = 1.46141630\n",
      "Iteration 16951, loss = 1.46141347\n",
      "Iteration 16952, loss = 1.46141063\n",
      "Iteration 16953, loss = 1.46140780\n",
      "Iteration 16954, loss = 1.46140496\n",
      "Iteration 16955, loss = 1.46140213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16956, loss = 1.46139929\n",
      "Iteration 16957, loss = 1.46139646\n",
      "Iteration 16958, loss = 1.46139363\n",
      "Iteration 16959, loss = 1.46139079\n",
      "Iteration 16960, loss = 1.46138796\n",
      "Iteration 16961, loss = 1.46138513\n",
      "Iteration 16962, loss = 1.46138230\n",
      "Iteration 16963, loss = 1.46137946\n",
      "Iteration 16964, loss = 1.46137663\n",
      "Iteration 16965, loss = 1.46137380\n",
      "Iteration 16966, loss = 1.46137097\n",
      "Iteration 16967, loss = 1.46136814\n",
      "Iteration 16968, loss = 1.46136531\n",
      "Iteration 16969, loss = 1.46136248\n",
      "Iteration 16970, loss = 1.46135965\n",
      "Iteration 16971, loss = 1.46135683\n",
      "Iteration 16972, loss = 1.46135400\n",
      "Iteration 16973, loss = 1.46135117\n",
      "Iteration 16974, loss = 1.46134834\n",
      "Iteration 16975, loss = 1.46134552\n",
      "Iteration 16976, loss = 1.46134269\n",
      "Iteration 16977, loss = 1.46133986\n",
      "Iteration 16978, loss = 1.46133704\n",
      "Iteration 16979, loss = 1.46133421\n",
      "Iteration 16980, loss = 1.46133139\n",
      "Iteration 16981, loss = 1.46132856\n",
      "Iteration 16982, loss = 1.46132574\n",
      "Iteration 16983, loss = 1.46132291\n",
      "Iteration 16984, loss = 1.46132009\n",
      "Iteration 16985, loss = 1.46131727\n",
      "Iteration 16986, loss = 1.46131444\n",
      "Iteration 16987, loss = 1.46131162\n",
      "Iteration 16988, loss = 1.46130880\n",
      "Iteration 16989, loss = 1.46130598\n",
      "Iteration 16990, loss = 1.46130316\n",
      "Iteration 16991, loss = 1.46130034\n",
      "Iteration 16992, loss = 1.46129752\n",
      "Iteration 16993, loss = 1.46129470\n",
      "Iteration 16994, loss = 1.46129188\n",
      "Iteration 16995, loss = 1.46128906\n",
      "Iteration 16996, loss = 1.46128624\n",
      "Iteration 16997, loss = 1.46128342\n",
      "Iteration 16998, loss = 1.46128060\n",
      "Iteration 16999, loss = 1.46127778\n",
      "Iteration 17000, loss = 1.46127496\n",
      "Iteration 17001, loss = 1.46127215\n",
      "Iteration 17002, loss = 1.46126933\n",
      "Iteration 17003, loss = 1.46126651\n",
      "Iteration 17004, loss = 1.46126370\n",
      "Iteration 17005, loss = 1.46126088\n",
      "Iteration 17006, loss = 1.46125807\n",
      "Iteration 17007, loss = 1.46125525\n",
      "Iteration 17008, loss = 1.46125244\n",
      "Iteration 17009, loss = 1.46124962\n",
      "Iteration 17010, loss = 1.46124681\n",
      "Iteration 17011, loss = 1.46124400\n",
      "Iteration 17012, loss = 1.46124118\n",
      "Iteration 17013, loss = 1.46123837\n",
      "Iteration 17014, loss = 1.46123556\n",
      "Iteration 17015, loss = 1.46123275\n",
      "Iteration 17016, loss = 1.46122993\n",
      "Iteration 17017, loss = 1.46122712\n",
      "Iteration 17018, loss = 1.46122431\n",
      "Iteration 17019, loss = 1.46122150\n",
      "Iteration 17020, loss = 1.46121869\n",
      "Iteration 17021, loss = 1.46121588\n",
      "Iteration 17022, loss = 1.46121307\n",
      "Iteration 17023, loss = 1.46121026\n",
      "Iteration 17024, loss = 1.46120746\n",
      "Iteration 17025, loss = 1.46120465\n",
      "Iteration 17026, loss = 1.46120184\n",
      "Iteration 17027, loss = 1.46119903\n",
      "Iteration 17028, loss = 1.46119622\n",
      "Iteration 17029, loss = 1.46119342\n",
      "Iteration 17030, loss = 1.46119061\n",
      "Iteration 17031, loss = 1.46118781\n",
      "Iteration 17032, loss = 1.46118500\n",
      "Iteration 17033, loss = 1.46118219\n",
      "Iteration 17034, loss = 1.46117939\n",
      "Iteration 17035, loss = 1.46117659\n",
      "Iteration 17036, loss = 1.46117378\n",
      "Iteration 17037, loss = 1.46117098\n",
      "Iteration 17038, loss = 1.46116817\n",
      "Iteration 17039, loss = 1.46116537\n",
      "Iteration 17040, loss = 1.46116257\n",
      "Iteration 17041, loss = 1.46115977\n",
      "Iteration 17042, loss = 1.46115696\n",
      "Iteration 17043, loss = 1.46115416\n",
      "Iteration 17044, loss = 1.46115136\n",
      "Iteration 17045, loss = 1.46114856\n",
      "Iteration 17046, loss = 1.46114576\n",
      "Iteration 17047, loss = 1.46114296\n",
      "Iteration 17048, loss = 1.46114016\n",
      "Iteration 17049, loss = 1.46113736\n",
      "Iteration 17050, loss = 1.46113456\n",
      "Iteration 17051, loss = 1.46113176\n",
      "Iteration 17052, loss = 1.46112897\n",
      "Iteration 17053, loss = 1.46112617\n",
      "Iteration 17054, loss = 1.46112337\n",
      "Iteration 17055, loss = 1.46112057\n",
      "Iteration 17056, loss = 1.46111778\n",
      "Iteration 17057, loss = 1.46111498\n",
      "Iteration 17058, loss = 1.46111218\n",
      "Iteration 17059, loss = 1.46110939\n",
      "Iteration 17060, loss = 1.46110659\n",
      "Iteration 17061, loss = 1.46110380\n",
      "Iteration 17062, loss = 1.46110100\n",
      "Iteration 17063, loss = 1.46109821\n",
      "Iteration 17064, loss = 1.46109542\n",
      "Iteration 17065, loss = 1.46109262\n",
      "Iteration 17066, loss = 1.46108983\n",
      "Iteration 17067, loss = 1.46108704\n",
      "Iteration 17068, loss = 1.46108424\n",
      "Iteration 17069, loss = 1.46108145\n",
      "Iteration 17070, loss = 1.46107866\n",
      "Iteration 17071, loss = 1.46107587\n",
      "Iteration 17072, loss = 1.46107308\n",
      "Iteration 17073, loss = 1.46107029\n",
      "Iteration 17074, loss = 1.46106750\n",
      "Iteration 17075, loss = 1.46106471\n",
      "Iteration 17076, loss = 1.46106192\n",
      "Iteration 17077, loss = 1.46105913\n",
      "Iteration 17078, loss = 1.46105634\n",
      "Iteration 17079, loss = 1.46105355\n",
      "Iteration 17080, loss = 1.46105077\n",
      "Iteration 17081, loss = 1.46104798\n",
      "Iteration 17082, loss = 1.46104519\n",
      "Iteration 17083, loss = 1.46104240\n",
      "Iteration 17084, loss = 1.46103962\n",
      "Iteration 17085, loss = 1.46103683\n",
      "Iteration 17086, loss = 1.46103405\n",
      "Iteration 17087, loss = 1.46103126\n",
      "Iteration 17088, loss = 1.46102848\n",
      "Iteration 17089, loss = 1.46102569\n",
      "Iteration 17090, loss = 1.46102291\n",
      "Iteration 17091, loss = 1.46102012\n",
      "Iteration 17092, loss = 1.46101734\n",
      "Iteration 17093, loss = 1.46101456\n",
      "Iteration 17094, loss = 1.46101177\n",
      "Iteration 17095, loss = 1.46100899\n",
      "Iteration 17096, loss = 1.46100621\n",
      "Iteration 17097, loss = 1.46100343\n",
      "Iteration 17098, loss = 1.46100065\n",
      "Iteration 17099, loss = 1.46099787\n",
      "Iteration 17100, loss = 1.46099509\n",
      "Iteration 17101, loss = 1.46099231\n",
      "Iteration 17102, loss = 1.46098953\n",
      "Iteration 17103, loss = 1.46098675\n",
      "Iteration 17104, loss = 1.46098397\n",
      "Iteration 17105, loss = 1.46098119\n",
      "Iteration 17106, loss = 1.46097841\n",
      "Iteration 17107, loss = 1.46097563\n",
      "Iteration 17108, loss = 1.46097285\n",
      "Iteration 17109, loss = 1.46097008\n",
      "Iteration 17110, loss = 1.46096730\n",
      "Iteration 17111, loss = 1.46096452\n",
      "Iteration 17112, loss = 1.46096175\n",
      "Iteration 17113, loss = 1.46095897\n",
      "Iteration 17114, loss = 1.46095620\n",
      "Iteration 17115, loss = 1.46095342\n",
      "Iteration 17116, loss = 1.46095065\n",
      "Iteration 17117, loss = 1.46094787\n",
      "Iteration 17118, loss = 1.46094510\n",
      "Iteration 17119, loss = 1.46094233\n",
      "Iteration 17120, loss = 1.46093955\n",
      "Iteration 17121, loss = 1.46093678\n",
      "Iteration 17122, loss = 1.46093401\n",
      "Iteration 17123, loss = 1.46093123\n",
      "Iteration 17124, loss = 1.46092846\n",
      "Iteration 17125, loss = 1.46092569\n",
      "Iteration 17126, loss = 1.46092292\n",
      "Iteration 17127, loss = 1.46092015\n",
      "Iteration 17128, loss = 1.46091738\n",
      "Iteration 17129, loss = 1.46091461\n",
      "Iteration 17130, loss = 1.46091184\n",
      "Iteration 17131, loss = 1.46090907\n",
      "Iteration 17132, loss = 1.46090630\n",
      "Iteration 17133, loss = 1.46090353\n",
      "Iteration 17134, loss = 1.46090077\n",
      "Iteration 17135, loss = 1.46089800\n",
      "Iteration 17136, loss = 1.46089523\n",
      "Iteration 17137, loss = 1.46089246\n",
      "Iteration 17138, loss = 1.46088970\n",
      "Iteration 17139, loss = 1.46088693\n",
      "Iteration 17140, loss = 1.46088417\n",
      "Iteration 17141, loss = 1.46088140\n",
      "Iteration 17142, loss = 1.46087863\n",
      "Iteration 17143, loss = 1.46087587\n",
      "Iteration 17144, loss = 1.46087311\n",
      "Iteration 17145, loss = 1.46087034\n",
      "Iteration 17146, loss = 1.46086758\n",
      "Iteration 17147, loss = 1.46086481\n",
      "Iteration 17148, loss = 1.46086205\n",
      "Iteration 17149, loss = 1.46085929\n",
      "Iteration 17150, loss = 1.46085653\n",
      "Iteration 17151, loss = 1.46085376\n",
      "Iteration 17152, loss = 1.46085100\n",
      "Iteration 17153, loss = 1.46084824\n",
      "Iteration 17154, loss = 1.46084548\n",
      "Iteration 17155, loss = 1.46084272\n",
      "Iteration 17156, loss = 1.46083996\n",
      "Iteration 17157, loss = 1.46083720\n",
      "Iteration 17158, loss = 1.46083444\n",
      "Iteration 17159, loss = 1.46083168\n",
      "Iteration 17160, loss = 1.46082892\n",
      "Iteration 17161, loss = 1.46082617\n",
      "Iteration 17162, loss = 1.46082341\n",
      "Iteration 17163, loss = 1.46082065\n",
      "Iteration 17164, loss = 1.46081789\n",
      "Iteration 17165, loss = 1.46081514\n",
      "Iteration 17166, loss = 1.46081238\n",
      "Iteration 17167, loss = 1.46080962\n",
      "Iteration 17168, loss = 1.46080687\n",
      "Iteration 17169, loss = 1.46080411\n",
      "Iteration 17170, loss = 1.46080136\n",
      "Iteration 17171, loss = 1.46079860\n",
      "Iteration 17172, loss = 1.46079585\n",
      "Iteration 17173, loss = 1.46079310\n",
      "Iteration 17174, loss = 1.46079034\n",
      "Iteration 17175, loss = 1.46078759\n",
      "Iteration 17176, loss = 1.46078484\n",
      "Iteration 17177, loss = 1.46078208\n",
      "Iteration 17178, loss = 1.46077933\n",
      "Iteration 17179, loss = 1.46077658\n",
      "Iteration 17180, loss = 1.46077383\n",
      "Iteration 17181, loss = 1.46077108\n",
      "Iteration 17182, loss = 1.46076833\n",
      "Iteration 17183, loss = 1.46076558\n",
      "Iteration 17184, loss = 1.46076283\n",
      "Iteration 17185, loss = 1.46076008\n",
      "Iteration 17186, loss = 1.46075733\n",
      "Iteration 17187, loss = 1.46075458\n",
      "Iteration 17188, loss = 1.46075183\n",
      "Iteration 17189, loss = 1.46074908\n",
      "Iteration 17190, loss = 1.46074633\n",
      "Iteration 17191, loss = 1.46074359\n",
      "Iteration 17192, loss = 1.46074084\n",
      "Iteration 17193, loss = 1.46073809\n",
      "Iteration 17194, loss = 1.46073535\n",
      "Iteration 17195, loss = 1.46073260\n",
      "Iteration 17196, loss = 1.46072986\n",
      "Iteration 17197, loss = 1.46072711\n",
      "Iteration 17198, loss = 1.46072437\n",
      "Iteration 17199, loss = 1.46072162\n",
      "Iteration 17200, loss = 1.46071888\n",
      "Iteration 17201, loss = 1.46071613\n",
      "Iteration 17202, loss = 1.46071339\n",
      "Iteration 17203, loss = 1.46071065\n",
      "Iteration 17204, loss = 1.46070790\n",
      "Iteration 17205, loss = 1.46070516\n",
      "Iteration 17206, loss = 1.46070242\n",
      "Iteration 17207, loss = 1.46069968\n",
      "Iteration 17208, loss = 1.46069694\n",
      "Iteration 17209, loss = 1.46069420\n",
      "Iteration 17210, loss = 1.46069146\n",
      "Iteration 17211, loss = 1.46068872\n",
      "Iteration 17212, loss = 1.46068598\n",
      "Iteration 17213, loss = 1.46068324\n",
      "Iteration 17214, loss = 1.46068050\n",
      "Iteration 17215, loss = 1.46067776\n",
      "Iteration 17216, loss = 1.46067502\n",
      "Iteration 17217, loss = 1.46067228\n",
      "Iteration 17218, loss = 1.46066954\n",
      "Iteration 17219, loss = 1.46066681\n",
      "Iteration 17220, loss = 1.46066407\n",
      "Iteration 17221, loss = 1.46066133\n",
      "Iteration 17222, loss = 1.46065860\n",
      "Iteration 17223, loss = 1.46065586\n",
      "Iteration 17224, loss = 1.46065313\n",
      "Iteration 17225, loss = 1.46065039\n",
      "Iteration 17226, loss = 1.46064766\n",
      "Iteration 17227, loss = 1.46064492\n",
      "Iteration 17228, loss = 1.46064219\n",
      "Iteration 17229, loss = 1.46063945\n",
      "Iteration 17230, loss = 1.46063672\n",
      "Iteration 17231, loss = 1.46063399\n",
      "Iteration 17232, loss = 1.46063125\n",
      "Iteration 17233, loss = 1.46062852\n",
      "Iteration 17234, loss = 1.46062579\n",
      "Iteration 17235, loss = 1.46062306\n",
      "Iteration 17236, loss = 1.46062033\n",
      "Iteration 17237, loss = 1.46061760\n",
      "Iteration 17238, loss = 1.46061487\n",
      "Iteration 17239, loss = 1.46061214\n",
      "Iteration 17240, loss = 1.46060941\n",
      "Iteration 17241, loss = 1.46060668\n",
      "Iteration 17242, loss = 1.46060395\n",
      "Iteration 17243, loss = 1.46060122\n",
      "Iteration 17244, loss = 1.46059849\n",
      "Iteration 17245, loss = 1.46059576\n",
      "Iteration 17246, loss = 1.46059304\n",
      "Iteration 17247, loss = 1.46059031\n",
      "Iteration 17248, loss = 1.46058758\n",
      "Iteration 17249, loss = 1.46058485\n",
      "Iteration 17250, loss = 1.46058213\n",
      "Iteration 17251, loss = 1.46057940\n",
      "Iteration 17252, loss = 1.46057668\n",
      "Iteration 17253, loss = 1.46057395\n",
      "Iteration 17254, loss = 1.46057123\n",
      "Iteration 17255, loss = 1.46056850\n",
      "Iteration 17256, loss = 1.46056578\n",
      "Iteration 17257, loss = 1.46056306\n",
      "Iteration 17258, loss = 1.46056033\n",
      "Iteration 17259, loss = 1.46055761\n",
      "Iteration 17260, loss = 1.46055489\n",
      "Iteration 17261, loss = 1.46055216\n",
      "Iteration 17262, loss = 1.46054944\n",
      "Iteration 17263, loss = 1.46054672\n",
      "Iteration 17264, loss = 1.46054400\n",
      "Iteration 17265, loss = 1.46054128\n",
      "Iteration 17266, loss = 1.46053856\n",
      "Iteration 17267, loss = 1.46053584\n",
      "Iteration 17268, loss = 1.46053312\n",
      "Iteration 17269, loss = 1.46053040\n",
      "Iteration 17270, loss = 1.46052768\n",
      "Iteration 17271, loss = 1.46052496\n",
      "Iteration 17272, loss = 1.46052224\n",
      "Iteration 17273, loss = 1.46051952\n",
      "Iteration 17274, loss = 1.46051681\n",
      "Iteration 17275, loss = 1.46051409\n",
      "Iteration 17276, loss = 1.46051137\n",
      "Iteration 17277, loss = 1.46050866\n",
      "Iteration 17278, loss = 1.46050594\n",
      "Iteration 17279, loss = 1.46050322\n",
      "Iteration 17280, loss = 1.46050051\n",
      "Iteration 17281, loss = 1.46049779\n",
      "Iteration 17282, loss = 1.46049508\n",
      "Iteration 17283, loss = 1.46049236\n",
      "Iteration 17284, loss = 1.46048965\n",
      "Iteration 17285, loss = 1.46048694\n",
      "Iteration 17286, loss = 1.46048422\n",
      "Iteration 17287, loss = 1.46048151\n",
      "Iteration 17288, loss = 1.46047880\n",
      "Iteration 17289, loss = 1.46047609\n",
      "Iteration 17290, loss = 1.46047337\n",
      "Iteration 17291, loss = 1.46047066\n",
      "Iteration 17292, loss = 1.46046795\n",
      "Iteration 17293, loss = 1.46046524\n",
      "Iteration 17294, loss = 1.46046253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17295, loss = 1.46045982\n",
      "Iteration 17296, loss = 1.46045711\n",
      "Iteration 17297, loss = 1.46045440\n",
      "Iteration 17298, loss = 1.46045169\n",
      "Iteration 17299, loss = 1.46044898\n",
      "Iteration 17300, loss = 1.46044627\n",
      "Iteration 17301, loss = 1.46044357\n",
      "Iteration 17302, loss = 1.46044086\n",
      "Iteration 17303, loss = 1.46043815\n",
      "Iteration 17304, loss = 1.46043544\n",
      "Iteration 17305, loss = 1.46043274\n",
      "Iteration 17306, loss = 1.46043003\n",
      "Iteration 17307, loss = 1.46042732\n",
      "Iteration 17308, loss = 1.46042462\n",
      "Iteration 17309, loss = 1.46042191\n",
      "Iteration 17310, loss = 1.46041921\n",
      "Iteration 17311, loss = 1.46041650\n",
      "Iteration 17312, loss = 1.46041380\n",
      "Iteration 17313, loss = 1.46041110\n",
      "Iteration 17314, loss = 1.46040839\n",
      "Iteration 17315, loss = 1.46040569\n",
      "Iteration 17316, loss = 1.46040299\n",
      "Iteration 17317, loss = 1.46040029\n",
      "Iteration 17318, loss = 1.46039758\n",
      "Iteration 17319, loss = 1.46039488\n",
      "Iteration 17320, loss = 1.46039218\n",
      "Iteration 17321, loss = 1.46038948\n",
      "Iteration 17322, loss = 1.46038678\n",
      "Iteration 17323, loss = 1.46038408\n",
      "Iteration 17324, loss = 1.46038138\n",
      "Iteration 17325, loss = 1.46037868\n",
      "Iteration 17326, loss = 1.46037598\n",
      "Iteration 17327, loss = 1.46037328\n",
      "Iteration 17328, loss = 1.46037058\n",
      "Iteration 17329, loss = 1.46036788\n",
      "Iteration 17330, loss = 1.46036519\n",
      "Iteration 17331, loss = 1.46036249\n",
      "Iteration 17332, loss = 1.46035979\n",
      "Iteration 17333, loss = 1.46035709\n",
      "Iteration 17334, loss = 1.46035440\n",
      "Iteration 17335, loss = 1.46035170\n",
      "Iteration 17336, loss = 1.46034901\n",
      "Iteration 17337, loss = 1.46034631\n",
      "Iteration 17338, loss = 1.46034362\n",
      "Iteration 17339, loss = 1.46034092\n",
      "Iteration 17340, loss = 1.46033823\n",
      "Iteration 17341, loss = 1.46033553\n",
      "Iteration 17342, loss = 1.46033284\n",
      "Iteration 17343, loss = 1.46033015\n",
      "Iteration 17344, loss = 1.46032745\n",
      "Iteration 17345, loss = 1.46032476\n",
      "Iteration 17346, loss = 1.46032207\n",
      "Iteration 17347, loss = 1.46031938\n",
      "Iteration 17348, loss = 1.46031669\n",
      "Iteration 17349, loss = 1.46031399\n",
      "Iteration 17350, loss = 1.46031130\n",
      "Iteration 17351, loss = 1.46030861\n",
      "Iteration 17352, loss = 1.46030592\n",
      "Iteration 17353, loss = 1.46030323\n",
      "Iteration 17354, loss = 1.46030054\n",
      "Iteration 17355, loss = 1.46029786\n",
      "Iteration 17356, loss = 1.46029517\n",
      "Iteration 17357, loss = 1.46029248\n",
      "Iteration 17358, loss = 1.46028979\n",
      "Iteration 17359, loss = 1.46028710\n",
      "Iteration 17360, loss = 1.46028442\n",
      "Iteration 17361, loss = 1.46028173\n",
      "Iteration 17362, loss = 1.46027904\n",
      "Iteration 17363, loss = 1.46027636\n",
      "Iteration 17364, loss = 1.46027367\n",
      "Iteration 17365, loss = 1.46027098\n",
      "Iteration 17366, loss = 1.46026830\n",
      "Iteration 17367, loss = 1.46026561\n",
      "Iteration 17368, loss = 1.46026293\n",
      "Iteration 17369, loss = 1.46026025\n",
      "Iteration 17370, loss = 1.46025756\n",
      "Iteration 17371, loss = 1.46025488\n",
      "Iteration 17372, loss = 1.46025220\n",
      "Iteration 17373, loss = 1.46024951\n",
      "Iteration 17374, loss = 1.46024683\n",
      "Iteration 17375, loss = 1.46024415\n",
      "Iteration 17376, loss = 1.46024147\n",
      "Iteration 17377, loss = 1.46023879\n",
      "Iteration 17378, loss = 1.46023610\n",
      "Iteration 17379, loss = 1.46023342\n",
      "Iteration 17380, loss = 1.46023074\n",
      "Iteration 17381, loss = 1.46022806\n",
      "Iteration 17382, loss = 1.46022538\n",
      "Iteration 17383, loss = 1.46022270\n",
      "Iteration 17384, loss = 1.46022003\n",
      "Iteration 17385, loss = 1.46021735\n",
      "Iteration 17386, loss = 1.46021467\n",
      "Iteration 17387, loss = 1.46021199\n",
      "Iteration 17388, loss = 1.46020931\n",
      "Iteration 17389, loss = 1.46020664\n",
      "Iteration 17390, loss = 1.46020396\n",
      "Iteration 17391, loss = 1.46020128\n",
      "Iteration 17392, loss = 1.46019861\n",
      "Iteration 17393, loss = 1.46019593\n",
      "Iteration 17394, loss = 1.46019326\n",
      "Iteration 17395, loss = 1.46019058\n",
      "Iteration 17396, loss = 1.46018791\n",
      "Iteration 17397, loss = 1.46018523\n",
      "Iteration 17398, loss = 1.46018256\n",
      "Iteration 17399, loss = 1.46017988\n",
      "Iteration 17400, loss = 1.46017721\n",
      "Iteration 17401, loss = 1.46017454\n",
      "Iteration 17402, loss = 1.46017187\n",
      "Iteration 17403, loss = 1.46016919\n",
      "Iteration 17404, loss = 1.46016652\n",
      "Iteration 17405, loss = 1.46016385\n",
      "Iteration 17406, loss = 1.46016118\n",
      "Iteration 17407, loss = 1.46015851\n",
      "Iteration 17408, loss = 1.46015584\n",
      "Iteration 17409, loss = 1.46015317\n",
      "Iteration 17410, loss = 1.46015050\n",
      "Iteration 17411, loss = 1.46014783\n",
      "Iteration 17412, loss = 1.46014516\n",
      "Iteration 17413, loss = 1.46014249\n",
      "Iteration 17414, loss = 1.46013982\n",
      "Iteration 17415, loss = 1.46013715\n",
      "Iteration 17416, loss = 1.46013449\n",
      "Iteration 17417, loss = 1.46013182\n",
      "Iteration 17418, loss = 1.46012915\n",
      "Iteration 17419, loss = 1.46012648\n",
      "Iteration 17420, loss = 1.46012382\n",
      "Iteration 17421, loss = 1.46012115\n",
      "Iteration 17422, loss = 1.46011849\n",
      "Iteration 17423, loss = 1.46011582\n",
      "Iteration 17424, loss = 1.46011316\n",
      "Iteration 17425, loss = 1.46011049\n",
      "Iteration 17426, loss = 1.46010783\n",
      "Iteration 17427, loss = 1.46010516\n",
      "Iteration 17428, loss = 1.46010250\n",
      "Iteration 17429, loss = 1.46009984\n",
      "Iteration 17430, loss = 1.46009717\n",
      "Iteration 17431, loss = 1.46009451\n",
      "Iteration 17432, loss = 1.46009185\n",
      "Iteration 17433, loss = 1.46008919\n",
      "Iteration 17434, loss = 1.46008653\n",
      "Iteration 17435, loss = 1.46008386\n",
      "Iteration 17436, loss = 1.46008120\n",
      "Iteration 17437, loss = 1.46007854\n",
      "Iteration 17438, loss = 1.46007588\n",
      "Iteration 17439, loss = 1.46007322\n",
      "Iteration 17440, loss = 1.46007056\n",
      "Iteration 17441, loss = 1.46006790\n",
      "Iteration 17442, loss = 1.46006525\n",
      "Iteration 17443, loss = 1.46006259\n",
      "Iteration 17444, loss = 1.46005993\n",
      "Iteration 17445, loss = 1.46005727\n",
      "Iteration 17446, loss = 1.46005461\n",
      "Iteration 17447, loss = 1.46005196\n",
      "Iteration 17448, loss = 1.46004930\n",
      "Iteration 17449, loss = 1.46004664\n",
      "Iteration 17450, loss = 1.46004399\n",
      "Iteration 17451, loss = 1.46004133\n",
      "Iteration 17452, loss = 1.46003868\n",
      "Iteration 17453, loss = 1.46003602\n",
      "Iteration 17454, loss = 1.46003337\n",
      "Iteration 17455, loss = 1.46003071\n",
      "Iteration 17456, loss = 1.46002806\n",
      "Iteration 17457, loss = 1.46002541\n",
      "Iteration 17458, loss = 1.46002275\n",
      "Iteration 17459, loss = 1.46002010\n",
      "Iteration 17460, loss = 1.46001745\n",
      "Iteration 17461, loss = 1.46001480\n",
      "Iteration 17462, loss = 1.46001214\n",
      "Iteration 17463, loss = 1.46000949\n",
      "Iteration 17464, loss = 1.46000684\n",
      "Iteration 17465, loss = 1.46000419\n",
      "Iteration 17466, loss = 1.46000154\n",
      "Iteration 17467, loss = 1.45999889\n",
      "Iteration 17468, loss = 1.45999624\n",
      "Iteration 17469, loss = 1.45999359\n",
      "Iteration 17470, loss = 1.45999094\n",
      "Iteration 17471, loss = 1.45998829\n",
      "Iteration 17472, loss = 1.45998564\n",
      "Iteration 17473, loss = 1.45998300\n",
      "Iteration 17474, loss = 1.45998035\n",
      "Iteration 17475, loss = 1.45997770\n",
      "Iteration 17476, loss = 1.45997505\n",
      "Iteration 17477, loss = 1.45997241\n",
      "Iteration 17478, loss = 1.45996976\n",
      "Iteration 17479, loss = 1.45996711\n",
      "Iteration 17480, loss = 1.45996447\n",
      "Iteration 17481, loss = 1.45996182\n",
      "Iteration 17482, loss = 1.45995918\n",
      "Iteration 17483, loss = 1.45995653\n",
      "Iteration 17484, loss = 1.45995389\n",
      "Iteration 17485, loss = 1.45995125\n",
      "Iteration 17486, loss = 1.45994860\n",
      "Iteration 17487, loss = 1.45994596\n",
      "Iteration 17488, loss = 1.45994332\n",
      "Iteration 17489, loss = 1.45994067\n",
      "Iteration 17490, loss = 1.45993803\n",
      "Iteration 17491, loss = 1.45993539\n",
      "Iteration 17492, loss = 1.45993275\n",
      "Iteration 17493, loss = 1.45993011\n",
      "Iteration 17494, loss = 1.45992747\n",
      "Iteration 17495, loss = 1.45992483\n",
      "Iteration 17496, loss = 1.45992218\n",
      "Iteration 17497, loss = 1.45991955\n",
      "Iteration 17498, loss = 1.45991691\n",
      "Iteration 17499, loss = 1.45991427\n",
      "Iteration 17500, loss = 1.45991163\n",
      "Iteration 17501, loss = 1.45990899\n",
      "Iteration 17502, loss = 1.45990635\n",
      "Iteration 17503, loss = 1.45990371\n",
      "Iteration 17504, loss = 1.45990108\n",
      "Iteration 17505, loss = 1.45989844\n",
      "Iteration 17506, loss = 1.45989580\n",
      "Iteration 17507, loss = 1.45989317\n",
      "Iteration 17508, loss = 1.45989053\n",
      "Iteration 17509, loss = 1.45988789\n",
      "Iteration 17510, loss = 1.45988526\n",
      "Iteration 17511, loss = 1.45988262\n",
      "Iteration 17512, loss = 1.45987999\n",
      "Iteration 17513, loss = 1.45987735\n",
      "Iteration 17514, loss = 1.45987472\n",
      "Iteration 17515, loss = 1.45987209\n",
      "Iteration 17516, loss = 1.45986945\n",
      "Iteration 17517, loss = 1.45986682\n",
      "Iteration 17518, loss = 1.45986419\n",
      "Iteration 17519, loss = 1.45986155\n",
      "Iteration 17520, loss = 1.45985892\n",
      "Iteration 17521, loss = 1.45985629\n",
      "Iteration 17522, loss = 1.45985366\n",
      "Iteration 17523, loss = 1.45985103\n",
      "Iteration 17524, loss = 1.45984840\n",
      "Iteration 17525, loss = 1.45984577\n",
      "Iteration 17526, loss = 1.45984314\n",
      "Iteration 17527, loss = 1.45984051\n",
      "Iteration 17528, loss = 1.45983788\n",
      "Iteration 17529, loss = 1.45983525\n",
      "Iteration 17530, loss = 1.45983262\n",
      "Iteration 17531, loss = 1.45982999\n",
      "Iteration 17532, loss = 1.45982737\n",
      "Iteration 17533, loss = 1.45982474\n",
      "Iteration 17534, loss = 1.45982211\n",
      "Iteration 17535, loss = 1.45981948\n",
      "Iteration 17536, loss = 1.45981686\n",
      "Iteration 17537, loss = 1.45981423\n",
      "Iteration 17538, loss = 1.45981160\n",
      "Iteration 17539, loss = 1.45980898\n",
      "Iteration 17540, loss = 1.45980635\n",
      "Iteration 17541, loss = 1.45980373\n",
      "Iteration 17542, loss = 1.45980110\n",
      "Iteration 17543, loss = 1.45979848\n",
      "Iteration 17544, loss = 1.45979586\n",
      "Iteration 17545, loss = 1.45979323\n",
      "Iteration 17546, loss = 1.45979061\n",
      "Iteration 17547, loss = 1.45978799\n",
      "Iteration 17548, loss = 1.45978536\n",
      "Iteration 17549, loss = 1.45978274\n",
      "Iteration 17550, loss = 1.45978012\n",
      "Iteration 17551, loss = 1.45977750\n",
      "Iteration 17552, loss = 1.45977488\n",
      "Iteration 17553, loss = 1.45977226\n",
      "Iteration 17554, loss = 1.45976964\n",
      "Iteration 17555, loss = 1.45976702\n",
      "Iteration 17556, loss = 1.45976440\n",
      "Iteration 17557, loss = 1.45976178\n",
      "Iteration 17558, loss = 1.45975916\n",
      "Iteration 17559, loss = 1.45975654\n",
      "Iteration 17560, loss = 1.45975392\n",
      "Iteration 17561, loss = 1.45975130\n",
      "Iteration 17562, loss = 1.45974869\n",
      "Iteration 17563, loss = 1.45974607\n",
      "Iteration 17564, loss = 1.45974345\n",
      "Iteration 17565, loss = 1.45974083\n",
      "Iteration 17566, loss = 1.45973822\n",
      "Iteration 17567, loss = 1.45973560\n",
      "Iteration 17568, loss = 1.45973299\n",
      "Iteration 17569, loss = 1.45973037\n",
      "Iteration 17570, loss = 1.45972776\n",
      "Iteration 17571, loss = 1.45972514\n",
      "Iteration 17572, loss = 1.45972253\n",
      "Iteration 17573, loss = 1.45971991\n",
      "Iteration 17574, loss = 1.45971730\n",
      "Iteration 17575, loss = 1.45971469\n",
      "Iteration 17576, loss = 1.45971207\n",
      "Iteration 17577, loss = 1.45970946\n",
      "Iteration 17578, loss = 1.45970685\n",
      "Iteration 17579, loss = 1.45970424\n",
      "Iteration 17580, loss = 1.45970162\n",
      "Iteration 17581, loss = 1.45969901\n",
      "Iteration 17582, loss = 1.45969640\n",
      "Iteration 17583, loss = 1.45969379\n",
      "Iteration 17584, loss = 1.45969118\n",
      "Iteration 17585, loss = 1.45968857\n",
      "Iteration 17586, loss = 1.45968596\n",
      "Iteration 17587, loss = 1.45968335\n",
      "Iteration 17588, loss = 1.45968074\n",
      "Iteration 17589, loss = 1.45967813\n",
      "Iteration 17590, loss = 1.45967553\n",
      "Iteration 17591, loss = 1.45967292\n",
      "Iteration 17592, loss = 1.45967031\n",
      "Iteration 17593, loss = 1.45966770\n",
      "Iteration 17594, loss = 1.45966510\n",
      "Iteration 17595, loss = 1.45966249\n",
      "Iteration 17596, loss = 1.45965988\n",
      "Iteration 17597, loss = 1.45965728\n",
      "Iteration 17598, loss = 1.45965467\n",
      "Iteration 17599, loss = 1.45965207\n",
      "Iteration 17600, loss = 1.45964946\n",
      "Iteration 17601, loss = 1.45964686\n",
      "Iteration 17602, loss = 1.45964425\n",
      "Iteration 17603, loss = 1.45964165\n",
      "Iteration 17604, loss = 1.45963904\n",
      "Iteration 17605, loss = 1.45963644\n",
      "Iteration 17606, loss = 1.45963384\n",
      "Iteration 17607, loss = 1.45963124\n",
      "Iteration 17608, loss = 1.45962863\n",
      "Iteration 17609, loss = 1.45962603\n",
      "Iteration 17610, loss = 1.45962343\n",
      "Iteration 17611, loss = 1.45962083\n",
      "Iteration 17612, loss = 1.45961823\n",
      "Iteration 17613, loss = 1.45961563\n",
      "Iteration 17614, loss = 1.45961303\n",
      "Iteration 17615, loss = 1.45961043\n",
      "Iteration 17616, loss = 1.45960783\n",
      "Iteration 17617, loss = 1.45960523\n",
      "Iteration 17618, loss = 1.45960263\n",
      "Iteration 17619, loss = 1.45960003\n",
      "Iteration 17620, loss = 1.45959743\n",
      "Iteration 17621, loss = 1.45959483\n",
      "Iteration 17622, loss = 1.45959224\n",
      "Iteration 17623, loss = 1.45958964\n",
      "Iteration 17624, loss = 1.45958704\n",
      "Iteration 17625, loss = 1.45958445\n",
      "Iteration 17626, loss = 1.45958185\n",
      "Iteration 17627, loss = 1.45957925\n",
      "Iteration 17628, loss = 1.45957666\n",
      "Iteration 17629, loss = 1.45957406\n",
      "Iteration 17630, loss = 1.45957147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17631, loss = 1.45956887\n",
      "Iteration 17632, loss = 1.45956628\n",
      "Iteration 17633, loss = 1.45956368\n",
      "Iteration 17634, loss = 1.45956109\n",
      "Iteration 17635, loss = 1.45955850\n",
      "Iteration 17636, loss = 1.45955591\n",
      "Iteration 17637, loss = 1.45955331\n",
      "Iteration 17638, loss = 1.45955072\n",
      "Iteration 17639, loss = 1.45954813\n",
      "Iteration 17640, loss = 1.45954554\n",
      "Iteration 17641, loss = 1.45954295\n",
      "Iteration 17642, loss = 1.45954035\n",
      "Iteration 17643, loss = 1.45953776\n",
      "Iteration 17644, loss = 1.45953517\n",
      "Iteration 17645, loss = 1.45953258\n",
      "Iteration 17646, loss = 1.45952999\n",
      "Iteration 17647, loss = 1.45952741\n",
      "Iteration 17648, loss = 1.45952482\n",
      "Iteration 17649, loss = 1.45952223\n",
      "Iteration 17650, loss = 1.45951964\n",
      "Iteration 17651, loss = 1.45951705\n",
      "Iteration 17652, loss = 1.45951446\n",
      "Iteration 17653, loss = 1.45951188\n",
      "Iteration 17654, loss = 1.45950929\n",
      "Iteration 17655, loss = 1.45950670\n",
      "Iteration 17656, loss = 1.45950412\n",
      "Iteration 17657, loss = 1.45950153\n",
      "Iteration 17658, loss = 1.45949895\n",
      "Iteration 17659, loss = 1.45949636\n",
      "Iteration 17660, loss = 1.45949378\n",
      "Iteration 17661, loss = 1.45949119\n",
      "Iteration 17662, loss = 1.45948861\n",
      "Iteration 17663, loss = 1.45948602\n",
      "Iteration 17664, loss = 1.45948344\n",
      "Iteration 17665, loss = 1.45948086\n",
      "Iteration 17666, loss = 1.45947827\n",
      "Iteration 17667, loss = 1.45947569\n",
      "Iteration 17668, loss = 1.45947311\n",
      "Iteration 17669, loss = 1.45947053\n",
      "Iteration 17670, loss = 1.45946794\n",
      "Iteration 17671, loss = 1.45946536\n",
      "Iteration 17672, loss = 1.45946278\n",
      "Iteration 17673, loss = 1.45946020\n",
      "Iteration 17674, loss = 1.45945762\n",
      "Iteration 17675, loss = 1.45945504\n",
      "Iteration 17676, loss = 1.45945246\n",
      "Iteration 17677, loss = 1.45944988\n",
      "Iteration 17678, loss = 1.45944730\n",
      "Iteration 17679, loss = 1.45944473\n",
      "Iteration 17680, loss = 1.45944215\n",
      "Iteration 17681, loss = 1.45943957\n",
      "Iteration 17682, loss = 1.45943699\n",
      "Iteration 17683, loss = 1.45943441\n",
      "Iteration 17684, loss = 1.45943184\n",
      "Iteration 17685, loss = 1.45942926\n",
      "Iteration 17686, loss = 1.45942668\n",
      "Iteration 17687, loss = 1.45942411\n",
      "Iteration 17688, loss = 1.45942153\n",
      "Iteration 17689, loss = 1.45941896\n",
      "Iteration 17690, loss = 1.45941638\n",
      "Iteration 17691, loss = 1.45941381\n",
      "Iteration 17692, loss = 1.45941123\n",
      "Iteration 17693, loss = 1.45940866\n",
      "Iteration 17694, loss = 1.45940609\n",
      "Iteration 17695, loss = 1.45940351\n",
      "Iteration 17696, loss = 1.45940094\n",
      "Iteration 17697, loss = 1.45939837\n",
      "Iteration 17698, loss = 1.45939579\n",
      "Iteration 17699, loss = 1.45939322\n",
      "Iteration 17700, loss = 1.45939065\n",
      "Iteration 17701, loss = 1.45938808\n",
      "Iteration 17702, loss = 1.45938551\n",
      "Iteration 17703, loss = 1.45938294\n",
      "Iteration 17704, loss = 1.45938037\n",
      "Iteration 17705, loss = 1.45937780\n",
      "Iteration 17706, loss = 1.45937523\n",
      "Iteration 17707, loss = 1.45937266\n",
      "Iteration 17708, loss = 1.45937009\n",
      "Iteration 17709, loss = 1.45936752\n",
      "Iteration 17710, loss = 1.45936495\n",
      "Iteration 17711, loss = 1.45936238\n",
      "Iteration 17712, loss = 1.45935982\n",
      "Iteration 17713, loss = 1.45935725\n",
      "Iteration 17714, loss = 1.45935468\n",
      "Iteration 17715, loss = 1.45935211\n",
      "Iteration 17716, loss = 1.45934955\n",
      "Iteration 17717, loss = 1.45934698\n",
      "Iteration 17718, loss = 1.45934442\n",
      "Iteration 17719, loss = 1.45934185\n",
      "Iteration 17720, loss = 1.45933929\n",
      "Iteration 17721, loss = 1.45933672\n",
      "Iteration 17722, loss = 1.45933416\n",
      "Iteration 17723, loss = 1.45933159\n",
      "Iteration 17724, loss = 1.45932903\n",
      "Iteration 17725, loss = 1.45932647\n",
      "Iteration 17726, loss = 1.45932390\n",
      "Iteration 17727, loss = 1.45932134\n",
      "Iteration 17728, loss = 1.45931878\n",
      "Iteration 17729, loss = 1.45931622\n",
      "Iteration 17730, loss = 1.45931365\n",
      "Iteration 17731, loss = 1.45931109\n",
      "Iteration 17732, loss = 1.45930853\n",
      "Iteration 17733, loss = 1.45930597\n",
      "Iteration 17734, loss = 1.45930341\n",
      "Iteration 17735, loss = 1.45930085\n",
      "Iteration 17736, loss = 1.45929829\n",
      "Iteration 17737, loss = 1.45929573\n",
      "Iteration 17738, loss = 1.45929317\n",
      "Iteration 17739, loss = 1.45929061\n",
      "Iteration 17740, loss = 1.45928805\n",
      "Iteration 17741, loss = 1.45928550\n",
      "Iteration 17742, loss = 1.45928294\n",
      "Iteration 17743, loss = 1.45928038\n",
      "Iteration 17744, loss = 1.45927782\n",
      "Iteration 17745, loss = 1.45927527\n",
      "Iteration 17746, loss = 1.45927271\n",
      "Iteration 17747, loss = 1.45927015\n",
      "Iteration 17748, loss = 1.45926760\n",
      "Iteration 17749, loss = 1.45926504\n",
      "Iteration 17750, loss = 1.45926249\n",
      "Iteration 17751, loss = 1.45925993\n",
      "Iteration 17752, loss = 1.45925738\n",
      "Iteration 17753, loss = 1.45925482\n",
      "Iteration 17754, loss = 1.45925227\n",
      "Iteration 17755, loss = 1.45924971\n",
      "Iteration 17756, loss = 1.45924716\n",
      "Iteration 17757, loss = 1.45924461\n",
      "Iteration 17758, loss = 1.45924206\n",
      "Iteration 17759, loss = 1.45923950\n",
      "Iteration 17760, loss = 1.45923695\n",
      "Iteration 17761, loss = 1.45923440\n",
      "Iteration 17762, loss = 1.45923185\n",
      "Iteration 17763, loss = 1.45922930\n",
      "Iteration 17764, loss = 1.45922675\n",
      "Iteration 17765, loss = 1.45922420\n",
      "Iteration 17766, loss = 1.45922165\n",
      "Iteration 17767, loss = 1.45921910\n",
      "Iteration 17768, loss = 1.45921655\n",
      "Iteration 17769, loss = 1.45921400\n",
      "Iteration 17770, loss = 1.45921145\n",
      "Iteration 17771, loss = 1.45920890\n",
      "Iteration 17772, loss = 1.45920635\n",
      "Iteration 17773, loss = 1.45920381\n",
      "Iteration 17774, loss = 1.45920126\n",
      "Iteration 17775, loss = 1.45919871\n",
      "Iteration 17776, loss = 1.45919616\n",
      "Iteration 17777, loss = 1.45919362\n",
      "Iteration 17778, loss = 1.45919107\n",
      "Iteration 17779, loss = 1.45918853\n",
      "Iteration 17780, loss = 1.45918598\n",
      "Iteration 17781, loss = 1.45918343\n",
      "Iteration 17782, loss = 1.45918089\n",
      "Iteration 17783, loss = 1.45917835\n",
      "Iteration 17784, loss = 1.45917580\n",
      "Iteration 17785, loss = 1.45917326\n",
      "Iteration 17786, loss = 1.45917071\n",
      "Iteration 17787, loss = 1.45916817\n",
      "Iteration 17788, loss = 1.45916563\n",
      "Iteration 17789, loss = 1.45916309\n",
      "Iteration 17790, loss = 1.45916054\n",
      "Iteration 17791, loss = 1.45915800\n",
      "Iteration 17792, loss = 1.45915546\n",
      "Iteration 17793, loss = 1.45915292\n",
      "Iteration 17794, loss = 1.45915038\n",
      "Iteration 17795, loss = 1.45914784\n",
      "Iteration 17796, loss = 1.45914530\n",
      "Iteration 17797, loss = 1.45914276\n",
      "Iteration 17798, loss = 1.45914022\n",
      "Iteration 17799, loss = 1.45913768\n",
      "Iteration 17800, loss = 1.45913514\n",
      "Iteration 17801, loss = 1.45913260\n",
      "Iteration 17802, loss = 1.45913006\n",
      "Iteration 17803, loss = 1.45912752\n",
      "Iteration 17804, loss = 1.45912499\n",
      "Iteration 17805, loss = 1.45912245\n",
      "Iteration 17806, loss = 1.45911991\n",
      "Iteration 17807, loss = 1.45911738\n",
      "Iteration 17808, loss = 1.45911484\n",
      "Iteration 17809, loss = 1.45911230\n",
      "Iteration 17810, loss = 1.45910977\n",
      "Iteration 17811, loss = 1.45910723\n",
      "Iteration 17812, loss = 1.45910470\n",
      "Iteration 17813, loss = 1.45910216\n",
      "Iteration 17814, loss = 1.45909963\n",
      "Iteration 17815, loss = 1.45909709\n",
      "Iteration 17816, loss = 1.45909456\n",
      "Iteration 17817, loss = 1.45909203\n",
      "Iteration 17818, loss = 1.45908949\n",
      "Iteration 17819, loss = 1.45908696\n",
      "Iteration 17820, loss = 1.45908443\n",
      "Iteration 17821, loss = 1.45908190\n",
      "Iteration 17822, loss = 1.45907936\n",
      "Iteration 17823, loss = 1.45907683\n",
      "Iteration 17824, loss = 1.45907430\n",
      "Iteration 17825, loss = 1.45907177\n",
      "Iteration 17826, loss = 1.45906924\n",
      "Iteration 17827, loss = 1.45906671\n",
      "Iteration 17828, loss = 1.45906418\n",
      "Iteration 17829, loss = 1.45906165\n",
      "Iteration 17830, loss = 1.45905912\n",
      "Iteration 17831, loss = 1.45905659\n",
      "Iteration 17832, loss = 1.45905406\n",
      "Iteration 17833, loss = 1.45905153\n",
      "Iteration 17834, loss = 1.45904901\n",
      "Iteration 17835, loss = 1.45904648\n",
      "Iteration 17836, loss = 1.45904395\n",
      "Iteration 17837, loss = 1.45904142\n",
      "Iteration 17838, loss = 1.45903890\n",
      "Iteration 17839, loss = 1.45903637\n",
      "Iteration 17840, loss = 1.45903385\n",
      "Iteration 17841, loss = 1.45903132\n",
      "Iteration 17842, loss = 1.45902879\n",
      "Iteration 17843, loss = 1.45902627\n",
      "Iteration 17844, loss = 1.45902374\n",
      "Iteration 17845, loss = 1.45902122\n",
      "Iteration 17846, loss = 1.45901870\n",
      "Iteration 17847, loss = 1.45901617\n",
      "Iteration 17848, loss = 1.45901365\n",
      "Iteration 17849, loss = 1.45901112\n",
      "Iteration 17850, loss = 1.45900860\n",
      "Iteration 17851, loss = 1.45900608\n",
      "Iteration 17852, loss = 1.45900356\n",
      "Iteration 17853, loss = 1.45900104\n",
      "Iteration 17854, loss = 1.45899851\n",
      "Iteration 17855, loss = 1.45899599\n",
      "Iteration 17856, loss = 1.45899347\n",
      "Iteration 17857, loss = 1.45899095\n",
      "Iteration 17858, loss = 1.45898843\n",
      "Iteration 17859, loss = 1.45898591\n",
      "Iteration 17860, loss = 1.45898339\n",
      "Iteration 17861, loss = 1.45898087\n",
      "Iteration 17862, loss = 1.45897835\n",
      "Iteration 17863, loss = 1.45897583\n",
      "Iteration 17864, loss = 1.45897332\n",
      "Iteration 17865, loss = 1.45897080\n",
      "Iteration 17866, loss = 1.45896828\n",
      "Iteration 17867, loss = 1.45896576\n",
      "Iteration 17868, loss = 1.45896325\n",
      "Iteration 17869, loss = 1.45896073\n",
      "Iteration 17870, loss = 1.45895821\n",
      "Iteration 17871, loss = 1.45895570\n",
      "Iteration 17872, loss = 1.45895318\n",
      "Iteration 17873, loss = 1.45895067\n",
      "Iteration 17874, loss = 1.45894815\n",
      "Iteration 17875, loss = 1.45894564\n",
      "Iteration 17876, loss = 1.45894312\n",
      "Iteration 17877, loss = 1.45894061\n",
      "Iteration 17878, loss = 1.45893809\n",
      "Iteration 17879, loss = 1.45893558\n",
      "Iteration 17880, loss = 1.45893307\n",
      "Iteration 17881, loss = 1.45893055\n",
      "Iteration 17882, loss = 1.45892804\n",
      "Iteration 17883, loss = 1.45892553\n",
      "Iteration 17884, loss = 1.45892302\n",
      "Iteration 17885, loss = 1.45892050\n",
      "Iteration 17886, loss = 1.45891799\n",
      "Iteration 17887, loss = 1.45891548\n",
      "Iteration 17888, loss = 1.45891297\n",
      "Iteration 17889, loss = 1.45891046\n",
      "Iteration 17890, loss = 1.45890795\n",
      "Iteration 17891, loss = 1.45890544\n",
      "Iteration 17892, loss = 1.45890293\n",
      "Iteration 17893, loss = 1.45890042\n",
      "Iteration 17894, loss = 1.45889791\n",
      "Iteration 17895, loss = 1.45889541\n",
      "Iteration 17896, loss = 1.45889290\n",
      "Iteration 17897, loss = 1.45889039\n",
      "Iteration 17898, loss = 1.45888788\n",
      "Iteration 17899, loss = 1.45888538\n",
      "Iteration 17900, loss = 1.45888287\n",
      "Iteration 17901, loss = 1.45888036\n",
      "Iteration 17902, loss = 1.45887786\n",
      "Iteration 17903, loss = 1.45887535\n",
      "Iteration 17904, loss = 1.45887284\n",
      "Iteration 17905, loss = 1.45887034\n",
      "Iteration 17906, loss = 1.45886783\n",
      "Iteration 17907, loss = 1.45886533\n",
      "Iteration 17908, loss = 1.45886283\n",
      "Iteration 17909, loss = 1.45886032\n",
      "Iteration 17910, loss = 1.45885782\n",
      "Iteration 17911, loss = 1.45885531\n",
      "Iteration 17912, loss = 1.45885281\n",
      "Iteration 17913, loss = 1.45885031\n",
      "Iteration 17914, loss = 1.45884781\n",
      "Iteration 17915, loss = 1.45884530\n",
      "Iteration 17916, loss = 1.45884280\n",
      "Iteration 17917, loss = 1.45884030\n",
      "Iteration 17918, loss = 1.45883780\n",
      "Iteration 17919, loss = 1.45883530\n",
      "Iteration 17920, loss = 1.45883280\n",
      "Iteration 17921, loss = 1.45883030\n",
      "Iteration 17922, loss = 1.45882780\n",
      "Iteration 17923, loss = 1.45882530\n",
      "Iteration 17924, loss = 1.45882280\n",
      "Iteration 17925, loss = 1.45882030\n",
      "Iteration 17926, loss = 1.45881780\n",
      "Iteration 17927, loss = 1.45881530\n",
      "Iteration 17928, loss = 1.45881281\n",
      "Iteration 17929, loss = 1.45881031\n",
      "Iteration 17930, loss = 1.45880781\n",
      "Iteration 17931, loss = 1.45880531\n",
      "Iteration 17932, loss = 1.45880282\n",
      "Iteration 17933, loss = 1.45880032\n",
      "Iteration 17934, loss = 1.45879783\n",
      "Iteration 17935, loss = 1.45879533\n",
      "Iteration 17936, loss = 1.45879283\n",
      "Iteration 17937, loss = 1.45879034\n",
      "Iteration 17938, loss = 1.45878784\n",
      "Iteration 17939, loss = 1.45878535\n",
      "Iteration 17940, loss = 1.45878286\n",
      "Iteration 17941, loss = 1.45878036\n",
      "Iteration 17942, loss = 1.45877787\n",
      "Iteration 17943, loss = 1.45877537\n",
      "Iteration 17944, loss = 1.45877288\n",
      "Iteration 17945, loss = 1.45877039\n",
      "Iteration 17946, loss = 1.45876790\n",
      "Iteration 17947, loss = 1.45876541\n",
      "Iteration 17948, loss = 1.45876291\n",
      "Iteration 17949, loss = 1.45876042\n",
      "Iteration 17950, loss = 1.45875793\n",
      "Iteration 17951, loss = 1.45875544\n",
      "Iteration 17952, loss = 1.45875295\n",
      "Iteration 17953, loss = 1.45875046\n",
      "Iteration 17954, loss = 1.45874797\n",
      "Iteration 17955, loss = 1.45874548\n",
      "Iteration 17956, loss = 1.45874299\n",
      "Iteration 17957, loss = 1.45874050\n",
      "Iteration 17958, loss = 1.45873801\n",
      "Iteration 17959, loss = 1.45873553\n",
      "Iteration 17960, loss = 1.45873304\n",
      "Iteration 17961, loss = 1.45873055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17962, loss = 1.45872806\n",
      "Iteration 17963, loss = 1.45872558\n",
      "Iteration 17964, loss = 1.45872309\n",
      "Iteration 17965, loss = 1.45872060\n",
      "Iteration 17966, loss = 1.45871812\n",
      "Iteration 17967, loss = 1.45871563\n",
      "Iteration 17968, loss = 1.45871315\n",
      "Iteration 17969, loss = 1.45871066\n",
      "Iteration 17970, loss = 1.45870818\n",
      "Iteration 17971, loss = 1.45870569\n",
      "Iteration 17972, loss = 1.45870321\n",
      "Iteration 17973, loss = 1.45870073\n",
      "Iteration 17974, loss = 1.45869824\n",
      "Iteration 17975, loss = 1.45869576\n",
      "Iteration 17976, loss = 1.45869328\n",
      "Iteration 17977, loss = 1.45869079\n",
      "Iteration 17978, loss = 1.45868831\n",
      "Iteration 17979, loss = 1.45868583\n",
      "Iteration 17980, loss = 1.45868335\n",
      "Iteration 17981, loss = 1.45868087\n",
      "Iteration 17982, loss = 1.45867839\n",
      "Iteration 17983, loss = 1.45867591\n",
      "Iteration 17984, loss = 1.45867343\n",
      "Iteration 17985, loss = 1.45867095\n",
      "Iteration 17986, loss = 1.45866847\n",
      "Iteration 17987, loss = 1.45866599\n",
      "Iteration 17988, loss = 1.45866351\n",
      "Iteration 17989, loss = 1.45866103\n",
      "Iteration 17990, loss = 1.45865855\n",
      "Iteration 17991, loss = 1.45865607\n",
      "Iteration 17992, loss = 1.45865359\n",
      "Iteration 17993, loss = 1.45865112\n",
      "Iteration 17994, loss = 1.45864864\n",
      "Iteration 17995, loss = 1.45864616\n",
      "Iteration 17996, loss = 1.45864369\n",
      "Iteration 17997, loss = 1.45864121\n",
      "Iteration 17998, loss = 1.45863873\n",
      "Iteration 17999, loss = 1.45863626\n",
      "Iteration 18000, loss = 1.45863378\n",
      "Iteration 18001, loss = 1.45863131\n",
      "Iteration 18002, loss = 1.45862883\n",
      "Iteration 18003, loss = 1.45862636\n",
      "Iteration 18004, loss = 1.45862389\n",
      "Iteration 18005, loss = 1.45862141\n",
      "Iteration 18006, loss = 1.45861894\n",
      "Iteration 18007, loss = 1.45861647\n",
      "Iteration 18008, loss = 1.45861399\n",
      "Iteration 18009, loss = 1.45861152\n",
      "Iteration 18010, loss = 1.45860905\n",
      "Iteration 18011, loss = 1.45860658\n",
      "Iteration 18012, loss = 1.45860410\n",
      "Iteration 18013, loss = 1.45860163\n",
      "Iteration 18014, loss = 1.45859916\n",
      "Iteration 18015, loss = 1.45859669\n",
      "Iteration 18016, loss = 1.45859422\n",
      "Iteration 18017, loss = 1.45859175\n",
      "Iteration 18018, loss = 1.45858928\n",
      "Iteration 18019, loss = 1.45858681\n",
      "Iteration 18020, loss = 1.45858434\n",
      "Iteration 18021, loss = 1.45858188\n",
      "Iteration 18022, loss = 1.45857941\n",
      "Iteration 18023, loss = 1.45857694\n",
      "Iteration 18024, loss = 1.45857447\n",
      "Iteration 18025, loss = 1.45857200\n",
      "Iteration 18026, loss = 1.45856954\n",
      "Iteration 18027, loss = 1.45856707\n",
      "Iteration 18028, loss = 1.45856460\n",
      "Iteration 18029, loss = 1.45856214\n",
      "Iteration 18030, loss = 1.45855967\n",
      "Iteration 18031, loss = 1.45855721\n",
      "Iteration 18032, loss = 1.45855474\n",
      "Iteration 18033, loss = 1.45855228\n",
      "Iteration 18034, loss = 1.45854981\n",
      "Iteration 18035, loss = 1.45854735\n",
      "Iteration 18036, loss = 1.45854488\n",
      "Iteration 18037, loss = 1.45854242\n",
      "Iteration 18038, loss = 1.45853996\n",
      "Iteration 18039, loss = 1.45853749\n",
      "Iteration 18040, loss = 1.45853503\n",
      "Iteration 18041, loss = 1.45853257\n",
      "Iteration 18042, loss = 1.45853010\n",
      "Iteration 18043, loss = 1.45852764\n",
      "Iteration 18044, loss = 1.45852518\n",
      "Iteration 18045, loss = 1.45852272\n",
      "Iteration 18046, loss = 1.45852026\n",
      "Iteration 18047, loss = 1.45851780\n",
      "Iteration 18048, loss = 1.45851534\n",
      "Iteration 18049, loss = 1.45851288\n",
      "Iteration 18050, loss = 1.45851042\n",
      "Iteration 18051, loss = 1.45850796\n",
      "Iteration 18052, loss = 1.45850550\n",
      "Iteration 18053, loss = 1.45850304\n",
      "Iteration 18054, loss = 1.45850058\n",
      "Iteration 18055, loss = 1.45849812\n",
      "Iteration 18056, loss = 1.45849567\n",
      "Iteration 18057, loss = 1.45849321\n",
      "Iteration 18058, loss = 1.45849075\n",
      "Iteration 18059, loss = 1.45848830\n",
      "Iteration 18060, loss = 1.45848584\n",
      "Iteration 18061, loss = 1.45848338\n",
      "Iteration 18062, loss = 1.45848093\n",
      "Iteration 18063, loss = 1.45847847\n",
      "Iteration 18064, loss = 1.45847602\n",
      "Iteration 18065, loss = 1.45847356\n",
      "Iteration 18066, loss = 1.45847111\n",
      "Iteration 18067, loss = 1.45846865\n",
      "Iteration 18068, loss = 1.45846620\n",
      "Iteration 18069, loss = 1.45846374\n",
      "Iteration 18070, loss = 1.45846129\n",
      "Iteration 18071, loss = 1.45845884\n",
      "Iteration 18072, loss = 1.45845638\n",
      "Iteration 18073, loss = 1.45845393\n",
      "Iteration 18074, loss = 1.45845148\n",
      "Iteration 18075, loss = 1.45844903\n",
      "Iteration 18076, loss = 1.45844658\n",
      "Iteration 18077, loss = 1.45844412\n",
      "Iteration 18078, loss = 1.45844167\n",
      "Iteration 18079, loss = 1.45843922\n",
      "Iteration 18080, loss = 1.45843677\n",
      "Iteration 18081, loss = 1.45843432\n",
      "Iteration 18082, loss = 1.45843187\n",
      "Iteration 18083, loss = 1.45842942\n",
      "Iteration 18084, loss = 1.45842697\n",
      "Iteration 18085, loss = 1.45842452\n",
      "Iteration 18086, loss = 1.45842208\n",
      "Iteration 18087, loss = 1.45841963\n",
      "Iteration 18088, loss = 1.45841718\n",
      "Iteration 18089, loss = 1.45841473\n",
      "Iteration 18090, loss = 1.45841229\n",
      "Iteration 18091, loss = 1.45840984\n",
      "Iteration 18092, loss = 1.45840739\n",
      "Iteration 18093, loss = 1.45840495\n",
      "Iteration 18094, loss = 1.45840250\n",
      "Iteration 18095, loss = 1.45840005\n",
      "Iteration 18096, loss = 1.45839761\n",
      "Iteration 18097, loss = 1.45839516\n",
      "Iteration 18098, loss = 1.45839272\n",
      "Iteration 18099, loss = 1.45839027\n",
      "Iteration 18100, loss = 1.45838783\n",
      "Iteration 18101, loss = 1.45838539\n",
      "Iteration 18102, loss = 1.45838294\n",
      "Iteration 18103, loss = 1.45838050\n",
      "Iteration 18104, loss = 1.45837806\n",
      "Iteration 18105, loss = 1.45837561\n",
      "Iteration 18106, loss = 1.45837317\n",
      "Iteration 18107, loss = 1.45837073\n",
      "Iteration 18108, loss = 1.45836829\n",
      "Iteration 18109, loss = 1.45836584\n",
      "Iteration 18110, loss = 1.45836340\n",
      "Iteration 18111, loss = 1.45836096\n",
      "Iteration 18112, loss = 1.45835852\n",
      "Iteration 18113, loss = 1.45835608\n",
      "Iteration 18114, loss = 1.45835364\n",
      "Iteration 18115, loss = 1.45835120\n",
      "Iteration 18116, loss = 1.45834876\n",
      "Iteration 18117, loss = 1.45834632\n",
      "Iteration 18118, loss = 1.45834389\n",
      "Iteration 18119, loss = 1.45834145\n",
      "Iteration 18120, loss = 1.45833901\n",
      "Iteration 18121, loss = 1.45833657\n",
      "Iteration 18122, loss = 1.45833413\n",
      "Iteration 18123, loss = 1.45833170\n",
      "Iteration 18124, loss = 1.45832926\n",
      "Iteration 18125, loss = 1.45832682\n",
      "Iteration 18126, loss = 1.45832439\n",
      "Iteration 18127, loss = 1.45832195\n",
      "Iteration 18128, loss = 1.45831952\n",
      "Iteration 18129, loss = 1.45831708\n",
      "Iteration 18130, loss = 1.45831465\n",
      "Iteration 18131, loss = 1.45831221\n",
      "Iteration 18132, loss = 1.45830978\n",
      "Iteration 18133, loss = 1.45830734\n",
      "Iteration 18134, loss = 1.45830491\n",
      "Iteration 18135, loss = 1.45830247\n",
      "Iteration 18136, loss = 1.45830004\n",
      "Iteration 18137, loss = 1.45829761\n",
      "Iteration 18138, loss = 1.45829518\n",
      "Iteration 18139, loss = 1.45829274\n",
      "Iteration 18140, loss = 1.45829031\n",
      "Iteration 18141, loss = 1.45828788\n",
      "Iteration 18142, loss = 1.45828545\n",
      "Iteration 18143, loss = 1.45828302\n",
      "Iteration 18144, loss = 1.45828059\n",
      "Iteration 18145, loss = 1.45827816\n",
      "Iteration 18146, loss = 1.45827573\n",
      "Iteration 18147, loss = 1.45827330\n",
      "Iteration 18148, loss = 1.45827087\n",
      "Iteration 18149, loss = 1.45826844\n",
      "Iteration 18150, loss = 1.45826601\n",
      "Iteration 18151, loss = 1.45826358\n",
      "Iteration 18152, loss = 1.45826115\n",
      "Iteration 18153, loss = 1.45825872\n",
      "Iteration 18154, loss = 1.45825630\n",
      "Iteration 18155, loss = 1.45825387\n",
      "Iteration 18156, loss = 1.45825144\n",
      "Iteration 18157, loss = 1.45824902\n",
      "Iteration 18158, loss = 1.45824659\n",
      "Iteration 18159, loss = 1.45824416\n",
      "Iteration 18160, loss = 1.45824174\n",
      "Iteration 18161, loss = 1.45823931\n",
      "Iteration 18162, loss = 1.45823689\n",
      "Iteration 18163, loss = 1.45823446\n",
      "Iteration 18164, loss = 1.45823204\n",
      "Iteration 18165, loss = 1.45822961\n",
      "Iteration 18166, loss = 1.45822719\n",
      "Iteration 18167, loss = 1.45822476\n",
      "Iteration 18168, loss = 1.45822234\n",
      "Iteration 18169, loss = 1.45821992\n",
      "Iteration 18170, loss = 1.45821750\n",
      "Iteration 18171, loss = 1.45821507\n",
      "Iteration 18172, loss = 1.45821265\n",
      "Iteration 18173, loss = 1.45821023\n",
      "Iteration 18174, loss = 1.45820781\n",
      "Iteration 18175, loss = 1.45820539\n",
      "Iteration 18176, loss = 1.45820296\n",
      "Iteration 18177, loss = 1.45820054\n",
      "Iteration 18178, loss = 1.45819812\n",
      "Iteration 18179, loss = 1.45819570\n",
      "Iteration 18180, loss = 1.45819328\n",
      "Iteration 18181, loss = 1.45819086\n",
      "Iteration 18182, loss = 1.45818845\n",
      "Iteration 18183, loss = 1.45818603\n",
      "Iteration 18184, loss = 1.45818361\n",
      "Iteration 18185, loss = 1.45818119\n",
      "Iteration 18186, loss = 1.45817877\n",
      "Iteration 18187, loss = 1.45817635\n",
      "Iteration 18188, loss = 1.45817394\n",
      "Iteration 18189, loss = 1.45817152\n",
      "Iteration 18190, loss = 1.45816910\n",
      "Iteration 18191, loss = 1.45816669\n",
      "Iteration 18192, loss = 1.45816427\n",
      "Iteration 18193, loss = 1.45816185\n",
      "Iteration 18194, loss = 1.45815944\n",
      "Iteration 18195, loss = 1.45815702\n",
      "Iteration 18196, loss = 1.45815461\n",
      "Iteration 18197, loss = 1.45815219\n",
      "Iteration 18198, loss = 1.45814978\n",
      "Iteration 18199, loss = 1.45814737\n",
      "Iteration 18200, loss = 1.45814495\n",
      "Iteration 18201, loss = 1.45814254\n",
      "Iteration 18202, loss = 1.45814013\n",
      "Iteration 18203, loss = 1.45813771\n",
      "Iteration 18204, loss = 1.45813530\n",
      "Iteration 18205, loss = 1.45813289\n",
      "Iteration 18206, loss = 1.45813048\n",
      "Iteration 18207, loss = 1.45812806\n",
      "Iteration 18208, loss = 1.45812565\n",
      "Iteration 18209, loss = 1.45812324\n",
      "Iteration 18210, loss = 1.45812083\n",
      "Iteration 18211, loss = 1.45811842\n",
      "Iteration 18212, loss = 1.45811601\n",
      "Iteration 18213, loss = 1.45811360\n",
      "Iteration 18214, loss = 1.45811119\n",
      "Iteration 18215, loss = 1.45810878\n",
      "Iteration 18216, loss = 1.45810637\n",
      "Iteration 18217, loss = 1.45810397\n",
      "Iteration 18218, loss = 1.45810156\n",
      "Iteration 18219, loss = 1.45809915\n",
      "Iteration 18220, loss = 1.45809674\n",
      "Iteration 18221, loss = 1.45809433\n",
      "Iteration 18222, loss = 1.45809193\n",
      "Iteration 18223, loss = 1.45808952\n",
      "Iteration 18224, loss = 1.45808711\n",
      "Iteration 18225, loss = 1.45808471\n",
      "Iteration 18226, loss = 1.45808230\n",
      "Iteration 18227, loss = 1.45807990\n",
      "Iteration 18228, loss = 1.45807749\n",
      "Iteration 18229, loss = 1.45807509\n",
      "Iteration 18230, loss = 1.45807268\n",
      "Iteration 18231, loss = 1.45807028\n",
      "Iteration 18232, loss = 1.45806787\n",
      "Iteration 18233, loss = 1.45806547\n",
      "Iteration 18234, loss = 1.45806307\n",
      "Iteration 18235, loss = 1.45806066\n",
      "Iteration 18236, loss = 1.45805826\n",
      "Iteration 18237, loss = 1.45805586\n",
      "Iteration 18238, loss = 1.45805345\n",
      "Iteration 18239, loss = 1.45805105\n",
      "Iteration 18240, loss = 1.45804865\n",
      "Iteration 18241, loss = 1.45804625\n",
      "Iteration 18242, loss = 1.45804385\n",
      "Iteration 18243, loss = 1.45804145\n",
      "Iteration 18244, loss = 1.45803905\n",
      "Iteration 18245, loss = 1.45803665\n",
      "Iteration 18246, loss = 1.45803425\n",
      "Iteration 18247, loss = 1.45803185\n",
      "Iteration 18248, loss = 1.45802945\n",
      "Iteration 18249, loss = 1.45802705\n",
      "Iteration 18250, loss = 1.45802465\n",
      "Iteration 18251, loss = 1.45802225\n",
      "Iteration 18252, loss = 1.45801985\n",
      "Iteration 18253, loss = 1.45801746\n",
      "Iteration 18254, loss = 1.45801506\n",
      "Iteration 18255, loss = 1.45801266\n",
      "Iteration 18256, loss = 1.45801027\n",
      "Iteration 18257, loss = 1.45800787\n",
      "Iteration 18258, loss = 1.45800547\n",
      "Iteration 18259, loss = 1.45800308\n",
      "Iteration 18260, loss = 1.45800068\n",
      "Iteration 18261, loss = 1.45799829\n",
      "Iteration 18262, loss = 1.45799589\n",
      "Iteration 18263, loss = 1.45799350\n",
      "Iteration 18264, loss = 1.45799110\n",
      "Iteration 18265, loss = 1.45798871\n",
      "Iteration 18266, loss = 1.45798631\n",
      "Iteration 18267, loss = 1.45798392\n",
      "Iteration 18268, loss = 1.45798153\n",
      "Iteration 18269, loss = 1.45797913\n",
      "Iteration 18270, loss = 1.45797674\n",
      "Iteration 18271, loss = 1.45797435\n",
      "Iteration 18272, loss = 1.45797196\n",
      "Iteration 18273, loss = 1.45796956\n",
      "Iteration 18274, loss = 1.45796717\n",
      "Iteration 18275, loss = 1.45796478\n",
      "Iteration 18276, loss = 1.45796239\n",
      "Iteration 18277, loss = 1.45796000\n",
      "Iteration 18278, loss = 1.45795761\n",
      "Iteration 18279, loss = 1.45795522\n",
      "Iteration 18280, loss = 1.45795283\n",
      "Iteration 18281, loss = 1.45795044\n",
      "Iteration 18282, loss = 1.45794805\n",
      "Iteration 18283, loss = 1.45794566\n",
      "Iteration 18284, loss = 1.45794327\n",
      "Iteration 18285, loss = 1.45794089\n",
      "Iteration 18286, loss = 1.45793850\n",
      "Iteration 18287, loss = 1.45793611\n",
      "Iteration 18288, loss = 1.45793372\n",
      "Iteration 18289, loss = 1.45793134\n",
      "Iteration 18290, loss = 1.45792895\n",
      "Iteration 18291, loss = 1.45792656\n",
      "Iteration 18292, loss = 1.45792418\n",
      "Iteration 18293, loss = 1.45792179\n",
      "Iteration 18294, loss = 1.45791941\n",
      "Iteration 18295, loss = 1.45791702\n",
      "Iteration 18296, loss = 1.45791464\n",
      "Iteration 18297, loss = 1.45791225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18298, loss = 1.45790987\n",
      "Iteration 18299, loss = 1.45790748\n",
      "Iteration 18300, loss = 1.45790510\n",
      "Iteration 18301, loss = 1.45790272\n",
      "Iteration 18302, loss = 1.45790033\n",
      "Iteration 18303, loss = 1.45789795\n",
      "Iteration 18304, loss = 1.45789557\n",
      "Iteration 18305, loss = 1.45789319\n",
      "Iteration 18306, loss = 1.45789080\n",
      "Iteration 18307, loss = 1.45788842\n",
      "Iteration 18308, loss = 1.45788604\n",
      "Iteration 18309, loss = 1.45788366\n",
      "Iteration 18310, loss = 1.45788128\n",
      "Iteration 18311, loss = 1.45787890\n",
      "Iteration 18312, loss = 1.45787652\n",
      "Iteration 18313, loss = 1.45787414\n",
      "Iteration 18314, loss = 1.45787176\n",
      "Iteration 18315, loss = 1.45786938\n",
      "Iteration 18316, loss = 1.45786700\n",
      "Iteration 18317, loss = 1.45786462\n",
      "Iteration 18318, loss = 1.45786224\n",
      "Iteration 18319, loss = 1.45785987\n",
      "Iteration 18320, loss = 1.45785749\n",
      "Iteration 18321, loss = 1.45785511\n",
      "Iteration 18322, loss = 1.45785273\n",
      "Iteration 18323, loss = 1.45785036\n",
      "Iteration 18324, loss = 1.45784798\n",
      "Iteration 18325, loss = 1.45784560\n",
      "Iteration 18326, loss = 1.45784323\n",
      "Iteration 18327, loss = 1.45784085\n",
      "Iteration 18328, loss = 1.45783848\n",
      "Iteration 18329, loss = 1.45783610\n",
      "Iteration 18330, loss = 1.45783373\n",
      "Iteration 18331, loss = 1.45783135\n",
      "Iteration 18332, loss = 1.45782898\n",
      "Iteration 18333, loss = 1.45782660\n",
      "Iteration 18334, loss = 1.45782423\n",
      "Iteration 18335, loss = 1.45782186\n",
      "Iteration 18336, loss = 1.45781948\n",
      "Iteration 18337, loss = 1.45781711\n",
      "Iteration 18338, loss = 1.45781474\n",
      "Iteration 18339, loss = 1.45781237\n",
      "Iteration 18340, loss = 1.45780999\n",
      "Iteration 18341, loss = 1.45780762\n",
      "Iteration 18342, loss = 1.45780525\n",
      "Iteration 18343, loss = 1.45780288\n",
      "Iteration 18344, loss = 1.45780051\n",
      "Iteration 18345, loss = 1.45779814\n",
      "Iteration 18346, loss = 1.45779577\n",
      "Iteration 18347, loss = 1.45779340\n",
      "Iteration 18348, loss = 1.45779103\n",
      "Iteration 18349, loss = 1.45778866\n",
      "Iteration 18350, loss = 1.45778629\n",
      "Iteration 18351, loss = 1.45778392\n",
      "Iteration 18352, loss = 1.45778155\n",
      "Iteration 18353, loss = 1.45777919\n",
      "Iteration 18354, loss = 1.45777682\n",
      "Iteration 18355, loss = 1.45777445\n",
      "Iteration 18356, loss = 1.45777208\n",
      "Iteration 18357, loss = 1.45776972\n",
      "Iteration 18358, loss = 1.45776735\n",
      "Iteration 18359, loss = 1.45776498\n",
      "Iteration 18360, loss = 1.45776262\n",
      "Iteration 18361, loss = 1.45776025\n",
      "Iteration 18362, loss = 1.45775789\n",
      "Iteration 18363, loss = 1.45775552\n",
      "Iteration 18364, loss = 1.45775316\n",
      "Iteration 18365, loss = 1.45775079\n",
      "Iteration 18366, loss = 1.45774843\n",
      "Iteration 18367, loss = 1.45774607\n",
      "Iteration 18368, loss = 1.45774370\n",
      "Iteration 18369, loss = 1.45774134\n",
      "Iteration 18370, loss = 1.45773898\n",
      "Iteration 18371, loss = 1.45773661\n",
      "Iteration 18372, loss = 1.45773425\n",
      "Iteration 18373, loss = 1.45773189\n",
      "Iteration 18374, loss = 1.45772953\n",
      "Iteration 18375, loss = 1.45772716\n",
      "Iteration 18376, loss = 1.45772480\n",
      "Iteration 18377, loss = 1.45772244\n",
      "Iteration 18378, loss = 1.45772008\n",
      "Iteration 18379, loss = 1.45771772\n",
      "Iteration 18380, loss = 1.45771536\n",
      "Iteration 18381, loss = 1.45771300\n",
      "Iteration 18382, loss = 1.45771064\n",
      "Iteration 18383, loss = 1.45770828\n",
      "Iteration 18384, loss = 1.45770592\n",
      "Iteration 18385, loss = 1.45770357\n",
      "Iteration 18386, loss = 1.45770121\n",
      "Iteration 18387, loss = 1.45769885\n",
      "Iteration 18388, loss = 1.45769649\n",
      "Iteration 18389, loss = 1.45769413\n",
      "Iteration 18390, loss = 1.45769178\n",
      "Iteration 18391, loss = 1.45768942\n",
      "Iteration 18392, loss = 1.45768706\n",
      "Iteration 18393, loss = 1.45768471\n",
      "Iteration 18394, loss = 1.45768235\n",
      "Iteration 18395, loss = 1.45768000\n",
      "Iteration 18396, loss = 1.45767764\n",
      "Iteration 18397, loss = 1.45767528\n",
      "Iteration 18398, loss = 1.45767293\n",
      "Iteration 18399, loss = 1.45767058\n",
      "Iteration 18400, loss = 1.45766822\n",
      "Iteration 18401, loss = 1.45766587\n",
      "Iteration 18402, loss = 1.45766351\n",
      "Iteration 18403, loss = 1.45766116\n",
      "Iteration 18404, loss = 1.45765881\n",
      "Iteration 18405, loss = 1.45765645\n",
      "Iteration 18406, loss = 1.45765410\n",
      "Iteration 18407, loss = 1.45765175\n",
      "Iteration 18408, loss = 1.45764940\n",
      "Iteration 18409, loss = 1.45764705\n",
      "Iteration 18410, loss = 1.45764469\n",
      "Iteration 18411, loss = 1.45764234\n",
      "Iteration 18412, loss = 1.45763999\n",
      "Iteration 18413, loss = 1.45763764\n",
      "Iteration 18414, loss = 1.45763529\n",
      "Iteration 18415, loss = 1.45763294\n",
      "Iteration 18416, loss = 1.45763059\n",
      "Iteration 18417, loss = 1.45762824\n",
      "Iteration 18418, loss = 1.45762589\n",
      "Iteration 18419, loss = 1.45762355\n",
      "Iteration 18420, loss = 1.45762120\n",
      "Iteration 18421, loss = 1.45761885\n",
      "Iteration 18422, loss = 1.45761650\n",
      "Iteration 18423, loss = 1.45761415\n",
      "Iteration 18424, loss = 1.45761181\n",
      "Iteration 18425, loss = 1.45760946\n",
      "Iteration 18426, loss = 1.45760711\n",
      "Iteration 18427, loss = 1.45760477\n",
      "Iteration 18428, loss = 1.45760242\n",
      "Iteration 18429, loss = 1.45760008\n",
      "Iteration 18430, loss = 1.45759773\n",
      "Iteration 18431, loss = 1.45759538\n",
      "Iteration 18432, loss = 1.45759304\n",
      "Iteration 18433, loss = 1.45759070\n",
      "Iteration 18434, loss = 1.45758835\n",
      "Iteration 18435, loss = 1.45758601\n",
      "Iteration 18436, loss = 1.45758366\n",
      "Iteration 18437, loss = 1.45758132\n",
      "Iteration 18438, loss = 1.45757898\n",
      "Iteration 18439, loss = 1.45757663\n",
      "Iteration 18440, loss = 1.45757429\n",
      "Iteration 18441, loss = 1.45757195\n",
      "Iteration 18442, loss = 1.45756961\n",
      "Iteration 18443, loss = 1.45756726\n",
      "Iteration 18444, loss = 1.45756492\n",
      "Iteration 18445, loss = 1.45756258\n",
      "Iteration 18446, loss = 1.45756024\n",
      "Iteration 18447, loss = 1.45755790\n",
      "Iteration 18448, loss = 1.45755556\n",
      "Iteration 18449, loss = 1.45755322\n",
      "Iteration 18450, loss = 1.45755088\n",
      "Iteration 18451, loss = 1.45754854\n",
      "Iteration 18452, loss = 1.45754620\n",
      "Iteration 18453, loss = 1.45754386\n",
      "Iteration 18454, loss = 1.45754153\n",
      "Iteration 18455, loss = 1.45753919\n",
      "Iteration 18456, loss = 1.45753685\n",
      "Iteration 18457, loss = 1.45753451\n",
      "Iteration 18458, loss = 1.45753217\n",
      "Iteration 18459, loss = 1.45752984\n",
      "Iteration 18460, loss = 1.45752750\n",
      "Iteration 18461, loss = 1.45752516\n",
      "Iteration 18462, loss = 1.45752283\n",
      "Iteration 18463, loss = 1.45752049\n",
      "Iteration 18464, loss = 1.45751816\n",
      "Iteration 18465, loss = 1.45751582\n",
      "Iteration 18466, loss = 1.45751349\n",
      "Iteration 18467, loss = 1.45751115\n",
      "Iteration 18468, loss = 1.45750882\n",
      "Iteration 18469, loss = 1.45750648\n",
      "Iteration 18470, loss = 1.45750415\n",
      "Iteration 18471, loss = 1.45750181\n",
      "Iteration 18472, loss = 1.45749948\n",
      "Iteration 18473, loss = 1.45749715\n",
      "Iteration 18474, loss = 1.45749482\n",
      "Iteration 18475, loss = 1.45749248\n",
      "Iteration 18476, loss = 1.45749015\n",
      "Iteration 18477, loss = 1.45748782\n",
      "Iteration 18478, loss = 1.45748549\n",
      "Iteration 18479, loss = 1.45748316\n",
      "Iteration 18480, loss = 1.45748083\n",
      "Iteration 18481, loss = 1.45747849\n",
      "Iteration 18482, loss = 1.45747616\n",
      "Iteration 18483, loss = 1.45747383\n",
      "Iteration 18484, loss = 1.45747150\n",
      "Iteration 18485, loss = 1.45746917\n",
      "Iteration 18486, loss = 1.45746685\n",
      "Iteration 18487, loss = 1.45746452\n",
      "Iteration 18488, loss = 1.45746219\n",
      "Iteration 18489, loss = 1.45745986\n",
      "Iteration 18490, loss = 1.45745753\n",
      "Iteration 18491, loss = 1.45745520\n",
      "Iteration 18492, loss = 1.45745288\n",
      "Iteration 18493, loss = 1.45745055\n",
      "Iteration 18494, loss = 1.45744822\n",
      "Iteration 18495, loss = 1.45744590\n",
      "Iteration 18496, loss = 1.45744357\n",
      "Iteration 18497, loss = 1.45744124\n",
      "Iteration 18498, loss = 1.45743892\n",
      "Iteration 18499, loss = 1.45743659\n",
      "Iteration 18500, loss = 1.45743427\n",
      "Iteration 18501, loss = 1.45743194\n",
      "Iteration 18502, loss = 1.45742962\n",
      "Iteration 18503, loss = 1.45742729\n",
      "Iteration 18504, loss = 1.45742497\n",
      "Iteration 18505, loss = 1.45742264\n",
      "Iteration 18506, loss = 1.45742032\n",
      "Iteration 18507, loss = 1.45741800\n",
      "Iteration 18508, loss = 1.45741567\n",
      "Iteration 18509, loss = 1.45741335\n",
      "Iteration 18510, loss = 1.45741103\n",
      "Iteration 18511, loss = 1.45740871\n",
      "Iteration 18512, loss = 1.45740639\n",
      "Iteration 18513, loss = 1.45740406\n",
      "Iteration 18514, loss = 1.45740174\n",
      "Iteration 18515, loss = 1.45739942\n",
      "Iteration 18516, loss = 1.45739710\n",
      "Iteration 18517, loss = 1.45739478\n",
      "Iteration 18518, loss = 1.45739246\n",
      "Iteration 18519, loss = 1.45739014\n",
      "Iteration 18520, loss = 1.45738782\n",
      "Iteration 18521, loss = 1.45738550\n",
      "Iteration 18522, loss = 1.45738318\n",
      "Iteration 18523, loss = 1.45738086\n",
      "Iteration 18524, loss = 1.45737855\n",
      "Iteration 18525, loss = 1.45737623\n",
      "Iteration 18526, loss = 1.45737391\n",
      "Iteration 18527, loss = 1.45737159\n",
      "Iteration 18528, loss = 1.45736928\n",
      "Iteration 18529, loss = 1.45736696\n",
      "Iteration 18530, loss = 1.45736464\n",
      "Iteration 18531, loss = 1.45736233\n",
      "Iteration 18532, loss = 1.45736001\n",
      "Iteration 18533, loss = 1.45735769\n",
      "Iteration 18534, loss = 1.45735538\n",
      "Iteration 18535, loss = 1.45735306\n",
      "Iteration 18536, loss = 1.45735075\n",
      "Iteration 18537, loss = 1.45734843\n",
      "Iteration 18538, loss = 1.45734612\n",
      "Iteration 18539, loss = 1.45734381\n",
      "Iteration 18540, loss = 1.45734149\n",
      "Iteration 18541, loss = 1.45733918\n",
      "Iteration 18542, loss = 1.45733687\n",
      "Iteration 18543, loss = 1.45733455\n",
      "Iteration 18544, loss = 1.45733224\n",
      "Iteration 18545, loss = 1.45732993\n",
      "Iteration 18546, loss = 1.45732762\n",
      "Iteration 18547, loss = 1.45732530\n",
      "Iteration 18548, loss = 1.45732299\n",
      "Iteration 18549, loss = 1.45732068\n",
      "Iteration 18550, loss = 1.45731837\n",
      "Iteration 18551, loss = 1.45731606\n",
      "Iteration 18552, loss = 1.45731375\n",
      "Iteration 18553, loss = 1.45731144\n",
      "Iteration 18554, loss = 1.45730913\n",
      "Iteration 18555, loss = 1.45730682\n",
      "Iteration 18556, loss = 1.45730451\n",
      "Iteration 18557, loss = 1.45730220\n",
      "Iteration 18558, loss = 1.45729989\n",
      "Iteration 18559, loss = 1.45729758\n",
      "Iteration 18560, loss = 1.45729528\n",
      "Iteration 18561, loss = 1.45729297\n",
      "Iteration 18562, loss = 1.45729066\n",
      "Iteration 18563, loss = 1.45728835\n",
      "Iteration 18564, loss = 1.45728605\n",
      "Iteration 18565, loss = 1.45728374\n",
      "Iteration 18566, loss = 1.45728143\n",
      "Iteration 18567, loss = 1.45727913\n",
      "Iteration 18568, loss = 1.45727682\n",
      "Iteration 18569, loss = 1.45727452\n",
      "Iteration 18570, loss = 1.45727221\n",
      "Iteration 18571, loss = 1.45726991\n",
      "Iteration 18572, loss = 1.45726760\n",
      "Iteration 18573, loss = 1.45726530\n",
      "Iteration 18574, loss = 1.45726299\n",
      "Iteration 18575, loss = 1.45726069\n",
      "Iteration 18576, loss = 1.45725839\n",
      "Iteration 18577, loss = 1.45725608\n",
      "Iteration 18578, loss = 1.45725378\n",
      "Iteration 18579, loss = 1.45725148\n",
      "Iteration 18580, loss = 1.45724917\n",
      "Iteration 18581, loss = 1.45724687\n",
      "Iteration 18582, loss = 1.45724457\n",
      "Iteration 18583, loss = 1.45724227\n",
      "Iteration 18584, loss = 1.45723997\n",
      "Iteration 18585, loss = 1.45723767\n",
      "Iteration 18586, loss = 1.45723537\n",
      "Iteration 18587, loss = 1.45723307\n",
      "Iteration 18588, loss = 1.45723077\n",
      "Iteration 18589, loss = 1.45722847\n",
      "Iteration 18590, loss = 1.45722617\n",
      "Iteration 18591, loss = 1.45722387\n",
      "Iteration 18592, loss = 1.45722157\n",
      "Iteration 18593, loss = 1.45721927\n",
      "Iteration 18594, loss = 1.45721697\n",
      "Iteration 18595, loss = 1.45721467\n",
      "Iteration 18596, loss = 1.45721237\n",
      "Iteration 18597, loss = 1.45721008\n",
      "Iteration 18598, loss = 1.45720778\n",
      "Iteration 18599, loss = 1.45720548\n",
      "Iteration 18600, loss = 1.45720319\n",
      "Iteration 18601, loss = 1.45720089\n",
      "Iteration 18602, loss = 1.45719859\n",
      "Iteration 18603, loss = 1.45719630\n",
      "Iteration 18604, loss = 1.45719400\n",
      "Iteration 18605, loss = 1.45719171\n",
      "Iteration 18606, loss = 1.45718941\n",
      "Iteration 18607, loss = 1.45718712\n",
      "Iteration 18608, loss = 1.45718482\n",
      "Iteration 18609, loss = 1.45718253\n",
      "Iteration 18610, loss = 1.45718023\n",
      "Iteration 18611, loss = 1.45717794\n",
      "Iteration 18612, loss = 1.45717565\n",
      "Iteration 18613, loss = 1.45717335\n",
      "Iteration 18614, loss = 1.45717106\n",
      "Iteration 18615, loss = 1.45716877\n",
      "Iteration 18616, loss = 1.45716648\n",
      "Iteration 18617, loss = 1.45716419\n",
      "Iteration 18618, loss = 1.45716189\n",
      "Iteration 18619, loss = 1.45715960\n",
      "Iteration 18620, loss = 1.45715731\n",
      "Iteration 18621, loss = 1.45715502\n",
      "Iteration 18622, loss = 1.45715273\n",
      "Iteration 18623, loss = 1.45715044\n",
      "Iteration 18624, loss = 1.45714815\n",
      "Iteration 18625, loss = 1.45714586\n",
      "Iteration 18626, loss = 1.45714357\n",
      "Iteration 18627, loss = 1.45714128\n",
      "Iteration 18628, loss = 1.45713899\n",
      "Iteration 18629, loss = 1.45713670\n",
      "Iteration 18630, loss = 1.45713442\n",
      "Iteration 18631, loss = 1.45713213\n",
      "Iteration 18632, loss = 1.45712984\n",
      "Iteration 18633, loss = 1.45712755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18634, loss = 1.45712527\n",
      "Iteration 18635, loss = 1.45712298\n",
      "Iteration 18636, loss = 1.45712069\n",
      "Iteration 18637, loss = 1.45711841\n",
      "Iteration 18638, loss = 1.45711612\n",
      "Iteration 18639, loss = 1.45711383\n",
      "Iteration 18640, loss = 1.45711155\n",
      "Iteration 18641, loss = 1.45710926\n",
      "Iteration 18642, loss = 1.45710698\n",
      "Iteration 18643, loss = 1.45710469\n",
      "Iteration 18644, loss = 1.45710241\n",
      "Iteration 18645, loss = 1.45710013\n",
      "Iteration 18646, loss = 1.45709784\n",
      "Iteration 18647, loss = 1.45709556\n",
      "Iteration 18648, loss = 1.45709328\n",
      "Iteration 18649, loss = 1.45709099\n",
      "Iteration 18650, loss = 1.45708871\n",
      "Iteration 18651, loss = 1.45708643\n",
      "Iteration 18652, loss = 1.45708414\n",
      "Iteration 18653, loss = 1.45708186\n",
      "Iteration 18654, loss = 1.45707958\n",
      "Iteration 18655, loss = 1.45707730\n",
      "Iteration 18656, loss = 1.45707502\n",
      "Iteration 18657, loss = 1.45707274\n",
      "Iteration 18658, loss = 1.45707046\n",
      "Iteration 18659, loss = 1.45706818\n",
      "Iteration 18660, loss = 1.45706590\n",
      "Iteration 18661, loss = 1.45706362\n",
      "Iteration 18662, loss = 1.45706134\n",
      "Iteration 18663, loss = 1.45705906\n",
      "Iteration 18664, loss = 1.45705678\n",
      "Iteration 18665, loss = 1.45705450\n",
      "Iteration 18666, loss = 1.45705223\n",
      "Iteration 18667, loss = 1.45704995\n",
      "Iteration 18668, loss = 1.45704767\n",
      "Iteration 18669, loss = 1.45704539\n",
      "Iteration 18670, loss = 1.45704312\n",
      "Iteration 18671, loss = 1.45704084\n",
      "Iteration 18672, loss = 1.45703856\n",
      "Iteration 18673, loss = 1.45703629\n",
      "Iteration 18674, loss = 1.45703401\n",
      "Iteration 18675, loss = 1.45703173\n",
      "Iteration 18676, loss = 1.45702946\n",
      "Iteration 18677, loss = 1.45702718\n",
      "Iteration 18678, loss = 1.45702491\n",
      "Iteration 18679, loss = 1.45702263\n",
      "Iteration 18680, loss = 1.45702036\n",
      "Iteration 18681, loss = 1.45701809\n",
      "Iteration 18682, loss = 1.45701581\n",
      "Iteration 18683, loss = 1.45701354\n",
      "Iteration 18684, loss = 1.45701127\n",
      "Iteration 18685, loss = 1.45700899\n",
      "Iteration 18686, loss = 1.45700672\n",
      "Iteration 18687, loss = 1.45700445\n",
      "Iteration 18688, loss = 1.45700218\n",
      "Iteration 18689, loss = 1.45699990\n",
      "Iteration 18690, loss = 1.45699763\n",
      "Iteration 18691, loss = 1.45699536\n",
      "Iteration 18692, loss = 1.45699309\n",
      "Iteration 18693, loss = 1.45699082\n",
      "Iteration 18694, loss = 1.45698855\n",
      "Iteration 18695, loss = 1.45698628\n",
      "Iteration 18696, loss = 1.45698401\n",
      "Iteration 18697, loss = 1.45698174\n",
      "Iteration 18698, loss = 1.45697947\n",
      "Iteration 18699, loss = 1.45697720\n",
      "Iteration 18700, loss = 1.45697493\n",
      "Iteration 18701, loss = 1.45697267\n",
      "Iteration 18702, loss = 1.45697040\n",
      "Iteration 18703, loss = 1.45696813\n",
      "Iteration 18704, loss = 1.45696586\n",
      "Iteration 18705, loss = 1.45696359\n",
      "Iteration 18706, loss = 1.45696133\n",
      "Iteration 18707, loss = 1.45695906\n",
      "Iteration 18708, loss = 1.45695679\n",
      "Iteration 18709, loss = 1.45695453\n",
      "Iteration 18710, loss = 1.45695226\n",
      "Iteration 18711, loss = 1.45695000\n",
      "Iteration 18712, loss = 1.45694773\n",
      "Iteration 18713, loss = 1.45694547\n",
      "Iteration 18714, loss = 1.45694320\n",
      "Iteration 18715, loss = 1.45694094\n",
      "Iteration 18716, loss = 1.45693867\n",
      "Iteration 18717, loss = 1.45693641\n",
      "Iteration 18718, loss = 1.45693414\n",
      "Iteration 18719, loss = 1.45693188\n",
      "Iteration 18720, loss = 1.45692962\n",
      "Iteration 18721, loss = 1.45692736\n",
      "Iteration 18722, loss = 1.45692509\n",
      "Iteration 18723, loss = 1.45692283\n",
      "Iteration 18724, loss = 1.45692057\n",
      "Iteration 18725, loss = 1.45691831\n",
      "Iteration 18726, loss = 1.45691605\n",
      "Iteration 18727, loss = 1.45691378\n",
      "Iteration 18728, loss = 1.45691152\n",
      "Iteration 18729, loss = 1.45690926\n",
      "Iteration 18730, loss = 1.45690700\n",
      "Iteration 18731, loss = 1.45690474\n",
      "Iteration 18732, loss = 1.45690248\n",
      "Iteration 18733, loss = 1.45690022\n",
      "Iteration 18734, loss = 1.45689796\n",
      "Iteration 18735, loss = 1.45689570\n",
      "Iteration 18736, loss = 1.45689345\n",
      "Iteration 18737, loss = 1.45689119\n",
      "Iteration 18738, loss = 1.45688893\n",
      "Iteration 18739, loss = 1.45688667\n",
      "Iteration 18740, loss = 1.45688441\n",
      "Iteration 18741, loss = 1.45688216\n",
      "Iteration 18742, loss = 1.45687990\n",
      "Iteration 18743, loss = 1.45687764\n",
      "Iteration 18744, loss = 1.45687539\n",
      "Iteration 18745, loss = 1.45687313\n",
      "Iteration 18746, loss = 1.45687087\n",
      "Iteration 18747, loss = 1.45686862\n",
      "Iteration 18748, loss = 1.45686636\n",
      "Iteration 18749, loss = 1.45686411\n",
      "Iteration 18750, loss = 1.45686185\n",
      "Iteration 18751, loss = 1.45685960\n",
      "Iteration 18752, loss = 1.45685734\n",
      "Iteration 18753, loss = 1.45685509\n",
      "Iteration 18754, loss = 1.45685284\n",
      "Iteration 18755, loss = 1.45685058\n",
      "Iteration 18756, loss = 1.45684833\n",
      "Iteration 18757, loss = 1.45684608\n",
      "Iteration 18758, loss = 1.45684382\n",
      "Iteration 18759, loss = 1.45684157\n",
      "Iteration 18760, loss = 1.45683932\n",
      "Iteration 18761, loss = 1.45683707\n",
      "Iteration 18762, loss = 1.45683482\n",
      "Iteration 18763, loss = 1.45683256\n",
      "Iteration 18764, loss = 1.45683031\n",
      "Iteration 18765, loss = 1.45682806\n",
      "Iteration 18766, loss = 1.45682581\n",
      "Iteration 18767, loss = 1.45682356\n",
      "Iteration 18768, loss = 1.45682131\n",
      "Iteration 18769, loss = 1.45681906\n",
      "Iteration 18770, loss = 1.45681681\n",
      "Iteration 18771, loss = 1.45681456\n",
      "Iteration 18772, loss = 1.45681232\n",
      "Iteration 18773, loss = 1.45681007\n",
      "Iteration 18774, loss = 1.45680782\n",
      "Iteration 18775, loss = 1.45680557\n",
      "Iteration 18776, loss = 1.45680332\n",
      "Iteration 18777, loss = 1.45680108\n",
      "Iteration 18778, loss = 1.45679883\n",
      "Iteration 18779, loss = 1.45679658\n",
      "Iteration 18780, loss = 1.45679434\n",
      "Iteration 18781, loss = 1.45679209\n",
      "Iteration 18782, loss = 1.45678984\n",
      "Iteration 18783, loss = 1.45678760\n",
      "Iteration 18784, loss = 1.45678535\n",
      "Iteration 18785, loss = 1.45678311\n",
      "Iteration 18786, loss = 1.45678086\n",
      "Iteration 18787, loss = 1.45677862\n",
      "Iteration 18788, loss = 1.45677637\n",
      "Iteration 18789, loss = 1.45677413\n",
      "Iteration 18790, loss = 1.45677188\n",
      "Iteration 18791, loss = 1.45676964\n",
      "Iteration 18792, loss = 1.45676740\n",
      "Iteration 18793, loss = 1.45676515\n",
      "Iteration 18794, loss = 1.45676291\n",
      "Iteration 18795, loss = 1.45676067\n",
      "Iteration 18796, loss = 1.45675843\n",
      "Iteration 18797, loss = 1.45675619\n",
      "Iteration 18798, loss = 1.45675394\n",
      "Iteration 18799, loss = 1.45675170\n",
      "Iteration 18800, loss = 1.45674946\n",
      "Iteration 18801, loss = 1.45674722\n",
      "Iteration 18802, loss = 1.45674498\n",
      "Iteration 18803, loss = 1.45674274\n",
      "Iteration 18804, loss = 1.45674050\n",
      "Iteration 18805, loss = 1.45673826\n",
      "Iteration 18806, loss = 1.45673602\n",
      "Iteration 18807, loss = 1.45673378\n",
      "Iteration 18808, loss = 1.45673154\n",
      "Iteration 18809, loss = 1.45672930\n",
      "Iteration 18810, loss = 1.45672706\n",
      "Iteration 18811, loss = 1.45672483\n",
      "Iteration 18812, loss = 1.45672259\n",
      "Iteration 18813, loss = 1.45672035\n",
      "Iteration 18814, loss = 1.45671811\n",
      "Iteration 18815, loss = 1.45671588\n",
      "Iteration 18816, loss = 1.45671364\n",
      "Iteration 18817, loss = 1.45671140\n",
      "Iteration 18818, loss = 1.45670917\n",
      "Iteration 18819, loss = 1.45670693\n",
      "Iteration 18820, loss = 1.45670470\n",
      "Iteration 18821, loss = 1.45670246\n",
      "Iteration 18822, loss = 1.45670023\n",
      "Iteration 18823, loss = 1.45669799\n",
      "Iteration 18824, loss = 1.45669576\n",
      "Iteration 18825, loss = 1.45669352\n",
      "Iteration 18826, loss = 1.45669129\n",
      "Iteration 18827, loss = 1.45668905\n",
      "Iteration 18828, loss = 1.45668682\n",
      "Iteration 18829, loss = 1.45668459\n",
      "Iteration 18830, loss = 1.45668235\n",
      "Iteration 18831, loss = 1.45668012\n",
      "Iteration 18832, loss = 1.45667789\n",
      "Iteration 18833, loss = 1.45667566\n",
      "Iteration 18834, loss = 1.45667343\n",
      "Iteration 18835, loss = 1.45667119\n",
      "Iteration 18836, loss = 1.45666896\n",
      "Iteration 18837, loss = 1.45666673\n",
      "Iteration 18838, loss = 1.45666450\n",
      "Iteration 18839, loss = 1.45666227\n",
      "Iteration 18840, loss = 1.45666004\n",
      "Iteration 18841, loss = 1.45665781\n",
      "Iteration 18842, loss = 1.45665558\n",
      "Iteration 18843, loss = 1.45665335\n",
      "Iteration 18844, loss = 1.45665112\n",
      "Iteration 18845, loss = 1.45664889\n",
      "Iteration 18846, loss = 1.45664666\n",
      "Iteration 18847, loss = 1.45664444\n",
      "Iteration 18848, loss = 1.45664221\n",
      "Iteration 18849, loss = 1.45663998\n",
      "Iteration 18850, loss = 1.45663775\n",
      "Iteration 18851, loss = 1.45663553\n",
      "Iteration 18852, loss = 1.45663330\n",
      "Iteration 18853, loss = 1.45663107\n",
      "Iteration 18854, loss = 1.45662885\n",
      "Iteration 18855, loss = 1.45662662\n",
      "Iteration 18856, loss = 1.45662439\n",
      "Iteration 18857, loss = 1.45662217\n",
      "Iteration 18858, loss = 1.45661994\n",
      "Iteration 18859, loss = 1.45661772\n",
      "Iteration 18860, loss = 1.45661549\n",
      "Iteration 18861, loss = 1.45661327\n",
      "Iteration 18862, loss = 1.45661104\n",
      "Iteration 18863, loss = 1.45660882\n",
      "Iteration 18864, loss = 1.45660660\n",
      "Iteration 18865, loss = 1.45660437\n",
      "Iteration 18866, loss = 1.45660215\n",
      "Iteration 18867, loss = 1.45659993\n",
      "Iteration 18868, loss = 1.45659770\n",
      "Iteration 18869, loss = 1.45659548\n",
      "Iteration 18870, loss = 1.45659326\n",
      "Iteration 18871, loss = 1.45659104\n",
      "Iteration 18872, loss = 1.45658882\n",
      "Iteration 18873, loss = 1.45658660\n",
      "Iteration 18874, loss = 1.45658437\n",
      "Iteration 18875, loss = 1.45658215\n",
      "Iteration 18876, loss = 1.45657993\n",
      "Iteration 18877, loss = 1.45657771\n",
      "Iteration 18878, loss = 1.45657549\n",
      "Iteration 18879, loss = 1.45657327\n",
      "Iteration 18880, loss = 1.45657105\n",
      "Iteration 18881, loss = 1.45656883\n",
      "Iteration 18882, loss = 1.45656662\n",
      "Iteration 18883, loss = 1.45656440\n",
      "Iteration 18884, loss = 1.45656218\n",
      "Iteration 18885, loss = 1.45655996\n",
      "Iteration 18886, loss = 1.45655774\n",
      "Iteration 18887, loss = 1.45655553\n",
      "Iteration 18888, loss = 1.45655331\n",
      "Iteration 18889, loss = 1.45655109\n",
      "Iteration 18890, loss = 1.45654887\n",
      "Iteration 18891, loss = 1.45654666\n",
      "Iteration 18892, loss = 1.45654444\n",
      "Iteration 18893, loss = 1.45654223\n",
      "Iteration 18894, loss = 1.45654001\n",
      "Iteration 18895, loss = 1.45653780\n",
      "Iteration 18896, loss = 1.45653558\n",
      "Iteration 18897, loss = 1.45653337\n",
      "Iteration 18898, loss = 1.45653115\n",
      "Iteration 18899, loss = 1.45652894\n",
      "Iteration 18900, loss = 1.45652672\n",
      "Iteration 18901, loss = 1.45652451\n",
      "Iteration 18902, loss = 1.45652230\n",
      "Iteration 18903, loss = 1.45652008\n",
      "Iteration 18904, loss = 1.45651787\n",
      "Iteration 18905, loss = 1.45651566\n",
      "Iteration 18906, loss = 1.45651344\n",
      "Iteration 18907, loss = 1.45651123\n",
      "Iteration 18908, loss = 1.45650902\n",
      "Iteration 18909, loss = 1.45650681\n",
      "Iteration 18910, loss = 1.45650460\n",
      "Iteration 18911, loss = 1.45650239\n",
      "Iteration 18912, loss = 1.45650018\n",
      "Iteration 18913, loss = 1.45649797\n",
      "Iteration 18914, loss = 1.45649576\n",
      "Iteration 18915, loss = 1.45649355\n",
      "Iteration 18916, loss = 1.45649134\n",
      "Iteration 18917, loss = 1.45648913\n",
      "Iteration 18918, loss = 1.45648692\n",
      "Iteration 18919, loss = 1.45648471\n",
      "Iteration 18920, loss = 1.45648250\n",
      "Iteration 18921, loss = 1.45648029\n",
      "Iteration 18922, loss = 1.45647808\n",
      "Iteration 18923, loss = 1.45647588\n",
      "Iteration 18924, loss = 1.45647367\n",
      "Iteration 18925, loss = 1.45647146\n",
      "Iteration 18926, loss = 1.45646925\n",
      "Iteration 18927, loss = 1.45646705\n",
      "Iteration 18928, loss = 1.45646484\n",
      "Iteration 18929, loss = 1.45646263\n",
      "Iteration 18930, loss = 1.45646043\n",
      "Iteration 18931, loss = 1.45645822\n",
      "Iteration 18932, loss = 1.45645602\n",
      "Iteration 18933, loss = 1.45645381\n",
      "Iteration 18934, loss = 1.45645161\n",
      "Iteration 18935, loss = 1.45644940\n",
      "Iteration 18936, loss = 1.45644720\n",
      "Iteration 18937, loss = 1.45644499\n",
      "Iteration 18938, loss = 1.45644279\n",
      "Iteration 18939, loss = 1.45644059\n",
      "Iteration 18940, loss = 1.45643838\n",
      "Iteration 18941, loss = 1.45643618\n",
      "Iteration 18942, loss = 1.45643398\n",
      "Iteration 18943, loss = 1.45643177\n",
      "Iteration 18944, loss = 1.45642957\n",
      "Iteration 18945, loss = 1.45642737\n",
      "Iteration 18946, loss = 1.45642517\n",
      "Iteration 18947, loss = 1.45642297\n",
      "Iteration 18948, loss = 1.45642077\n",
      "Iteration 18949, loss = 1.45641857\n",
      "Iteration 18950, loss = 1.45641637\n",
      "Iteration 18951, loss = 1.45641416\n",
      "Iteration 18952, loss = 1.45641196\n",
      "Iteration 18953, loss = 1.45640977\n",
      "Iteration 18954, loss = 1.45640757\n",
      "Iteration 18955, loss = 1.45640537\n",
      "Iteration 18956, loss = 1.45640317\n",
      "Iteration 18957, loss = 1.45640097\n",
      "Iteration 18958, loss = 1.45639877\n",
      "Iteration 18959, loss = 1.45639657\n",
      "Iteration 18960, loss = 1.45639437\n",
      "Iteration 18961, loss = 1.45639218\n",
      "Iteration 18962, loss = 1.45638998\n",
      "Iteration 18963, loss = 1.45638778\n",
      "Iteration 18964, loss = 1.45638558\n",
      "Iteration 18965, loss = 1.45638339\n",
      "Iteration 18966, loss = 1.45638119\n",
      "Iteration 18967, loss = 1.45637900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18968, loss = 1.45637680\n",
      "Iteration 18969, loss = 1.45637460\n",
      "Iteration 18970, loss = 1.45637241\n",
      "Iteration 18971, loss = 1.45637021\n",
      "Iteration 18972, loss = 1.45636802\n",
      "Iteration 18973, loss = 1.45636582\n",
      "Iteration 18974, loss = 1.45636363\n",
      "Iteration 18975, loss = 1.45636144\n",
      "Iteration 18976, loss = 1.45635924\n",
      "Iteration 18977, loss = 1.45635705\n",
      "Iteration 18978, loss = 1.45635486\n",
      "Iteration 18979, loss = 1.45635266\n",
      "Iteration 18980, loss = 1.45635047\n",
      "Iteration 18981, loss = 1.45634828\n",
      "Iteration 18982, loss = 1.45634609\n",
      "Iteration 18983, loss = 1.45634389\n",
      "Iteration 18984, loss = 1.45634170\n",
      "Iteration 18985, loss = 1.45633951\n",
      "Iteration 18986, loss = 1.45633732\n",
      "Iteration 18987, loss = 1.45633513\n",
      "Iteration 18988, loss = 1.45633294\n",
      "Iteration 18989, loss = 1.45633075\n",
      "Iteration 18990, loss = 1.45632856\n",
      "Iteration 18991, loss = 1.45632637\n",
      "Iteration 18992, loss = 1.45632418\n",
      "Iteration 18993, loss = 1.45632199\n",
      "Iteration 18994, loss = 1.45631980\n",
      "Iteration 18995, loss = 1.45631761\n",
      "Iteration 18996, loss = 1.45631543\n",
      "Iteration 18997, loss = 1.45631324\n",
      "Iteration 18998, loss = 1.45631105\n",
      "Iteration 18999, loss = 1.45630886\n",
      "Iteration 19000, loss = 1.45630667\n",
      "Iteration 19001, loss = 1.45630449\n",
      "Iteration 19002, loss = 1.45630230\n",
      "Iteration 19003, loss = 1.45630011\n",
      "Iteration 19004, loss = 1.45629793\n",
      "Iteration 19005, loss = 1.45629574\n",
      "Iteration 19006, loss = 1.45629356\n",
      "Iteration 19007, loss = 1.45629137\n",
      "Iteration 19008, loss = 1.45628919\n",
      "Iteration 19009, loss = 1.45628700\n",
      "Iteration 19010, loss = 1.45628482\n",
      "Iteration 19011, loss = 1.45628263\n",
      "Iteration 19012, loss = 1.45628045\n",
      "Iteration 19013, loss = 1.45627826\n",
      "Iteration 19014, loss = 1.45627608\n",
      "Iteration 19015, loss = 1.45627390\n",
      "Iteration 19016, loss = 1.45627171\n",
      "Iteration 19017, loss = 1.45626953\n",
      "Iteration 19018, loss = 1.45626735\n",
      "Iteration 19019, loss = 1.45626517\n",
      "Iteration 19020, loss = 1.45626298\n",
      "Iteration 19021, loss = 1.45626080\n",
      "Iteration 19022, loss = 1.45625862\n",
      "Iteration 19023, loss = 1.45625644\n",
      "Iteration 19024, loss = 1.45625426\n",
      "Iteration 19025, loss = 1.45625208\n",
      "Iteration 19026, loss = 1.45624990\n",
      "Iteration 19027, loss = 1.45624772\n",
      "Iteration 19028, loss = 1.45624554\n",
      "Iteration 19029, loss = 1.45624336\n",
      "Iteration 19030, loss = 1.45624118\n",
      "Iteration 19031, loss = 1.45623900\n",
      "Iteration 19032, loss = 1.45623682\n",
      "Iteration 19033, loss = 1.45623464\n",
      "Iteration 19034, loss = 1.45623246\n",
      "Iteration 19035, loss = 1.45623028\n",
      "Iteration 19036, loss = 1.45622811\n",
      "Iteration 19037, loss = 1.45622593\n",
      "Iteration 19038, loss = 1.45622375\n",
      "Iteration 19039, loss = 1.45622158\n",
      "Iteration 19040, loss = 1.45621940\n",
      "Iteration 19041, loss = 1.45621722\n",
      "Iteration 19042, loss = 1.45621505\n",
      "Iteration 19043, loss = 1.45621287\n",
      "Iteration 19044, loss = 1.45621069\n",
      "Iteration 19045, loss = 1.45620852\n",
      "Iteration 19046, loss = 1.45620634\n",
      "Iteration 19047, loss = 1.45620417\n",
      "Iteration 19048, loss = 1.45620199\n",
      "Iteration 19049, loss = 1.45619982\n",
      "Iteration 19050, loss = 1.45619765\n",
      "Iteration 19051, loss = 1.45619547\n",
      "Iteration 19052, loss = 1.45619330\n",
      "Iteration 19053, loss = 1.45619112\n",
      "Iteration 19054, loss = 1.45618895\n",
      "Iteration 19055, loss = 1.45618678\n",
      "Iteration 19056, loss = 1.45618461\n",
      "Iteration 19057, loss = 1.45618243\n",
      "Iteration 19058, loss = 1.45618026\n",
      "Iteration 19059, loss = 1.45617809\n",
      "Iteration 19060, loss = 1.45617592\n",
      "Iteration 19061, loss = 1.45617375\n",
      "Iteration 19062, loss = 1.45617158\n",
      "Iteration 19063, loss = 1.45616941\n",
      "Iteration 19064, loss = 1.45616723\n",
      "Iteration 19065, loss = 1.45616506\n",
      "Iteration 19066, loss = 1.45616289\n",
      "Iteration 19067, loss = 1.45616072\n",
      "Iteration 19068, loss = 1.45615856\n",
      "Iteration 19069, loss = 1.45615639\n",
      "Iteration 19070, loss = 1.45615422\n",
      "Iteration 19071, loss = 1.45615205\n",
      "Iteration 19072, loss = 1.45614988\n",
      "Iteration 19073, loss = 1.45614771\n",
      "Iteration 19074, loss = 1.45614554\n",
      "Iteration 19075, loss = 1.45614338\n",
      "Iteration 19076, loss = 1.45614121\n",
      "Iteration 19077, loss = 1.45613904\n",
      "Iteration 19078, loss = 1.45613688\n",
      "Iteration 19079, loss = 1.45613471\n",
      "Iteration 19080, loss = 1.45613254\n",
      "Iteration 19081, loss = 1.45613038\n",
      "Iteration 19082, loss = 1.45612821\n",
      "Iteration 19083, loss = 1.45612605\n",
      "Iteration 19084, loss = 1.45612388\n",
      "Iteration 19085, loss = 1.45612172\n",
      "Iteration 19086, loss = 1.45611955\n",
      "Iteration 19087, loss = 1.45611739\n",
      "Iteration 19088, loss = 1.45611522\n",
      "Iteration 19089, loss = 1.45611306\n",
      "Iteration 19090, loss = 1.45611089\n",
      "Iteration 19091, loss = 1.45610873\n",
      "Iteration 19092, loss = 1.45610657\n",
      "Iteration 19093, loss = 1.45610440\n",
      "Iteration 19094, loss = 1.45610224\n",
      "Iteration 19095, loss = 1.45610008\n",
      "Iteration 19096, loss = 1.45609792\n",
      "Iteration 19097, loss = 1.45609576\n",
      "Iteration 19098, loss = 1.45609359\n",
      "Iteration 19099, loss = 1.45609143\n",
      "Iteration 19100, loss = 1.45608927\n",
      "Iteration 19101, loss = 1.45608711\n",
      "Iteration 19102, loss = 1.45608495\n",
      "Iteration 19103, loss = 1.45608279\n",
      "Iteration 19104, loss = 1.45608063\n",
      "Iteration 19105, loss = 1.45607847\n",
      "Iteration 19106, loss = 1.45607631\n",
      "Iteration 19107, loss = 1.45607415\n",
      "Iteration 19108, loss = 1.45607199\n",
      "Iteration 19109, loss = 1.45606983\n",
      "Iteration 19110, loss = 1.45606768\n",
      "Iteration 19111, loss = 1.45606552\n",
      "Iteration 19112, loss = 1.45606336\n",
      "Iteration 19113, loss = 1.45606120\n",
      "Iteration 19114, loss = 1.45605904\n",
      "Iteration 19115, loss = 1.45605689\n",
      "Iteration 19116, loss = 1.45605473\n",
      "Iteration 19117, loss = 1.45605257\n",
      "Iteration 19118, loss = 1.45605042\n",
      "Iteration 19119, loss = 1.45604826\n",
      "Iteration 19120, loss = 1.45604611\n",
      "Iteration 19121, loss = 1.45604395\n",
      "Iteration 19122, loss = 1.45604179\n",
      "Iteration 19123, loss = 1.45603964\n",
      "Iteration 19124, loss = 1.45603748\n",
      "Iteration 19125, loss = 1.45603533\n",
      "Iteration 19126, loss = 1.45603318\n",
      "Iteration 19127, loss = 1.45603102\n",
      "Iteration 19128, loss = 1.45602887\n",
      "Iteration 19129, loss = 1.45602671\n",
      "Iteration 19130, loss = 1.45602456\n",
      "Iteration 19131, loss = 1.45602241\n",
      "Iteration 19132, loss = 1.45602025\n",
      "Iteration 19133, loss = 1.45601810\n",
      "Iteration 19134, loss = 1.45601595\n",
      "Iteration 19135, loss = 1.45601380\n",
      "Iteration 19136, loss = 1.45601165\n",
      "Iteration 19137, loss = 1.45600949\n",
      "Iteration 19138, loss = 1.45600734\n",
      "Iteration 19139, loss = 1.45600519\n",
      "Iteration 19140, loss = 1.45600304\n",
      "Iteration 19141, loss = 1.45600089\n",
      "Iteration 19142, loss = 1.45599874\n",
      "Iteration 19143, loss = 1.45599659\n",
      "Iteration 19144, loss = 1.45599444\n",
      "Iteration 19145, loss = 1.45599229\n",
      "Iteration 19146, loss = 1.45599014\n",
      "Iteration 19147, loss = 1.45598799\n",
      "Iteration 19148, loss = 1.45598585\n",
      "Iteration 19149, loss = 1.45598370\n",
      "Iteration 19150, loss = 1.45598155\n",
      "Iteration 19151, loss = 1.45597940\n",
      "Iteration 19152, loss = 1.45597725\n",
      "Iteration 19153, loss = 1.45597511\n",
      "Iteration 19154, loss = 1.45597296\n",
      "Iteration 19155, loss = 1.45597081\n",
      "Iteration 19156, loss = 1.45596867\n",
      "Iteration 19157, loss = 1.45596652\n",
      "Iteration 19158, loss = 1.45596437\n",
      "Iteration 19159, loss = 1.45596223\n",
      "Iteration 19160, loss = 1.45596008\n",
      "Iteration 19161, loss = 1.45595794\n",
      "Iteration 19162, loss = 1.45595579\n",
      "Iteration 19163, loss = 1.45595365\n",
      "Iteration 19164, loss = 1.45595150\n",
      "Iteration 19165, loss = 1.45594936\n",
      "Iteration 19166, loss = 1.45594721\n",
      "Iteration 19167, loss = 1.45594507\n",
      "Iteration 19168, loss = 1.45594293\n",
      "Iteration 19169, loss = 1.45594078\n",
      "Iteration 19170, loss = 1.45593864\n",
      "Iteration 19171, loss = 1.45593650\n",
      "Iteration 19172, loss = 1.45593436\n",
      "Iteration 19173, loss = 1.45593221\n",
      "Iteration 19174, loss = 1.45593007\n",
      "Iteration 19175, loss = 1.45592793\n",
      "Iteration 19176, loss = 1.45592579\n",
      "Iteration 19177, loss = 1.45592365\n",
      "Iteration 19178, loss = 1.45592151\n",
      "Iteration 19179, loss = 1.45591937\n",
      "Iteration 19180, loss = 1.45591723\n",
      "Iteration 19181, loss = 1.45591509\n",
      "Iteration 19182, loss = 1.45591295\n",
      "Iteration 19183, loss = 1.45591081\n",
      "Iteration 19184, loss = 1.45590867\n",
      "Iteration 19185, loss = 1.45590653\n",
      "Iteration 19186, loss = 1.45590439\n",
      "Iteration 19187, loss = 1.45590225\n",
      "Iteration 19188, loss = 1.45590011\n",
      "Iteration 19189, loss = 1.45589797\n",
      "Iteration 19190, loss = 1.45589584\n",
      "Iteration 19191, loss = 1.45589370\n",
      "Iteration 19192, loss = 1.45589156\n",
      "Iteration 19193, loss = 1.45588942\n",
      "Iteration 19194, loss = 1.45588729\n",
      "Iteration 19195, loss = 1.45588515\n",
      "Iteration 19196, loss = 1.45588301\n",
      "Iteration 19197, loss = 1.45588088\n",
      "Iteration 19198, loss = 1.45587874\n",
      "Iteration 19199, loss = 1.45587661\n",
      "Iteration 19200, loss = 1.45587447\n",
      "Iteration 19201, loss = 1.45587234\n",
      "Iteration 19202, loss = 1.45587020\n",
      "Iteration 19203, loss = 1.45586807\n",
      "Iteration 19204, loss = 1.45586593\n",
      "Iteration 19205, loss = 1.45586380\n",
      "Iteration 19206, loss = 1.45586167\n",
      "Iteration 19207, loss = 1.45585953\n",
      "Iteration 19208, loss = 1.45585740\n",
      "Iteration 19209, loss = 1.45585527\n",
      "Iteration 19210, loss = 1.45585313\n",
      "Iteration 19211, loss = 1.45585100\n",
      "Iteration 19212, loss = 1.45584887\n",
      "Iteration 19213, loss = 1.45584674\n",
      "Iteration 19214, loss = 1.45584461\n",
      "Iteration 19215, loss = 1.45584247\n",
      "Iteration 19216, loss = 1.45584034\n",
      "Iteration 19217, loss = 1.45583821\n",
      "Iteration 19218, loss = 1.45583608\n",
      "Iteration 19219, loss = 1.45583395\n",
      "Iteration 19220, loss = 1.45583182\n",
      "Iteration 19221, loss = 1.45582969\n",
      "Iteration 19222, loss = 1.45582756\n",
      "Iteration 19223, loss = 1.45582543\n",
      "Iteration 19224, loss = 1.45582330\n",
      "Iteration 19225, loss = 1.45582117\n",
      "Iteration 19226, loss = 1.45581905\n",
      "Iteration 19227, loss = 1.45581692\n",
      "Iteration 19228, loss = 1.45581479\n",
      "Iteration 19229, loss = 1.45581266\n",
      "Iteration 19230, loss = 1.45581053\n",
      "Iteration 19231, loss = 1.45580841\n",
      "Iteration 19232, loss = 1.45580628\n",
      "Iteration 19233, loss = 1.45580415\n",
      "Iteration 19234, loss = 1.45580203\n",
      "Iteration 19235, loss = 1.45579990\n",
      "Iteration 19236, loss = 1.45579777\n",
      "Iteration 19237, loss = 1.45579565\n",
      "Iteration 19238, loss = 1.45579352\n",
      "Iteration 19239, loss = 1.45579140\n",
      "Iteration 19240, loss = 1.45578927\n",
      "Iteration 19241, loss = 1.45578715\n",
      "Iteration 19242, loss = 1.45578502\n",
      "Iteration 19243, loss = 1.45578290\n",
      "Iteration 19244, loss = 1.45578077\n",
      "Iteration 19245, loss = 1.45577865\n",
      "Iteration 19246, loss = 1.45577653\n",
      "Iteration 19247, loss = 1.45577440\n",
      "Iteration 19248, loss = 1.45577228\n",
      "Iteration 19249, loss = 1.45577016\n",
      "Iteration 19250, loss = 1.45576804\n",
      "Iteration 19251, loss = 1.45576591\n",
      "Iteration 19252, loss = 1.45576379\n",
      "Iteration 19253, loss = 1.45576167\n",
      "Iteration 19254, loss = 1.45575955\n",
      "Iteration 19255, loss = 1.45575743\n",
      "Iteration 19256, loss = 1.45575531\n",
      "Iteration 19257, loss = 1.45575318\n",
      "Iteration 19258, loss = 1.45575106\n",
      "Iteration 19259, loss = 1.45574894\n",
      "Iteration 19260, loss = 1.45574682\n",
      "Iteration 19261, loss = 1.45574470\n",
      "Iteration 19262, loss = 1.45574259\n",
      "Iteration 19263, loss = 1.45574047\n",
      "Iteration 19264, loss = 1.45573835\n",
      "Iteration 19265, loss = 1.45573623\n",
      "Iteration 19266, loss = 1.45573411\n",
      "Iteration 19267, loss = 1.45573199\n",
      "Iteration 19268, loss = 1.45572987\n",
      "Iteration 19269, loss = 1.45572776\n",
      "Iteration 19270, loss = 1.45572564\n",
      "Iteration 19271, loss = 1.45572352\n",
      "Iteration 19272, loss = 1.45572140\n",
      "Iteration 19273, loss = 1.45571929\n",
      "Iteration 19274, loss = 1.45571717\n",
      "Iteration 19275, loss = 1.45571506\n",
      "Iteration 19276, loss = 1.45571294\n",
      "Iteration 19277, loss = 1.45571082\n",
      "Iteration 19278, loss = 1.45570871\n",
      "Iteration 19279, loss = 1.45570659\n",
      "Iteration 19280, loss = 1.45570448\n",
      "Iteration 19281, loss = 1.45570236\n",
      "Iteration 19282, loss = 1.45570025\n",
      "Iteration 19283, loss = 1.45569814\n",
      "Iteration 19284, loss = 1.45569602\n",
      "Iteration 19285, loss = 1.45569391\n",
      "Iteration 19286, loss = 1.45569179\n",
      "Iteration 19287, loss = 1.45568968\n",
      "Iteration 19288, loss = 1.45568757\n",
      "Iteration 19289, loss = 1.45568546\n",
      "Iteration 19290, loss = 1.45568334\n",
      "Iteration 19291, loss = 1.45568123\n",
      "Iteration 19292, loss = 1.45567912\n",
      "Iteration 19293, loss = 1.45567701\n",
      "Iteration 19294, loss = 1.45567490\n",
      "Iteration 19295, loss = 1.45567279\n",
      "Iteration 19296, loss = 1.45567067\n",
      "Iteration 19297, loss = 1.45566856\n",
      "Iteration 19298, loss = 1.45566645\n",
      "Iteration 19299, loss = 1.45566434\n",
      "Iteration 19300, loss = 1.45566223\n",
      "Iteration 19301, loss = 1.45566012\n",
      "Iteration 19302, loss = 1.45565801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19303, loss = 1.45565591\n",
      "Iteration 19304, loss = 1.45565380\n",
      "Iteration 19305, loss = 1.45565169\n",
      "Iteration 19306, loss = 1.45564958\n",
      "Iteration 19307, loss = 1.45564747\n",
      "Iteration 19308, loss = 1.45564536\n",
      "Iteration 19309, loss = 1.45564326\n",
      "Iteration 19310, loss = 1.45564115\n",
      "Iteration 19311, loss = 1.45563904\n",
      "Iteration 19312, loss = 1.45563693\n",
      "Iteration 19313, loss = 1.45563483\n",
      "Iteration 19314, loss = 1.45563272\n",
      "Iteration 19315, loss = 1.45563062\n",
      "Iteration 19316, loss = 1.45562851\n",
      "Iteration 19317, loss = 1.45562640\n",
      "Iteration 19318, loss = 1.45562430\n",
      "Iteration 19319, loss = 1.45562219\n",
      "Iteration 19320, loss = 1.45562009\n",
      "Iteration 19321, loss = 1.45561798\n",
      "Iteration 19322, loss = 1.45561588\n",
      "Iteration 19323, loss = 1.45561378\n",
      "Iteration 19324, loss = 1.45561167\n",
      "Iteration 19325, loss = 1.45560957\n",
      "Iteration 19326, loss = 1.45560747\n",
      "Iteration 19327, loss = 1.45560536\n",
      "Iteration 19328, loss = 1.45560326\n",
      "Iteration 19329, loss = 1.45560116\n",
      "Iteration 19330, loss = 1.45559905\n",
      "Iteration 19331, loss = 1.45559695\n",
      "Iteration 19332, loss = 1.45559485\n",
      "Iteration 19333, loss = 1.45559275\n",
      "Iteration 19334, loss = 1.45559065\n",
      "Iteration 19335, loss = 1.45558855\n",
      "Iteration 19336, loss = 1.45558645\n",
      "Iteration 19337, loss = 1.45558435\n",
      "Iteration 19338, loss = 1.45558225\n",
      "Iteration 19339, loss = 1.45558015\n",
      "Iteration 19340, loss = 1.45557805\n",
      "Iteration 19341, loss = 1.45557595\n",
      "Iteration 19342, loss = 1.45557385\n",
      "Iteration 19343, loss = 1.45557175\n",
      "Iteration 19344, loss = 1.45556965\n",
      "Iteration 19345, loss = 1.45556755\n",
      "Iteration 19346, loss = 1.45556545\n",
      "Iteration 19347, loss = 1.45556335\n",
      "Iteration 19348, loss = 1.45556126\n",
      "Iteration 19349, loss = 1.45555916\n",
      "Iteration 19350, loss = 1.45555706\n",
      "Iteration 19351, loss = 1.45555496\n",
      "Iteration 19352, loss = 1.45555287\n",
      "Iteration 19353, loss = 1.45555077\n",
      "Iteration 19354, loss = 1.45554867\n",
      "Iteration 19355, loss = 1.45554658\n",
      "Iteration 19356, loss = 1.45554448\n",
      "Iteration 19357, loss = 1.45554239\n",
      "Iteration 19358, loss = 1.45554029\n",
      "Iteration 19359, loss = 1.45553820\n",
      "Iteration 19360, loss = 1.45553610\n",
      "Iteration 19361, loss = 1.45553401\n",
      "Iteration 19362, loss = 1.45553191\n",
      "Iteration 19363, loss = 1.45552982\n",
      "Iteration 19364, loss = 1.45552773\n",
      "Iteration 19365, loss = 1.45552563\n",
      "Iteration 19366, loss = 1.45552354\n",
      "Iteration 19367, loss = 1.45552145\n",
      "Iteration 19368, loss = 1.45551935\n",
      "Iteration 19369, loss = 1.45551726\n",
      "Iteration 19370, loss = 1.45551517\n",
      "Iteration 19371, loss = 1.45551308\n",
      "Iteration 19372, loss = 1.45551098\n",
      "Iteration 19373, loss = 1.45550889\n",
      "Iteration 19374, loss = 1.45550680\n",
      "Iteration 19375, loss = 1.45550471\n",
      "Iteration 19376, loss = 1.45550262\n",
      "Iteration 19377, loss = 1.45550053\n",
      "Iteration 19378, loss = 1.45549844\n",
      "Iteration 19379, loss = 1.45549635\n",
      "Iteration 19380, loss = 1.45549426\n",
      "Iteration 19381, loss = 1.45549217\n",
      "Iteration 19382, loss = 1.45549008\n",
      "Iteration 19383, loss = 1.45548799\n",
      "Iteration 19384, loss = 1.45548590\n",
      "Iteration 19385, loss = 1.45548381\n",
      "Iteration 19386, loss = 1.45548172\n",
      "Iteration 19387, loss = 1.45547964\n",
      "Iteration 19388, loss = 1.45547755\n",
      "Iteration 19389, loss = 1.45547546\n",
      "Iteration 19390, loss = 1.45547337\n",
      "Iteration 19391, loss = 1.45547129\n",
      "Iteration 19392, loss = 1.45546920\n",
      "Iteration 19393, loss = 1.45546711\n",
      "Iteration 19394, loss = 1.45546503\n",
      "Iteration 19395, loss = 1.45546294\n",
      "Iteration 19396, loss = 1.45546086\n",
      "Iteration 19397, loss = 1.45545877\n",
      "Iteration 19398, loss = 1.45545668\n",
      "Iteration 19399, loss = 1.45545460\n",
      "Iteration 19400, loss = 1.45545251\n",
      "Iteration 19401, loss = 1.45545043\n",
      "Iteration 19402, loss = 1.45544835\n",
      "Iteration 19403, loss = 1.45544626\n",
      "Iteration 19404, loss = 1.45544418\n",
      "Iteration 19405, loss = 1.45544209\n",
      "Iteration 19406, loss = 1.45544001\n",
      "Iteration 19407, loss = 1.45543793\n",
      "Iteration 19408, loss = 1.45543584\n",
      "Iteration 19409, loss = 1.45543376\n",
      "Iteration 19410, loss = 1.45543168\n",
      "Iteration 19411, loss = 1.45542960\n",
      "Iteration 19412, loss = 1.45542752\n",
      "Iteration 19413, loss = 1.45542543\n",
      "Iteration 19414, loss = 1.45542335\n",
      "Iteration 19415, loss = 1.45542127\n",
      "Iteration 19416, loss = 1.45541919\n",
      "Iteration 19417, loss = 1.45541711\n",
      "Iteration 19418, loss = 1.45541503\n",
      "Iteration 19419, loss = 1.45541295\n",
      "Iteration 19420, loss = 1.45541087\n",
      "Iteration 19421, loss = 1.45540879\n",
      "Iteration 19422, loss = 1.45540671\n",
      "Iteration 19423, loss = 1.45540463\n",
      "Iteration 19424, loss = 1.45540255\n",
      "Iteration 19425, loss = 1.45540047\n",
      "Iteration 19426, loss = 1.45539840\n",
      "Iteration 19427, loss = 1.45539632\n",
      "Iteration 19428, loss = 1.45539424\n",
      "Iteration 19429, loss = 1.45539216\n",
      "Iteration 19430, loss = 1.45539008\n",
      "Iteration 19431, loss = 1.45538801\n",
      "Iteration 19432, loss = 1.45538593\n",
      "Iteration 19433, loss = 1.45538385\n",
      "Iteration 19434, loss = 1.45538178\n",
      "Iteration 19435, loss = 1.45537970\n",
      "Iteration 19436, loss = 1.45537762\n",
      "Iteration 19437, loss = 1.45537555\n",
      "Iteration 19438, loss = 1.45537347\n",
      "Iteration 19439, loss = 1.45537140\n",
      "Iteration 19440, loss = 1.45536932\n",
      "Iteration 19441, loss = 1.45536725\n",
      "Iteration 19442, loss = 1.45536517\n",
      "Iteration 19443, loss = 1.45536310\n",
      "Iteration 19444, loss = 1.45536103\n",
      "Iteration 19445, loss = 1.45535895\n",
      "Iteration 19446, loss = 1.45535688\n",
      "Iteration 19447, loss = 1.45535481\n",
      "Iteration 19448, loss = 1.45535273\n",
      "Iteration 19449, loss = 1.45535066\n",
      "Iteration 19450, loss = 1.45534859\n",
      "Iteration 19451, loss = 1.45534651\n",
      "Iteration 19452, loss = 1.45534444\n",
      "Iteration 19453, loss = 1.45534237\n",
      "Iteration 19454, loss = 1.45534030\n",
      "Iteration 19455, loss = 1.45533823\n",
      "Iteration 19456, loss = 1.45533616\n",
      "Iteration 19457, loss = 1.45533409\n",
      "Iteration 19458, loss = 1.45533202\n",
      "Iteration 19459, loss = 1.45532995\n",
      "Iteration 19460, loss = 1.45532788\n",
      "Iteration 19461, loss = 1.45532581\n",
      "Iteration 19462, loss = 1.45532374\n",
      "Iteration 19463, loss = 1.45532167\n",
      "Iteration 19464, loss = 1.45531960\n",
      "Iteration 19465, loss = 1.45531753\n",
      "Iteration 19466, loss = 1.45531546\n",
      "Iteration 19467, loss = 1.45531339\n",
      "Iteration 19468, loss = 1.45531132\n",
      "Iteration 19469, loss = 1.45530926\n",
      "Iteration 19470, loss = 1.45530719\n",
      "Iteration 19471, loss = 1.45530512\n",
      "Iteration 19472, loss = 1.45530305\n",
      "Iteration 19473, loss = 1.45530099\n",
      "Iteration 19474, loss = 1.45529892\n",
      "Iteration 19475, loss = 1.45529685\n",
      "Iteration 19476, loss = 1.45529479\n",
      "Iteration 19477, loss = 1.45529272\n",
      "Iteration 19478, loss = 1.45529066\n",
      "Iteration 19479, loss = 1.45528859\n",
      "Iteration 19480, loss = 1.45528653\n",
      "Iteration 19481, loss = 1.45528446\n",
      "Iteration 19482, loss = 1.45528240\n",
      "Iteration 19483, loss = 1.45528033\n",
      "Iteration 19484, loss = 1.45527827\n",
      "Iteration 19485, loss = 1.45527620\n",
      "Iteration 19486, loss = 1.45527414\n",
      "Iteration 19487, loss = 1.45527208\n",
      "Iteration 19488, loss = 1.45527001\n",
      "Iteration 19489, loss = 1.45526795\n",
      "Iteration 19490, loss = 1.45526589\n",
      "Iteration 19491, loss = 1.45526383\n",
      "Iteration 19492, loss = 1.45526176\n",
      "Iteration 19493, loss = 1.45525970\n",
      "Iteration 19494, loss = 1.45525764\n",
      "Iteration 19495, loss = 1.45525558\n",
      "Iteration 19496, loss = 1.45525352\n",
      "Iteration 19497, loss = 1.45525146\n",
      "Iteration 19498, loss = 1.45524940\n",
      "Iteration 19499, loss = 1.45524734\n",
      "Iteration 19500, loss = 1.45524527\n",
      "Iteration 19501, loss = 1.45524321\n",
      "Iteration 19502, loss = 1.45524116\n",
      "Iteration 19503, loss = 1.45523910\n",
      "Iteration 19504, loss = 1.45523704\n",
      "Iteration 19505, loss = 1.45523498\n",
      "Iteration 19506, loss = 1.45523292\n",
      "Iteration 19507, loss = 1.45523086\n",
      "Iteration 19508, loss = 1.45522880\n",
      "Iteration 19509, loss = 1.45522674\n",
      "Iteration 19510, loss = 1.45522469\n",
      "Iteration 19511, loss = 1.45522263\n",
      "Iteration 19512, loss = 1.45522057\n",
      "Iteration 19513, loss = 1.45521851\n",
      "Iteration 19514, loss = 1.45521646\n",
      "Iteration 19515, loss = 1.45521440\n",
      "Iteration 19516, loss = 1.45521234\n",
      "Iteration 19517, loss = 1.45521029\n",
      "Iteration 19518, loss = 1.45520823\n",
      "Iteration 19519, loss = 1.45520618\n",
      "Iteration 19520, loss = 1.45520412\n",
      "Iteration 19521, loss = 1.45520207\n",
      "Iteration 19522, loss = 1.45520001\n",
      "Iteration 19523, loss = 1.45519796\n",
      "Iteration 19524, loss = 1.45519590\n",
      "Iteration 19525, loss = 1.45519385\n",
      "Iteration 19526, loss = 1.45519179\n",
      "Iteration 19527, loss = 1.45518974\n",
      "Iteration 19528, loss = 1.45518769\n",
      "Iteration 19529, loss = 1.45518563\n",
      "Iteration 19530, loss = 1.45518358\n",
      "Iteration 19531, loss = 1.45518153\n",
      "Iteration 19532, loss = 1.45517948\n",
      "Iteration 19533, loss = 1.45517742\n",
      "Iteration 19534, loss = 1.45517537\n",
      "Iteration 19535, loss = 1.45517332\n",
      "Iteration 19536, loss = 1.45517127\n",
      "Iteration 19537, loss = 1.45516922\n",
      "Iteration 19538, loss = 1.45516717\n",
      "Iteration 19539, loss = 1.45516511\n",
      "Iteration 19540, loss = 1.45516306\n",
      "Iteration 19541, loss = 1.45516101\n",
      "Iteration 19542, loss = 1.45515896\n",
      "Iteration 19543, loss = 1.45515691\n",
      "Iteration 19544, loss = 1.45515486\n",
      "Iteration 19545, loss = 1.45515282\n",
      "Iteration 19546, loss = 1.45515077\n",
      "Iteration 19547, loss = 1.45514872\n",
      "Iteration 19548, loss = 1.45514667\n",
      "Iteration 19549, loss = 1.45514462\n",
      "Iteration 19550, loss = 1.45514257\n",
      "Iteration 19551, loss = 1.45514052\n",
      "Iteration 19552, loss = 1.45513848\n",
      "Iteration 19553, loss = 1.45513643\n",
      "Iteration 19554, loss = 1.45513438\n",
      "Iteration 19555, loss = 1.45513234\n",
      "Iteration 19556, loss = 1.45513029\n",
      "Iteration 19557, loss = 1.45512824\n",
      "Iteration 19558, loss = 1.45512620\n",
      "Iteration 19559, loss = 1.45512415\n",
      "Iteration 19560, loss = 1.45512210\n",
      "Iteration 19561, loss = 1.45512006\n",
      "Iteration 19562, loss = 1.45511801\n",
      "Iteration 19563, loss = 1.45511597\n",
      "Iteration 19564, loss = 1.45511392\n",
      "Iteration 19565, loss = 1.45511188\n",
      "Iteration 19566, loss = 1.45510984\n",
      "Iteration 19567, loss = 1.45510779\n",
      "Iteration 19568, loss = 1.45510575\n",
      "Iteration 19569, loss = 1.45510370\n",
      "Iteration 19570, loss = 1.45510166\n",
      "Iteration 19571, loss = 1.45509962\n",
      "Iteration 19572, loss = 1.45509758\n",
      "Iteration 19573, loss = 1.45509553\n",
      "Iteration 19574, loss = 1.45509349\n",
      "Iteration 19575, loss = 1.45509145\n",
      "Iteration 19576, loss = 1.45508941\n",
      "Iteration 19577, loss = 1.45508737\n",
      "Iteration 19578, loss = 1.45508532\n",
      "Iteration 19579, loss = 1.45508328\n",
      "Iteration 19580, loss = 1.45508124\n",
      "Iteration 19581, loss = 1.45507920\n",
      "Iteration 19582, loss = 1.45507716\n",
      "Iteration 19583, loss = 1.45507512\n",
      "Iteration 19584, loss = 1.45507308\n",
      "Iteration 19585, loss = 1.45507104\n",
      "Iteration 19586, loss = 1.45506900\n",
      "Iteration 19587, loss = 1.45506696\n",
      "Iteration 19588, loss = 1.45506492\n",
      "Iteration 19589, loss = 1.45506288\n",
      "Iteration 19590, loss = 1.45506085\n",
      "Iteration 19591, loss = 1.45505881\n",
      "Iteration 19592, loss = 1.45505677\n",
      "Iteration 19593, loss = 1.45505473\n",
      "Iteration 19594, loss = 1.45505270\n",
      "Iteration 19595, loss = 1.45505066\n",
      "Iteration 19596, loss = 1.45504862\n",
      "Iteration 19597, loss = 1.45504658\n",
      "Iteration 19598, loss = 1.45504455\n",
      "Iteration 19599, loss = 1.45504251\n",
      "Iteration 19600, loss = 1.45504048\n",
      "Iteration 19601, loss = 1.45503844\n",
      "Iteration 19602, loss = 1.45503640\n",
      "Iteration 19603, loss = 1.45503437\n",
      "Iteration 19604, loss = 1.45503233\n",
      "Iteration 19605, loss = 1.45503030\n",
      "Iteration 19606, loss = 1.45502826\n",
      "Iteration 19607, loss = 1.45502623\n",
      "Iteration 19608, loss = 1.45502420\n",
      "Iteration 19609, loss = 1.45502216\n",
      "Iteration 19610, loss = 1.45502013\n",
      "Iteration 19611, loss = 1.45501809\n",
      "Iteration 19612, loss = 1.45501606\n",
      "Iteration 19613, loss = 1.45501403\n",
      "Iteration 19614, loss = 1.45501200\n",
      "Iteration 19615, loss = 1.45500996\n",
      "Iteration 19616, loss = 1.45500793\n",
      "Iteration 19617, loss = 1.45500590\n",
      "Iteration 19618, loss = 1.45500387\n",
      "Iteration 19619, loss = 1.45500184\n",
      "Iteration 19620, loss = 1.45499980\n",
      "Iteration 19621, loss = 1.45499777\n",
      "Iteration 19622, loss = 1.45499574\n",
      "Iteration 19623, loss = 1.45499371\n",
      "Iteration 19624, loss = 1.45499168\n",
      "Iteration 19625, loss = 1.45498965\n",
      "Iteration 19626, loss = 1.45498762\n",
      "Iteration 19627, loss = 1.45498559\n",
      "Iteration 19628, loss = 1.45498356\n",
      "Iteration 19629, loss = 1.45498153\n",
      "Iteration 19630, loss = 1.45497951\n",
      "Iteration 19631, loss = 1.45497748\n",
      "Iteration 19632, loss = 1.45497545\n",
      "Iteration 19633, loss = 1.45497342\n",
      "Iteration 19634, loss = 1.45497139\n",
      "Iteration 19635, loss = 1.45496936\n",
      "Iteration 19636, loss = 1.45496734\n",
      "Iteration 19637, loss = 1.45496531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19638, loss = 1.45496328\n",
      "Iteration 19639, loss = 1.45496126\n",
      "Iteration 19640, loss = 1.45495923\n",
      "Iteration 19641, loss = 1.45495720\n",
      "Iteration 19642, loss = 1.45495518\n",
      "Iteration 19643, loss = 1.45495315\n",
      "Iteration 19644, loss = 1.45495113\n",
      "Iteration 19645, loss = 1.45494910\n",
      "Iteration 19646, loss = 1.45494708\n",
      "Iteration 19647, loss = 1.45494505\n",
      "Iteration 19648, loss = 1.45494303\n",
      "Iteration 19649, loss = 1.45494100\n",
      "Iteration 19650, loss = 1.45493898\n",
      "Iteration 19651, loss = 1.45493695\n",
      "Iteration 19652, loss = 1.45493493\n",
      "Iteration 19653, loss = 1.45493291\n",
      "Iteration 19654, loss = 1.45493088\n",
      "Iteration 19655, loss = 1.45492886\n",
      "Iteration 19656, loss = 1.45492684\n",
      "Iteration 19657, loss = 1.45492482\n",
      "Iteration 19658, loss = 1.45492279\n",
      "Iteration 19659, loss = 1.45492077\n",
      "Iteration 19660, loss = 1.45491875\n",
      "Iteration 19661, loss = 1.45491673\n",
      "Iteration 19662, loss = 1.45491471\n",
      "Iteration 19663, loss = 1.45491269\n",
      "Iteration 19664, loss = 1.45491067\n",
      "Iteration 19665, loss = 1.45490865\n",
      "Iteration 19666, loss = 1.45490663\n",
      "Iteration 19667, loss = 1.45490461\n",
      "Iteration 19668, loss = 1.45490259\n",
      "Iteration 19669, loss = 1.45490057\n",
      "Iteration 19670, loss = 1.45489855\n",
      "Iteration 19671, loss = 1.45489653\n",
      "Iteration 19672, loss = 1.45489451\n",
      "Iteration 19673, loss = 1.45489249\n",
      "Iteration 19674, loss = 1.45489047\n",
      "Iteration 19675, loss = 1.45488845\n",
      "Iteration 19676, loss = 1.45488644\n",
      "Iteration 19677, loss = 1.45488442\n",
      "Iteration 19678, loss = 1.45488240\n",
      "Iteration 19679, loss = 1.45488038\n",
      "Iteration 19680, loss = 1.45487837\n",
      "Iteration 19681, loss = 1.45487635\n",
      "Iteration 19682, loss = 1.45487433\n",
      "Iteration 19683, loss = 1.45487232\n",
      "Iteration 19684, loss = 1.45487030\n",
      "Iteration 19685, loss = 1.45486829\n",
      "Iteration 19686, loss = 1.45486627\n",
      "Iteration 19687, loss = 1.45486425\n",
      "Iteration 19688, loss = 1.45486224\n",
      "Iteration 19689, loss = 1.45486022\n",
      "Iteration 19690, loss = 1.45485821\n",
      "Iteration 19691, loss = 1.45485620\n",
      "Iteration 19692, loss = 1.45485418\n",
      "Iteration 19693, loss = 1.45485217\n",
      "Iteration 19694, loss = 1.45485015\n",
      "Iteration 19695, loss = 1.45484814\n",
      "Iteration 19696, loss = 1.45484613\n",
      "Iteration 19697, loss = 1.45484411\n",
      "Iteration 19698, loss = 1.45484210\n",
      "Iteration 19699, loss = 1.45484009\n",
      "Iteration 19700, loss = 1.45483808\n",
      "Iteration 19701, loss = 1.45483607\n",
      "Iteration 19702, loss = 1.45483405\n",
      "Iteration 19703, loss = 1.45483204\n",
      "Iteration 19704, loss = 1.45483003\n",
      "Iteration 19705, loss = 1.45482802\n",
      "Iteration 19706, loss = 1.45482601\n",
      "Iteration 19707, loss = 1.45482400\n",
      "Iteration 19708, loss = 1.45482199\n",
      "Iteration 19709, loss = 1.45481998\n",
      "Iteration 19710, loss = 1.45481797\n",
      "Iteration 19711, loss = 1.45481596\n",
      "Iteration 19712, loss = 1.45481395\n",
      "Iteration 19713, loss = 1.45481194\n",
      "Iteration 19714, loss = 1.45480993\n",
      "Iteration 19715, loss = 1.45480792\n",
      "Iteration 19716, loss = 1.45480591\n",
      "Iteration 19717, loss = 1.45480391\n",
      "Iteration 19718, loss = 1.45480190\n",
      "Iteration 19719, loss = 1.45479989\n",
      "Iteration 19720, loss = 1.45479788\n",
      "Iteration 19721, loss = 1.45479588\n",
      "Iteration 19722, loss = 1.45479387\n",
      "Iteration 19723, loss = 1.45479186\n",
      "Iteration 19724, loss = 1.45478985\n",
      "Iteration 19725, loss = 1.45478785\n",
      "Iteration 19726, loss = 1.45478584\n",
      "Iteration 19727, loss = 1.45478384\n",
      "Iteration 19728, loss = 1.45478183\n",
      "Iteration 19729, loss = 1.45477983\n",
      "Iteration 19730, loss = 1.45477782\n",
      "Iteration 19731, loss = 1.45477582\n",
      "Iteration 19732, loss = 1.45477381\n",
      "Iteration 19733, loss = 1.45477181\n",
      "Iteration 19734, loss = 1.45476980\n",
      "Iteration 19735, loss = 1.45476780\n",
      "Iteration 19736, loss = 1.45476579\n",
      "Iteration 19737, loss = 1.45476379\n",
      "Iteration 19738, loss = 1.45476179\n",
      "Iteration 19739, loss = 1.45475978\n",
      "Iteration 19740, loss = 1.45475778\n",
      "Iteration 19741, loss = 1.45475578\n",
      "Iteration 19742, loss = 1.45475378\n",
      "Iteration 19743, loss = 1.45475177\n",
      "Iteration 19744, loss = 1.45474977\n",
      "Iteration 19745, loss = 1.45474777\n",
      "Iteration 19746, loss = 1.45474577\n",
      "Iteration 19747, loss = 1.45474377\n",
      "Iteration 19748, loss = 1.45474177\n",
      "Iteration 19749, loss = 1.45473977\n",
      "Iteration 19750, loss = 1.45473777\n",
      "Iteration 19751, loss = 1.45473577\n",
      "Iteration 19752, loss = 1.45473377\n",
      "Iteration 19753, loss = 1.45473177\n",
      "Iteration 19754, loss = 1.45472977\n",
      "Iteration 19755, loss = 1.45472777\n",
      "Iteration 19756, loss = 1.45472577\n",
      "Iteration 19757, loss = 1.45472377\n",
      "Iteration 19758, loss = 1.45472177\n",
      "Iteration 19759, loss = 1.45471977\n",
      "Iteration 19760, loss = 1.45471778\n",
      "Iteration 19761, loss = 1.45471578\n",
      "Iteration 19762, loss = 1.45471378\n",
      "Iteration 19763, loss = 1.45471178\n",
      "Iteration 19764, loss = 1.45470979\n",
      "Iteration 19765, loss = 1.45470779\n",
      "Iteration 19766, loss = 1.45470579\n",
      "Iteration 19767, loss = 1.45470380\n",
      "Iteration 19768, loss = 1.45470180\n",
      "Iteration 19769, loss = 1.45469980\n",
      "Iteration 19770, loss = 1.45469781\n",
      "Iteration 19771, loss = 1.45469581\n",
      "Iteration 19772, loss = 1.45469382\n",
      "Iteration 19773, loss = 1.45469182\n",
      "Iteration 19774, loss = 1.45468983\n",
      "Iteration 19775, loss = 1.45468783\n",
      "Iteration 19776, loss = 1.45468584\n",
      "Iteration 19777, loss = 1.45468384\n",
      "Iteration 19778, loss = 1.45468185\n",
      "Iteration 19779, loss = 1.45467986\n",
      "Iteration 19780, loss = 1.45467786\n",
      "Iteration 19781, loss = 1.45467587\n",
      "Iteration 19782, loss = 1.45467388\n",
      "Iteration 19783, loss = 1.45467188\n",
      "Iteration 19784, loss = 1.45466989\n",
      "Iteration 19785, loss = 1.45466790\n",
      "Iteration 19786, loss = 1.45466591\n",
      "Iteration 19787, loss = 1.45466391\n",
      "Iteration 19788, loss = 1.45466192\n",
      "Iteration 19789, loss = 1.45465993\n",
      "Iteration 19790, loss = 1.45465794\n",
      "Iteration 19791, loss = 1.45465595\n",
      "Iteration 19792, loss = 1.45465396\n",
      "Iteration 19793, loss = 1.45465197\n",
      "Iteration 19794, loss = 1.45464998\n",
      "Iteration 19795, loss = 1.45464799\n",
      "Iteration 19796, loss = 1.45464600\n",
      "Iteration 19797, loss = 1.45464401\n",
      "Iteration 19798, loss = 1.45464202\n",
      "Iteration 19799, loss = 1.45464003\n",
      "Iteration 19800, loss = 1.45463804\n",
      "Iteration 19801, loss = 1.45463605\n",
      "Iteration 19802, loss = 1.45463407\n",
      "Iteration 19803, loss = 1.45463208\n",
      "Iteration 19804, loss = 1.45463009\n",
      "Iteration 19805, loss = 1.45462810\n",
      "Iteration 19806, loss = 1.45462611\n",
      "Iteration 19807, loss = 1.45462413\n",
      "Iteration 19808, loss = 1.45462214\n",
      "Iteration 19809, loss = 1.45462015\n",
      "Iteration 19810, loss = 1.45461817\n",
      "Iteration 19811, loss = 1.45461618\n",
      "Iteration 19812, loss = 1.45461420\n",
      "Iteration 19813, loss = 1.45461221\n",
      "Iteration 19814, loss = 1.45461022\n",
      "Iteration 19815, loss = 1.45460824\n",
      "Iteration 19816, loss = 1.45460625\n",
      "Iteration 19817, loss = 1.45460427\n",
      "Iteration 19818, loss = 1.45460228\n",
      "Iteration 19819, loss = 1.45460030\n",
      "Iteration 19820, loss = 1.45459832\n",
      "Iteration 19821, loss = 1.45459633\n",
      "Iteration 19822, loss = 1.45459435\n",
      "Iteration 19823, loss = 1.45459237\n",
      "Iteration 19824, loss = 1.45459038\n",
      "Iteration 19825, loss = 1.45458840\n",
      "Iteration 19826, loss = 1.45458642\n",
      "Iteration 19827, loss = 1.45458443\n",
      "Iteration 19828, loss = 1.45458245\n",
      "Iteration 19829, loss = 1.45458047\n",
      "Iteration 19830, loss = 1.45457849\n",
      "Iteration 19831, loss = 1.45457651\n",
      "Iteration 19832, loss = 1.45457452\n",
      "Iteration 19833, loss = 1.45457254\n",
      "Iteration 19834, loss = 1.45457056\n",
      "Iteration 19835, loss = 1.45456858\n",
      "Iteration 19836, loss = 1.45456660\n",
      "Iteration 19837, loss = 1.45456462\n",
      "Iteration 19838, loss = 1.45456264\n",
      "Iteration 19839, loss = 1.45456066\n",
      "Iteration 19840, loss = 1.45455868\n",
      "Iteration 19841, loss = 1.45455670\n",
      "Iteration 19842, loss = 1.45455472\n",
      "Iteration 19843, loss = 1.45455275\n",
      "Iteration 19844, loss = 1.45455077\n",
      "Iteration 19845, loss = 1.45454879\n",
      "Iteration 19846, loss = 1.45454681\n",
      "Iteration 19847, loss = 1.45454483\n",
      "Iteration 19848, loss = 1.45454285\n",
      "Iteration 19849, loss = 1.45454088\n",
      "Iteration 19850, loss = 1.45453890\n",
      "Iteration 19851, loss = 1.45453692\n",
      "Iteration 19852, loss = 1.45453495\n",
      "Iteration 19853, loss = 1.45453297\n",
      "Iteration 19854, loss = 1.45453099\n",
      "Iteration 19855, loss = 1.45452902\n",
      "Iteration 19856, loss = 1.45452704\n",
      "Iteration 19857, loss = 1.45452507\n",
      "Iteration 19858, loss = 1.45452309\n",
      "Iteration 19859, loss = 1.45452112\n",
      "Iteration 19860, loss = 1.45451914\n",
      "Iteration 19861, loss = 1.45451717\n",
      "Iteration 19862, loss = 1.45451519\n",
      "Iteration 19863, loss = 1.45451322\n",
      "Iteration 19864, loss = 1.45451124\n",
      "Iteration 19865, loss = 1.45450927\n",
      "Iteration 19866, loss = 1.45450730\n",
      "Iteration 19867, loss = 1.45450532\n",
      "Iteration 19868, loss = 1.45450335\n",
      "Iteration 19869, loss = 1.45450138\n",
      "Iteration 19870, loss = 1.45449940\n",
      "Iteration 19871, loss = 1.45449743\n",
      "Iteration 19872, loss = 1.45449546\n",
      "Iteration 19873, loss = 1.45449349\n",
      "Iteration 19874, loss = 1.45449152\n",
      "Iteration 19875, loss = 1.45448955\n",
      "Iteration 19876, loss = 1.45448757\n",
      "Iteration 19877, loss = 1.45448560\n",
      "Iteration 19878, loss = 1.45448363\n",
      "Iteration 19879, loss = 1.45448166\n",
      "Iteration 19880, loss = 1.45447969\n",
      "Iteration 19881, loss = 1.45447772\n",
      "Iteration 19882, loss = 1.45447575\n",
      "Iteration 19883, loss = 1.45447378\n",
      "Iteration 19884, loss = 1.45447181\n",
      "Iteration 19885, loss = 1.45446984\n",
      "Iteration 19886, loss = 1.45446788\n",
      "Iteration 19887, loss = 1.45446591\n",
      "Iteration 19888, loss = 1.45446394\n",
      "Iteration 19889, loss = 1.45446197\n",
      "Iteration 19890, loss = 1.45446000\n",
      "Iteration 19891, loss = 1.45445803\n",
      "Iteration 19892, loss = 1.45445607\n",
      "Iteration 19893, loss = 1.45445410\n",
      "Iteration 19894, loss = 1.45445213\n",
      "Iteration 19895, loss = 1.45445017\n",
      "Iteration 19896, loss = 1.45444820\n",
      "Iteration 19897, loss = 1.45444623\n",
      "Iteration 19898, loss = 1.45444427\n",
      "Iteration 19899, loss = 1.45444230\n",
      "Iteration 19900, loss = 1.45444033\n",
      "Iteration 19901, loss = 1.45443837\n",
      "Iteration 19902, loss = 1.45443640\n",
      "Iteration 19903, loss = 1.45443444\n",
      "Iteration 19904, loss = 1.45443247\n",
      "Iteration 19905, loss = 1.45443051\n",
      "Iteration 19906, loss = 1.45442855\n",
      "Iteration 19907, loss = 1.45442658\n",
      "Iteration 19908, loss = 1.45442462\n",
      "Iteration 19909, loss = 1.45442265\n",
      "Iteration 19910, loss = 1.45442069\n",
      "Iteration 19911, loss = 1.45441873\n",
      "Iteration 19912, loss = 1.45441676\n",
      "Iteration 19913, loss = 1.45441480\n",
      "Iteration 19914, loss = 1.45441284\n",
      "Iteration 19915, loss = 1.45441088\n",
      "Iteration 19916, loss = 1.45440891\n",
      "Iteration 19917, loss = 1.45440695\n",
      "Iteration 19918, loss = 1.45440499\n",
      "Iteration 19919, loss = 1.45440303\n",
      "Iteration 19920, loss = 1.45440107\n",
      "Iteration 19921, loss = 1.45439911\n",
      "Iteration 19922, loss = 1.45439715\n",
      "Iteration 19923, loss = 1.45439519\n",
      "Iteration 19924, loss = 1.45439323\n",
      "Iteration 19925, loss = 1.45439127\n",
      "Iteration 19926, loss = 1.45438931\n",
      "Iteration 19927, loss = 1.45438735\n",
      "Iteration 19928, loss = 1.45438539\n",
      "Iteration 19929, loss = 1.45438343\n",
      "Iteration 19930, loss = 1.45438147\n",
      "Iteration 19931, loss = 1.45437951\n",
      "Iteration 19932, loss = 1.45437755\n",
      "Iteration 19933, loss = 1.45437560\n",
      "Iteration 19934, loss = 1.45437364\n",
      "Iteration 19935, loss = 1.45437168\n",
      "Iteration 19936, loss = 1.45436972\n",
      "Iteration 19937, loss = 1.45436777\n",
      "Iteration 19938, loss = 1.45436581\n",
      "Iteration 19939, loss = 1.45436385\n",
      "Iteration 19940, loss = 1.45436189\n",
      "Iteration 19941, loss = 1.45435994\n",
      "Iteration 19942, loss = 1.45435798\n",
      "Iteration 19943, loss = 1.45435603\n",
      "Iteration 19944, loss = 1.45435407\n",
      "Iteration 19945, loss = 1.45435212\n",
      "Iteration 19946, loss = 1.45435016\n",
      "Iteration 19947, loss = 1.45434821\n",
      "Iteration 19948, loss = 1.45434625\n",
      "Iteration 19949, loss = 1.45434430\n",
      "Iteration 19950, loss = 1.45434234\n",
      "Iteration 19951, loss = 1.45434039\n",
      "Iteration 19952, loss = 1.45433843\n",
      "Iteration 19953, loss = 1.45433648\n",
      "Iteration 19954, loss = 1.45433453\n",
      "Iteration 19955, loss = 1.45433257\n",
      "Iteration 19956, loss = 1.45433062\n",
      "Iteration 19957, loss = 1.45432867\n",
      "Iteration 19958, loss = 1.45432672\n",
      "Iteration 19959, loss = 1.45432476\n",
      "Iteration 19960, loss = 1.45432281\n",
      "Iteration 19961, loss = 1.45432086\n",
      "Iteration 19962, loss = 1.45431891\n",
      "Iteration 19963, loss = 1.45431696\n",
      "Iteration 19964, loss = 1.45431501\n",
      "Iteration 19965, loss = 1.45431305\n",
      "Iteration 19966, loss = 1.45431110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19967, loss = 1.45430915\n",
      "Iteration 19968, loss = 1.45430720\n",
      "Iteration 19969, loss = 1.45430525\n",
      "Iteration 19970, loss = 1.45430330\n",
      "Iteration 19971, loss = 1.45430135\n",
      "Iteration 19972, loss = 1.45429941\n",
      "Iteration 19973, loss = 1.45429746\n",
      "Iteration 19974, loss = 1.45429551\n",
      "Iteration 19975, loss = 1.45429356\n",
      "Iteration 19976, loss = 1.45429161\n",
      "Iteration 19977, loss = 1.45428966\n",
      "Iteration 19978, loss = 1.45428771\n",
      "Iteration 19979, loss = 1.45428577\n",
      "Iteration 19980, loss = 1.45428382\n",
      "Iteration 19981, loss = 1.45428187\n",
      "Iteration 19982, loss = 1.45427992\n",
      "Iteration 19983, loss = 1.45427798\n",
      "Iteration 19984, loss = 1.45427603\n",
      "Iteration 19985, loss = 1.45427409\n",
      "Iteration 19986, loss = 1.45427214\n",
      "Iteration 19987, loss = 1.45427019\n",
      "Iteration 19988, loss = 1.45426825\n",
      "Iteration 19989, loss = 1.45426630\n",
      "Iteration 19990, loss = 1.45426436\n",
      "Iteration 19991, loss = 1.45426241\n",
      "Iteration 19992, loss = 1.45426047\n",
      "Iteration 19993, loss = 1.45425852\n",
      "Iteration 19994, loss = 1.45425658\n",
      "Iteration 19995, loss = 1.45425463\n",
      "Iteration 19996, loss = 1.45425269\n",
      "Iteration 19997, loss = 1.45425075\n",
      "Iteration 19998, loss = 1.45424880\n",
      "Iteration 19999, loss = 1.45424686\n",
      "Iteration 20000, loss = 1.45424492\n",
      "Iteration 20001, loss = 1.45424297\n",
      "Iteration 20002, loss = 1.45424103\n",
      "Iteration 20003, loss = 1.45423909\n",
      "Iteration 20004, loss = 1.45423715\n",
      "Iteration 20005, loss = 1.45423521\n",
      "Iteration 20006, loss = 1.45423326\n",
      "Iteration 20007, loss = 1.45423132\n",
      "Iteration 20008, loss = 1.45422938\n",
      "Iteration 20009, loss = 1.45422744\n",
      "Iteration 20010, loss = 1.45422550\n",
      "Iteration 20011, loss = 1.45422356\n",
      "Iteration 20012, loss = 1.45422162\n",
      "Iteration 20013, loss = 1.45421968\n",
      "Iteration 20014, loss = 1.45421774\n",
      "Iteration 20015, loss = 1.45421580\n",
      "Iteration 20016, loss = 1.45421386\n",
      "Iteration 20017, loss = 1.45421192\n",
      "Iteration 20018, loss = 1.45420998\n",
      "Iteration 20019, loss = 1.45420804\n",
      "Iteration 20020, loss = 1.45420611\n",
      "Iteration 20021, loss = 1.45420417\n",
      "Iteration 20022, loss = 1.45420223\n",
      "Iteration 20023, loss = 1.45420029\n",
      "Iteration 20024, loss = 1.45419835\n",
      "Iteration 20025, loss = 1.45419642\n",
      "Iteration 20026, loss = 1.45419448\n",
      "Iteration 20027, loss = 1.45419254\n",
      "Iteration 20028, loss = 1.45419061\n",
      "Iteration 20029, loss = 1.45418867\n",
      "Iteration 20030, loss = 1.45418673\n",
      "Iteration 20031, loss = 1.45418480\n",
      "Iteration 20032, loss = 1.45418286\n",
      "Iteration 20033, loss = 1.45418093\n",
      "Iteration 20034, loss = 1.45417899\n",
      "Iteration 20035, loss = 1.45417706\n",
      "Iteration 20036, loss = 1.45417512\n",
      "Iteration 20037, loss = 1.45417319\n",
      "Iteration 20038, loss = 1.45417125\n",
      "Iteration 20039, loss = 1.45416932\n",
      "Iteration 20040, loss = 1.45416738\n",
      "Iteration 20041, loss = 1.45416545\n",
      "Iteration 20042, loss = 1.45416352\n",
      "Iteration 20043, loss = 1.45416158\n",
      "Iteration 20044, loss = 1.45415965\n",
      "Iteration 20045, loss = 1.45415772\n",
      "Iteration 20046, loss = 1.45415578\n",
      "Iteration 20047, loss = 1.45415385\n",
      "Iteration 20048, loss = 1.45415192\n",
      "Iteration 20049, loss = 1.45414999\n",
      "Iteration 20050, loss = 1.45414806\n",
      "Iteration 20051, loss = 1.45414612\n",
      "Iteration 20052, loss = 1.45414419\n",
      "Iteration 20053, loss = 1.45414226\n",
      "Iteration 20054, loss = 1.45414033\n",
      "Iteration 20055, loss = 1.45413840\n",
      "Iteration 20056, loss = 1.45413647\n",
      "Iteration 20057, loss = 1.45413454\n",
      "Iteration 20058, loss = 1.45413261\n",
      "Iteration 20059, loss = 1.45413068\n",
      "Iteration 20060, loss = 1.45412875\n",
      "Iteration 20061, loss = 1.45412682\n",
      "Iteration 20062, loss = 1.45412489\n",
      "Iteration 20063, loss = 1.45412296\n",
      "Iteration 20064, loss = 1.45412104\n",
      "Iteration 20065, loss = 1.45411911\n",
      "Iteration 20066, loss = 1.45411718\n",
      "Iteration 20067, loss = 1.45411525\n",
      "Iteration 20068, loss = 1.45411332\n",
      "Iteration 20069, loss = 1.45411140\n",
      "Iteration 20070, loss = 1.45410947\n",
      "Iteration 20071, loss = 1.45410754\n",
      "Iteration 20072, loss = 1.45410561\n",
      "Iteration 20073, loss = 1.45410369\n",
      "Iteration 20074, loss = 1.45410176\n",
      "Iteration 20075, loss = 1.45409984\n",
      "Iteration 20076, loss = 1.45409791\n",
      "Iteration 20077, loss = 1.45409598\n",
      "Iteration 20078, loss = 1.45409406\n",
      "Iteration 20079, loss = 1.45409213\n",
      "Iteration 20080, loss = 1.45409021\n",
      "Iteration 20081, loss = 1.45408828\n",
      "Iteration 20082, loss = 1.45408636\n",
      "Iteration 20083, loss = 1.45408443\n",
      "Iteration 20084, loss = 1.45408251\n",
      "Iteration 20085, loss = 1.45408059\n",
      "Iteration 20086, loss = 1.45407866\n",
      "Iteration 20087, loss = 1.45407674\n",
      "Iteration 20088, loss = 1.45407482\n",
      "Iteration 20089, loss = 1.45407289\n",
      "Iteration 20090, loss = 1.45407097\n",
      "Iteration 20091, loss = 1.45406905\n",
      "Iteration 20092, loss = 1.45406712\n",
      "Iteration 20093, loss = 1.45406520\n",
      "Iteration 20094, loss = 1.45406328\n",
      "Iteration 20095, loss = 1.45406136\n",
      "Iteration 20096, loss = 1.45405944\n",
      "Iteration 20097, loss = 1.45405752\n",
      "Iteration 20098, loss = 1.45405560\n",
      "Iteration 20099, loss = 1.45405367\n",
      "Iteration 20100, loss = 1.45405175\n",
      "Iteration 20101, loss = 1.45404983\n",
      "Iteration 20102, loss = 1.45404791\n",
      "Iteration 20103, loss = 1.45404599\n",
      "Iteration 20104, loss = 1.45404407\n",
      "Iteration 20105, loss = 1.45404215\n",
      "Iteration 20106, loss = 1.45404024\n",
      "Iteration 20107, loss = 1.45403832\n",
      "Iteration 20108, loss = 1.45403640\n",
      "Iteration 20109, loss = 1.45403448\n",
      "Iteration 20110, loss = 1.45403256\n",
      "Iteration 20111, loss = 1.45403064\n",
      "Iteration 20112, loss = 1.45402872\n",
      "Iteration 20113, loss = 1.45402681\n",
      "Iteration 20114, loss = 1.45402489\n",
      "Iteration 20115, loss = 1.45402297\n",
      "Iteration 20116, loss = 1.45402105\n",
      "Iteration 20117, loss = 1.45401914\n",
      "Iteration 20118, loss = 1.45401722\n",
      "Iteration 20119, loss = 1.45401531\n",
      "Iteration 20120, loss = 1.45401339\n",
      "Iteration 20121, loss = 1.45401147\n",
      "Iteration 20122, loss = 1.45400956\n",
      "Iteration 20123, loss = 1.45400764\n",
      "Iteration 20124, loss = 1.45400573\n",
      "Iteration 20125, loss = 1.45400381\n",
      "Iteration 20126, loss = 1.45400190\n",
      "Iteration 20127, loss = 1.45399998\n",
      "Iteration 20128, loss = 1.45399807\n",
      "Iteration 20129, loss = 1.45399615\n",
      "Iteration 20130, loss = 1.45399424\n",
      "Iteration 20131, loss = 1.45399233\n",
      "Iteration 20132, loss = 1.45399041\n",
      "Iteration 20133, loss = 1.45398850\n",
      "Iteration 20134, loss = 1.45398659\n",
      "Iteration 20135, loss = 1.45398467\n",
      "Iteration 20136, loss = 1.45398276\n",
      "Iteration 20137, loss = 1.45398085\n",
      "Iteration 20138, loss = 1.45397894\n",
      "Iteration 20139, loss = 1.45397702\n",
      "Iteration 20140, loss = 1.45397511\n",
      "Iteration 20141, loss = 1.45397320\n",
      "Iteration 20142, loss = 1.45397129\n",
      "Iteration 20143, loss = 1.45396938\n",
      "Iteration 20144, loss = 1.45396747\n",
      "Iteration 20145, loss = 1.45396556\n",
      "Iteration 20146, loss = 1.45396365\n",
      "Iteration 20147, loss = 1.45396174\n",
      "Iteration 20148, loss = 1.45395983\n",
      "Iteration 20149, loss = 1.45395792\n",
      "Iteration 20150, loss = 1.45395601\n",
      "Iteration 20151, loss = 1.45395410\n",
      "Iteration 20152, loss = 1.45395219\n",
      "Iteration 20153, loss = 1.45395028\n",
      "Iteration 20154, loss = 1.45394837\n",
      "Iteration 20155, loss = 1.45394646\n",
      "Iteration 20156, loss = 1.45394455\n",
      "Iteration 20157, loss = 1.45394265\n",
      "Iteration 20158, loss = 1.45394074\n",
      "Iteration 20159, loss = 1.45393883\n",
      "Iteration 20160, loss = 1.45393692\n",
      "Iteration 20161, loss = 1.45393502\n",
      "Iteration 20162, loss = 1.45393311\n",
      "Iteration 20163, loss = 1.45393120\n",
      "Iteration 20164, loss = 1.45392930\n",
      "Iteration 20165, loss = 1.45392739\n",
      "Iteration 20166, loss = 1.45392548\n",
      "Iteration 20167, loss = 1.45392358\n",
      "Iteration 20168, loss = 1.45392167\n",
      "Iteration 20169, loss = 1.45391977\n",
      "Iteration 20170, loss = 1.45391786\n",
      "Iteration 20171, loss = 1.45391596\n",
      "Iteration 20172, loss = 1.45391405\n",
      "Iteration 20173, loss = 1.45391215\n",
      "Iteration 20174, loss = 1.45391024\n",
      "Iteration 20175, loss = 1.45390834\n",
      "Iteration 20176, loss = 1.45390644\n",
      "Iteration 20177, loss = 1.45390453\n",
      "Iteration 20178, loss = 1.45390263\n",
      "Iteration 20179, loss = 1.45390073\n",
      "Iteration 20180, loss = 1.45389882\n",
      "Iteration 20181, loss = 1.45389692\n",
      "Iteration 20182, loss = 1.45389502\n",
      "Iteration 20183, loss = 1.45389312\n",
      "Iteration 20184, loss = 1.45389121\n",
      "Iteration 20185, loss = 1.45388931\n",
      "Iteration 20186, loss = 1.45388741\n",
      "Iteration 20187, loss = 1.45388551\n",
      "Iteration 20188, loss = 1.45388361\n",
      "Iteration 20189, loss = 1.45388171\n",
      "Iteration 20190, loss = 1.45387981\n",
      "Iteration 20191, loss = 1.45387791\n",
      "Iteration 20192, loss = 1.45387601\n",
      "Iteration 20193, loss = 1.45387411\n",
      "Iteration 20194, loss = 1.45387221\n",
      "Iteration 20195, loss = 1.45387031\n",
      "Iteration 20196, loss = 1.45386841\n",
      "Iteration 20197, loss = 1.45386651\n",
      "Iteration 20198, loss = 1.45386461\n",
      "Iteration 20199, loss = 1.45386271\n",
      "Iteration 20200, loss = 1.45386081\n",
      "Iteration 20201, loss = 1.45385891\n",
      "Iteration 20202, loss = 1.45385701\n",
      "Iteration 20203, loss = 1.45385512\n",
      "Iteration 20204, loss = 1.45385322\n",
      "Iteration 20205, loss = 1.45385132\n",
      "Iteration 20206, loss = 1.45384942\n",
      "Iteration 20207, loss = 1.45384753\n",
      "Iteration 20208, loss = 1.45384563\n",
      "Iteration 20209, loss = 1.45384373\n",
      "Iteration 20210, loss = 1.45384184\n",
      "Iteration 20211, loss = 1.45383994\n",
      "Iteration 20212, loss = 1.45383805\n",
      "Iteration 20213, loss = 1.45383615\n",
      "Iteration 20214, loss = 1.45383425\n",
      "Iteration 20215, loss = 1.45383236\n",
      "Iteration 20216, loss = 1.45383046\n",
      "Iteration 20217, loss = 1.45382857\n",
      "Iteration 20218, loss = 1.45382667\n",
      "Iteration 20219, loss = 1.45382478\n",
      "Iteration 20220, loss = 1.45382289\n",
      "Iteration 20221, loss = 1.45382099\n",
      "Iteration 20222, loss = 1.45381910\n",
      "Iteration 20223, loss = 1.45381720\n",
      "Iteration 20224, loss = 1.45381531\n",
      "Iteration 20225, loss = 1.45381342\n",
      "Iteration 20226, loss = 1.45381153\n",
      "Iteration 20227, loss = 1.45380963\n",
      "Iteration 20228, loss = 1.45380774\n",
      "Iteration 20229, loss = 1.45380585\n",
      "Iteration 20230, loss = 1.45380396\n",
      "Iteration 20231, loss = 1.45380206\n",
      "Iteration 20232, loss = 1.45380017\n",
      "Iteration 20233, loss = 1.45379828\n",
      "Iteration 20234, loss = 1.45379639\n",
      "Iteration 20235, loss = 1.45379450\n",
      "Iteration 20236, loss = 1.45379261\n",
      "Iteration 20237, loss = 1.45379072\n",
      "Iteration 20238, loss = 1.45378883\n",
      "Iteration 20239, loss = 1.45378694\n",
      "Iteration 20240, loss = 1.45378505\n",
      "Iteration 20241, loss = 1.45378316\n",
      "Iteration 20242, loss = 1.45378127\n",
      "Iteration 20243, loss = 1.45377938\n",
      "Iteration 20244, loss = 1.45377749\n",
      "Iteration 20245, loss = 1.45377560\n",
      "Iteration 20246, loss = 1.45377371\n",
      "Iteration 20247, loss = 1.45377182\n",
      "Iteration 20248, loss = 1.45376994\n",
      "Iteration 20249, loss = 1.45376805\n",
      "Iteration 20250, loss = 1.45376616\n",
      "Iteration 20251, loss = 1.45376427\n",
      "Iteration 20252, loss = 1.45376239\n",
      "Iteration 20253, loss = 1.45376050\n",
      "Iteration 20254, loss = 1.45375861\n",
      "Iteration 20255, loss = 1.45375673\n",
      "Iteration 20256, loss = 1.45375484\n",
      "Iteration 20257, loss = 1.45375295\n",
      "Iteration 20258, loss = 1.45375107\n",
      "Iteration 20259, loss = 1.45374918\n",
      "Iteration 20260, loss = 1.45374730\n",
      "Iteration 20261, loss = 1.45374541\n",
      "Iteration 20262, loss = 1.45374353\n",
      "Iteration 20263, loss = 1.45374164\n",
      "Iteration 20264, loss = 1.45373976\n",
      "Iteration 20265, loss = 1.45373787\n",
      "Iteration 20266, loss = 1.45373599\n",
      "Iteration 20267, loss = 1.45373410\n",
      "Iteration 20268, loss = 1.45373222\n",
      "Iteration 20269, loss = 1.45373034\n",
      "Iteration 20270, loss = 1.45372845\n",
      "Iteration 20271, loss = 1.45372657\n",
      "Iteration 20272, loss = 1.45372469\n",
      "Iteration 20273, loss = 1.45372280\n",
      "Iteration 20274, loss = 1.45372092\n",
      "Iteration 20275, loss = 1.45371904\n",
      "Iteration 20276, loss = 1.45371716\n",
      "Iteration 20277, loss = 1.45371528\n",
      "Iteration 20278, loss = 1.45371339\n",
      "Iteration 20279, loss = 1.45371151\n",
      "Iteration 20280, loss = 1.45370963\n",
      "Iteration 20281, loss = 1.45370775\n",
      "Iteration 20282, loss = 1.45370587\n",
      "Iteration 20283, loss = 1.45370399\n",
      "Iteration 20284, loss = 1.45370211\n",
      "Iteration 20285, loss = 1.45370023\n",
      "Iteration 20286, loss = 1.45369835\n",
      "Iteration 20287, loss = 1.45369647\n",
      "Iteration 20288, loss = 1.45369459\n",
      "Iteration 20289, loss = 1.45369271\n",
      "Iteration 20290, loss = 1.45369083\n",
      "Iteration 20291, loss = 1.45368895\n",
      "Iteration 20292, loss = 1.45368707\n",
      "Iteration 20293, loss = 1.45368520\n",
      "Iteration 20294, loss = 1.45368332\n",
      "Iteration 20295, loss = 1.45368144\n",
      "Iteration 20296, loss = 1.45367956\n",
      "Iteration 20297, loss = 1.45367768\n",
      "Iteration 20298, loss = 1.45367581\n",
      "Iteration 20299, loss = 1.45367393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20300, loss = 1.45367205\n",
      "Iteration 20301, loss = 1.45367018\n",
      "Iteration 20302, loss = 1.45366830\n",
      "Iteration 20303, loss = 1.45366642\n",
      "Iteration 20304, loss = 1.45366455\n",
      "Iteration 20305, loss = 1.45366267\n",
      "Iteration 20306, loss = 1.45366080\n",
      "Iteration 20307, loss = 1.45365892\n",
      "Iteration 20308, loss = 1.45365705\n",
      "Iteration 20309, loss = 1.45365517\n",
      "Iteration 20310, loss = 1.45365330\n",
      "Iteration 20311, loss = 1.45365142\n",
      "Iteration 20312, loss = 1.45364955\n",
      "Iteration 20313, loss = 1.45364767\n",
      "Iteration 20314, loss = 1.45364580\n",
      "Iteration 20315, loss = 1.45364393\n",
      "Iteration 20316, loss = 1.45364205\n",
      "Iteration 20317, loss = 1.45364018\n",
      "Iteration 20318, loss = 1.45363831\n",
      "Iteration 20319, loss = 1.45363643\n",
      "Iteration 20320, loss = 1.45363456\n",
      "Iteration 20321, loss = 1.45363269\n",
      "Iteration 20322, loss = 1.45363082\n",
      "Iteration 20323, loss = 1.45362894\n",
      "Iteration 20324, loss = 1.45362707\n",
      "Iteration 20325, loss = 1.45362520\n",
      "Iteration 20326, loss = 1.45362333\n",
      "Iteration 20327, loss = 1.45362146\n",
      "Iteration 20328, loss = 1.45361959\n",
      "Iteration 20329, loss = 1.45361772\n",
      "Iteration 20330, loss = 1.45361585\n",
      "Iteration 20331, loss = 1.45361398\n",
      "Iteration 20332, loss = 1.45361211\n",
      "Iteration 20333, loss = 1.45361024\n",
      "Iteration 20334, loss = 1.45360837\n",
      "Iteration 20335, loss = 1.45360650\n",
      "Iteration 20336, loss = 1.45360463\n",
      "Iteration 20337, loss = 1.45360276\n",
      "Iteration 20338, loss = 1.45360089\n",
      "Iteration 20339, loss = 1.45359902\n",
      "Iteration 20340, loss = 1.45359715\n",
      "Iteration 20341, loss = 1.45359529\n",
      "Iteration 20342, loss = 1.45359342\n",
      "Iteration 20343, loss = 1.45359155\n",
      "Iteration 20344, loss = 1.45358968\n",
      "Iteration 20345, loss = 1.45358782\n",
      "Iteration 20346, loss = 1.45358595\n",
      "Iteration 20347, loss = 1.45358408\n",
      "Iteration 20348, loss = 1.45358222\n",
      "Iteration 20349, loss = 1.45358035\n",
      "Iteration 20350, loss = 1.45357848\n",
      "Iteration 20351, loss = 1.45357662\n",
      "Iteration 20352, loss = 1.45357475\n",
      "Iteration 20353, loss = 1.45357289\n",
      "Iteration 20354, loss = 1.45357102\n",
      "Iteration 20355, loss = 1.45356916\n",
      "Iteration 20356, loss = 1.45356729\n",
      "Iteration 20357, loss = 1.45356543\n",
      "Iteration 20358, loss = 1.45356356\n",
      "Iteration 20359, loss = 1.45356170\n",
      "Iteration 20360, loss = 1.45355983\n",
      "Iteration 20361, loss = 1.45355797\n",
      "Iteration 20362, loss = 1.45355611\n",
      "Iteration 20363, loss = 1.45355424\n",
      "Iteration 20364, loss = 1.45355238\n",
      "Iteration 20365, loss = 1.45355052\n",
      "Iteration 20366, loss = 1.45354865\n",
      "Iteration 20367, loss = 1.45354679\n",
      "Iteration 20368, loss = 1.45354493\n",
      "Iteration 20369, loss = 1.45354307\n",
      "Iteration 20370, loss = 1.45354121\n",
      "Iteration 20371, loss = 1.45353934\n",
      "Iteration 20372, loss = 1.45353748\n",
      "Iteration 20373, loss = 1.45353562\n",
      "Iteration 20374, loss = 1.45353376\n",
      "Iteration 20375, loss = 1.45353190\n",
      "Iteration 20376, loss = 1.45353004\n",
      "Iteration 20377, loss = 1.45352818\n",
      "Iteration 20378, loss = 1.45352632\n",
      "Iteration 20379, loss = 1.45352446\n",
      "Iteration 20380, loss = 1.45352260\n",
      "Iteration 20381, loss = 1.45352074\n",
      "Iteration 20382, loss = 1.45351888\n",
      "Iteration 20383, loss = 1.45351702\n",
      "Iteration 20384, loss = 1.45351516\n",
      "Iteration 20385, loss = 1.45351330\n",
      "Iteration 20386, loss = 1.45351144\n",
      "Iteration 20387, loss = 1.45350959\n",
      "Iteration 20388, loss = 1.45350773\n",
      "Iteration 20389, loss = 1.45350587\n",
      "Iteration 20390, loss = 1.45350401\n",
      "Iteration 20391, loss = 1.45350216\n",
      "Iteration 20392, loss = 1.45350030\n",
      "Iteration 20393, loss = 1.45349844\n",
      "Iteration 20394, loss = 1.45349658\n",
      "Iteration 20395, loss = 1.45349473\n",
      "Iteration 20396, loss = 1.45349287\n",
      "Iteration 20397, loss = 1.45349102\n",
      "Iteration 20398, loss = 1.45348916\n",
      "Iteration 20399, loss = 1.45348730\n",
      "Iteration 20400, loss = 1.45348545\n",
      "Iteration 20401, loss = 1.45348359\n",
      "Iteration 20402, loss = 1.45348174\n",
      "Iteration 20403, loss = 1.45347988\n",
      "Iteration 20404, loss = 1.45347803\n",
      "Iteration 20405, loss = 1.45347617\n",
      "Iteration 20406, loss = 1.45347432\n",
      "Iteration 20407, loss = 1.45347247\n",
      "Iteration 20408, loss = 1.45347061\n",
      "Iteration 20409, loss = 1.45346876\n",
      "Iteration 20410, loss = 1.45346690\n",
      "Iteration 20411, loss = 1.45346505\n",
      "Iteration 20412, loss = 1.45346320\n",
      "Iteration 20413, loss = 1.45346135\n",
      "Iteration 20414, loss = 1.45345949\n",
      "Iteration 20415, loss = 1.45345764\n",
      "Iteration 20416, loss = 1.45345579\n",
      "Iteration 20417, loss = 1.45345394\n",
      "Iteration 20418, loss = 1.45345209\n",
      "Iteration 20419, loss = 1.45345023\n",
      "Iteration 20420, loss = 1.45344838\n",
      "Iteration 20421, loss = 1.45344653\n",
      "Iteration 20422, loss = 1.45344468\n",
      "Iteration 20423, loss = 1.45344283\n",
      "Iteration 20424, loss = 1.45344098\n",
      "Iteration 20425, loss = 1.45343913\n",
      "Iteration 20426, loss = 1.45343728\n",
      "Iteration 20427, loss = 1.45343543\n",
      "Iteration 20428, loss = 1.45343358\n",
      "Iteration 20429, loss = 1.45343173\n",
      "Iteration 20430, loss = 1.45342988\n",
      "Iteration 20431, loss = 1.45342803\n",
      "Iteration 20432, loss = 1.45342618\n",
      "Iteration 20433, loss = 1.45342434\n",
      "Iteration 20434, loss = 1.45342249\n",
      "Iteration 20435, loss = 1.45342064\n",
      "Iteration 20436, loss = 1.45341879\n",
      "Iteration 20437, loss = 1.45341694\n",
      "Iteration 20438, loss = 1.45341510\n",
      "Iteration 20439, loss = 1.45341325\n",
      "Iteration 20440, loss = 1.45341140\n",
      "Iteration 20441, loss = 1.45340955\n",
      "Iteration 20442, loss = 1.45340771\n",
      "Iteration 20443, loss = 1.45340586\n",
      "Iteration 20444, loss = 1.45340402\n",
      "Iteration 20445, loss = 1.45340217\n",
      "Iteration 20446, loss = 1.45340032\n",
      "Iteration 20447, loss = 1.45339848\n",
      "Iteration 20448, loss = 1.45339663\n",
      "Iteration 20449, loss = 1.45339479\n",
      "Iteration 20450, loss = 1.45339294\n",
      "Iteration 20451, loss = 1.45339110\n",
      "Iteration 20452, loss = 1.45338925\n",
      "Iteration 20453, loss = 1.45338741\n",
      "Iteration 20454, loss = 1.45338556\n",
      "Iteration 20455, loss = 1.45338372\n",
      "Iteration 20456, loss = 1.45338188\n",
      "Iteration 20457, loss = 1.45338003\n",
      "Iteration 20458, loss = 1.45337819\n",
      "Iteration 20459, loss = 1.45337635\n",
      "Iteration 20460, loss = 1.45337450\n",
      "Iteration 20461, loss = 1.45337266\n",
      "Iteration 20462, loss = 1.45337082\n",
      "Iteration 20463, loss = 1.45336898\n",
      "Iteration 20464, loss = 1.45336714\n",
      "Iteration 20465, loss = 1.45336529\n",
      "Iteration 20466, loss = 1.45336345\n",
      "Iteration 20467, loss = 1.45336161\n",
      "Iteration 20468, loss = 1.45335977\n",
      "Iteration 20469, loss = 1.45335793\n",
      "Iteration 20470, loss = 1.45335609\n",
      "Iteration 20471, loss = 1.45335425\n",
      "Iteration 20472, loss = 1.45335241\n",
      "Iteration 20473, loss = 1.45335057\n",
      "Iteration 20474, loss = 1.45334873\n",
      "Iteration 20475, loss = 1.45334689\n",
      "Iteration 20476, loss = 1.45334505\n",
      "Iteration 20477, loss = 1.45334321\n",
      "Iteration 20478, loss = 1.45334137\n",
      "Iteration 20479, loss = 1.45333953\n",
      "Iteration 20480, loss = 1.45333769\n",
      "Iteration 20481, loss = 1.45333585\n",
      "Iteration 20482, loss = 1.45333401\n",
      "Iteration 20483, loss = 1.45333218\n",
      "Iteration 20484, loss = 1.45333034\n",
      "Iteration 20485, loss = 1.45332850\n",
      "Iteration 20486, loss = 1.45332666\n",
      "Iteration 20487, loss = 1.45332483\n",
      "Iteration 20488, loss = 1.45332299\n",
      "Iteration 20489, loss = 1.45332115\n",
      "Iteration 20490, loss = 1.45331932\n",
      "Iteration 20491, loss = 1.45331748\n",
      "Iteration 20492, loss = 1.45331564\n",
      "Iteration 20493, loss = 1.45331381\n",
      "Iteration 20494, loss = 1.45331197\n",
      "Iteration 20495, loss = 1.45331014\n",
      "Iteration 20496, loss = 1.45330830\n",
      "Iteration 20497, loss = 1.45330647\n",
      "Iteration 20498, loss = 1.45330463\n",
      "Iteration 20499, loss = 1.45330280\n",
      "Iteration 20500, loss = 1.45330096\n",
      "Iteration 20501, loss = 1.45329913\n",
      "Iteration 20502, loss = 1.45329729\n",
      "Iteration 20503, loss = 1.45329546\n",
      "Iteration 20504, loss = 1.45329363\n",
      "Iteration 20505, loss = 1.45329179\n",
      "Iteration 20506, loss = 1.45328996\n",
      "Iteration 20507, loss = 1.45328813\n",
      "Iteration 20508, loss = 1.45328629\n",
      "Iteration 20509, loss = 1.45328446\n",
      "Iteration 20510, loss = 1.45328263\n",
      "Iteration 20511, loss = 1.45328080\n",
      "Iteration 20512, loss = 1.45327897\n",
      "Iteration 20513, loss = 1.45327713\n",
      "Iteration 20514, loss = 1.45327530\n",
      "Iteration 20515, loss = 1.45327347\n",
      "Iteration 20516, loss = 1.45327164\n",
      "Iteration 20517, loss = 1.45326981\n",
      "Iteration 20518, loss = 1.45326798\n",
      "Iteration 20519, loss = 1.45326615\n",
      "Iteration 20520, loss = 1.45326432\n",
      "Iteration 20521, loss = 1.45326249\n",
      "Iteration 20522, loss = 1.45326066\n",
      "Iteration 20523, loss = 1.45325883\n",
      "Iteration 20524, loss = 1.45325700\n",
      "Iteration 20525, loss = 1.45325517\n",
      "Iteration 20526, loss = 1.45325334\n",
      "Iteration 20527, loss = 1.45325151\n",
      "Iteration 20528, loss = 1.45324968\n",
      "Iteration 20529, loss = 1.45324785\n",
      "Iteration 20530, loss = 1.45324603\n",
      "Iteration 20531, loss = 1.45324420\n",
      "Iteration 20532, loss = 1.45324237\n",
      "Iteration 20533, loss = 1.45324054\n",
      "Iteration 20534, loss = 1.45323871\n",
      "Iteration 20535, loss = 1.45323689\n",
      "Iteration 20536, loss = 1.45323506\n",
      "Iteration 20537, loss = 1.45323323\n",
      "Iteration 20538, loss = 1.45323141\n",
      "Iteration 20539, loss = 1.45322958\n",
      "Iteration 20540, loss = 1.45322775\n",
      "Iteration 20541, loss = 1.45322593\n",
      "Iteration 20542, loss = 1.45322410\n",
      "Iteration 20543, loss = 1.45322228\n",
      "Iteration 20544, loss = 1.45322045\n",
      "Iteration 20545, loss = 1.45321863\n",
      "Iteration 20546, loss = 1.45321680\n",
      "Iteration 20547, loss = 1.45321498\n",
      "Iteration 20548, loss = 1.45321315\n",
      "Iteration 20549, loss = 1.45321133\n",
      "Iteration 20550, loss = 1.45320950\n",
      "Iteration 20551, loss = 1.45320768\n",
      "Iteration 20552, loss = 1.45320586\n",
      "Iteration 20553, loss = 1.45320403\n",
      "Iteration 20554, loss = 1.45320221\n",
      "Iteration 20555, loss = 1.45320039\n",
      "Iteration 20556, loss = 1.45319856\n",
      "Iteration 20557, loss = 1.45319674\n",
      "Iteration 20558, loss = 1.45319492\n",
      "Iteration 20559, loss = 1.45319310\n",
      "Iteration 20560, loss = 1.45319128\n",
      "Iteration 20561, loss = 1.45318945\n",
      "Iteration 20562, loss = 1.45318763\n",
      "Iteration 20563, loss = 1.45318581\n",
      "Iteration 20564, loss = 1.45318399\n",
      "Iteration 20565, loss = 1.45318217\n",
      "Iteration 20566, loss = 1.45318035\n",
      "Iteration 20567, loss = 1.45317853\n",
      "Iteration 20568, loss = 1.45317671\n",
      "Iteration 20569, loss = 1.45317489\n",
      "Iteration 20570, loss = 1.45317307\n",
      "Iteration 20571, loss = 1.45317125\n",
      "Iteration 20572, loss = 1.45316943\n",
      "Iteration 20573, loss = 1.45316761\n",
      "Iteration 20574, loss = 1.45316579\n",
      "Iteration 20575, loss = 1.45316397\n",
      "Iteration 20576, loss = 1.45316215\n",
      "Iteration 20577, loss = 1.45316033\n",
      "Iteration 20578, loss = 1.45315851\n",
      "Iteration 20579, loss = 1.45315670\n",
      "Iteration 20580, loss = 1.45315488\n",
      "Iteration 20581, loss = 1.45315306\n",
      "Iteration 20582, loss = 1.45315124\n",
      "Iteration 20583, loss = 1.45314943\n",
      "Iteration 20584, loss = 1.45314761\n",
      "Iteration 20585, loss = 1.45314579\n",
      "Iteration 20586, loss = 1.45314398\n",
      "Iteration 20587, loss = 1.45314216\n",
      "Iteration 20588, loss = 1.45314034\n",
      "Iteration 20589, loss = 1.45313853\n",
      "Iteration 20590, loss = 1.45313671\n",
      "Iteration 20591, loss = 1.45313490\n",
      "Iteration 20592, loss = 1.45313308\n",
      "Iteration 20593, loss = 1.45313126\n",
      "Iteration 20594, loss = 1.45312945\n",
      "Iteration 20595, loss = 1.45312763\n",
      "Iteration 20596, loss = 1.45312582\n",
      "Iteration 20597, loss = 1.45312401\n",
      "Iteration 20598, loss = 1.45312219\n",
      "Iteration 20599, loss = 1.45312038\n",
      "Iteration 20600, loss = 1.45311856\n",
      "Iteration 20601, loss = 1.45311675\n",
      "Iteration 20602, loss = 1.45311494\n",
      "Iteration 20603, loss = 1.45311312\n",
      "Iteration 20604, loss = 1.45311131\n",
      "Iteration 20605, loss = 1.45310950\n",
      "Iteration 20606, loss = 1.45310769\n",
      "Iteration 20607, loss = 1.45310587\n",
      "Iteration 20608, loss = 1.45310406\n",
      "Iteration 20609, loss = 1.45310225\n",
      "Iteration 20610, loss = 1.45310044\n",
      "Iteration 20611, loss = 1.45309863\n",
      "Iteration 20612, loss = 1.45309682\n",
      "Iteration 20613, loss = 1.45309500\n",
      "Iteration 20614, loss = 1.45309319\n",
      "Iteration 20615, loss = 1.45309138\n",
      "Iteration 20616, loss = 1.45308957\n",
      "Iteration 20617, loss = 1.45308776\n",
      "Iteration 20618, loss = 1.45308595\n",
      "Iteration 20619, loss = 1.45308414\n",
      "Iteration 20620, loss = 1.45308233\n",
      "Iteration 20621, loss = 1.45308052\n",
      "Iteration 20622, loss = 1.45307871\n",
      "Iteration 20623, loss = 1.45307690\n",
      "Iteration 20624, loss = 1.45307510\n",
      "Iteration 20625, loss = 1.45307329\n",
      "Iteration 20626, loss = 1.45307148\n",
      "Iteration 20627, loss = 1.45306967\n",
      "Iteration 20628, loss = 1.45306786\n",
      "Iteration 20629, loss = 1.45306605\n",
      "Iteration 20630, loss = 1.45306425\n",
      "Iteration 20631, loss = 1.45306244\n",
      "Iteration 20632, loss = 1.45306063\n",
      "Iteration 20633, loss = 1.45305883\n",
      "Iteration 20634, loss = 1.45305702\n",
      "Iteration 20635, loss = 1.45305521\n",
      "Iteration 20636, loss = 1.45305341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20637, loss = 1.45305160\n",
      "Iteration 20638, loss = 1.45304979\n",
      "Iteration 20639, loss = 1.45304799\n",
      "Iteration 20640, loss = 1.45304618\n",
      "Iteration 20641, loss = 1.45304438\n",
      "Iteration 20642, loss = 1.45304257\n",
      "Iteration 20643, loss = 1.45304077\n",
      "Iteration 20644, loss = 1.45303896\n",
      "Iteration 20645, loss = 1.45303716\n",
      "Iteration 20646, loss = 1.45303535\n",
      "Iteration 20647, loss = 1.45303355\n",
      "Iteration 20648, loss = 1.45303175\n",
      "Iteration 20649, loss = 1.45302994\n",
      "Iteration 20650, loss = 1.45302814\n",
      "Iteration 20651, loss = 1.45302633\n",
      "Iteration 20652, loss = 1.45302453\n",
      "Iteration 20653, loss = 1.45302273\n",
      "Iteration 20654, loss = 1.45302093\n",
      "Iteration 20655, loss = 1.45301912\n",
      "Iteration 20656, loss = 1.45301732\n",
      "Iteration 20657, loss = 1.45301552\n",
      "Iteration 20658, loss = 1.45301372\n",
      "Iteration 20659, loss = 1.45301192\n",
      "Iteration 20660, loss = 1.45301011\n",
      "Iteration 20661, loss = 1.45300831\n",
      "Iteration 20662, loss = 1.45300651\n",
      "Iteration 20663, loss = 1.45300471\n",
      "Iteration 20664, loss = 1.45300291\n",
      "Iteration 20665, loss = 1.45300111\n",
      "Iteration 20666, loss = 1.45299931\n",
      "Iteration 20667, loss = 1.45299751\n",
      "Iteration 20668, loss = 1.45299571\n",
      "Iteration 20669, loss = 1.45299391\n",
      "Iteration 20670, loss = 1.45299211\n",
      "Iteration 20671, loss = 1.45299031\n",
      "Iteration 20672, loss = 1.45298851\n",
      "Iteration 20673, loss = 1.45298671\n",
      "Iteration 20674, loss = 1.45298491\n",
      "Iteration 20675, loss = 1.45298312\n",
      "Iteration 20676, loss = 1.45298132\n",
      "Iteration 20677, loss = 1.45297952\n",
      "Iteration 20678, loss = 1.45297772\n",
      "Iteration 20679, loss = 1.45297592\n",
      "Iteration 20680, loss = 1.45297413\n",
      "Iteration 20681, loss = 1.45297233\n",
      "Iteration 20682, loss = 1.45297053\n",
      "Iteration 20683, loss = 1.45296874\n",
      "Iteration 20684, loss = 1.45296694\n",
      "Iteration 20685, loss = 1.45296514\n",
      "Iteration 20686, loss = 1.45296335\n",
      "Iteration 20687, loss = 1.45296155\n",
      "Iteration 20688, loss = 1.45295976\n",
      "Iteration 20689, loss = 1.45295796\n",
      "Iteration 20690, loss = 1.45295616\n",
      "Iteration 20691, loss = 1.45295437\n",
      "Iteration 20692, loss = 1.45295257\n",
      "Iteration 20693, loss = 1.45295078\n",
      "Iteration 20694, loss = 1.45294898\n",
      "Iteration 20695, loss = 1.45294719\n",
      "Iteration 20696, loss = 1.45294540\n",
      "Iteration 20697, loss = 1.45294360\n",
      "Iteration 20698, loss = 1.45294181\n",
      "Iteration 20699, loss = 1.45294001\n",
      "Iteration 20700, loss = 1.45293822\n",
      "Iteration 20701, loss = 1.45293643\n",
      "Iteration 20702, loss = 1.45293464\n",
      "Iteration 20703, loss = 1.45293284\n",
      "Iteration 20704, loss = 1.45293105\n",
      "Iteration 20705, loss = 1.45292926\n",
      "Iteration 20706, loss = 1.45292747\n",
      "Iteration 20707, loss = 1.45292567\n",
      "Iteration 20708, loss = 1.45292388\n",
      "Iteration 20709, loss = 1.45292209\n",
      "Iteration 20710, loss = 1.45292030\n",
      "Iteration 20711, loss = 1.45291851\n",
      "Iteration 20712, loss = 1.45291672\n",
      "Iteration 20713, loss = 1.45291493\n",
      "Iteration 20714, loss = 1.45291314\n",
      "Iteration 20715, loss = 1.45291135\n",
      "Iteration 20716, loss = 1.45290956\n",
      "Iteration 20717, loss = 1.45290777\n",
      "Iteration 20718, loss = 1.45290598\n",
      "Iteration 20719, loss = 1.45290419\n",
      "Iteration 20720, loss = 1.45290240\n",
      "Iteration 20721, loss = 1.45290061\n",
      "Iteration 20722, loss = 1.45289882\n",
      "Iteration 20723, loss = 1.45289703\n",
      "Iteration 20724, loss = 1.45289524\n",
      "Iteration 20725, loss = 1.45289345\n",
      "Iteration 20726, loss = 1.45289167\n",
      "Iteration 20727, loss = 1.45288988\n",
      "Iteration 20728, loss = 1.45288809\n",
      "Iteration 20729, loss = 1.45288630\n",
      "Iteration 20730, loss = 1.45288452\n",
      "Iteration 20731, loss = 1.45288273\n",
      "Iteration 20732, loss = 1.45288094\n",
      "Iteration 20733, loss = 1.45287915\n",
      "Iteration 20734, loss = 1.45287737\n",
      "Iteration 20735, loss = 1.45287558\n",
      "Iteration 20736, loss = 1.45287380\n",
      "Iteration 20737, loss = 1.45287201\n",
      "Iteration 20738, loss = 1.45287022\n",
      "Iteration 20739, loss = 1.45286844\n",
      "Iteration 20740, loss = 1.45286665\n",
      "Iteration 20741, loss = 1.45286487\n",
      "Iteration 20742, loss = 1.45286308\n",
      "Iteration 20743, loss = 1.45286130\n",
      "Iteration 20744, loss = 1.45285951\n",
      "Iteration 20745, loss = 1.45285773\n",
      "Iteration 20746, loss = 1.45285595\n",
      "Iteration 20747, loss = 1.45285416\n",
      "Iteration 20748, loss = 1.45285238\n",
      "Iteration 20749, loss = 1.45285059\n",
      "Iteration 20750, loss = 1.45284881\n",
      "Iteration 20751, loss = 1.45284703\n",
      "Iteration 20752, loss = 1.45284525\n",
      "Iteration 20753, loss = 1.45284346\n",
      "Iteration 20754, loss = 1.45284168\n",
      "Iteration 20755, loss = 1.45283990\n",
      "Iteration 20756, loss = 1.45283812\n",
      "Iteration 20757, loss = 1.45283633\n",
      "Iteration 20758, loss = 1.45283455\n",
      "Iteration 20759, loss = 1.45283277\n",
      "Iteration 20760, loss = 1.45283099\n",
      "Iteration 20761, loss = 1.45282921\n",
      "Iteration 20762, loss = 1.45282743\n",
      "Iteration 20763, loss = 1.45282565\n",
      "Iteration 20764, loss = 1.45282387\n",
      "Iteration 20765, loss = 1.45282209\n",
      "Iteration 20766, loss = 1.45282031\n",
      "Iteration 20767, loss = 1.45281853\n",
      "Iteration 20768, loss = 1.45281675\n",
      "Iteration 20769, loss = 1.45281497\n",
      "Iteration 20770, loss = 1.45281319\n",
      "Iteration 20771, loss = 1.45281141\n",
      "Iteration 20772, loss = 1.45280963\n",
      "Iteration 20773, loss = 1.45280785\n",
      "Iteration 20774, loss = 1.45280607\n",
      "Iteration 20775, loss = 1.45280430\n",
      "Iteration 20776, loss = 1.45280252\n",
      "Iteration 20777, loss = 1.45280074\n",
      "Iteration 20778, loss = 1.45279896\n",
      "Iteration 20779, loss = 1.45279718\n",
      "Iteration 20780, loss = 1.45279541\n",
      "Iteration 20781, loss = 1.45279363\n",
      "Iteration 20782, loss = 1.45279185\n",
      "Iteration 20783, loss = 1.45279008\n",
      "Iteration 20784, loss = 1.45278830\n",
      "Iteration 20785, loss = 1.45278652\n",
      "Iteration 20786, loss = 1.45278475\n",
      "Iteration 20787, loss = 1.45278297\n",
      "Iteration 20788, loss = 1.45278120\n",
      "Iteration 20789, loss = 1.45277942\n",
      "Iteration 20790, loss = 1.45277765\n",
      "Iteration 20791, loss = 1.45277587\n",
      "Iteration 20792, loss = 1.45277410\n",
      "Iteration 20793, loss = 1.45277232\n",
      "Iteration 20794, loss = 1.45277055\n",
      "Iteration 20795, loss = 1.45276877\n",
      "Iteration 20796, loss = 1.45276700\n",
      "Iteration 20797, loss = 1.45276522\n",
      "Iteration 20798, loss = 1.45276345\n",
      "Iteration 20799, loss = 1.45276168\n",
      "Iteration 20800, loss = 1.45275990\n",
      "Iteration 20801, loss = 1.45275813\n",
      "Iteration 20802, loss = 1.45275636\n",
      "Iteration 20803, loss = 1.45275459\n",
      "Iteration 20804, loss = 1.45275281\n",
      "Iteration 20805, loss = 1.45275104\n",
      "Iteration 20806, loss = 1.45274927\n",
      "Iteration 20807, loss = 1.45274750\n",
      "Iteration 20808, loss = 1.45274573\n",
      "Iteration 20809, loss = 1.45274395\n",
      "Iteration 20810, loss = 1.45274218\n",
      "Iteration 20811, loss = 1.45274041\n",
      "Iteration 20812, loss = 1.45273864\n",
      "Iteration 20813, loss = 1.45273687\n",
      "Iteration 20814, loss = 1.45273510\n",
      "Iteration 20815, loss = 1.45273333\n",
      "Iteration 20816, loss = 1.45273156\n",
      "Iteration 20817, loss = 1.45272979\n",
      "Iteration 20818, loss = 1.45272802\n",
      "Iteration 20819, loss = 1.45272625\n",
      "Iteration 20820, loss = 1.45272448\n",
      "Iteration 20821, loss = 1.45272271\n",
      "Iteration 20822, loss = 1.45272094\n",
      "Iteration 20823, loss = 1.45271917\n",
      "Iteration 20824, loss = 1.45271741\n",
      "Iteration 20825, loss = 1.45271564\n",
      "Iteration 20826, loss = 1.45271387\n",
      "Iteration 20827, loss = 1.45271210\n",
      "Iteration 20828, loss = 1.45271033\n",
      "Iteration 20829, loss = 1.45270857\n",
      "Iteration 20830, loss = 1.45270680\n",
      "Iteration 20831, loss = 1.45270503\n",
      "Iteration 20832, loss = 1.45270326\n",
      "Iteration 20833, loss = 1.45270150\n",
      "Iteration 20834, loss = 1.45269973\n",
      "Iteration 20835, loss = 1.45269797\n",
      "Iteration 20836, loss = 1.45269620\n",
      "Iteration 20837, loss = 1.45269443\n",
      "Iteration 20838, loss = 1.45269267\n",
      "Iteration 20839, loss = 1.45269090\n",
      "Iteration 20840, loss = 1.45268914\n",
      "Iteration 20841, loss = 1.45268737\n",
      "Iteration 20842, loss = 1.45268561\n",
      "Iteration 20843, loss = 1.45268384\n",
      "Iteration 20844, loss = 1.45268208\n",
      "Iteration 20845, loss = 1.45268031\n",
      "Iteration 20846, loss = 1.45267855\n",
      "Iteration 20847, loss = 1.45267679\n",
      "Iteration 20848, loss = 1.45267502\n",
      "Iteration 20849, loss = 1.45267326\n",
      "Iteration 20850, loss = 1.45267149\n",
      "Iteration 20851, loss = 1.45266973\n",
      "Iteration 20852, loss = 1.45266797\n",
      "Iteration 20853, loss = 1.45266621\n",
      "Iteration 20854, loss = 1.45266444\n",
      "Iteration 20855, loss = 1.45266268\n",
      "Iteration 20856, loss = 1.45266092\n",
      "Iteration 20857, loss = 1.45265916\n",
      "Iteration 20858, loss = 1.45265740\n",
      "Iteration 20859, loss = 1.45265563\n",
      "Iteration 20860, loss = 1.45265387\n",
      "Iteration 20861, loss = 1.45265211\n",
      "Iteration 20862, loss = 1.45265035\n",
      "Iteration 20863, loss = 1.45264859\n",
      "Iteration 20864, loss = 1.45264683\n",
      "Iteration 20865, loss = 1.45264507\n",
      "Iteration 20866, loss = 1.45264331\n",
      "Iteration 20867, loss = 1.45264155\n",
      "Iteration 20868, loss = 1.45263979\n",
      "Iteration 20869, loss = 1.45263803\n",
      "Iteration 20870, loss = 1.45263627\n",
      "Iteration 20871, loss = 1.45263451\n",
      "Iteration 20872, loss = 1.45263275\n",
      "Iteration 20873, loss = 1.45263099\n",
      "Iteration 20874, loss = 1.45262923\n",
      "Iteration 20875, loss = 1.45262748\n",
      "Iteration 20876, loss = 1.45262572\n",
      "Iteration 20877, loss = 1.45262396\n",
      "Iteration 20878, loss = 1.45262220\n",
      "Iteration 20879, loss = 1.45262044\n",
      "Iteration 20880, loss = 1.45261869\n",
      "Iteration 20881, loss = 1.45261693\n",
      "Iteration 20882, loss = 1.45261517\n",
      "Iteration 20883, loss = 1.45261342\n",
      "Iteration 20884, loss = 1.45261166\n",
      "Iteration 20885, loss = 1.45260990\n",
      "Iteration 20886, loss = 1.45260815\n",
      "Iteration 20887, loss = 1.45260639\n",
      "Iteration 20888, loss = 1.45260464\n",
      "Iteration 20889, loss = 1.45260288\n",
      "Iteration 20890, loss = 1.45260112\n",
      "Iteration 20891, loss = 1.45259937\n",
      "Iteration 20892, loss = 1.45259761\n",
      "Iteration 20893, loss = 1.45259586\n",
      "Iteration 20894, loss = 1.45259410\n",
      "Iteration 20895, loss = 1.45259235\n",
      "Iteration 20896, loss = 1.45259060\n",
      "Iteration 20897, loss = 1.45258884\n",
      "Iteration 20898, loss = 1.45258709\n",
      "Iteration 20899, loss = 1.45258533\n",
      "Iteration 20900, loss = 1.45258358\n",
      "Iteration 20901, loss = 1.45258183\n",
      "Iteration 20902, loss = 1.45258007\n",
      "Iteration 20903, loss = 1.45257832\n",
      "Iteration 20904, loss = 1.45257657\n",
      "Iteration 20905, loss = 1.45257482\n",
      "Iteration 20906, loss = 1.45257306\n",
      "Iteration 20907, loss = 1.45257131\n",
      "Iteration 20908, loss = 1.45256956\n",
      "Iteration 20909, loss = 1.45256781\n",
      "Iteration 20910, loss = 1.45256606\n",
      "Iteration 20911, loss = 1.45256431\n",
      "Iteration 20912, loss = 1.45256255\n",
      "Iteration 20913, loss = 1.45256080\n",
      "Iteration 20914, loss = 1.45255905\n",
      "Iteration 20915, loss = 1.45255730\n",
      "Iteration 20916, loss = 1.45255555\n",
      "Iteration 20917, loss = 1.45255380\n",
      "Iteration 20918, loss = 1.45255205\n",
      "Iteration 20919, loss = 1.45255030\n",
      "Iteration 20920, loss = 1.45254855\n",
      "Iteration 20921, loss = 1.45254680\n",
      "Iteration 20922, loss = 1.45254505\n",
      "Iteration 20923, loss = 1.45254331\n",
      "Iteration 20924, loss = 1.45254156\n",
      "Iteration 20925, loss = 1.45253981\n",
      "Iteration 20926, loss = 1.45253806\n",
      "Iteration 20927, loss = 1.45253631\n",
      "Iteration 20928, loss = 1.45253456\n",
      "Iteration 20929, loss = 1.45253282\n",
      "Iteration 20930, loss = 1.45253107\n",
      "Iteration 20931, loss = 1.45252932\n",
      "Iteration 20932, loss = 1.45252757\n",
      "Iteration 20933, loss = 1.45252583\n",
      "Iteration 20934, loss = 1.45252408\n",
      "Iteration 20935, loss = 1.45252233\n",
      "Iteration 20936, loss = 1.45252059\n",
      "Iteration 20937, loss = 1.45251884\n",
      "Iteration 20938, loss = 1.45251709\n",
      "Iteration 20939, loss = 1.45251535\n",
      "Iteration 20940, loss = 1.45251360\n",
      "Iteration 20941, loss = 1.45251186\n",
      "Iteration 20942, loss = 1.45251011\n",
      "Iteration 20943, loss = 1.45250837\n",
      "Iteration 20944, loss = 1.45250662\n",
      "Iteration 20945, loss = 1.45250488\n",
      "Iteration 20946, loss = 1.45250313\n",
      "Iteration 20947, loss = 1.45250139\n",
      "Iteration 20948, loss = 1.45249965\n",
      "Iteration 20949, loss = 1.45249790\n",
      "Iteration 20950, loss = 1.45249616\n",
      "Iteration 20951, loss = 1.45249441\n",
      "Iteration 20952, loss = 1.45249267\n",
      "Iteration 20953, loss = 1.45249093\n",
      "Iteration 20954, loss = 1.45248919\n",
      "Iteration 20955, loss = 1.45248744\n",
      "Iteration 20956, loss = 1.45248570\n",
      "Iteration 20957, loss = 1.45248396\n",
      "Iteration 20958, loss = 1.45248222\n",
      "Iteration 20959, loss = 1.45248047\n",
      "Iteration 20960, loss = 1.45247873\n",
      "Iteration 20961, loss = 1.45247699\n",
      "Iteration 20962, loss = 1.45247525\n",
      "Iteration 20963, loss = 1.45247351\n",
      "Iteration 20964, loss = 1.45247177\n",
      "Iteration 20965, loss = 1.45247003\n",
      "Iteration 20966, loss = 1.45246829\n",
      "Iteration 20967, loss = 1.45246655\n",
      "Iteration 20968, loss = 1.45246481\n",
      "Iteration 20969, loss = 1.45246307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20970, loss = 1.45246133\n",
      "Iteration 20971, loss = 1.45245959\n",
      "Iteration 20972, loss = 1.45245785\n",
      "Iteration 20973, loss = 1.45245611\n",
      "Iteration 20974, loss = 1.45245437\n",
      "Iteration 20975, loss = 1.45245263\n",
      "Iteration 20976, loss = 1.45245089\n",
      "Iteration 20977, loss = 1.45244915\n",
      "Iteration 20978, loss = 1.45244741\n",
      "Iteration 20979, loss = 1.45244568\n",
      "Iteration 20980, loss = 1.45244394\n",
      "Iteration 20981, loss = 1.45244220\n",
      "Iteration 20982, loss = 1.45244046\n",
      "Iteration 20983, loss = 1.45243873\n",
      "Iteration 20984, loss = 1.45243699\n",
      "Iteration 20985, loss = 1.45243525\n",
      "Iteration 20986, loss = 1.45243352\n",
      "Iteration 20987, loss = 1.45243178\n",
      "Iteration 20988, loss = 1.45243004\n",
      "Iteration 20989, loss = 1.45242831\n",
      "Iteration 20990, loss = 1.45242657\n",
      "Iteration 20991, loss = 1.45242484\n",
      "Iteration 20992, loss = 1.45242310\n",
      "Iteration 20993, loss = 1.45242136\n",
      "Iteration 20994, loss = 1.45241963\n",
      "Iteration 20995, loss = 1.45241789\n",
      "Iteration 20996, loss = 1.45241616\n",
      "Iteration 20997, loss = 1.45241443\n",
      "Iteration 20998, loss = 1.45241269\n",
      "Iteration 20999, loss = 1.45241096\n",
      "Iteration 21000, loss = 1.45240922\n",
      "Iteration 21001, loss = 1.45240749\n",
      "Iteration 21002, loss = 1.45240576\n",
      "Iteration 21003, loss = 1.45240402\n",
      "Iteration 21004, loss = 1.45240229\n",
      "Iteration 21005, loss = 1.45240056\n",
      "Iteration 21006, loss = 1.45239882\n",
      "Iteration 21007, loss = 1.45239709\n",
      "Iteration 21008, loss = 1.45239536\n",
      "Iteration 21009, loss = 1.45239363\n",
      "Iteration 21010, loss = 1.45239189\n",
      "Iteration 21011, loss = 1.45239016\n",
      "Iteration 21012, loss = 1.45238843\n",
      "Iteration 21013, loss = 1.45238670\n",
      "Iteration 21014, loss = 1.45238497\n",
      "Iteration 21015, loss = 1.45238324\n",
      "Iteration 21016, loss = 1.45238151\n",
      "Iteration 21017, loss = 1.45237978\n",
      "Iteration 21018, loss = 1.45237804\n",
      "Iteration 21019, loss = 1.45237631\n",
      "Iteration 21020, loss = 1.45237458\n",
      "Iteration 21021, loss = 1.45237285\n",
      "Iteration 21022, loss = 1.45237112\n",
      "Iteration 21023, loss = 1.45236940\n",
      "Iteration 21024, loss = 1.45236767\n",
      "Iteration 21025, loss = 1.45236594\n",
      "Iteration 21026, loss = 1.45236421\n",
      "Iteration 21027, loss = 1.45236248\n",
      "Iteration 21028, loss = 1.45236075\n",
      "Iteration 21029, loss = 1.45235902\n",
      "Iteration 21030, loss = 1.45235729\n",
      "Iteration 21031, loss = 1.45235557\n",
      "Iteration 21032, loss = 1.45235384\n",
      "Iteration 21033, loss = 1.45235211\n",
      "Iteration 21034, loss = 1.45235038\n",
      "Iteration 21035, loss = 1.45234866\n",
      "Iteration 21036, loss = 1.45234693\n",
      "Iteration 21037, loss = 1.45234520\n",
      "Iteration 21038, loss = 1.45234348\n",
      "Iteration 21039, loss = 1.45234175\n",
      "Iteration 21040, loss = 1.45234002\n",
      "Iteration 21041, loss = 1.45233830\n",
      "Iteration 21042, loss = 1.45233657\n",
      "Iteration 21043, loss = 1.45233485\n",
      "Iteration 21044, loss = 1.45233312\n",
      "Iteration 21045, loss = 1.45233139\n",
      "Iteration 21046, loss = 1.45232967\n",
      "Iteration 21047, loss = 1.45232794\n",
      "Iteration 21048, loss = 1.45232622\n",
      "Iteration 21049, loss = 1.45232450\n",
      "Iteration 21050, loss = 1.45232277\n",
      "Iteration 21051, loss = 1.45232105\n",
      "Iteration 21052, loss = 1.45231932\n",
      "Iteration 21053, loss = 1.45231760\n",
      "Iteration 21054, loss = 1.45231588\n",
      "Iteration 21055, loss = 1.45231415\n",
      "Iteration 21056, loss = 1.45231243\n",
      "Iteration 21057, loss = 1.45231071\n",
      "Iteration 21058, loss = 1.45230898\n",
      "Iteration 21059, loss = 1.45230726\n",
      "Iteration 21060, loss = 1.45230554\n",
      "Iteration 21061, loss = 1.45230382\n",
      "Iteration 21062, loss = 1.45230209\n",
      "Iteration 21063, loss = 1.45230037\n",
      "Iteration 21064, loss = 1.45229865\n",
      "Iteration 21065, loss = 1.45229693\n",
      "Iteration 21066, loss = 1.45229521\n",
      "Iteration 21067, loss = 1.45229349\n",
      "Iteration 21068, loss = 1.45229177\n",
      "Iteration 21069, loss = 1.45229005\n",
      "Iteration 21070, loss = 1.45228833\n",
      "Iteration 21071, loss = 1.45228661\n",
      "Iteration 21072, loss = 1.45228489\n",
      "Iteration 21073, loss = 1.45228317\n",
      "Iteration 21074, loss = 1.45228145\n",
      "Iteration 21075, loss = 1.45227973\n",
      "Iteration 21076, loss = 1.45227801\n",
      "Iteration 21077, loss = 1.45227629\n",
      "Iteration 21078, loss = 1.45227457\n",
      "Iteration 21079, loss = 1.45227285\n",
      "Iteration 21080, loss = 1.45227113\n",
      "Iteration 21081, loss = 1.45226941\n",
      "Iteration 21082, loss = 1.45226769\n",
      "Iteration 21083, loss = 1.45226598\n",
      "Iteration 21084, loss = 1.45226426\n",
      "Iteration 21085, loss = 1.45226254\n",
      "Iteration 21086, loss = 1.45226082\n",
      "Iteration 21087, loss = 1.45225911\n",
      "Iteration 21088, loss = 1.45225739\n",
      "Iteration 21089, loss = 1.45225567\n",
      "Iteration 21090, loss = 1.45225396\n",
      "Iteration 21091, loss = 1.45225224\n",
      "Iteration 21092, loss = 1.45225052\n",
      "Iteration 21093, loss = 1.45224881\n",
      "Iteration 21094, loss = 1.45224709\n",
      "Iteration 21095, loss = 1.45224538\n",
      "Iteration 21096, loss = 1.45224366\n",
      "Iteration 21097, loss = 1.45224195\n",
      "Iteration 21098, loss = 1.45224023\n",
      "Iteration 21099, loss = 1.45223852\n",
      "Iteration 21100, loss = 1.45223680\n",
      "Iteration 21101, loss = 1.45223509\n",
      "Iteration 21102, loss = 1.45223337\n",
      "Iteration 21103, loss = 1.45223166\n",
      "Iteration 21104, loss = 1.45222994\n",
      "Iteration 21105, loss = 1.45222823\n",
      "Iteration 21106, loss = 1.45222652\n",
      "Iteration 21107, loss = 1.45222480\n",
      "Iteration 21108, loss = 1.45222309\n",
      "Iteration 21109, loss = 1.45222138\n",
      "Iteration 21110, loss = 1.45221966\n",
      "Iteration 21111, loss = 1.45221795\n",
      "Iteration 21112, loss = 1.45221624\n",
      "Iteration 21113, loss = 1.45221453\n",
      "Iteration 21114, loss = 1.45221281\n",
      "Iteration 21115, loss = 1.45221110\n",
      "Iteration 21116, loss = 1.45220939\n",
      "Iteration 21117, loss = 1.45220768\n",
      "Iteration 21118, loss = 1.45220597\n",
      "Iteration 21119, loss = 1.45220426\n",
      "Iteration 21120, loss = 1.45220255\n",
      "Iteration 21121, loss = 1.45220084\n",
      "Iteration 21122, loss = 1.45219912\n",
      "Iteration 21123, loss = 1.45219741\n",
      "Iteration 21124, loss = 1.45219570\n",
      "Iteration 21125, loss = 1.45219399\n",
      "Iteration 21126, loss = 1.45219228\n",
      "Iteration 21127, loss = 1.45219057\n",
      "Iteration 21128, loss = 1.45218887\n",
      "Iteration 21129, loss = 1.45218716\n",
      "Iteration 21130, loss = 1.45218545\n",
      "Iteration 21131, loss = 1.45218374\n",
      "Iteration 21132, loss = 1.45218203\n",
      "Iteration 21133, loss = 1.45218032\n",
      "Iteration 21134, loss = 1.45217861\n",
      "Iteration 21135, loss = 1.45217690\n",
      "Iteration 21136, loss = 1.45217520\n",
      "Iteration 21137, loss = 1.45217349\n",
      "Iteration 21138, loss = 1.45217178\n",
      "Iteration 21139, loss = 1.45217007\n",
      "Iteration 21140, loss = 1.45216837\n",
      "Iteration 21141, loss = 1.45216666\n",
      "Iteration 21142, loss = 1.45216495\n",
      "Iteration 21143, loss = 1.45216325\n",
      "Iteration 21144, loss = 1.45216154\n",
      "Iteration 21145, loss = 1.45215983\n",
      "Iteration 21146, loss = 1.45215813\n",
      "Iteration 21147, loss = 1.45215642\n",
      "Iteration 21148, loss = 1.45215472\n",
      "Iteration 21149, loss = 1.45215301\n",
      "Iteration 21150, loss = 1.45215131\n",
      "Iteration 21151, loss = 1.45214960\n",
      "Iteration 21152, loss = 1.45214790\n",
      "Iteration 21153, loss = 1.45214619\n",
      "Iteration 21154, loss = 1.45214449\n",
      "Iteration 21155, loss = 1.45214278\n",
      "Iteration 21156, loss = 1.45214108\n",
      "Iteration 21157, loss = 1.45213938\n",
      "Iteration 21158, loss = 1.45213767\n",
      "Iteration 21159, loss = 1.45213597\n",
      "Iteration 21160, loss = 1.45213426\n",
      "Iteration 21161, loss = 1.45213256\n",
      "Iteration 21162, loss = 1.45213086\n",
      "Iteration 21163, loss = 1.45212916\n",
      "Iteration 21164, loss = 1.45212745\n",
      "Iteration 21165, loss = 1.45212575\n",
      "Iteration 21166, loss = 1.45212405\n",
      "Iteration 21167, loss = 1.45212235\n",
      "Iteration 21168, loss = 1.45212064\n",
      "Iteration 21169, loss = 1.45211894\n",
      "Iteration 21170, loss = 1.45211724\n",
      "Iteration 21171, loss = 1.45211554\n",
      "Iteration 21172, loss = 1.45211384\n",
      "Iteration 21173, loss = 1.45211214\n",
      "Iteration 21174, loss = 1.45211044\n",
      "Iteration 21175, loss = 1.45210874\n",
      "Iteration 21176, loss = 1.45210704\n",
      "Iteration 21177, loss = 1.45210534\n",
      "Iteration 21178, loss = 1.45210364\n",
      "Iteration 21179, loss = 1.45210194\n",
      "Iteration 21180, loss = 1.45210024\n",
      "Iteration 21181, loss = 1.45209854\n",
      "Iteration 21182, loss = 1.45209684\n",
      "Iteration 21183, loss = 1.45209514\n",
      "Iteration 21184, loss = 1.45209344\n",
      "Iteration 21185, loss = 1.45209174\n",
      "Iteration 21186, loss = 1.45209004\n",
      "Iteration 21187, loss = 1.45208835\n",
      "Iteration 21188, loss = 1.45208665\n",
      "Iteration 21189, loss = 1.45208495\n",
      "Iteration 21190, loss = 1.45208325\n",
      "Iteration 21191, loss = 1.45208155\n",
      "Iteration 21192, loss = 1.45207986\n",
      "Iteration 21193, loss = 1.45207816\n",
      "Iteration 21194, loss = 1.45207646\n",
      "Iteration 21195, loss = 1.45207477\n",
      "Iteration 21196, loss = 1.45207307\n",
      "Iteration 21197, loss = 1.45207137\n",
      "Iteration 21198, loss = 1.45206968\n",
      "Iteration 21199, loss = 1.45206798\n",
      "Iteration 21200, loss = 1.45206629\n",
      "Iteration 21201, loss = 1.45206459\n",
      "Iteration 21202, loss = 1.45206289\n",
      "Iteration 21203, loss = 1.45206120\n",
      "Iteration 21204, loss = 1.45205950\n",
      "Iteration 21205, loss = 1.45205781\n",
      "Iteration 21206, loss = 1.45205611\n",
      "Iteration 21207, loss = 1.45205442\n",
      "Iteration 21208, loss = 1.45205273\n",
      "Iteration 21209, loss = 1.45205103\n",
      "Iteration 21210, loss = 1.45204934\n",
      "Iteration 21211, loss = 1.45204764\n",
      "Iteration 21212, loss = 1.45204595\n",
      "Iteration 21213, loss = 1.45204426\n",
      "Iteration 21214, loss = 1.45204256\n",
      "Iteration 21215, loss = 1.45204087\n",
      "Iteration 21216, loss = 1.45203918\n",
      "Iteration 21217, loss = 1.45203749\n",
      "Iteration 21218, loss = 1.45203579\n",
      "Iteration 21219, loss = 1.45203410\n",
      "Iteration 21220, loss = 1.45203241\n",
      "Iteration 21221, loss = 1.45203072\n",
      "Iteration 21222, loss = 1.45202903\n",
      "Iteration 21223, loss = 1.45202733\n",
      "Iteration 21224, loss = 1.45202564\n",
      "Iteration 21225, loss = 1.45202395\n",
      "Iteration 21226, loss = 1.45202226\n",
      "Iteration 21227, loss = 1.45202057\n",
      "Iteration 21228, loss = 1.45201888\n",
      "Iteration 21229, loss = 1.45201719\n",
      "Iteration 21230, loss = 1.45201550\n",
      "Iteration 21231, loss = 1.45201381\n",
      "Iteration 21232, loss = 1.45201212\n",
      "Iteration 21233, loss = 1.45201043\n",
      "Iteration 21234, loss = 1.45200874\n",
      "Iteration 21235, loss = 1.45200705\n",
      "Iteration 21236, loss = 1.45200536\n",
      "Iteration 21237, loss = 1.45200367\n",
      "Iteration 21238, loss = 1.45200198\n",
      "Iteration 21239, loss = 1.45200030\n",
      "Iteration 21240, loss = 1.45199861\n",
      "Iteration 21241, loss = 1.45199692\n",
      "Iteration 21242, loss = 1.45199523\n",
      "Iteration 21243, loss = 1.45199354\n",
      "Iteration 21244, loss = 1.45199186\n",
      "Iteration 21245, loss = 1.45199017\n",
      "Iteration 21246, loss = 1.45198848\n",
      "Iteration 21247, loss = 1.45198679\n",
      "Iteration 21248, loss = 1.45198511\n",
      "Iteration 21249, loss = 1.45198342\n",
      "Iteration 21250, loss = 1.45198173\n",
      "Iteration 21251, loss = 1.45198005\n",
      "Iteration 21252, loss = 1.45197836\n",
      "Iteration 21253, loss = 1.45197668\n",
      "Iteration 21254, loss = 1.45197499\n",
      "Iteration 21255, loss = 1.45197330\n",
      "Iteration 21256, loss = 1.45197162\n",
      "Iteration 21257, loss = 1.45196993\n",
      "Iteration 21258, loss = 1.45196825\n",
      "Iteration 21259, loss = 1.45196656\n",
      "Iteration 21260, loss = 1.45196488\n",
      "Iteration 21261, loss = 1.45196320\n",
      "Iteration 21262, loss = 1.45196151\n",
      "Iteration 21263, loss = 1.45195983\n",
      "Iteration 21264, loss = 1.45195814\n",
      "Iteration 21265, loss = 1.45195646\n",
      "Iteration 21266, loss = 1.45195478\n",
      "Iteration 21267, loss = 1.45195309\n",
      "Iteration 21268, loss = 1.45195141\n",
      "Iteration 21269, loss = 1.45194973\n",
      "Iteration 21270, loss = 1.45194804\n",
      "Iteration 21271, loss = 1.45194636\n",
      "Iteration 21272, loss = 1.45194468\n",
      "Iteration 21273, loss = 1.45194300\n",
      "Iteration 21274, loss = 1.45194132\n",
      "Iteration 21275, loss = 1.45193963\n",
      "Iteration 21276, loss = 1.45193795\n",
      "Iteration 21277, loss = 1.45193627\n",
      "Iteration 21278, loss = 1.45193459\n",
      "Iteration 21279, loss = 1.45193291\n",
      "Iteration 21280, loss = 1.45193123\n",
      "Iteration 21281, loss = 1.45192955\n",
      "Iteration 21282, loss = 1.45192787\n",
      "Iteration 21283, loss = 1.45192619\n",
      "Iteration 21284, loss = 1.45192451\n",
      "Iteration 21285, loss = 1.45192283\n",
      "Iteration 21286, loss = 1.45192115\n",
      "Iteration 21287, loss = 1.45191947\n",
      "Iteration 21288, loss = 1.45191779\n",
      "Iteration 21289, loss = 1.45191611\n",
      "Iteration 21290, loss = 1.45191443\n",
      "Iteration 21291, loss = 1.45191275\n",
      "Iteration 21292, loss = 1.45191107\n",
      "Iteration 21293, loss = 1.45190939\n",
      "Iteration 21294, loss = 1.45190772\n",
      "Iteration 21295, loss = 1.45190604\n",
      "Iteration 21296, loss = 1.45190436\n",
      "Iteration 21297, loss = 1.45190268\n",
      "Iteration 21298, loss = 1.45190100\n",
      "Iteration 21299, loss = 1.45189933\n",
      "Iteration 21300, loss = 1.45189765\n",
      "Iteration 21301, loss = 1.45189597\n",
      "Iteration 21302, loss = 1.45189430\n",
      "Iteration 21303, loss = 1.45189262\n",
      "Iteration 21304, loss = 1.45189094\n",
      "Iteration 21305, loss = 1.45188927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21306, loss = 1.45188759\n",
      "Iteration 21307, loss = 1.45188591\n",
      "Iteration 21308, loss = 1.45188424\n",
      "Iteration 21309, loss = 1.45188256\n",
      "Iteration 21310, loss = 1.45188089\n",
      "Iteration 21311, loss = 1.45187921\n",
      "Iteration 21312, loss = 1.45187754\n",
      "Iteration 21313, loss = 1.45187586\n",
      "Iteration 21314, loss = 1.45187419\n",
      "Iteration 21315, loss = 1.45187251\n",
      "Iteration 21316, loss = 1.45187084\n",
      "Iteration 21317, loss = 1.45186917\n",
      "Iteration 21318, loss = 1.45186749\n",
      "Iteration 21319, loss = 1.45186582\n",
      "Iteration 21320, loss = 1.45186415\n",
      "Iteration 21321, loss = 1.45186247\n",
      "Iteration 21322, loss = 1.45186080\n",
      "Iteration 21323, loss = 1.45185913\n",
      "Iteration 21324, loss = 1.45185745\n",
      "Iteration 21325, loss = 1.45185578\n",
      "Iteration 21326, loss = 1.45185411\n",
      "Iteration 21327, loss = 1.45185244\n",
      "Iteration 21328, loss = 1.45185076\n",
      "Iteration 21329, loss = 1.45184909\n",
      "Iteration 21330, loss = 1.45184742\n",
      "Iteration 21331, loss = 1.45184575\n",
      "Iteration 21332, loss = 1.45184408\n",
      "Iteration 21333, loss = 1.45184241\n",
      "Iteration 21334, loss = 1.45184074\n",
      "Iteration 21335, loss = 1.45183907\n",
      "Iteration 21336, loss = 1.45183740\n",
      "Iteration 21337, loss = 1.45183573\n",
      "Iteration 21338, loss = 1.45183406\n",
      "Iteration 21339, loss = 1.45183239\n",
      "Iteration 21340, loss = 1.45183072\n",
      "Iteration 21341, loss = 1.45182905\n",
      "Iteration 21342, loss = 1.45182738\n",
      "Iteration 21343, loss = 1.45182571\n",
      "Iteration 21344, loss = 1.45182404\n",
      "Iteration 21345, loss = 1.45182237\n",
      "Iteration 21346, loss = 1.45182070\n",
      "Iteration 21347, loss = 1.45181903\n",
      "Iteration 21348, loss = 1.45181736\n",
      "Iteration 21349, loss = 1.45181570\n",
      "Iteration 21350, loss = 1.45181403\n",
      "Iteration 21351, loss = 1.45181236\n",
      "Iteration 21352, loss = 1.45181069\n",
      "Iteration 21353, loss = 1.45180903\n",
      "Iteration 21354, loss = 1.45180736\n",
      "Iteration 21355, loss = 1.45180569\n",
      "Iteration 21356, loss = 1.45180402\n",
      "Iteration 21357, loss = 1.45180236\n",
      "Iteration 21358, loss = 1.45180069\n",
      "Iteration 21359, loss = 1.45179902\n",
      "Iteration 21360, loss = 1.45179736\n",
      "Iteration 21361, loss = 1.45179569\n",
      "Iteration 21362, loss = 1.45179403\n",
      "Iteration 21363, loss = 1.45179236\n",
      "Iteration 21364, loss = 1.45179070\n",
      "Iteration 21365, loss = 1.45178903\n",
      "Iteration 21366, loss = 1.45178737\n",
      "Iteration 21367, loss = 1.45178570\n",
      "Iteration 21368, loss = 1.45178404\n",
      "Iteration 21369, loss = 1.45178237\n",
      "Iteration 21370, loss = 1.45178071\n",
      "Iteration 21371, loss = 1.45177904\n",
      "Iteration 21372, loss = 1.45177738\n",
      "Iteration 21373, loss = 1.45177572\n",
      "Iteration 21374, loss = 1.45177405\n",
      "Iteration 21375, loss = 1.45177239\n",
      "Iteration 21376, loss = 1.45177073\n",
      "Iteration 21377, loss = 1.45176906\n",
      "Iteration 21378, loss = 1.45176740\n",
      "Iteration 21379, loss = 1.45176574\n",
      "Iteration 21380, loss = 1.45176408\n",
      "Iteration 21381, loss = 1.45176241\n",
      "Iteration 21382, loss = 1.45176075\n",
      "Iteration 21383, loss = 1.45175909\n",
      "Iteration 21384, loss = 1.45175743\n",
      "Iteration 21385, loss = 1.45175577\n",
      "Iteration 21386, loss = 1.45175410\n",
      "Iteration 21387, loss = 1.45175244\n",
      "Iteration 21388, loss = 1.45175078\n",
      "Iteration 21389, loss = 1.45174912\n",
      "Iteration 21390, loss = 1.45174746\n",
      "Iteration 21391, loss = 1.45174580\n",
      "Iteration 21392, loss = 1.45174414\n",
      "Iteration 21393, loss = 1.45174248\n",
      "Iteration 21394, loss = 1.45174082\n",
      "Iteration 21395, loss = 1.45173916\n",
      "Iteration 21396, loss = 1.45173750\n",
      "Iteration 21397, loss = 1.45173584\n",
      "Iteration 21398, loss = 1.45173418\n",
      "Iteration 21399, loss = 1.45173252\n",
      "Iteration 21400, loss = 1.45173087\n",
      "Iteration 21401, loss = 1.45172921\n",
      "Iteration 21402, loss = 1.45172755\n",
      "Iteration 21403, loss = 1.45172589\n",
      "Iteration 21404, loss = 1.45172423\n",
      "Iteration 21405, loss = 1.45172257\n",
      "Iteration 21406, loss = 1.45172092\n",
      "Iteration 21407, loss = 1.45171926\n",
      "Iteration 21408, loss = 1.45171760\n",
      "Iteration 21409, loss = 1.45171594\n",
      "Iteration 21410, loss = 1.45171429\n",
      "Iteration 21411, loss = 1.45171263\n",
      "Iteration 21412, loss = 1.45171097\n",
      "Iteration 21413, loss = 1.45170932\n",
      "Iteration 21414, loss = 1.45170766\n",
      "Iteration 21415, loss = 1.45170601\n",
      "Iteration 21416, loss = 1.45170435\n",
      "Iteration 21417, loss = 1.45170269\n",
      "Iteration 21418, loss = 1.45170104\n",
      "Iteration 21419, loss = 1.45169938\n",
      "Iteration 21420, loss = 1.45169773\n",
      "Iteration 21421, loss = 1.45169607\n",
      "Iteration 21422, loss = 1.45169442\n",
      "Iteration 21423, loss = 1.45169276\n",
      "Iteration 21424, loss = 1.45169111\n",
      "Iteration 21425, loss = 1.45168946\n",
      "Iteration 21426, loss = 1.45168780\n",
      "Iteration 21427, loss = 1.45168615\n",
      "Iteration 21428, loss = 1.45168449\n",
      "Iteration 21429, loss = 1.45168284\n",
      "Iteration 21430, loss = 1.45168119\n",
      "Iteration 21431, loss = 1.45167953\n",
      "Iteration 21432, loss = 1.45167788\n",
      "Iteration 21433, loss = 1.45167623\n",
      "Iteration 21434, loss = 1.45167458\n",
      "Iteration 21435, loss = 1.45167292\n",
      "Iteration 21436, loss = 1.45167127\n",
      "Iteration 21437, loss = 1.45166962\n",
      "Iteration 21438, loss = 1.45166797\n",
      "Iteration 21439, loss = 1.45166632\n",
      "Iteration 21440, loss = 1.45166466\n",
      "Iteration 21441, loss = 1.45166301\n",
      "Iteration 21442, loss = 1.45166136\n",
      "Iteration 21443, loss = 1.45165971\n",
      "Iteration 21444, loss = 1.45165806\n",
      "Iteration 21445, loss = 1.45165641\n",
      "Iteration 21446, loss = 1.45165476\n",
      "Iteration 21447, loss = 1.45165311\n",
      "Iteration 21448, loss = 1.45165146\n",
      "Iteration 21449, loss = 1.45164981\n",
      "Iteration 21450, loss = 1.45164816\n",
      "Iteration 21451, loss = 1.45164651\n",
      "Iteration 21452, loss = 1.45164486\n",
      "Iteration 21453, loss = 1.45164321\n",
      "Iteration 21454, loss = 1.45164156\n",
      "Iteration 21455, loss = 1.45163991\n",
      "Iteration 21456, loss = 1.45163827\n",
      "Iteration 21457, loss = 1.45163662\n",
      "Iteration 21458, loss = 1.45163497\n",
      "Iteration 21459, loss = 1.45163332\n",
      "Iteration 21460, loss = 1.45163167\n",
      "Iteration 21461, loss = 1.45163003\n",
      "Iteration 21462, loss = 1.45162838\n",
      "Iteration 21463, loss = 1.45162673\n",
      "Iteration 21464, loss = 1.45162508\n",
      "Iteration 21465, loss = 1.45162344\n",
      "Iteration 21466, loss = 1.45162179\n",
      "Iteration 21467, loss = 1.45162014\n",
      "Iteration 21468, loss = 1.45161850\n",
      "Iteration 21469, loss = 1.45161685\n",
      "Iteration 21470, loss = 1.45161520\n",
      "Iteration 21471, loss = 1.45161356\n",
      "Iteration 21472, loss = 1.45161191\n",
      "Iteration 21473, loss = 1.45161027\n",
      "Iteration 21474, loss = 1.45160862\n",
      "Iteration 21475, loss = 1.45160698\n",
      "Iteration 21476, loss = 1.45160533\n",
      "Iteration 21477, loss = 1.45160369\n",
      "Iteration 21478, loss = 1.45160204\n",
      "Iteration 21479, loss = 1.45160040\n",
      "Iteration 21480, loss = 1.45159875\n",
      "Iteration 21481, loss = 1.45159711\n",
      "Iteration 21482, loss = 1.45159547\n",
      "Iteration 21483, loss = 1.45159382\n",
      "Iteration 21484, loss = 1.45159218\n",
      "Iteration 21485, loss = 1.45159053\n",
      "Iteration 21486, loss = 1.45158889\n",
      "Iteration 21487, loss = 1.45158725\n",
      "Iteration 21488, loss = 1.45158561\n",
      "Iteration 21489, loss = 1.45158396\n",
      "Iteration 21490, loss = 1.45158232\n",
      "Iteration 21491, loss = 1.45158068\n",
      "Iteration 21492, loss = 1.45157904\n",
      "Iteration 21493, loss = 1.45157739\n",
      "Iteration 21494, loss = 1.45157575\n",
      "Iteration 21495, loss = 1.45157411\n",
      "Iteration 21496, loss = 1.45157247\n",
      "Iteration 21497, loss = 1.45157083\n",
      "Iteration 21498, loss = 1.45156919\n",
      "Iteration 21499, loss = 1.45156755\n",
      "Iteration 21500, loss = 1.45156591\n",
      "Iteration 21501, loss = 1.45156427\n",
      "Iteration 21502, loss = 1.45156263\n",
      "Iteration 21503, loss = 1.45156099\n",
      "Iteration 21504, loss = 1.45155935\n",
      "Iteration 21505, loss = 1.45155771\n",
      "Iteration 21506, loss = 1.45155607\n",
      "Iteration 21507, loss = 1.45155443\n",
      "Iteration 21508, loss = 1.45155279\n",
      "Iteration 21509, loss = 1.45155115\n",
      "Iteration 21510, loss = 1.45154951\n",
      "Iteration 21511, loss = 1.45154787\n",
      "Iteration 21512, loss = 1.45154623\n",
      "Iteration 21513, loss = 1.45154459\n",
      "Iteration 21514, loss = 1.45154296\n",
      "Iteration 21515, loss = 1.45154132\n",
      "Iteration 21516, loss = 1.45153968\n",
      "Iteration 21517, loss = 1.45153804\n",
      "Iteration 21518, loss = 1.45153641\n",
      "Iteration 21519, loss = 1.45153477\n",
      "Iteration 21520, loss = 1.45153313\n",
      "Iteration 21521, loss = 1.45153149\n",
      "Iteration 21522, loss = 1.45152986\n",
      "Iteration 21523, loss = 1.45152822\n",
      "Iteration 21524, loss = 1.45152658\n",
      "Iteration 21525, loss = 1.45152495\n",
      "Iteration 21526, loss = 1.45152331\n",
      "Iteration 21527, loss = 1.45152168\n",
      "Iteration 21528, loss = 1.45152004\n",
      "Iteration 21529, loss = 1.45151841\n",
      "Iteration 21530, loss = 1.45151677\n",
      "Iteration 21531, loss = 1.45151514\n",
      "Iteration 21532, loss = 1.45151350\n",
      "Iteration 21533, loss = 1.45151187\n",
      "Iteration 21534, loss = 1.45151023\n",
      "Iteration 21535, loss = 1.45150860\n",
      "Iteration 21536, loss = 1.45150696\n",
      "Iteration 21537, loss = 1.45150533\n",
      "Iteration 21538, loss = 1.45150369\n",
      "Iteration 21539, loss = 1.45150206\n",
      "Iteration 21540, loss = 1.45150043\n",
      "Iteration 21541, loss = 1.45149879\n",
      "Iteration 21542, loss = 1.45149716\n",
      "Iteration 21543, loss = 1.45149553\n",
      "Iteration 21544, loss = 1.45149390\n",
      "Iteration 21545, loss = 1.45149226\n",
      "Iteration 21546, loss = 1.45149063\n",
      "Iteration 21547, loss = 1.45148900\n",
      "Iteration 21548, loss = 1.45148737\n",
      "Iteration 21549, loss = 1.45148573\n",
      "Iteration 21550, loss = 1.45148410\n",
      "Iteration 21551, loss = 1.45148247\n",
      "Iteration 21552, loss = 1.45148084\n",
      "Iteration 21553, loss = 1.45147921\n",
      "Iteration 21554, loss = 1.45147758\n",
      "Iteration 21555, loss = 1.45147595\n",
      "Iteration 21556, loss = 1.45147432\n",
      "Iteration 21557, loss = 1.45147269\n",
      "Iteration 21558, loss = 1.45147106\n",
      "Iteration 21559, loss = 1.45146943\n",
      "Iteration 21560, loss = 1.45146780\n",
      "Iteration 21561, loss = 1.45146617\n",
      "Iteration 21562, loss = 1.45146454\n",
      "Iteration 21563, loss = 1.45146291\n",
      "Iteration 21564, loss = 1.45146128\n",
      "Iteration 21565, loss = 1.45145965\n",
      "Iteration 21566, loss = 1.45145802\n",
      "Iteration 21567, loss = 1.45145639\n",
      "Iteration 21568, loss = 1.45145476\n",
      "Iteration 21569, loss = 1.45145313\n",
      "Iteration 21570, loss = 1.45145151\n",
      "Iteration 21571, loss = 1.45144988\n",
      "Iteration 21572, loss = 1.45144825\n",
      "Iteration 21573, loss = 1.45144662\n",
      "Iteration 21574, loss = 1.45144500\n",
      "Iteration 21575, loss = 1.45144337\n",
      "Iteration 21576, loss = 1.45144174\n",
      "Iteration 21577, loss = 1.45144011\n",
      "Iteration 21578, loss = 1.45143849\n",
      "Iteration 21579, loss = 1.45143686\n",
      "Iteration 21580, loss = 1.45143523\n",
      "Iteration 21581, loss = 1.45143361\n",
      "Iteration 21582, loss = 1.45143198\n",
      "Iteration 21583, loss = 1.45143036\n",
      "Iteration 21584, loss = 1.45142873\n",
      "Iteration 21585, loss = 1.45142711\n",
      "Iteration 21586, loss = 1.45142548\n",
      "Iteration 21587, loss = 1.45142386\n",
      "Iteration 21588, loss = 1.45142223\n",
      "Iteration 21589, loss = 1.45142061\n",
      "Iteration 21590, loss = 1.45141898\n",
      "Iteration 21591, loss = 1.45141736\n",
      "Iteration 21592, loss = 1.45141573\n",
      "Iteration 21593, loss = 1.45141411\n",
      "Iteration 21594, loss = 1.45141248\n",
      "Iteration 21595, loss = 1.45141086\n",
      "Iteration 21596, loss = 1.45140924\n",
      "Iteration 21597, loss = 1.45140761\n",
      "Iteration 21598, loss = 1.45140599\n",
      "Iteration 21599, loss = 1.45140437\n",
      "Iteration 21600, loss = 1.45140275\n",
      "Iteration 21601, loss = 1.45140112\n",
      "Iteration 21602, loss = 1.45139950\n",
      "Iteration 21603, loss = 1.45139788\n",
      "Iteration 21604, loss = 1.45139626\n",
      "Iteration 21605, loss = 1.45139463\n",
      "Iteration 21606, loss = 1.45139301\n",
      "Iteration 21607, loss = 1.45139139\n",
      "Iteration 21608, loss = 1.45138977\n",
      "Iteration 21609, loss = 1.45138815\n",
      "Iteration 21610, loss = 1.45138653\n",
      "Iteration 21611, loss = 1.45138491\n",
      "Iteration 21612, loss = 1.45138329\n",
      "Iteration 21613, loss = 1.45138167\n",
      "Iteration 21614, loss = 1.45138004\n",
      "Iteration 21615, loss = 1.45137842\n",
      "Iteration 21616, loss = 1.45137680\n",
      "Iteration 21617, loss = 1.45137518\n",
      "Iteration 21618, loss = 1.45137357\n",
      "Iteration 21619, loss = 1.45137195\n",
      "Iteration 21620, loss = 1.45137033\n",
      "Iteration 21621, loss = 1.45136871\n",
      "Iteration 21622, loss = 1.45136709\n",
      "Iteration 21623, loss = 1.45136547\n",
      "Iteration 21624, loss = 1.45136385\n",
      "Iteration 21625, loss = 1.45136223\n",
      "Iteration 21626, loss = 1.45136061\n",
      "Iteration 21627, loss = 1.45135900\n",
      "Iteration 21628, loss = 1.45135738\n",
      "Iteration 21629, loss = 1.45135576\n",
      "Iteration 21630, loss = 1.45135414\n",
      "Iteration 21631, loss = 1.45135253\n",
      "Iteration 21632, loss = 1.45135091\n",
      "Iteration 21633, loss = 1.45134929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21634, loss = 1.45134768\n",
      "Iteration 21635, loss = 1.45134606\n",
      "Iteration 21636, loss = 1.45134444\n",
      "Iteration 21637, loss = 1.45134283\n",
      "Iteration 21638, loss = 1.45134121\n",
      "Iteration 21639, loss = 1.45133959\n",
      "Iteration 21640, loss = 1.45133798\n",
      "Iteration 21641, loss = 1.45133636\n",
      "Iteration 21642, loss = 1.45133475\n",
      "Iteration 21643, loss = 1.45133313\n",
      "Iteration 21644, loss = 1.45133152\n",
      "Iteration 21645, loss = 1.45132990\n",
      "Iteration 21646, loss = 1.45132829\n",
      "Iteration 21647, loss = 1.45132667\n",
      "Iteration 21648, loss = 1.45132506\n",
      "Iteration 21649, loss = 1.45132345\n",
      "Iteration 21650, loss = 1.45132183\n",
      "Iteration 21651, loss = 1.45132022\n",
      "Iteration 21652, loss = 1.45131860\n",
      "Iteration 21653, loss = 1.45131699\n",
      "Iteration 21654, loss = 1.45131538\n",
      "Iteration 21655, loss = 1.45131376\n",
      "Iteration 21656, loss = 1.45131215\n",
      "Iteration 21657, loss = 1.45131054\n",
      "Iteration 21658, loss = 1.45130893\n",
      "Iteration 21659, loss = 1.45130731\n",
      "Iteration 21660, loss = 1.45130570\n",
      "Iteration 21661, loss = 1.45130409\n",
      "Iteration 21662, loss = 1.45130248\n",
      "Iteration 21663, loss = 1.45130087\n",
      "Iteration 21664, loss = 1.45129925\n",
      "Iteration 21665, loss = 1.45129764\n",
      "Iteration 21666, loss = 1.45129603\n",
      "Iteration 21667, loss = 1.45129442\n",
      "Iteration 21668, loss = 1.45129281\n",
      "Iteration 21669, loss = 1.45129120\n",
      "Iteration 21670, loss = 1.45128959\n",
      "Iteration 21671, loss = 1.45128798\n",
      "Iteration 21672, loss = 1.45128637\n",
      "Iteration 21673, loss = 1.45128476\n",
      "Iteration 21674, loss = 1.45128315\n",
      "Iteration 21675, loss = 1.45128154\n",
      "Iteration 21676, loss = 1.45127993\n",
      "Iteration 21677, loss = 1.45127832\n",
      "Iteration 21678, loss = 1.45127671\n",
      "Iteration 21679, loss = 1.45127510\n",
      "Iteration 21680, loss = 1.45127349\n",
      "Iteration 21681, loss = 1.45127189\n",
      "Iteration 21682, loss = 1.45127028\n",
      "Iteration 21683, loss = 1.45126867\n",
      "Iteration 21684, loss = 1.45126706\n",
      "Iteration 21685, loss = 1.45126545\n",
      "Iteration 21686, loss = 1.45126385\n",
      "Iteration 21687, loss = 1.45126224\n",
      "Iteration 21688, loss = 1.45126063\n",
      "Iteration 21689, loss = 1.45125902\n",
      "Iteration 21690, loss = 1.45125742\n",
      "Iteration 21691, loss = 1.45125581\n",
      "Iteration 21692, loss = 1.45125420\n",
      "Iteration 21693, loss = 1.45125260\n",
      "Iteration 21694, loss = 1.45125099\n",
      "Iteration 21695, loss = 1.45124938\n",
      "Iteration 21696, loss = 1.45124778\n",
      "Iteration 21697, loss = 1.45124617\n",
      "Iteration 21698, loss = 1.45124457\n",
      "Iteration 21699, loss = 1.45124296\n",
      "Iteration 21700, loss = 1.45124136\n",
      "Iteration 21701, loss = 1.45123975\n",
      "Iteration 21702, loss = 1.45123815\n",
      "Iteration 21703, loss = 1.45123654\n",
      "Iteration 21704, loss = 1.45123494\n",
      "Iteration 21705, loss = 1.45123333\n",
      "Iteration 21706, loss = 1.45123173\n",
      "Iteration 21707, loss = 1.45123013\n",
      "Iteration 21708, loss = 1.45122852\n",
      "Iteration 21709, loss = 1.45122692\n",
      "Iteration 21710, loss = 1.45122531\n",
      "Iteration 21711, loss = 1.45122371\n",
      "Iteration 21712, loss = 1.45122211\n",
      "Iteration 21713, loss = 1.45122051\n",
      "Iteration 21714, loss = 1.45121890\n",
      "Iteration 21715, loss = 1.45121730\n",
      "Iteration 21716, loss = 1.45121570\n",
      "Iteration 21717, loss = 1.45121410\n",
      "Iteration 21718, loss = 1.45121249\n",
      "Iteration 21719, loss = 1.45121089\n",
      "Iteration 21720, loss = 1.45120929\n",
      "Iteration 21721, loss = 1.45120769\n",
      "Iteration 21722, loss = 1.45120609\n",
      "Iteration 21723, loss = 1.45120449\n",
      "Iteration 21724, loss = 1.45120288\n",
      "Iteration 21725, loss = 1.45120128\n",
      "Iteration 21726, loss = 1.45119968\n",
      "Iteration 21727, loss = 1.45119808\n",
      "Iteration 21728, loss = 1.45119648\n",
      "Iteration 21729, loss = 1.45119488\n",
      "Iteration 21730, loss = 1.45119328\n",
      "Iteration 21731, loss = 1.45119168\n",
      "Iteration 21732, loss = 1.45119008\n",
      "Iteration 21733, loss = 1.45118848\n",
      "Iteration 21734, loss = 1.45118688\n",
      "Iteration 21735, loss = 1.45118528\n",
      "Iteration 21736, loss = 1.45118369\n",
      "Iteration 21737, loss = 1.45118209\n",
      "Iteration 21738, loss = 1.45118049\n",
      "Iteration 21739, loss = 1.45117889\n",
      "Iteration 21740, loss = 1.45117729\n",
      "Iteration 21741, loss = 1.45117569\n",
      "Iteration 21742, loss = 1.45117410\n",
      "Iteration 21743, loss = 1.45117250\n",
      "Iteration 21744, loss = 1.45117090\n",
      "Iteration 21745, loss = 1.45116930\n",
      "Iteration 21746, loss = 1.45116771\n",
      "Iteration 21747, loss = 1.45116611\n",
      "Iteration 21748, loss = 1.45116451\n",
      "Iteration 21749, loss = 1.45116292\n",
      "Iteration 21750, loss = 1.45116132\n",
      "Iteration 21751, loss = 1.45115972\n",
      "Iteration 21752, loss = 1.45115813\n",
      "Iteration 21753, loss = 1.45115653\n",
      "Iteration 21754, loss = 1.45115493\n",
      "Iteration 21755, loss = 1.45115334\n",
      "Iteration 21756, loss = 1.45115174\n",
      "Iteration 21757, loss = 1.45115015\n",
      "Iteration 21758, loss = 1.45114855\n",
      "Iteration 21759, loss = 1.45114696\n",
      "Iteration 21760, loss = 1.45114536\n",
      "Iteration 21761, loss = 1.45114377\n",
      "Iteration 21762, loss = 1.45114217\n",
      "Iteration 21763, loss = 1.45114058\n",
      "Iteration 21764, loss = 1.45113899\n",
      "Iteration 21765, loss = 1.45113739\n",
      "Iteration 21766, loss = 1.45113580\n",
      "Iteration 21767, loss = 1.45113421\n",
      "Iteration 21768, loss = 1.45113261\n",
      "Iteration 21769, loss = 1.45113102\n",
      "Iteration 21770, loss = 1.45112943\n",
      "Iteration 21771, loss = 1.45112783\n",
      "Iteration 21772, loss = 1.45112624\n",
      "Iteration 21773, loss = 1.45112465\n",
      "Iteration 21774, loss = 1.45112306\n",
      "Iteration 21775, loss = 1.45112146\n",
      "Iteration 21776, loss = 1.45111987\n",
      "Iteration 21777, loss = 1.45111828\n",
      "Iteration 21778, loss = 1.45111669\n",
      "Iteration 21779, loss = 1.45111510\n",
      "Iteration 21780, loss = 1.45111350\n",
      "Iteration 21781, loss = 1.45111191\n",
      "Iteration 21782, loss = 1.45111032\n",
      "Iteration 21783, loss = 1.45110873\n",
      "Iteration 21784, loss = 1.45110714\n",
      "Iteration 21785, loss = 1.45110555\n",
      "Iteration 21786, loss = 1.45110396\n",
      "Iteration 21787, loss = 1.45110237\n",
      "Iteration 21788, loss = 1.45110078\n",
      "Iteration 21789, loss = 1.45109919\n",
      "Iteration 21790, loss = 1.45109760\n",
      "Iteration 21791, loss = 1.45109601\n",
      "Iteration 21792, loss = 1.45109442\n",
      "Iteration 21793, loss = 1.45109283\n",
      "Iteration 21794, loss = 1.45109124\n",
      "Iteration 21795, loss = 1.45108966\n",
      "Iteration 21796, loss = 1.45108807\n",
      "Iteration 21797, loss = 1.45108648\n",
      "Iteration 21798, loss = 1.45108489\n",
      "Iteration 21799, loss = 1.45108330\n",
      "Iteration 21800, loss = 1.45108172\n",
      "Iteration 21801, loss = 1.45108013\n",
      "Iteration 21802, loss = 1.45107854\n",
      "Iteration 21803, loss = 1.45107695\n",
      "Iteration 21804, loss = 1.45107537\n",
      "Iteration 21805, loss = 1.45107378\n",
      "Iteration 21806, loss = 1.45107219\n",
      "Iteration 21807, loss = 1.45107060\n",
      "Iteration 21808, loss = 1.45106902\n",
      "Iteration 21809, loss = 1.45106743\n",
      "Iteration 21810, loss = 1.45106585\n",
      "Iteration 21811, loss = 1.45106426\n",
      "Iteration 21812, loss = 1.45106267\n",
      "Iteration 21813, loss = 1.45106109\n",
      "Iteration 21814, loss = 1.45105950\n",
      "Iteration 21815, loss = 1.45105792\n",
      "Iteration 21816, loss = 1.45105633\n",
      "Iteration 21817, loss = 1.45105475\n",
      "Iteration 21818, loss = 1.45105316\n",
      "Iteration 21819, loss = 1.45105158\n",
      "Iteration 21820, loss = 1.45104999\n",
      "Iteration 21821, loss = 1.45104841\n",
      "Iteration 21822, loss = 1.45104683\n",
      "Iteration 21823, loss = 1.45104524\n",
      "Iteration 21824, loss = 1.45104366\n",
      "Iteration 21825, loss = 1.45104207\n",
      "Iteration 21826, loss = 1.45104049\n",
      "Iteration 21827, loss = 1.45103891\n",
      "Iteration 21828, loss = 1.45103732\n",
      "Iteration 21829, loss = 1.45103574\n",
      "Iteration 21830, loss = 1.45103416\n",
      "Iteration 21831, loss = 1.45103258\n",
      "Iteration 21832, loss = 1.45103099\n",
      "Iteration 21833, loss = 1.45102941\n",
      "Iteration 21834, loss = 1.45102783\n",
      "Iteration 21835, loss = 1.45102625\n",
      "Iteration 21836, loss = 1.45102467\n",
      "Iteration 21837, loss = 1.45102309\n",
      "Iteration 21838, loss = 1.45102150\n",
      "Iteration 21839, loss = 1.45101992\n",
      "Iteration 21840, loss = 1.45101834\n",
      "Iteration 21841, loss = 1.45101676\n",
      "Iteration 21842, loss = 1.45101518\n",
      "Iteration 21843, loss = 1.45101360\n",
      "Iteration 21844, loss = 1.45101202\n",
      "Iteration 21845, loss = 1.45101044\n",
      "Iteration 21846, loss = 1.45100886\n",
      "Iteration 21847, loss = 1.45100728\n",
      "Iteration 21848, loss = 1.45100570\n",
      "Iteration 21849, loss = 1.45100412\n",
      "Iteration 21850, loss = 1.45100254\n",
      "Iteration 21851, loss = 1.45100096\n",
      "Iteration 21852, loss = 1.45099938\n",
      "Iteration 21853, loss = 1.45099780\n",
      "Iteration 21854, loss = 1.45099623\n",
      "Iteration 21855, loss = 1.45099465\n",
      "Iteration 21856, loss = 1.45099307\n",
      "Iteration 21857, loss = 1.45099149\n",
      "Iteration 21858, loss = 1.45098991\n",
      "Iteration 21859, loss = 1.45098834\n",
      "Iteration 21860, loss = 1.45098676\n",
      "Iteration 21861, loss = 1.45098518\n",
      "Iteration 21862, loss = 1.45098360\n",
      "Iteration 21863, loss = 1.45098203\n",
      "Iteration 21864, loss = 1.45098045\n",
      "Iteration 21865, loss = 1.45097887\n",
      "Iteration 21866, loss = 1.45097730\n",
      "Iteration 21867, loss = 1.45097572\n",
      "Iteration 21868, loss = 1.45097414\n",
      "Iteration 21869, loss = 1.45097257\n",
      "Iteration 21870, loss = 1.45097099\n",
      "Iteration 21871, loss = 1.45096942\n",
      "Iteration 21872, loss = 1.45096784\n",
      "Iteration 21873, loss = 1.45096627\n",
      "Iteration 21874, loss = 1.45096469\n",
      "Iteration 21875, loss = 1.45096312\n",
      "Iteration 21876, loss = 1.45096154\n",
      "Iteration 21877, loss = 1.45095997\n",
      "Iteration 21878, loss = 1.45095839\n",
      "Iteration 21879, loss = 1.45095682\n",
      "Iteration 21880, loss = 1.45095524\n",
      "Iteration 21881, loss = 1.45095367\n",
      "Iteration 21882, loss = 1.45095209\n",
      "Iteration 21883, loss = 1.45095052\n",
      "Iteration 21884, loss = 1.45094895\n",
      "Iteration 21885, loss = 1.45094737\n",
      "Iteration 21886, loss = 1.45094580\n",
      "Iteration 21887, loss = 1.45094423\n",
      "Iteration 21888, loss = 1.45094266\n",
      "Iteration 21889, loss = 1.45094108\n",
      "Iteration 21890, loss = 1.45093951\n",
      "Iteration 21891, loss = 1.45093794\n",
      "Iteration 21892, loss = 1.45093637\n",
      "Iteration 21893, loss = 1.45093479\n",
      "Iteration 21894, loss = 1.45093322\n",
      "Iteration 21895, loss = 1.45093165\n",
      "Iteration 21896, loss = 1.45093008\n",
      "Iteration 21897, loss = 1.45092851\n",
      "Iteration 21898, loss = 1.45092694\n",
      "Iteration 21899, loss = 1.45092537\n",
      "Iteration 21900, loss = 1.45092380\n",
      "Iteration 21901, loss = 1.45092222\n",
      "Iteration 21902, loss = 1.45092065\n",
      "Iteration 21903, loss = 1.45091908\n",
      "Iteration 21904, loss = 1.45091751\n",
      "Iteration 21905, loss = 1.45091594\n",
      "Iteration 21906, loss = 1.45091437\n",
      "Iteration 21907, loss = 1.45091280\n",
      "Iteration 21908, loss = 1.45091124\n",
      "Iteration 21909, loss = 1.45090967\n",
      "Iteration 21910, loss = 1.45090810\n",
      "Iteration 21911, loss = 1.45090653\n",
      "Iteration 21912, loss = 1.45090496\n",
      "Iteration 21913, loss = 1.45090339\n",
      "Iteration 21914, loss = 1.45090182\n",
      "Iteration 21915, loss = 1.45090025\n",
      "Iteration 21916, loss = 1.45089869\n",
      "Iteration 21917, loss = 1.45089712\n",
      "Iteration 21918, loss = 1.45089555\n",
      "Iteration 21919, loss = 1.45089398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21920, loss = 1.45089242\n",
      "Iteration 21921, loss = 1.45089085\n",
      "Iteration 21922, loss = 1.45088928\n",
      "Iteration 21923, loss = 1.45088771\n",
      "Iteration 21924, loss = 1.45088615\n",
      "Iteration 21925, loss = 1.45088458\n",
      "Iteration 21926, loss = 1.45088302\n",
      "Iteration 21927, loss = 1.45088145\n",
      "Iteration 21928, loss = 1.45087988\n",
      "Iteration 21929, loss = 1.45087832\n",
      "Iteration 21930, loss = 1.45087675\n",
      "Iteration 21931, loss = 1.45087519\n",
      "Iteration 21932, loss = 1.45087362\n",
      "Iteration 21933, loss = 1.45087206\n",
      "Iteration 21934, loss = 1.45087049\n",
      "Iteration 21935, loss = 1.45086893\n",
      "Iteration 21936, loss = 1.45086736\n",
      "Iteration 21937, loss = 1.45086580\n",
      "Iteration 21938, loss = 1.45086423\n",
      "Iteration 21939, loss = 1.45086267\n",
      "Iteration 21940, loss = 1.45086110\n",
      "Iteration 21941, loss = 1.45085954\n",
      "Iteration 21942, loss = 1.45085798\n",
      "Iteration 21943, loss = 1.45085641\n",
      "Iteration 21944, loss = 1.45085485\n",
      "Iteration 21945, loss = 1.45085329\n",
      "Iteration 21946, loss = 1.45085172\n",
      "Iteration 21947, loss = 1.45085016\n",
      "Iteration 21948, loss = 1.45084860\n",
      "Iteration 21949, loss = 1.45084704\n",
      "Iteration 21950, loss = 1.45084547\n",
      "Iteration 21951, loss = 1.45084391\n",
      "Iteration 21952, loss = 1.45084235\n",
      "Iteration 21953, loss = 1.45084079\n",
      "Iteration 21954, loss = 1.45083923\n",
      "Iteration 21955, loss = 1.45083766\n",
      "Iteration 21956, loss = 1.45083610\n",
      "Iteration 21957, loss = 1.45083454\n",
      "Iteration 21958, loss = 1.45083298\n",
      "Iteration 21959, loss = 1.45083142\n",
      "Iteration 21960, loss = 1.45082986\n",
      "Iteration 21961, loss = 1.45082830\n",
      "Iteration 21962, loss = 1.45082674\n",
      "Iteration 21963, loss = 1.45082518\n",
      "Iteration 21964, loss = 1.45082362\n",
      "Iteration 21965, loss = 1.45082206\n",
      "Iteration 21966, loss = 1.45082050\n",
      "Iteration 21967, loss = 1.45081894\n",
      "Iteration 21968, loss = 1.45081738\n",
      "Iteration 21969, loss = 1.45081582\n",
      "Iteration 21970, loss = 1.45081426\n",
      "Iteration 21971, loss = 1.45081270\n",
      "Iteration 21972, loss = 1.45081115\n",
      "Iteration 21973, loss = 1.45080959\n",
      "Iteration 21974, loss = 1.45080803\n",
      "Iteration 21975, loss = 1.45080647\n",
      "Iteration 21976, loss = 1.45080491\n",
      "Iteration 21977, loss = 1.45080335\n",
      "Iteration 21978, loss = 1.45080180\n",
      "Iteration 21979, loss = 1.45080024\n",
      "Iteration 21980, loss = 1.45079868\n",
      "Iteration 21981, loss = 1.45079713\n",
      "Iteration 21982, loss = 1.45079557\n",
      "Iteration 21983, loss = 1.45079401\n",
      "Iteration 21984, loss = 1.45079246\n",
      "Iteration 21985, loss = 1.45079090\n",
      "Iteration 21986, loss = 1.45078934\n",
      "Iteration 21987, loss = 1.45078779\n",
      "Iteration 21988, loss = 1.45078623\n",
      "Iteration 21989, loss = 1.45078467\n",
      "Iteration 21990, loss = 1.45078312\n",
      "Iteration 21991, loss = 1.45078156\n",
      "Iteration 21992, loss = 1.45078001\n",
      "Iteration 21993, loss = 1.45077845\n",
      "Iteration 21994, loss = 1.45077690\n",
      "Iteration 21995, loss = 1.45077534\n",
      "Iteration 21996, loss = 1.45077379\n",
      "Iteration 21997, loss = 1.45077223\n",
      "Iteration 21998, loss = 1.45077068\n",
      "Iteration 21999, loss = 1.45076913\n",
      "Iteration 22000, loss = 1.45076757\n",
      "Iteration 22001, loss = 1.45076602\n",
      "Iteration 22002, loss = 1.45076447\n",
      "Iteration 22003, loss = 1.45076291\n",
      "Iteration 22004, loss = 1.45076136\n",
      "Iteration 22005, loss = 1.45075981\n",
      "Iteration 22006, loss = 1.45075825\n",
      "Iteration 22007, loss = 1.45075670\n",
      "Iteration 22008, loss = 1.45075515\n",
      "Iteration 22009, loss = 1.45075360\n",
      "Iteration 22010, loss = 1.45075204\n",
      "Iteration 22011, loss = 1.45075049\n",
      "Iteration 22012, loss = 1.45074894\n",
      "Iteration 22013, loss = 1.45074739\n",
      "Iteration 22014, loss = 1.45074584\n",
      "Iteration 22015, loss = 1.45074428\n",
      "Iteration 22016, loss = 1.45074273\n",
      "Iteration 22017, loss = 1.45074118\n",
      "Iteration 22018, loss = 1.45073963\n",
      "Iteration 22019, loss = 1.45073808\n",
      "Iteration 22020, loss = 1.45073653\n",
      "Iteration 22021, loss = 1.45073498\n",
      "Iteration 22022, loss = 1.45073343\n",
      "Iteration 22023, loss = 1.45073188\n",
      "Iteration 22024, loss = 1.45073033\n",
      "Iteration 22025, loss = 1.45072878\n",
      "Iteration 22026, loss = 1.45072723\n",
      "Iteration 22027, loss = 1.45072568\n",
      "Iteration 22028, loss = 1.45072413\n",
      "Iteration 22029, loss = 1.45072258\n",
      "Iteration 22030, loss = 1.45072103\n",
      "Iteration 22031, loss = 1.45071948\n",
      "Iteration 22032, loss = 1.45071794\n",
      "Iteration 22033, loss = 1.45071639\n",
      "Iteration 22034, loss = 1.45071484\n",
      "Iteration 22035, loss = 1.45071329\n",
      "Iteration 22036, loss = 1.45071174\n",
      "Iteration 22037, loss = 1.45071020\n",
      "Iteration 22038, loss = 1.45070865\n",
      "Iteration 22039, loss = 1.45070710\n",
      "Iteration 22040, loss = 1.45070555\n",
      "Iteration 22041, loss = 1.45070401\n",
      "Iteration 22042, loss = 1.45070246\n",
      "Iteration 22043, loss = 1.45070091\n",
      "Iteration 22044, loss = 1.45069937\n",
      "Iteration 22045, loss = 1.45069782\n",
      "Iteration 22046, loss = 1.45069627\n",
      "Iteration 22047, loss = 1.45069473\n",
      "Iteration 22048, loss = 1.45069318\n",
      "Iteration 22049, loss = 1.45069164\n",
      "Iteration 22050, loss = 1.45069009\n",
      "Iteration 22051, loss = 1.45068855\n",
      "Iteration 22052, loss = 1.45068700\n",
      "Iteration 22053, loss = 1.45068545\n",
      "Iteration 22054, loss = 1.45068391\n",
      "Iteration 22055, loss = 1.45068237\n",
      "Iteration 22056, loss = 1.45068082\n",
      "Iteration 22057, loss = 1.45067928\n",
      "Iteration 22058, loss = 1.45067773\n",
      "Iteration 22059, loss = 1.45067619\n",
      "Iteration 22060, loss = 1.45067464\n",
      "Iteration 22061, loss = 1.45067310\n",
      "Iteration 22062, loss = 1.45067156\n",
      "Iteration 22063, loss = 1.45067001\n",
      "Iteration 22064, loss = 1.45066847\n",
      "Iteration 22065, loss = 1.45066693\n",
      "Iteration 22066, loss = 1.45066538\n",
      "Iteration 22067, loss = 1.45066384\n",
      "Iteration 22068, loss = 1.45066230\n",
      "Iteration 22069, loss = 1.45066076\n",
      "Iteration 22070, loss = 1.45065921\n",
      "Iteration 22071, loss = 1.45065767\n",
      "Iteration 22072, loss = 1.45065613\n",
      "Iteration 22073, loss = 1.45065459\n",
      "Iteration 22074, loss = 1.45065305\n",
      "Iteration 22075, loss = 1.45065151\n",
      "Iteration 22076, loss = 1.45064996\n",
      "Iteration 22077, loss = 1.45064842\n",
      "Iteration 22078, loss = 1.45064688\n",
      "Iteration 22079, loss = 1.45064534\n",
      "Iteration 22080, loss = 1.45064380\n",
      "Iteration 22081, loss = 1.45064226\n",
      "Iteration 22082, loss = 1.45064072\n",
      "Iteration 22083, loss = 1.45063918\n",
      "Iteration 22084, loss = 1.45063764\n",
      "Iteration 22085, loss = 1.45063610\n",
      "Iteration 22086, loss = 1.45063456\n",
      "Iteration 22087, loss = 1.45063302\n",
      "Iteration 22088, loss = 1.45063148\n",
      "Iteration 22089, loss = 1.45062994\n",
      "Iteration 22090, loss = 1.45062840\n",
      "Iteration 22091, loss = 1.45062687\n",
      "Iteration 22092, loss = 1.45062533\n",
      "Iteration 22093, loss = 1.45062379\n",
      "Iteration 22094, loss = 1.45062225\n",
      "Iteration 22095, loss = 1.45062071\n",
      "Iteration 22096, loss = 1.45061917\n",
      "Iteration 22097, loss = 1.45061764\n",
      "Iteration 22098, loss = 1.45061610\n",
      "Iteration 22099, loss = 1.45061456\n",
      "Iteration 22100, loss = 1.45061302\n",
      "Iteration 22101, loss = 1.45061149\n",
      "Iteration 22102, loss = 1.45060995\n",
      "Iteration 22103, loss = 1.45060841\n",
      "Iteration 22104, loss = 1.45060688\n",
      "Iteration 22105, loss = 1.45060534\n",
      "Iteration 22106, loss = 1.45060380\n",
      "Iteration 22107, loss = 1.45060227\n",
      "Iteration 22108, loss = 1.45060073\n",
      "Iteration 22109, loss = 1.45059920\n",
      "Iteration 22110, loss = 1.45059766\n",
      "Iteration 22111, loss = 1.45059612\n",
      "Iteration 22112, loss = 1.45059459\n",
      "Iteration 22113, loss = 1.45059305\n",
      "Iteration 22114, loss = 1.45059152\n",
      "Iteration 22115, loss = 1.45058998\n",
      "Iteration 22116, loss = 1.45058845\n",
      "Iteration 22117, loss = 1.45058691\n",
      "Iteration 22118, loss = 1.45058538\n",
      "Iteration 22119, loss = 1.45058385\n",
      "Iteration 22120, loss = 1.45058231\n",
      "Iteration 22121, loss = 1.45058078\n",
      "Iteration 22122, loss = 1.45057925\n",
      "Iteration 22123, loss = 1.45057771\n",
      "Iteration 22124, loss = 1.45057618\n",
      "Iteration 22125, loss = 1.45057465\n",
      "Iteration 22126, loss = 1.45057311\n",
      "Iteration 22127, loss = 1.45057158\n",
      "Iteration 22128, loss = 1.45057005\n",
      "Iteration 22129, loss = 1.45056851\n",
      "Iteration 22130, loss = 1.45056698\n",
      "Iteration 22131, loss = 1.45056545\n",
      "Iteration 22132, loss = 1.45056392\n",
      "Iteration 22133, loss = 1.45056239\n",
      "Iteration 22134, loss = 1.45056085\n",
      "Iteration 22135, loss = 1.45055932\n",
      "Iteration 22136, loss = 1.45055779\n",
      "Iteration 22137, loss = 1.45055626\n",
      "Iteration 22138, loss = 1.45055473\n",
      "Iteration 22139, loss = 1.45055320\n",
      "Iteration 22140, loss = 1.45055167\n",
      "Iteration 22141, loss = 1.45055014\n",
      "Iteration 22142, loss = 1.45054861\n",
      "Iteration 22143, loss = 1.45054708\n",
      "Iteration 22144, loss = 1.45054555\n",
      "Iteration 22145, loss = 1.45054402\n",
      "Iteration 22146, loss = 1.45054249\n",
      "Iteration 22147, loss = 1.45054096\n",
      "Iteration 22148, loss = 1.45053943\n",
      "Iteration 22149, loss = 1.45053790\n",
      "Iteration 22150, loss = 1.45053637\n",
      "Iteration 22151, loss = 1.45053484\n",
      "Iteration 22152, loss = 1.45053331\n",
      "Iteration 22153, loss = 1.45053178\n",
      "Iteration 22154, loss = 1.45053025\n",
      "Iteration 22155, loss = 1.45052873\n",
      "Iteration 22156, loss = 1.45052720\n",
      "Iteration 22157, loss = 1.45052567\n",
      "Iteration 22158, loss = 1.45052414\n",
      "Iteration 22159, loss = 1.45052261\n",
      "Iteration 22160, loss = 1.45052109\n",
      "Iteration 22161, loss = 1.45051956\n",
      "Iteration 22162, loss = 1.45051803\n",
      "Iteration 22163, loss = 1.45051651\n",
      "Iteration 22164, loss = 1.45051498\n",
      "Iteration 22165, loss = 1.45051345\n",
      "Iteration 22166, loss = 1.45051193\n",
      "Iteration 22167, loss = 1.45051040\n",
      "Iteration 22168, loss = 1.45050887\n",
      "Iteration 22169, loss = 1.45050735\n",
      "Iteration 22170, loss = 1.45050582\n",
      "Iteration 22171, loss = 1.45050430\n",
      "Iteration 22172, loss = 1.45050277\n",
      "Iteration 22173, loss = 1.45050125\n",
      "Iteration 22174, loss = 1.45049972\n",
      "Iteration 22175, loss = 1.45049820\n",
      "Iteration 22176, loss = 1.45049667\n",
      "Iteration 22177, loss = 1.45049515\n",
      "Iteration 22178, loss = 1.45049362\n",
      "Iteration 22179, loss = 1.45049210\n",
      "Iteration 22180, loss = 1.45049057\n",
      "Iteration 22181, loss = 1.45048905\n",
      "Iteration 22182, loss = 1.45048753\n",
      "Iteration 22183, loss = 1.45048600\n",
      "Iteration 22184, loss = 1.45048448\n",
      "Iteration 22185, loss = 1.45048295\n",
      "Iteration 22186, loss = 1.45048143\n",
      "Iteration 22187, loss = 1.45047991\n",
      "Iteration 22188, loss = 1.45047839\n",
      "Iteration 22189, loss = 1.45047686\n",
      "Iteration 22190, loss = 1.45047534\n",
      "Iteration 22191, loss = 1.45047382\n",
      "Iteration 22192, loss = 1.45047230\n",
      "Iteration 22193, loss = 1.45047077\n",
      "Iteration 22194, loss = 1.45046925\n",
      "Iteration 22195, loss = 1.45046773\n",
      "Iteration 22196, loss = 1.45046621\n",
      "Iteration 22197, loss = 1.45046469\n",
      "Iteration 22198, loss = 1.45046317\n",
      "Iteration 22199, loss = 1.45046165\n",
      "Iteration 22200, loss = 1.45046012\n",
      "Iteration 22201, loss = 1.45045860\n",
      "Iteration 22202, loss = 1.45045708\n",
      "Iteration 22203, loss = 1.45045556\n",
      "Iteration 22204, loss = 1.45045404\n",
      "Iteration 22205, loss = 1.45045252\n",
      "Iteration 22206, loss = 1.45045100\n",
      "Iteration 22207, loss = 1.45044948\n",
      "Iteration 22208, loss = 1.45044796\n",
      "Iteration 22209, loss = 1.45044644\n",
      "Iteration 22210, loss = 1.45044492\n",
      "Iteration 22211, loss = 1.45044340\n",
      "Iteration 22212, loss = 1.45044189\n",
      "Iteration 22213, loss = 1.45044037\n",
      "Iteration 22214, loss = 1.45043885\n",
      "Iteration 22215, loss = 1.45043733\n",
      "Iteration 22216, loss = 1.45043581\n",
      "Iteration 22217, loss = 1.45043429\n",
      "Iteration 22218, loss = 1.45043278\n",
      "Iteration 22219, loss = 1.45043126\n",
      "Iteration 22220, loss = 1.45042974\n",
      "Iteration 22221, loss = 1.45042822\n",
      "Iteration 22222, loss = 1.45042671\n",
      "Iteration 22223, loss = 1.45042519\n",
      "Iteration 22224, loss = 1.45042367\n",
      "Iteration 22225, loss = 1.45042215\n",
      "Iteration 22226, loss = 1.45042064\n",
      "Iteration 22227, loss = 1.45041912\n",
      "Iteration 22228, loss = 1.45041760\n",
      "Iteration 22229, loss = 1.45041609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22230, loss = 1.45041457\n",
      "Iteration 22231, loss = 1.45041306\n",
      "Iteration 22232, loss = 1.45041154\n",
      "Iteration 22233, loss = 1.45041003\n",
      "Iteration 22234, loss = 1.45040851\n",
      "Iteration 22235, loss = 1.45040699\n",
      "Iteration 22236, loss = 1.45040548\n",
      "Iteration 22237, loss = 1.45040396\n",
      "Iteration 22238, loss = 1.45040245\n",
      "Iteration 22239, loss = 1.45040094\n",
      "Iteration 22240, loss = 1.45039942\n",
      "Iteration 22241, loss = 1.45039791\n",
      "Iteration 22242, loss = 1.45039639\n",
      "Iteration 22243, loss = 1.45039488\n",
      "Iteration 22244, loss = 1.45039336\n",
      "Iteration 22245, loss = 1.45039185\n",
      "Iteration 22246, loss = 1.45039034\n",
      "Iteration 22247, loss = 1.45038882\n",
      "Iteration 22248, loss = 1.45038731\n",
      "Iteration 22249, loss = 1.45038580\n",
      "Iteration 22250, loss = 1.45038429\n",
      "Iteration 22251, loss = 1.45038277\n",
      "Iteration 22252, loss = 1.45038126\n",
      "Iteration 22253, loss = 1.45037975\n",
      "Iteration 22254, loss = 1.45037824\n",
      "Iteration 22255, loss = 1.45037672\n",
      "Iteration 22256, loss = 1.45037521\n",
      "Iteration 22257, loss = 1.45037370\n",
      "Iteration 22258, loss = 1.45037219\n",
      "Iteration 22259, loss = 1.45037068\n",
      "Iteration 22260, loss = 1.45036917\n",
      "Iteration 22261, loss = 1.45036766\n",
      "Iteration 22262, loss = 1.45036614\n",
      "Iteration 22263, loss = 1.45036463\n",
      "Iteration 22264, loss = 1.45036312\n",
      "Iteration 22265, loss = 1.45036161\n",
      "Iteration 22266, loss = 1.45036010\n",
      "Iteration 22267, loss = 1.45035859\n",
      "Iteration 22268, loss = 1.45035708\n",
      "Iteration 22269, loss = 1.45035557\n",
      "Iteration 22270, loss = 1.45035406\n",
      "Iteration 22271, loss = 1.45035255\n",
      "Iteration 22272, loss = 1.45035104\n",
      "Iteration 22273, loss = 1.45034954\n",
      "Iteration 22274, loss = 1.45034803\n",
      "Iteration 22275, loss = 1.45034652\n",
      "Iteration 22276, loss = 1.45034501\n",
      "Iteration 22277, loss = 1.45034350\n",
      "Iteration 22278, loss = 1.45034199\n",
      "Iteration 22279, loss = 1.45034048\n",
      "Iteration 22280, loss = 1.45033898\n",
      "Iteration 22281, loss = 1.45033747\n",
      "Iteration 22282, loss = 1.45033596\n",
      "Iteration 22283, loss = 1.45033445\n",
      "Iteration 22284, loss = 1.45033295\n",
      "Iteration 22285, loss = 1.45033144\n",
      "Iteration 22286, loss = 1.45032993\n",
      "Iteration 22287, loss = 1.45032843\n",
      "Iteration 22288, loss = 1.45032692\n",
      "Iteration 22289, loss = 1.45032541\n",
      "Iteration 22290, loss = 1.45032391\n",
      "Iteration 22291, loss = 1.45032240\n",
      "Iteration 22292, loss = 1.45032089\n",
      "Iteration 22293, loss = 1.45031939\n",
      "Iteration 22294, loss = 1.45031788\n",
      "Iteration 22295, loss = 1.45031638\n",
      "Iteration 22296, loss = 1.45031487\n",
      "Iteration 22297, loss = 1.45031337\n",
      "Iteration 22298, loss = 1.45031186\n",
      "Iteration 22299, loss = 1.45031036\n",
      "Iteration 22300, loss = 1.45030885\n",
      "Iteration 22301, loss = 1.45030735\n",
      "Iteration 22302, loss = 1.45030584\n",
      "Iteration 22303, loss = 1.45030434\n",
      "Iteration 22304, loss = 1.45030283\n",
      "Iteration 22305, loss = 1.45030133\n",
      "Iteration 22306, loss = 1.45029983\n",
      "Iteration 22307, loss = 1.45029832\n",
      "Iteration 22308, loss = 1.45029682\n",
      "Iteration 22309, loss = 1.45029532\n",
      "Iteration 22310, loss = 1.45029381\n",
      "Iteration 22311, loss = 1.45029231\n",
      "Iteration 22312, loss = 1.45029081\n",
      "Iteration 22313, loss = 1.45028930\n",
      "Iteration 22314, loss = 1.45028780\n",
      "Iteration 22315, loss = 1.45028630\n",
      "Iteration 22316, loss = 1.45028480\n",
      "Iteration 22317, loss = 1.45028329\n",
      "Iteration 22318, loss = 1.45028179\n",
      "Iteration 22319, loss = 1.45028029\n",
      "Iteration 22320, loss = 1.45027879\n",
      "Iteration 22321, loss = 1.45027729\n",
      "Iteration 22322, loss = 1.45027579\n",
      "Iteration 22323, loss = 1.45027429\n",
      "Iteration 22324, loss = 1.45027279\n",
      "Iteration 22325, loss = 1.45027128\n",
      "Iteration 22326, loss = 1.45026978\n",
      "Iteration 22327, loss = 1.45026828\n",
      "Iteration 22328, loss = 1.45026678\n",
      "Iteration 22329, loss = 1.45026528\n",
      "Iteration 22330, loss = 1.45026378\n",
      "Iteration 22331, loss = 1.45026228\n",
      "Iteration 22332, loss = 1.45026078\n",
      "Iteration 22333, loss = 1.45025928\n",
      "Iteration 22334, loss = 1.45025778\n",
      "Iteration 22335, loss = 1.45025629\n",
      "Iteration 22336, loss = 1.45025479\n",
      "Iteration 22337, loss = 1.45025329\n",
      "Iteration 22338, loss = 1.45025179\n",
      "Iteration 22339, loss = 1.45025029\n",
      "Iteration 22340, loss = 1.45024879\n",
      "Iteration 22341, loss = 1.45024729\n",
      "Iteration 22342, loss = 1.45024580\n",
      "Iteration 22343, loss = 1.45024430\n",
      "Iteration 22344, loss = 1.45024280\n",
      "Iteration 22345, loss = 1.45024130\n",
      "Iteration 22346, loss = 1.45023981\n",
      "Iteration 22347, loss = 1.45023831\n",
      "Iteration 22348, loss = 1.45023681\n",
      "Iteration 22349, loss = 1.45023531\n",
      "Iteration 22350, loss = 1.45023382\n",
      "Iteration 22351, loss = 1.45023232\n",
      "Iteration 22352, loss = 1.45023082\n",
      "Iteration 22353, loss = 1.45022933\n",
      "Iteration 22354, loss = 1.45022783\n",
      "Iteration 22355, loss = 1.45022634\n",
      "Iteration 22356, loss = 1.45022484\n",
      "Iteration 22357, loss = 1.45022334\n",
      "Iteration 22358, loss = 1.45022185\n",
      "Iteration 22359, loss = 1.45022035\n",
      "Iteration 22360, loss = 1.45021886\n",
      "Iteration 22361, loss = 1.45021736\n",
      "Iteration 22362, loss = 1.45021587\n",
      "Iteration 22363, loss = 1.45021437\n",
      "Iteration 22364, loss = 1.45021288\n",
      "Iteration 22365, loss = 1.45021138\n",
      "Iteration 22366, loss = 1.45020989\n",
      "Iteration 22367, loss = 1.45020840\n",
      "Iteration 22368, loss = 1.45020690\n",
      "Iteration 22369, loss = 1.45020541\n",
      "Iteration 22370, loss = 1.45020392\n",
      "Iteration 22371, loss = 1.45020242\n",
      "Iteration 22372, loss = 1.45020093\n",
      "Iteration 22373, loss = 1.45019944\n",
      "Iteration 22374, loss = 1.45019794\n",
      "Iteration 22375, loss = 1.45019645\n",
      "Iteration 22376, loss = 1.45019496\n",
      "Iteration 22377, loss = 1.45019346\n",
      "Iteration 22378, loss = 1.45019197\n",
      "Iteration 22379, loss = 1.45019048\n",
      "Iteration 22380, loss = 1.45018899\n",
      "Iteration 22381, loss = 1.45018750\n",
      "Iteration 22382, loss = 1.45018600\n",
      "Iteration 22383, loss = 1.45018451\n",
      "Iteration 22384, loss = 1.45018302\n",
      "Iteration 22385, loss = 1.45018153\n",
      "Iteration 22386, loss = 1.45018004\n",
      "Iteration 22387, loss = 1.45017855\n",
      "Iteration 22388, loss = 1.45017706\n",
      "Iteration 22389, loss = 1.45017557\n",
      "Iteration 22390, loss = 1.45017408\n",
      "Iteration 22391, loss = 1.45017259\n",
      "Iteration 22392, loss = 1.45017110\n",
      "Iteration 22393, loss = 1.45016961\n",
      "Iteration 22394, loss = 1.45016812\n",
      "Iteration 22395, loss = 1.45016663\n",
      "Iteration 22396, loss = 1.45016514\n",
      "Iteration 22397, loss = 1.45016365\n",
      "Iteration 22398, loss = 1.45016216\n",
      "Iteration 22399, loss = 1.45016067\n",
      "Iteration 22400, loss = 1.45015918\n",
      "Iteration 22401, loss = 1.45015769\n",
      "Iteration 22402, loss = 1.45015620\n",
      "Iteration 22403, loss = 1.45015472\n",
      "Iteration 22404, loss = 1.45015323\n",
      "Iteration 22405, loss = 1.45015174\n",
      "Iteration 22406, loss = 1.45015025\n",
      "Iteration 22407, loss = 1.45014876\n",
      "Iteration 22408, loss = 1.45014728\n",
      "Iteration 22409, loss = 1.45014579\n",
      "Iteration 22410, loss = 1.45014430\n",
      "Iteration 22411, loss = 1.45014282\n",
      "Iteration 22412, loss = 1.45014133\n",
      "Iteration 22413, loss = 1.45013984\n",
      "Iteration 22414, loss = 1.45013835\n",
      "Iteration 22415, loss = 1.45013687\n",
      "Iteration 22416, loss = 1.45013538\n",
      "Iteration 22417, loss = 1.45013390\n",
      "Iteration 22418, loss = 1.45013241\n",
      "Iteration 22419, loss = 1.45013092\n",
      "Iteration 22420, loss = 1.45012944\n",
      "Iteration 22421, loss = 1.45012795\n",
      "Iteration 22422, loss = 1.45012647\n",
      "Iteration 22423, loss = 1.45012498\n",
      "Iteration 22424, loss = 1.45012350\n",
      "Iteration 22425, loss = 1.45012201\n",
      "Iteration 22426, loss = 1.45012053\n",
      "Iteration 22427, loss = 1.45011904\n",
      "Iteration 22428, loss = 1.45011756\n",
      "Iteration 22429, loss = 1.45011607\n",
      "Iteration 22430, loss = 1.45011459\n",
      "Iteration 22431, loss = 1.45011311\n",
      "Iteration 22432, loss = 1.45011162\n",
      "Iteration 22433, loss = 1.45011014\n",
      "Iteration 22434, loss = 1.45010866\n",
      "Iteration 22435, loss = 1.45010717\n",
      "Iteration 22436, loss = 1.45010569\n",
      "Iteration 22437, loss = 1.45010421\n",
      "Iteration 22438, loss = 1.45010272\n",
      "Iteration 22439, loss = 1.45010124\n",
      "Iteration 22440, loss = 1.45009976\n",
      "Iteration 22441, loss = 1.45009828\n",
      "Iteration 22442, loss = 1.45009679\n",
      "Iteration 22443, loss = 1.45009531\n",
      "Iteration 22444, loss = 1.45009383\n",
      "Iteration 22445, loss = 1.45009235\n",
      "Iteration 22446, loss = 1.45009087\n",
      "Iteration 22447, loss = 1.45008938\n",
      "Iteration 22448, loss = 1.45008790\n",
      "Iteration 22449, loss = 1.45008642\n",
      "Iteration 22450, loss = 1.45008494\n",
      "Iteration 22451, loss = 1.45008346\n",
      "Iteration 22452, loss = 1.45008198\n",
      "Iteration 22453, loss = 1.45008050\n",
      "Iteration 22454, loss = 1.45007902\n",
      "Iteration 22455, loss = 1.45007754\n",
      "Iteration 22456, loss = 1.45007606\n",
      "Iteration 22457, loss = 1.45007458\n",
      "Iteration 22458, loss = 1.45007310\n",
      "Iteration 22459, loss = 1.45007162\n",
      "Iteration 22460, loss = 1.45007014\n",
      "Iteration 22461, loss = 1.45006866\n",
      "Iteration 22462, loss = 1.45006718\n",
      "Iteration 22463, loss = 1.45006570\n",
      "Iteration 22464, loss = 1.45006422\n",
      "Iteration 22465, loss = 1.45006275\n",
      "Iteration 22466, loss = 1.45006127\n",
      "Iteration 22467, loss = 1.45005979\n",
      "Iteration 22468, loss = 1.45005831\n",
      "Iteration 22469, loss = 1.45005683\n",
      "Iteration 22470, loss = 1.45005536\n",
      "Iteration 22471, loss = 1.45005388\n",
      "Iteration 22472, loss = 1.45005240\n",
      "Iteration 22473, loss = 1.45005092\n",
      "Iteration 22474, loss = 1.45004945\n",
      "Iteration 22475, loss = 1.45004797\n",
      "Iteration 22476, loss = 1.45004649\n",
      "Iteration 22477, loss = 1.45004502\n",
      "Iteration 22478, loss = 1.45004354\n",
      "Iteration 22479, loss = 1.45004206\n",
      "Iteration 22480, loss = 1.45004059\n",
      "Iteration 22481, loss = 1.45003911\n",
      "Iteration 22482, loss = 1.45003763\n",
      "Iteration 22483, loss = 1.45003616\n",
      "Iteration 22484, loss = 1.45003468\n",
      "Iteration 22485, loss = 1.45003321\n",
      "Iteration 22486, loss = 1.45003173\n",
      "Iteration 22487, loss = 1.45003026\n",
      "Iteration 22488, loss = 1.45002878\n",
      "Iteration 22489, loss = 1.45002731\n",
      "Iteration 22490, loss = 1.45002583\n",
      "Iteration 22491, loss = 1.45002436\n",
      "Iteration 22492, loss = 1.45002288\n",
      "Iteration 22493, loss = 1.45002141\n",
      "Iteration 22494, loss = 1.45001993\n",
      "Iteration 22495, loss = 1.45001846\n",
      "Iteration 22496, loss = 1.45001699\n",
      "Iteration 22497, loss = 1.45001551\n",
      "Iteration 22498, loss = 1.45001404\n",
      "Iteration 22499, loss = 1.45001257\n",
      "Iteration 22500, loss = 1.45001109\n",
      "Iteration 22501, loss = 1.45000962\n",
      "Iteration 22502, loss = 1.45000815\n",
      "Iteration 22503, loss = 1.45000668\n",
      "Iteration 22504, loss = 1.45000520\n",
      "Iteration 22505, loss = 1.45000373\n",
      "Iteration 22506, loss = 1.45000226\n",
      "Iteration 22507, loss = 1.45000079\n",
      "Iteration 22508, loss = 1.44999931\n",
      "Iteration 22509, loss = 1.44999784\n",
      "Iteration 22510, loss = 1.44999637\n",
      "Iteration 22511, loss = 1.44999490\n",
      "Iteration 22512, loss = 1.44999343\n",
      "Iteration 22513, loss = 1.44999196\n",
      "Iteration 22514, loss = 1.44999049\n",
      "Iteration 22515, loss = 1.44998902\n",
      "Iteration 22516, loss = 1.44998754\n",
      "Iteration 22517, loss = 1.44998607\n",
      "Iteration 22518, loss = 1.44998460\n",
      "Iteration 22519, loss = 1.44998313\n",
      "Iteration 22520, loss = 1.44998166\n",
      "Iteration 22521, loss = 1.44998019\n",
      "Iteration 22522, loss = 1.44997872\n",
      "Iteration 22523, loss = 1.44997725\n",
      "Iteration 22524, loss = 1.44997579\n",
      "Iteration 22525, loss = 1.44997432\n",
      "Iteration 22526, loss = 1.44997285\n",
      "Iteration 22527, loss = 1.44997138\n",
      "Iteration 22528, loss = 1.44996991\n",
      "Iteration 22529, loss = 1.44996844\n",
      "Iteration 22530, loss = 1.44996697\n",
      "Iteration 22531, loss = 1.44996550\n",
      "Iteration 22532, loss = 1.44996404\n",
      "Iteration 22533, loss = 1.44996257\n",
      "Iteration 22534, loss = 1.44996110\n",
      "Iteration 22535, loss = 1.44995963\n",
      "Iteration 22536, loss = 1.44995816\n",
      "Iteration 22537, loss = 1.44995670\n",
      "Iteration 22538, loss = 1.44995523\n",
      "Iteration 22539, loss = 1.44995376\n",
      "Iteration 22540, loss = 1.44995230\n",
      "Iteration 22541, loss = 1.44995083\n",
      "Iteration 22542, loss = 1.44994936\n",
      "Iteration 22543, loss = 1.44994790\n",
      "Iteration 22544, loss = 1.44994643\n",
      "Iteration 22545, loss = 1.44994496\n",
      "Iteration 22546, loss = 1.44994350\n",
      "Iteration 22547, loss = 1.44994203\n",
      "Iteration 22548, loss = 1.44994057\n",
      "Iteration 22549, loss = 1.44993910\n",
      "Iteration 22550, loss = 1.44993764\n",
      "Iteration 22551, loss = 1.44993617\n",
      "Iteration 22552, loss = 1.44993471\n",
      "Iteration 22553, loss = 1.44993324\n",
      "Iteration 22554, loss = 1.44993178\n",
      "Iteration 22555, loss = 1.44993031\n",
      "Iteration 22556, loss = 1.44992885\n",
      "Iteration 22557, loss = 1.44992738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22558, loss = 1.44992592\n",
      "Iteration 22559, loss = 1.44992445\n",
      "Iteration 22560, loss = 1.44992299\n",
      "Iteration 22561, loss = 1.44992153\n",
      "Iteration 22562, loss = 1.44992006\n",
      "Iteration 22563, loss = 1.44991860\n",
      "Iteration 22564, loss = 1.44991714\n",
      "Iteration 22565, loss = 1.44991567\n",
      "Iteration 22566, loss = 1.44991421\n",
      "Iteration 22567, loss = 1.44991275\n",
      "Iteration 22568, loss = 1.44991129\n",
      "Iteration 22569, loss = 1.44990982\n",
      "Iteration 22570, loss = 1.44990836\n",
      "Iteration 22571, loss = 1.44990690\n",
      "Iteration 22572, loss = 1.44990544\n",
      "Iteration 22573, loss = 1.44990398\n",
      "Iteration 22574, loss = 1.44990251\n",
      "Iteration 22575, loss = 1.44990105\n",
      "Iteration 22576, loss = 1.44989959\n",
      "Iteration 22577, loss = 1.44989813\n",
      "Iteration 22578, loss = 1.44989667\n",
      "Iteration 22579, loss = 1.44989521\n",
      "Iteration 22580, loss = 1.44989375\n",
      "Iteration 22581, loss = 1.44989229\n",
      "Iteration 22582, loss = 1.44989083\n",
      "Iteration 22583, loss = 1.44988937\n",
      "Iteration 22584, loss = 1.44988791\n",
      "Iteration 22585, loss = 1.44988645\n",
      "Iteration 22586, loss = 1.44988499\n",
      "Iteration 22587, loss = 1.44988353\n",
      "Iteration 22588, loss = 1.44988207\n",
      "Iteration 22589, loss = 1.44988061\n",
      "Iteration 22590, loss = 1.44987915\n",
      "Iteration 22591, loss = 1.44987769\n",
      "Iteration 22592, loss = 1.44987623\n",
      "Iteration 22593, loss = 1.44987477\n",
      "Iteration 22594, loss = 1.44987331\n",
      "Iteration 22595, loss = 1.44987185\n",
      "Iteration 22596, loss = 1.44987040\n",
      "Iteration 22597, loss = 1.44986894\n",
      "Iteration 22598, loss = 1.44986748\n",
      "Iteration 22599, loss = 1.44986602\n",
      "Iteration 22600, loss = 1.44986456\n",
      "Iteration 22601, loss = 1.44986311\n",
      "Iteration 22602, loss = 1.44986165\n",
      "Iteration 22603, loss = 1.44986019\n",
      "Iteration 22604, loss = 1.44985874\n",
      "Iteration 22605, loss = 1.44985728\n",
      "Iteration 22606, loss = 1.44985582\n",
      "Iteration 22607, loss = 1.44985437\n",
      "Iteration 22608, loss = 1.44985291\n",
      "Iteration 22609, loss = 1.44985145\n",
      "Iteration 22610, loss = 1.44985000\n",
      "Iteration 22611, loss = 1.44984854\n",
      "Iteration 22612, loss = 1.44984709\n",
      "Iteration 22613, loss = 1.44984563\n",
      "Iteration 22614, loss = 1.44984417\n",
      "Iteration 22615, loss = 1.44984272\n",
      "Iteration 22616, loss = 1.44984126\n",
      "Iteration 22617, loss = 1.44983981\n",
      "Iteration 22618, loss = 1.44983835\n",
      "Iteration 22619, loss = 1.44983690\n",
      "Iteration 22620, loss = 1.44983544\n",
      "Iteration 22621, loss = 1.44983399\n",
      "Iteration 22622, loss = 1.44983254\n",
      "Iteration 22623, loss = 1.44983108\n",
      "Iteration 22624, loss = 1.44982963\n",
      "Iteration 22625, loss = 1.44982817\n",
      "Iteration 22626, loss = 1.44982672\n",
      "Iteration 22627, loss = 1.44982527\n",
      "Iteration 22628, loss = 1.44982381\n",
      "Iteration 22629, loss = 1.44982236\n",
      "Iteration 22630, loss = 1.44982091\n",
      "Iteration 22631, loss = 1.44981945\n",
      "Iteration 22632, loss = 1.44981800\n",
      "Iteration 22633, loss = 1.44981655\n",
      "Iteration 22634, loss = 1.44981510\n",
      "Iteration 22635, loss = 1.44981364\n",
      "Iteration 22636, loss = 1.44981219\n",
      "Iteration 22637, loss = 1.44981074\n",
      "Iteration 22638, loss = 1.44980929\n",
      "Iteration 22639, loss = 1.44980784\n",
      "Iteration 22640, loss = 1.44980639\n",
      "Iteration 22641, loss = 1.44980493\n",
      "Iteration 22642, loss = 1.44980348\n",
      "Iteration 22643, loss = 1.44980203\n",
      "Iteration 22644, loss = 1.44980058\n",
      "Iteration 22645, loss = 1.44979913\n",
      "Iteration 22646, loss = 1.44979768\n",
      "Iteration 22647, loss = 1.44979623\n",
      "Iteration 22648, loss = 1.44979478\n",
      "Iteration 22649, loss = 1.44979333\n",
      "Iteration 22650, loss = 1.44979188\n",
      "Iteration 22651, loss = 1.44979043\n",
      "Iteration 22652, loss = 1.44978898\n",
      "Iteration 22653, loss = 1.44978753\n",
      "Iteration 22654, loss = 1.44978608\n",
      "Iteration 22655, loss = 1.44978463\n",
      "Iteration 22656, loss = 1.44978318\n",
      "Iteration 22657, loss = 1.44978173\n",
      "Iteration 22658, loss = 1.44978028\n",
      "Iteration 22659, loss = 1.44977884\n",
      "Iteration 22660, loss = 1.44977739\n",
      "Iteration 22661, loss = 1.44977594\n",
      "Iteration 22662, loss = 1.44977449\n",
      "Iteration 22663, loss = 1.44977304\n",
      "Iteration 22664, loss = 1.44977159\n",
      "Iteration 22665, loss = 1.44977015\n",
      "Iteration 22666, loss = 1.44976870\n",
      "Iteration 22667, loss = 1.44976725\n",
      "Iteration 22668, loss = 1.44976581\n",
      "Iteration 22669, loss = 1.44976436\n",
      "Iteration 22670, loss = 1.44976291\n",
      "Iteration 22671, loss = 1.44976146\n",
      "Iteration 22672, loss = 1.44976002\n",
      "Iteration 22673, loss = 1.44975857\n",
      "Iteration 22674, loss = 1.44975712\n",
      "Iteration 22675, loss = 1.44975568\n",
      "Iteration 22676, loss = 1.44975423\n",
      "Iteration 22677, loss = 1.44975279\n",
      "Iteration 22678, loss = 1.44975134\n",
      "Iteration 22679, loss = 1.44974990\n",
      "Iteration 22680, loss = 1.44974845\n",
      "Iteration 22681, loss = 1.44974700\n",
      "Iteration 22682, loss = 1.44974556\n",
      "Iteration 22683, loss = 1.44974411\n",
      "Iteration 22684, loss = 1.44974267\n",
      "Iteration 22685, loss = 1.44974123\n",
      "Iteration 22686, loss = 1.44973978\n",
      "Iteration 22687, loss = 1.44973834\n",
      "Iteration 22688, loss = 1.44973689\n",
      "Iteration 22689, loss = 1.44973545\n",
      "Iteration 22690, loss = 1.44973400\n",
      "Iteration 22691, loss = 1.44973256\n",
      "Iteration 22692, loss = 1.44973112\n",
      "Iteration 22693, loss = 1.44972967\n",
      "Iteration 22694, loss = 1.44972823\n",
      "Iteration 22695, loss = 1.44972679\n",
      "Iteration 22696, loss = 1.44972534\n",
      "Iteration 22697, loss = 1.44972390\n",
      "Iteration 22698, loss = 1.44972246\n",
      "Iteration 22699, loss = 1.44972102\n",
      "Iteration 22700, loss = 1.44971957\n",
      "Iteration 22701, loss = 1.44971813\n",
      "Iteration 22702, loss = 1.44971669\n",
      "Iteration 22703, loss = 1.44971525\n",
      "Iteration 22704, loss = 1.44971381\n",
      "Iteration 22705, loss = 1.44971236\n",
      "Iteration 22706, loss = 1.44971092\n",
      "Iteration 22707, loss = 1.44970948\n",
      "Iteration 22708, loss = 1.44970804\n",
      "Iteration 22709, loss = 1.44970660\n",
      "Iteration 22710, loss = 1.44970516\n",
      "Iteration 22711, loss = 1.44970372\n",
      "Iteration 22712, loss = 1.44970228\n",
      "Iteration 22713, loss = 1.44970084\n",
      "Iteration 22714, loss = 1.44969940\n",
      "Iteration 22715, loss = 1.44969796\n",
      "Iteration 22716, loss = 1.44969652\n",
      "Iteration 22717, loss = 1.44969508\n",
      "Iteration 22718, loss = 1.44969364\n",
      "Iteration 22719, loss = 1.44969220\n",
      "Iteration 22720, loss = 1.44969076\n",
      "Iteration 22721, loss = 1.44968932\n",
      "Iteration 22722, loss = 1.44968788\n",
      "Iteration 22723, loss = 1.44968644\n",
      "Iteration 22724, loss = 1.44968500\n",
      "Iteration 22725, loss = 1.44968356\n",
      "Iteration 22726, loss = 1.44968213\n",
      "Iteration 22727, loss = 1.44968069\n",
      "Iteration 22728, loss = 1.44967925\n",
      "Iteration 22729, loss = 1.44967781\n",
      "Iteration 22730, loss = 1.44967637\n",
      "Iteration 22731, loss = 1.44967494\n",
      "Iteration 22732, loss = 1.44967350\n",
      "Iteration 22733, loss = 1.44967206\n",
      "Iteration 22734, loss = 1.44967062\n",
      "Iteration 22735, loss = 1.44966919\n",
      "Iteration 22736, loss = 1.44966775\n",
      "Iteration 22737, loss = 1.44966631\n",
      "Iteration 22738, loss = 1.44966488\n",
      "Iteration 22739, loss = 1.44966344\n",
      "Iteration 22740, loss = 1.44966200\n",
      "Iteration 22741, loss = 1.44966057\n",
      "Iteration 22742, loss = 1.44965913\n",
      "Iteration 22743, loss = 1.44965770\n",
      "Iteration 22744, loss = 1.44965626\n",
      "Iteration 22745, loss = 1.44965482\n",
      "Iteration 22746, loss = 1.44965339\n",
      "Iteration 22747, loss = 1.44965195\n",
      "Iteration 22748, loss = 1.44965052\n",
      "Iteration 22749, loss = 1.44964908\n",
      "Iteration 22750, loss = 1.44964765\n",
      "Iteration 22751, loss = 1.44964621\n",
      "Iteration 22752, loss = 1.44964478\n",
      "Iteration 22753, loss = 1.44964334\n",
      "Iteration 22754, loss = 1.44964191\n",
      "Iteration 22755, loss = 1.44964048\n",
      "Iteration 22756, loss = 1.44963904\n",
      "Iteration 22757, loss = 1.44963761\n",
      "Iteration 22758, loss = 1.44963618\n",
      "Iteration 22759, loss = 1.44963474\n",
      "Iteration 22760, loss = 1.44963331\n",
      "Iteration 22761, loss = 1.44963188\n",
      "Iteration 22762, loss = 1.44963044\n",
      "Iteration 22763, loss = 1.44962901\n",
      "Iteration 22764, loss = 1.44962758\n",
      "Iteration 22765, loss = 1.44962614\n",
      "Iteration 22766, loss = 1.44962471\n",
      "Iteration 22767, loss = 1.44962328\n",
      "Iteration 22768, loss = 1.44962185\n",
      "Iteration 22769, loss = 1.44962042\n",
      "Iteration 22770, loss = 1.44961898\n",
      "Iteration 22771, loss = 1.44961755\n",
      "Iteration 22772, loss = 1.44961612\n",
      "Iteration 22773, loss = 1.44961469\n",
      "Iteration 22774, loss = 1.44961326\n",
      "Iteration 22775, loss = 1.44961183\n",
      "Iteration 22776, loss = 1.44961040\n",
      "Iteration 22777, loss = 1.44960897\n",
      "Iteration 22778, loss = 1.44960753\n",
      "Iteration 22779, loss = 1.44960610\n",
      "Iteration 22780, loss = 1.44960467\n",
      "Iteration 22781, loss = 1.44960324\n",
      "Iteration 22782, loss = 1.44960181\n",
      "Iteration 22783, loss = 1.44960038\n",
      "Iteration 22784, loss = 1.44959895\n",
      "Iteration 22785, loss = 1.44959752\n",
      "Iteration 22786, loss = 1.44959610\n",
      "Iteration 22787, loss = 1.44959467\n",
      "Iteration 22788, loss = 1.44959324\n",
      "Iteration 22789, loss = 1.44959181\n",
      "Iteration 22790, loss = 1.44959038\n",
      "Iteration 22791, loss = 1.44958895\n",
      "Iteration 22792, loss = 1.44958752\n",
      "Iteration 22793, loss = 1.44958609\n",
      "Iteration 22794, loss = 1.44958467\n",
      "Iteration 22795, loss = 1.44958324\n",
      "Iteration 22796, loss = 1.44958181\n",
      "Iteration 22797, loss = 1.44958038\n",
      "Iteration 22798, loss = 1.44957895\n",
      "Iteration 22799, loss = 1.44957753\n",
      "Iteration 22800, loss = 1.44957610\n",
      "Iteration 22801, loss = 1.44957467\n",
      "Iteration 22802, loss = 1.44957325\n",
      "Iteration 22803, loss = 1.44957182\n",
      "Iteration 22804, loss = 1.44957039\n",
      "Iteration 22805, loss = 1.44956897\n",
      "Iteration 22806, loss = 1.44956754\n",
      "Iteration 22807, loss = 1.44956611\n",
      "Iteration 22808, loss = 1.44956469\n",
      "Iteration 22809, loss = 1.44956326\n",
      "Iteration 22810, loss = 1.44956183\n",
      "Iteration 22811, loss = 1.44956041\n",
      "Iteration 22812, loss = 1.44955898\n",
      "Iteration 22813, loss = 1.44955756\n",
      "Iteration 22814, loss = 1.44955613\n",
      "Iteration 22815, loss = 1.44955471\n",
      "Iteration 22816, loss = 1.44955328\n",
      "Iteration 22817, loss = 1.44955186\n",
      "Iteration 22818, loss = 1.44955043\n",
      "Iteration 22819, loss = 1.44954901\n",
      "Iteration 22820, loss = 1.44954758\n",
      "Iteration 22821, loss = 1.44954616\n",
      "Iteration 22822, loss = 1.44954474\n",
      "Iteration 22823, loss = 1.44954331\n",
      "Iteration 22824, loss = 1.44954189\n",
      "Iteration 22825, loss = 1.44954047\n",
      "Iteration 22826, loss = 1.44953904\n",
      "Iteration 22827, loss = 1.44953762\n",
      "Iteration 22828, loss = 1.44953620\n",
      "Iteration 22829, loss = 1.44953477\n",
      "Iteration 22830, loss = 1.44953335\n",
      "Iteration 22831, loss = 1.44953193\n",
      "Iteration 22832, loss = 1.44953050\n",
      "Iteration 22833, loss = 1.44952908\n",
      "Iteration 22834, loss = 1.44952766\n",
      "Iteration 22835, loss = 1.44952624\n",
      "Iteration 22836, loss = 1.44952482\n",
      "Iteration 22837, loss = 1.44952339\n",
      "Iteration 22838, loss = 1.44952197\n",
      "Iteration 22839, loss = 1.44952055\n",
      "Iteration 22840, loss = 1.44951913\n",
      "Iteration 22841, loss = 1.44951771\n",
      "Iteration 22842, loss = 1.44951629\n",
      "Iteration 22843, loss = 1.44951487\n",
      "Iteration 22844, loss = 1.44951345\n",
      "Iteration 22845, loss = 1.44951203\n",
      "Iteration 22846, loss = 1.44951060\n",
      "Iteration 22847, loss = 1.44950918\n",
      "Iteration 22848, loss = 1.44950776\n",
      "Iteration 22849, loss = 1.44950634\n",
      "Iteration 22850, loss = 1.44950492\n",
      "Iteration 22851, loss = 1.44950350\n",
      "Iteration 22852, loss = 1.44950209\n",
      "Iteration 22853, loss = 1.44950067\n",
      "Iteration 22854, loss = 1.44949925\n",
      "Iteration 22855, loss = 1.44949783\n",
      "Iteration 22856, loss = 1.44949641\n",
      "Iteration 22857, loss = 1.44949499\n",
      "Iteration 22858, loss = 1.44949357\n",
      "Iteration 22859, loss = 1.44949215\n",
      "Iteration 22860, loss = 1.44949073\n",
      "Iteration 22861, loss = 1.44948932\n",
      "Iteration 22862, loss = 1.44948790\n",
      "Iteration 22863, loss = 1.44948648\n",
      "Iteration 22864, loss = 1.44948506\n",
      "Iteration 22865, loss = 1.44948364\n",
      "Iteration 22866, loss = 1.44948223\n",
      "Iteration 22867, loss = 1.44948081\n",
      "Iteration 22868, loss = 1.44947939\n",
      "Iteration 22869, loss = 1.44947798\n",
      "Iteration 22870, loss = 1.44947656\n",
      "Iteration 22871, loss = 1.44947514\n",
      "Iteration 22872, loss = 1.44947373\n",
      "Iteration 22873, loss = 1.44947231\n",
      "Iteration 22874, loss = 1.44947089\n",
      "Iteration 22875, loss = 1.44946948\n",
      "Iteration 22876, loss = 1.44946806\n",
      "Iteration 22877, loss = 1.44946664\n",
      "Iteration 22878, loss = 1.44946523\n",
      "Iteration 22879, loss = 1.44946381\n",
      "Iteration 22880, loss = 1.44946240\n",
      "Iteration 22881, loss = 1.44946098\n",
      "Iteration 22882, loss = 1.44945957\n",
      "Iteration 22883, loss = 1.44945815\n",
      "Iteration 22884, loss = 1.44945674\n",
      "Iteration 22885, loss = 1.44945532\n",
      "Iteration 22886, loss = 1.44945391\n",
      "Iteration 22887, loss = 1.44945249\n",
      "Iteration 22888, loss = 1.44945108\n",
      "Iteration 22889, loss = 1.44944967\n",
      "Iteration 22890, loss = 1.44944825\n",
      "Iteration 22891, loss = 1.44944684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22892, loss = 1.44944542\n",
      "Iteration 22893, loss = 1.44944401\n",
      "Iteration 22894, loss = 1.44944260\n",
      "Iteration 22895, loss = 1.44944118\n",
      "Iteration 22896, loss = 1.44943977\n",
      "Iteration 22897, loss = 1.44943836\n",
      "Iteration 22898, loss = 1.44943695\n",
      "Iteration 22899, loss = 1.44943553\n",
      "Iteration 22900, loss = 1.44943412\n",
      "Iteration 22901, loss = 1.44943271\n",
      "Iteration 22902, loss = 1.44943130\n",
      "Iteration 22903, loss = 1.44942988\n",
      "Iteration 22904, loss = 1.44942847\n",
      "Iteration 22905, loss = 1.44942706\n",
      "Iteration 22906, loss = 1.44942565\n",
      "Iteration 22907, loss = 1.44942424\n",
      "Iteration 22908, loss = 1.44942283\n",
      "Iteration 22909, loss = 1.44942142\n",
      "Iteration 22910, loss = 1.44942000\n",
      "Iteration 22911, loss = 1.44941859\n",
      "Iteration 22912, loss = 1.44941718\n",
      "Iteration 22913, loss = 1.44941577\n",
      "Iteration 22914, loss = 1.44941436\n",
      "Iteration 22915, loss = 1.44941295\n",
      "Iteration 22916, loss = 1.44941154\n",
      "Iteration 22917, loss = 1.44941013\n",
      "Iteration 22918, loss = 1.44940872\n",
      "Iteration 22919, loss = 1.44940731\n",
      "Iteration 22920, loss = 1.44940590\n",
      "Iteration 22921, loss = 1.44940449\n",
      "Iteration 22922, loss = 1.44940308\n",
      "Iteration 22923, loss = 1.44940168\n",
      "Iteration 22924, loss = 1.44940027\n",
      "Iteration 22925, loss = 1.44939886\n",
      "Iteration 22926, loss = 1.44939745\n",
      "Iteration 22927, loss = 1.44939604\n",
      "Iteration 22928, loss = 1.44939463\n",
      "Iteration 22929, loss = 1.44939322\n",
      "Iteration 22930, loss = 1.44939182\n",
      "Iteration 22931, loss = 1.44939041\n",
      "Iteration 22932, loss = 1.44938900\n",
      "Iteration 22933, loss = 1.44938759\n",
      "Iteration 22934, loss = 1.44938619\n",
      "Iteration 22935, loss = 1.44938478\n",
      "Iteration 22936, loss = 1.44938337\n",
      "Iteration 22937, loss = 1.44938196\n",
      "Iteration 22938, loss = 1.44938056\n",
      "Iteration 22939, loss = 1.44937915\n",
      "Iteration 22940, loss = 1.44937774\n",
      "Iteration 22941, loss = 1.44937634\n",
      "Iteration 22942, loss = 1.44937493\n",
      "Iteration 22943, loss = 1.44937352\n",
      "Iteration 22944, loss = 1.44937212\n",
      "Iteration 22945, loss = 1.44937071\n",
      "Iteration 22946, loss = 1.44936931\n",
      "Iteration 22947, loss = 1.44936790\n",
      "Iteration 22948, loss = 1.44936650\n",
      "Iteration 22949, loss = 1.44936509\n",
      "Iteration 22950, loss = 1.44936369\n",
      "Iteration 22951, loss = 1.44936228\n",
      "Iteration 22952, loss = 1.44936088\n",
      "Iteration 22953, loss = 1.44935947\n",
      "Iteration 22954, loss = 1.44935807\n",
      "Iteration 22955, loss = 1.44935666\n",
      "Iteration 22956, loss = 1.44935526\n",
      "Iteration 22957, loss = 1.44935385\n",
      "Iteration 22958, loss = 1.44935245\n",
      "Iteration 22959, loss = 1.44935105\n",
      "Iteration 22960, loss = 1.44934964\n",
      "Iteration 22961, loss = 1.44934824\n",
      "Iteration 22962, loss = 1.44934684\n",
      "Iteration 22963, loss = 1.44934543\n",
      "Iteration 22964, loss = 1.44934403\n",
      "Iteration 22965, loss = 1.44934263\n",
      "Iteration 22966, loss = 1.44934122\n",
      "Iteration 22967, loss = 1.44933982\n",
      "Iteration 22968, loss = 1.44933842\n",
      "Iteration 22969, loss = 1.44933702\n",
      "Iteration 22970, loss = 1.44933561\n",
      "Iteration 22971, loss = 1.44933421\n",
      "Iteration 22972, loss = 1.44933281\n",
      "Iteration 22973, loss = 1.44933141\n",
      "Iteration 22974, loss = 1.44933001\n",
      "Iteration 22975, loss = 1.44932861\n",
      "Iteration 22976, loss = 1.44932720\n",
      "Iteration 22977, loss = 1.44932580\n",
      "Iteration 22978, loss = 1.44932440\n",
      "Iteration 22979, loss = 1.44932300\n",
      "Iteration 22980, loss = 1.44932160\n",
      "Iteration 22981, loss = 1.44932020\n",
      "Iteration 22982, loss = 1.44931880\n",
      "Iteration 22983, loss = 1.44931740\n",
      "Iteration 22984, loss = 1.44931600\n",
      "Iteration 22985, loss = 1.44931460\n",
      "Iteration 22986, loss = 1.44931320\n",
      "Iteration 22987, loss = 1.44931180\n",
      "Iteration 22988, loss = 1.44931040\n",
      "Iteration 22989, loss = 1.44930900\n",
      "Iteration 22990, loss = 1.44930760\n",
      "Iteration 22991, loss = 1.44930620\n",
      "Iteration 22992, loss = 1.44930480\n",
      "Iteration 22993, loss = 1.44930341\n",
      "Iteration 22994, loss = 1.44930201\n",
      "Iteration 22995, loss = 1.44930061\n",
      "Iteration 22996, loss = 1.44929921\n",
      "Iteration 22997, loss = 1.44929781\n",
      "Iteration 22998, loss = 1.44929641\n",
      "Iteration 22999, loss = 1.44929502\n",
      "Iteration 23000, loss = 1.44929362\n",
      "Iteration 23001, loss = 1.44929222\n",
      "Iteration 23002, loss = 1.44929082\n",
      "Iteration 23003, loss = 1.44928943\n",
      "Iteration 23004, loss = 1.44928803\n",
      "Iteration 23005, loss = 1.44928663\n",
      "Iteration 23006, loss = 1.44928523\n",
      "Iteration 23007, loss = 1.44928384\n",
      "Iteration 23008, loss = 1.44928244\n",
      "Iteration 23009, loss = 1.44928104\n",
      "Iteration 23010, loss = 1.44927965\n",
      "Iteration 23011, loss = 1.44927825\n",
      "Iteration 23012, loss = 1.44927686\n",
      "Iteration 23013, loss = 1.44927546\n",
      "Iteration 23014, loss = 1.44927406\n",
      "Iteration 23015, loss = 1.44927267\n",
      "Iteration 23016, loss = 1.44927127\n",
      "Iteration 23017, loss = 1.44926988\n",
      "Iteration 23018, loss = 1.44926848\n",
      "Iteration 23019, loss = 1.44926709\n",
      "Iteration 23020, loss = 1.44926569\n",
      "Iteration 23021, loss = 1.44926430\n",
      "Iteration 23022, loss = 1.44926290\n",
      "Iteration 23023, loss = 1.44926151\n",
      "Iteration 23024, loss = 1.44926011\n",
      "Iteration 23025, loss = 1.44925872\n",
      "Iteration 23026, loss = 1.44925733\n",
      "Iteration 23027, loss = 1.44925593\n",
      "Iteration 23028, loss = 1.44925454\n",
      "Iteration 23029, loss = 1.44925314\n",
      "Iteration 23030, loss = 1.44925175\n",
      "Iteration 23031, loss = 1.44925036\n",
      "Iteration 23032, loss = 1.44924896\n",
      "Iteration 23033, loss = 1.44924757\n",
      "Iteration 23034, loss = 1.44924618\n",
      "Iteration 23035, loss = 1.44924479\n",
      "Iteration 23036, loss = 1.44924339\n",
      "Iteration 23037, loss = 1.44924200\n",
      "Iteration 23038, loss = 1.44924061\n",
      "Iteration 23039, loss = 1.44923922\n",
      "Iteration 23040, loss = 1.44923782\n",
      "Iteration 23041, loss = 1.44923643\n",
      "Iteration 23042, loss = 1.44923504\n",
      "Iteration 23043, loss = 1.44923365\n",
      "Iteration 23044, loss = 1.44923226\n",
      "Iteration 23045, loss = 1.44923087\n",
      "Iteration 23046, loss = 1.44922948\n",
      "Iteration 23047, loss = 1.44922808\n",
      "Iteration 23048, loss = 1.44922669\n",
      "Iteration 23049, loss = 1.44922530\n",
      "Iteration 23050, loss = 1.44922391\n",
      "Iteration 23051, loss = 1.44922252\n",
      "Iteration 23052, loss = 1.44922113\n",
      "Iteration 23053, loss = 1.44921974\n",
      "Iteration 23054, loss = 1.44921835\n",
      "Iteration 23055, loss = 1.44921696\n",
      "Iteration 23056, loss = 1.44921557\n",
      "Iteration 23057, loss = 1.44921418\n",
      "Iteration 23058, loss = 1.44921279\n",
      "Iteration 23059, loss = 1.44921140\n",
      "Iteration 23060, loss = 1.44921002\n",
      "Iteration 23061, loss = 1.44920863\n",
      "Iteration 23062, loss = 1.44920724\n",
      "Iteration 23063, loss = 1.44920585\n",
      "Iteration 23064, loss = 1.44920446\n",
      "Iteration 23065, loss = 1.44920307\n",
      "Iteration 23066, loss = 1.44920168\n",
      "Iteration 23067, loss = 1.44920030\n",
      "Iteration 23068, loss = 1.44919891\n",
      "Iteration 23069, loss = 1.44919752\n",
      "Iteration 23070, loss = 1.44919613\n",
      "Iteration 23071, loss = 1.44919474\n",
      "Iteration 23072, loss = 1.44919336\n",
      "Iteration 23073, loss = 1.44919197\n",
      "Iteration 23074, loss = 1.44919058\n",
      "Iteration 23075, loss = 1.44918920\n",
      "Iteration 23076, loss = 1.44918781\n",
      "Iteration 23077, loss = 1.44918642\n",
      "Iteration 23078, loss = 1.44918504\n",
      "Iteration 23079, loss = 1.44918365\n",
      "Iteration 23080, loss = 1.44918226\n",
      "Iteration 23081, loss = 1.44918088\n",
      "Iteration 23082, loss = 1.44917949\n",
      "Iteration 23083, loss = 1.44917811\n",
      "Iteration 23084, loss = 1.44917672\n",
      "Iteration 23085, loss = 1.44917533\n",
      "Iteration 23086, loss = 1.44917395\n",
      "Iteration 23087, loss = 1.44917256\n",
      "Iteration 23088, loss = 1.44917118\n",
      "Iteration 23089, loss = 1.44916979\n",
      "Iteration 23090, loss = 1.44916841\n",
      "Iteration 23091, loss = 1.44916702\n",
      "Iteration 23092, loss = 1.44916564\n",
      "Iteration 23093, loss = 1.44916426\n",
      "Iteration 23094, loss = 1.44916287\n",
      "Iteration 23095, loss = 1.44916149\n",
      "Iteration 23096, loss = 1.44916010\n",
      "Iteration 23097, loss = 1.44915872\n",
      "Iteration 23098, loss = 1.44915734\n",
      "Iteration 23099, loss = 1.44915595\n",
      "Iteration 23100, loss = 1.44915457\n",
      "Iteration 23101, loss = 1.44915319\n",
      "Iteration 23102, loss = 1.44915180\n",
      "Iteration 23103, loss = 1.44915042\n",
      "Iteration 23104, loss = 1.44914904\n",
      "Iteration 23105, loss = 1.44914765\n",
      "Iteration 23106, loss = 1.44914627\n",
      "Iteration 23107, loss = 1.44914489\n",
      "Iteration 23108, loss = 1.44914351\n",
      "Iteration 23109, loss = 1.44914213\n",
      "Iteration 23110, loss = 1.44914074\n",
      "Iteration 23111, loss = 1.44913936\n",
      "Iteration 23112, loss = 1.44913798\n",
      "Iteration 23113, loss = 1.44913660\n",
      "Iteration 23114, loss = 1.44913522\n",
      "Iteration 23115, loss = 1.44913384\n",
      "Iteration 23116, loss = 1.44913246\n",
      "Iteration 23117, loss = 1.44913107\n",
      "Iteration 23118, loss = 1.44912969\n",
      "Iteration 23119, loss = 1.44912831\n",
      "Iteration 23120, loss = 1.44912693\n",
      "Iteration 23121, loss = 1.44912555\n",
      "Iteration 23122, loss = 1.44912417\n",
      "Iteration 23123, loss = 1.44912279\n",
      "Iteration 23124, loss = 1.44912141\n",
      "Iteration 23125, loss = 1.44912003\n",
      "Iteration 23126, loss = 1.44911865\n",
      "Iteration 23127, loss = 1.44911727\n",
      "Iteration 23128, loss = 1.44911589\n",
      "Iteration 23129, loss = 1.44911451\n",
      "Iteration 23130, loss = 1.44911314\n",
      "Iteration 23131, loss = 1.44911176\n",
      "Iteration 23132, loss = 1.44911038\n",
      "Iteration 23133, loss = 1.44910900\n",
      "Iteration 23134, loss = 1.44910762\n",
      "Iteration 23135, loss = 1.44910624\n",
      "Iteration 23136, loss = 1.44910486\n",
      "Iteration 23137, loss = 1.44910349\n",
      "Iteration 23138, loss = 1.44910211\n",
      "Iteration 23139, loss = 1.44910073\n",
      "Iteration 23140, loss = 1.44909935\n",
      "Iteration 23141, loss = 1.44909798\n",
      "Iteration 23142, loss = 1.44909660\n",
      "Iteration 23143, loss = 1.44909522\n",
      "Iteration 23144, loss = 1.44909384\n",
      "Iteration 23145, loss = 1.44909247\n",
      "Iteration 23146, loss = 1.44909109\n",
      "Iteration 23147, loss = 1.44908971\n",
      "Iteration 23148, loss = 1.44908834\n",
      "Iteration 23149, loss = 1.44908696\n",
      "Iteration 23150, loss = 1.44908558\n",
      "Iteration 23151, loss = 1.44908421\n",
      "Iteration 23152, loss = 1.44908283\n",
      "Iteration 23153, loss = 1.44908146\n",
      "Iteration 23154, loss = 1.44908008\n",
      "Iteration 23155, loss = 1.44907871\n",
      "Iteration 23156, loss = 1.44907733\n",
      "Iteration 23157, loss = 1.44907596\n",
      "Iteration 23158, loss = 1.44907458\n",
      "Iteration 23159, loss = 1.44907321\n",
      "Iteration 23160, loss = 1.44907183\n",
      "Iteration 23161, loss = 1.44907046\n",
      "Iteration 23162, loss = 1.44906908\n",
      "Iteration 23163, loss = 1.44906771\n",
      "Iteration 23164, loss = 1.44906633\n",
      "Iteration 23165, loss = 1.44906496\n",
      "Iteration 23166, loss = 1.44906359\n",
      "Iteration 23167, loss = 1.44906221\n",
      "Iteration 23168, loss = 1.44906084\n",
      "Iteration 23169, loss = 1.44905946\n",
      "Iteration 23170, loss = 1.44905809\n",
      "Iteration 23171, loss = 1.44905672\n",
      "Iteration 23172, loss = 1.44905534\n",
      "Iteration 23173, loss = 1.44905397\n",
      "Iteration 23174, loss = 1.44905260\n",
      "Iteration 23175, loss = 1.44905123\n",
      "Iteration 23176, loss = 1.44904985\n",
      "Iteration 23177, loss = 1.44904848\n",
      "Iteration 23178, loss = 1.44904711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23179, loss = 1.44904574\n",
      "Iteration 23180, loss = 1.44904437\n",
      "Iteration 23181, loss = 1.44904299\n",
      "Iteration 23182, loss = 1.44904162\n",
      "Iteration 23183, loss = 1.44904025\n",
      "Iteration 23184, loss = 1.44903888\n",
      "Iteration 23185, loss = 1.44903751\n",
      "Iteration 23186, loss = 1.44903614\n",
      "Iteration 23187, loss = 1.44903477\n",
      "Iteration 23188, loss = 1.44903340\n",
      "Iteration 23189, loss = 1.44903202\n",
      "Iteration 23190, loss = 1.44903065\n",
      "Iteration 23191, loss = 1.44902928\n",
      "Iteration 23192, loss = 1.44902791\n",
      "Iteration 23193, loss = 1.44902654\n",
      "Iteration 23194, loss = 1.44902517\n",
      "Iteration 23195, loss = 1.44902380\n",
      "Iteration 23196, loss = 1.44902243\n",
      "Iteration 23197, loss = 1.44902106\n",
      "Iteration 23198, loss = 1.44901970\n",
      "Iteration 23199, loss = 1.44901833\n",
      "Iteration 23200, loss = 1.44901696\n",
      "Iteration 23201, loss = 1.44901559\n",
      "Iteration 23202, loss = 1.44901422\n",
      "Iteration 23203, loss = 1.44901285\n",
      "Iteration 23204, loss = 1.44901148\n",
      "Iteration 23205, loss = 1.44901011\n",
      "Iteration 23206, loss = 1.44900875\n",
      "Iteration 23207, loss = 1.44900738\n",
      "Iteration 23208, loss = 1.44900601\n",
      "Iteration 23209, loss = 1.44900464\n",
      "Iteration 23210, loss = 1.44900327\n",
      "Iteration 23211, loss = 1.44900191\n",
      "Iteration 23212, loss = 1.44900054\n",
      "Iteration 23213, loss = 1.44899917\n",
      "Iteration 23214, loss = 1.44899780\n",
      "Iteration 23215, loss = 1.44899644\n",
      "Iteration 23216, loss = 1.44899507\n",
      "Iteration 23217, loss = 1.44899370\n",
      "Iteration 23218, loss = 1.44899234\n",
      "Iteration 23219, loss = 1.44899097\n",
      "Iteration 23220, loss = 1.44898960\n",
      "Iteration 23221, loss = 1.44898824\n",
      "Iteration 23222, loss = 1.44898687\n",
      "Iteration 23223, loss = 1.44898551\n",
      "Iteration 23224, loss = 1.44898414\n",
      "Iteration 23225, loss = 1.44898278\n",
      "Iteration 23226, loss = 1.44898141\n",
      "Iteration 23227, loss = 1.44898005\n",
      "Iteration 23228, loss = 1.44897868\n",
      "Iteration 23229, loss = 1.44897732\n",
      "Iteration 23230, loss = 1.44897595\n",
      "Iteration 23231, loss = 1.44897459\n",
      "Iteration 23232, loss = 1.44897322\n",
      "Iteration 23233, loss = 1.44897186\n",
      "Iteration 23234, loss = 1.44897049\n",
      "Iteration 23235, loss = 1.44896913\n",
      "Iteration 23236, loss = 1.44896776\n",
      "Iteration 23237, loss = 1.44896640\n",
      "Iteration 23238, loss = 1.44896504\n",
      "Iteration 23239, loss = 1.44896367\n",
      "Iteration 23240, loss = 1.44896231\n",
      "Iteration 23241, loss = 1.44896095\n",
      "Iteration 23242, loss = 1.44895958\n",
      "Iteration 23243, loss = 1.44895822\n",
      "Iteration 23244, loss = 1.44895686\n",
      "Iteration 23245, loss = 1.44895549\n",
      "Iteration 23246, loss = 1.44895413\n",
      "Iteration 23247, loss = 1.44895277\n",
      "Iteration 23248, loss = 1.44895141\n",
      "Iteration 23249, loss = 1.44895004\n",
      "Iteration 23250, loss = 1.44894868\n",
      "Iteration 23251, loss = 1.44894732\n",
      "Iteration 23252, loss = 1.44894596\n",
      "Iteration 23253, loss = 1.44894460\n",
      "Iteration 23254, loss = 1.44894324\n",
      "Iteration 23255, loss = 1.44894187\n",
      "Iteration 23256, loss = 1.44894051\n",
      "Iteration 23257, loss = 1.44893915\n",
      "Iteration 23258, loss = 1.44893779\n",
      "Iteration 23259, loss = 1.44893643\n",
      "Iteration 23260, loss = 1.44893507\n",
      "Iteration 23261, loss = 1.44893371\n",
      "Iteration 23262, loss = 1.44893235\n",
      "Iteration 23263, loss = 1.44893099\n",
      "Iteration 23264, loss = 1.44892963\n",
      "Iteration 23265, loss = 1.44892827\n",
      "Iteration 23266, loss = 1.44892691\n",
      "Iteration 23267, loss = 1.44892555\n",
      "Iteration 23268, loss = 1.44892419\n",
      "Iteration 23269, loss = 1.44892283\n",
      "Iteration 23270, loss = 1.44892147\n",
      "Iteration 23271, loss = 1.44892011\n",
      "Iteration 23272, loss = 1.44891875\n",
      "Iteration 23273, loss = 1.44891740\n",
      "Iteration 23274, loss = 1.44891604\n",
      "Iteration 23275, loss = 1.44891468\n",
      "Iteration 23276, loss = 1.44891332\n",
      "Iteration 23277, loss = 1.44891196\n",
      "Iteration 23278, loss = 1.44891060\n",
      "Iteration 23279, loss = 1.44890925\n",
      "Iteration 23280, loss = 1.44890789\n",
      "Iteration 23281, loss = 1.44890653\n",
      "Iteration 23282, loss = 1.44890517\n",
      "Iteration 23283, loss = 1.44890381\n",
      "Iteration 23284, loss = 1.44890246\n",
      "Iteration 23285, loss = 1.44890110\n",
      "Iteration 23286, loss = 1.44889974\n",
      "Iteration 23287, loss = 1.44889839\n",
      "Iteration 23288, loss = 1.44889703\n",
      "Iteration 23289, loss = 1.44889567\n",
      "Iteration 23290, loss = 1.44889432\n",
      "Iteration 23291, loss = 1.44889296\n",
      "Iteration 23292, loss = 1.44889160\n",
      "Iteration 23293, loss = 1.44889025\n",
      "Iteration 23294, loss = 1.44888889\n",
      "Iteration 23295, loss = 1.44888754\n",
      "Iteration 23296, loss = 1.44888618\n",
      "Iteration 23297, loss = 1.44888483\n",
      "Iteration 23298, loss = 1.44888347\n",
      "Iteration 23299, loss = 1.44888212\n",
      "Iteration 23300, loss = 1.44888076\n",
      "Iteration 23301, loss = 1.44887941\n",
      "Iteration 23302, loss = 1.44887805\n",
      "Iteration 23303, loss = 1.44887670\n",
      "Iteration 23304, loss = 1.44887534\n",
      "Iteration 23305, loss = 1.44887399\n",
      "Iteration 23306, loss = 1.44887263\n",
      "Iteration 23307, loss = 1.44887128\n",
      "Iteration 23308, loss = 1.44886993\n",
      "Iteration 23309, loss = 1.44886857\n",
      "Iteration 23310, loss = 1.44886722\n",
      "Iteration 23311, loss = 1.44886586\n",
      "Iteration 23312, loss = 1.44886451\n",
      "Iteration 23313, loss = 1.44886316\n",
      "Iteration 23314, loss = 1.44886180\n",
      "Iteration 23315, loss = 1.44886045\n",
      "Iteration 23316, loss = 1.44885910\n",
      "Iteration 23317, loss = 1.44885775\n",
      "Iteration 23318, loss = 1.44885639\n",
      "Iteration 23319, loss = 1.44885504\n",
      "Iteration 23320, loss = 1.44885369\n",
      "Iteration 23321, loss = 1.44885234\n",
      "Iteration 23322, loss = 1.44885099\n",
      "Iteration 23323, loss = 1.44884963\n",
      "Iteration 23324, loss = 1.44884828\n",
      "Iteration 23325, loss = 1.44884693\n",
      "Iteration 23326, loss = 1.44884558\n",
      "Iteration 23327, loss = 1.44884423\n",
      "Iteration 23328, loss = 1.44884288\n",
      "Iteration 23329, loss = 1.44884153\n",
      "Iteration 23330, loss = 1.44884017\n",
      "Iteration 23331, loss = 1.44883882\n",
      "Iteration 23332, loss = 1.44883747\n",
      "Iteration 23333, loss = 1.44883612\n",
      "Iteration 23334, loss = 1.44883477\n",
      "Iteration 23335, loss = 1.44883342\n",
      "Iteration 23336, loss = 1.44883207\n",
      "Iteration 23337, loss = 1.44883072\n",
      "Iteration 23338, loss = 1.44882937\n",
      "Iteration 23339, loss = 1.44882802\n",
      "Iteration 23340, loss = 1.44882667\n",
      "Iteration 23341, loss = 1.44882532\n",
      "Iteration 23342, loss = 1.44882398\n",
      "Iteration 23343, loss = 1.44882263\n",
      "Iteration 23344, loss = 1.44882128\n",
      "Iteration 23345, loss = 1.44881993\n",
      "Iteration 23346, loss = 1.44881858\n",
      "Iteration 23347, loss = 1.44881723\n",
      "Iteration 23348, loss = 1.44881588\n",
      "Iteration 23349, loss = 1.44881454\n",
      "Iteration 23350, loss = 1.44881319\n",
      "Iteration 23351, loss = 1.44881184\n",
      "Iteration 23352, loss = 1.44881049\n",
      "Iteration 23353, loss = 1.44880914\n",
      "Iteration 23354, loss = 1.44880780\n",
      "Iteration 23355, loss = 1.44880645\n",
      "Iteration 23356, loss = 1.44880510\n",
      "Iteration 23357, loss = 1.44880375\n",
      "Iteration 23358, loss = 1.44880241\n",
      "Iteration 23359, loss = 1.44880106\n",
      "Iteration 23360, loss = 1.44879971\n",
      "Iteration 23361, loss = 1.44879837\n",
      "Iteration 23362, loss = 1.44879702\n",
      "Iteration 23363, loss = 1.44879568\n",
      "Iteration 23364, loss = 1.44879433\n",
      "Iteration 23365, loss = 1.44879298\n",
      "Iteration 23366, loss = 1.44879164\n",
      "Iteration 23367, loss = 1.44879029\n",
      "Iteration 23368, loss = 1.44878895\n",
      "Iteration 23369, loss = 1.44878760\n",
      "Iteration 23370, loss = 1.44878626\n",
      "Iteration 23371, loss = 1.44878491\n",
      "Iteration 23372, loss = 1.44878357\n",
      "Iteration 23373, loss = 1.44878222\n",
      "Iteration 23374, loss = 1.44878088\n",
      "Iteration 23375, loss = 1.44877953\n",
      "Iteration 23376, loss = 1.44877819\n",
      "Iteration 23377, loss = 1.44877684\n",
      "Iteration 23378, loss = 1.44877550\n",
      "Iteration 23379, loss = 1.44877415\n",
      "Iteration 23380, loss = 1.44877281\n",
      "Iteration 23381, loss = 1.44877147\n",
      "Iteration 23382, loss = 1.44877012\n",
      "Iteration 23383, loss = 1.44876878\n",
      "Iteration 23384, loss = 1.44876744\n",
      "Iteration 23385, loss = 1.44876609\n",
      "Iteration 23386, loss = 1.44876475\n",
      "Iteration 23387, loss = 1.44876341\n",
      "Iteration 23388, loss = 1.44876206\n",
      "Iteration 23389, loss = 1.44876072\n",
      "Iteration 23390, loss = 1.44875938\n",
      "Iteration 23391, loss = 1.44875804\n",
      "Iteration 23392, loss = 1.44875669\n",
      "Iteration 23393, loss = 1.44875535\n",
      "Iteration 23394, loss = 1.44875401\n",
      "Iteration 23395, loss = 1.44875267\n",
      "Iteration 23396, loss = 1.44875133\n",
      "Iteration 23397, loss = 1.44874999\n",
      "Iteration 23398, loss = 1.44874864\n",
      "Iteration 23399, loss = 1.44874730\n",
      "Iteration 23400, loss = 1.44874596\n",
      "Iteration 23401, loss = 1.44874462\n",
      "Iteration 23402, loss = 1.44874328\n",
      "Iteration 23403, loss = 1.44874194\n",
      "Iteration 23404, loss = 1.44874060\n",
      "Iteration 23405, loss = 1.44873926\n",
      "Iteration 23406, loss = 1.44873792\n",
      "Iteration 23407, loss = 1.44873658\n",
      "Iteration 23408, loss = 1.44873524\n",
      "Iteration 23409, loss = 1.44873390\n",
      "Iteration 23410, loss = 1.44873256\n",
      "Iteration 23411, loss = 1.44873122\n",
      "Iteration 23412, loss = 1.44872988\n",
      "Iteration 23413, loss = 1.44872854\n",
      "Iteration 23414, loss = 1.44872720\n",
      "Iteration 23415, loss = 1.44872586\n",
      "Iteration 23416, loss = 1.44872452\n",
      "Iteration 23417, loss = 1.44872318\n",
      "Iteration 23418, loss = 1.44872184\n",
      "Iteration 23419, loss = 1.44872051\n",
      "Iteration 23420, loss = 1.44871917\n",
      "Iteration 23421, loss = 1.44871783\n",
      "Iteration 23422, loss = 1.44871649\n",
      "Iteration 23423, loss = 1.44871515\n",
      "Iteration 23424, loss = 1.44871382\n",
      "Iteration 23425, loss = 1.44871248\n",
      "Iteration 23426, loss = 1.44871114\n",
      "Iteration 23427, loss = 1.44870980\n",
      "Iteration 23428, loss = 1.44870847\n",
      "Iteration 23429, loss = 1.44870713\n",
      "Iteration 23430, loss = 1.44870579\n",
      "Iteration 23431, loss = 1.44870445\n",
      "Iteration 23432, loss = 1.44870312\n",
      "Iteration 23433, loss = 1.44870178\n",
      "Iteration 23434, loss = 1.44870044\n",
      "Iteration 23435, loss = 1.44869911\n",
      "Iteration 23436, loss = 1.44869777\n",
      "Iteration 23437, loss = 1.44869644\n",
      "Iteration 23438, loss = 1.44869510\n",
      "Iteration 23439, loss = 1.44869376\n",
      "Iteration 23440, loss = 1.44869243\n",
      "Iteration 23441, loss = 1.44869109\n",
      "Iteration 23442, loss = 1.44868976\n",
      "Iteration 23443, loss = 1.44868842\n",
      "Iteration 23444, loss = 1.44868709\n",
      "Iteration 23445, loss = 1.44868575\n",
      "Iteration 23446, loss = 1.44868442\n",
      "Iteration 23447, loss = 1.44868308\n",
      "Iteration 23448, loss = 1.44868175\n",
      "Iteration 23449, loss = 1.44868041\n",
      "Iteration 23450, loss = 1.44867908\n",
      "Iteration 23451, loss = 1.44867775\n",
      "Iteration 23452, loss = 1.44867641\n",
      "Iteration 23453, loss = 1.44867508\n",
      "Iteration 23454, loss = 1.44867374\n",
      "Iteration 23455, loss = 1.44867241\n",
      "Iteration 23456, loss = 1.44867108\n",
      "Iteration 23457, loss = 1.44866974\n",
      "Iteration 23458, loss = 1.44866841\n",
      "Iteration 23459, loss = 1.44866708\n",
      "Iteration 23460, loss = 1.44866574\n",
      "Iteration 23461, loss = 1.44866441\n",
      "Iteration 23462, loss = 1.44866308\n",
      "Iteration 23463, loss = 1.44866175\n",
      "Iteration 23464, loss = 1.44866041\n",
      "Iteration 23465, loss = 1.44865908\n",
      "Iteration 23466, loss = 1.44865775\n",
      "Iteration 23467, loss = 1.44865642\n",
      "Iteration 23468, loss = 1.44865509\n",
      "Iteration 23469, loss = 1.44865375\n",
      "Iteration 23470, loss = 1.44865242\n",
      "Iteration 23471, loss = 1.44865109\n",
      "Iteration 23472, loss = 1.44864976\n",
      "Iteration 23473, loss = 1.44864843\n",
      "Iteration 23474, loss = 1.44864710\n",
      "Iteration 23475, loss = 1.44864577\n",
      "Iteration 23476, loss = 1.44864444\n",
      "Iteration 23477, loss = 1.44864311\n",
      "Iteration 23478, loss = 1.44864178\n",
      "Iteration 23479, loss = 1.44864045\n",
      "Iteration 23480, loss = 1.44863912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23481, loss = 1.44863779\n",
      "Iteration 23482, loss = 1.44863646\n",
      "Iteration 23483, loss = 1.44863513\n",
      "Iteration 23484, loss = 1.44863380\n",
      "Iteration 23485, loss = 1.44863247\n",
      "Iteration 23486, loss = 1.44863114\n",
      "Iteration 23487, loss = 1.44862981\n",
      "Iteration 23488, loss = 1.44862848\n",
      "Iteration 23489, loss = 1.44862715\n",
      "Iteration 23490, loss = 1.44862582\n",
      "Iteration 23491, loss = 1.44862449\n",
      "Iteration 23492, loss = 1.44862316\n",
      "Iteration 23493, loss = 1.44862184\n",
      "Iteration 23494, loss = 1.44862051\n",
      "Iteration 23495, loss = 1.44861918\n",
      "Iteration 23496, loss = 1.44861785\n",
      "Iteration 23497, loss = 1.44861652\n",
      "Iteration 23498, loss = 1.44861520\n",
      "Iteration 23499, loss = 1.44861387\n",
      "Iteration 23500, loss = 1.44861254\n",
      "Iteration 23501, loss = 1.44861121\n",
      "Iteration 23502, loss = 1.44860989\n",
      "Iteration 23503, loss = 1.44860856\n",
      "Iteration 23504, loss = 1.44860723\n",
      "Iteration 23505, loss = 1.44860591\n",
      "Iteration 23506, loss = 1.44860458\n",
      "Iteration 23507, loss = 1.44860325\n",
      "Iteration 23508, loss = 1.44860193\n",
      "Iteration 23509, loss = 1.44860060\n",
      "Iteration 23510, loss = 1.44859927\n",
      "Iteration 23511, loss = 1.44859795\n",
      "Iteration 23512, loss = 1.44859662\n",
      "Iteration 23513, loss = 1.44859530\n",
      "Iteration 23514, loss = 1.44859397\n",
      "Iteration 23515, loss = 1.44859264\n",
      "Iteration 23516, loss = 1.44859132\n",
      "Iteration 23517, loss = 1.44858999\n",
      "Iteration 23518, loss = 1.44858867\n",
      "Iteration 23519, loss = 1.44858734\n",
      "Iteration 23520, loss = 1.44858602\n",
      "Iteration 23521, loss = 1.44858470\n",
      "Iteration 23522, loss = 1.44858337\n",
      "Iteration 23523, loss = 1.44858205\n",
      "Iteration 23524, loss = 1.44858072\n",
      "Iteration 23525, loss = 1.44857940\n",
      "Iteration 23526, loss = 1.44857807\n",
      "Iteration 23527, loss = 1.44857675\n",
      "Iteration 23528, loss = 1.44857543\n",
      "Iteration 23529, loss = 1.44857410\n",
      "Iteration 23530, loss = 1.44857278\n",
      "Iteration 23531, loss = 1.44857146\n",
      "Iteration 23532, loss = 1.44857013\n",
      "Iteration 23533, loss = 1.44856881\n",
      "Iteration 23534, loss = 1.44856749\n",
      "Iteration 23535, loss = 1.44856617\n",
      "Iteration 23536, loss = 1.44856484\n",
      "Iteration 23537, loss = 1.44856352\n",
      "Iteration 23538, loss = 1.44856220\n",
      "Iteration 23539, loss = 1.44856088\n",
      "Iteration 23540, loss = 1.44855955\n",
      "Iteration 23541, loss = 1.44855823\n",
      "Iteration 23542, loss = 1.44855691\n",
      "Iteration 23543, loss = 1.44855559\n",
      "Iteration 23544, loss = 1.44855427\n",
      "Iteration 23545, loss = 1.44855295\n",
      "Iteration 23546, loss = 1.44855162\n",
      "Iteration 23547, loss = 1.44855030\n",
      "Iteration 23548, loss = 1.44854898\n",
      "Iteration 23549, loss = 1.44854766\n",
      "Iteration 23550, loss = 1.44854634\n",
      "Iteration 23551, loss = 1.44854502\n",
      "Iteration 23552, loss = 1.44854370\n",
      "Iteration 23553, loss = 1.44854238\n",
      "Iteration 23554, loss = 1.44854106\n",
      "Iteration 23555, loss = 1.44853974\n",
      "Iteration 23556, loss = 1.44853842\n",
      "Iteration 23557, loss = 1.44853710\n",
      "Iteration 23558, loss = 1.44853578\n",
      "Iteration 23559, loss = 1.44853446\n",
      "Iteration 23560, loss = 1.44853314\n",
      "Iteration 23561, loss = 1.44853182\n",
      "Iteration 23562, loss = 1.44853050\n",
      "Iteration 23563, loss = 1.44852918\n",
      "Iteration 23564, loss = 1.44852787\n",
      "Iteration 23565, loss = 1.44852655\n",
      "Iteration 23566, loss = 1.44852523\n",
      "Iteration 23567, loss = 1.44852391\n",
      "Iteration 23568, loss = 1.44852259\n",
      "Iteration 23569, loss = 1.44852127\n",
      "Iteration 23570, loss = 1.44851996\n",
      "Iteration 23571, loss = 1.44851864\n",
      "Iteration 23572, loss = 1.44851732\n",
      "Iteration 23573, loss = 1.44851600\n",
      "Iteration 23574, loss = 1.44851469\n",
      "Iteration 23575, loss = 1.44851337\n",
      "Iteration 23576, loss = 1.44851205\n",
      "Iteration 23577, loss = 1.44851073\n",
      "Iteration 23578, loss = 1.44850942\n",
      "Iteration 23579, loss = 1.44850810\n",
      "Iteration 23580, loss = 1.44850678\n",
      "Iteration 23581, loss = 1.44850547\n",
      "Iteration 23582, loss = 1.44850415\n",
      "Iteration 23583, loss = 1.44850283\n",
      "Iteration 23584, loss = 1.44850152\n",
      "Iteration 23585, loss = 1.44850020\n",
      "Iteration 23586, loss = 1.44849889\n",
      "Iteration 23587, loss = 1.44849757\n",
      "Iteration 23588, loss = 1.44849626\n",
      "Iteration 23589, loss = 1.44849494\n",
      "Iteration 23590, loss = 1.44849362\n",
      "Iteration 23591, loss = 1.44849231\n",
      "Iteration 23592, loss = 1.44849099\n",
      "Iteration 23593, loss = 1.44848968\n",
      "Iteration 23594, loss = 1.44848837\n",
      "Iteration 23595, loss = 1.44848705\n",
      "Iteration 23596, loss = 1.44848574\n",
      "Iteration 23597, loss = 1.44848442\n",
      "Iteration 23598, loss = 1.44848311\n",
      "Iteration 23599, loss = 1.44848179\n",
      "Iteration 23600, loss = 1.44848048\n",
      "Iteration 23601, loss = 1.44847917\n",
      "Iteration 23602, loss = 1.44847785\n",
      "Iteration 23603, loss = 1.44847654\n",
      "Iteration 23604, loss = 1.44847523\n",
      "Iteration 23605, loss = 1.44847391\n",
      "Iteration 23606, loss = 1.44847260\n",
      "Iteration 23607, loss = 1.44847129\n",
      "Iteration 23608, loss = 1.44846997\n",
      "Iteration 23609, loss = 1.44846866\n",
      "Iteration 23610, loss = 1.44846735\n",
      "Iteration 23611, loss = 1.44846604\n",
      "Iteration 23612, loss = 1.44846472\n",
      "Iteration 23613, loss = 1.44846341\n",
      "Iteration 23614, loss = 1.44846210\n",
      "Iteration 23615, loss = 1.44846079\n",
      "Iteration 23616, loss = 1.44845947\n",
      "Iteration 23617, loss = 1.44845816\n",
      "Iteration 23618, loss = 1.44845685\n",
      "Iteration 23619, loss = 1.44845554\n",
      "Iteration 23620, loss = 1.44845423\n",
      "Iteration 23621, loss = 1.44845292\n",
      "Iteration 23622, loss = 1.44845161\n",
      "Iteration 23623, loss = 1.44845030\n",
      "Iteration 23624, loss = 1.44844899\n",
      "Iteration 23625, loss = 1.44844767\n",
      "Iteration 23626, loss = 1.44844636\n",
      "Iteration 23627, loss = 1.44844505\n",
      "Iteration 23628, loss = 1.44844374\n",
      "Iteration 23629, loss = 1.44844243\n",
      "Iteration 23630, loss = 1.44844112\n",
      "Iteration 23631, loss = 1.44843981\n",
      "Iteration 23632, loss = 1.44843850\n",
      "Iteration 23633, loss = 1.44843719\n",
      "Iteration 23634, loss = 1.44843589\n",
      "Iteration 23635, loss = 1.44843458\n",
      "Iteration 23636, loss = 1.44843327\n",
      "Iteration 23637, loss = 1.44843196\n",
      "Iteration 23638, loss = 1.44843065\n",
      "Iteration 23639, loss = 1.44842934\n",
      "Iteration 23640, loss = 1.44842803\n",
      "Iteration 23641, loss = 1.44842672\n",
      "Iteration 23642, loss = 1.44842542\n",
      "Iteration 23643, loss = 1.44842411\n",
      "Iteration 23644, loss = 1.44842280\n",
      "Iteration 23645, loss = 1.44842149\n",
      "Iteration 23646, loss = 1.44842018\n",
      "Iteration 23647, loss = 1.44841888\n",
      "Iteration 23648, loss = 1.44841757\n",
      "Iteration 23649, loss = 1.44841626\n",
      "Iteration 23650, loss = 1.44841495\n",
      "Iteration 23651, loss = 1.44841365\n",
      "Iteration 23652, loss = 1.44841234\n",
      "Iteration 23653, loss = 1.44841103\n",
      "Iteration 23654, loss = 1.44840973\n",
      "Iteration 23655, loss = 1.44840842\n",
      "Iteration 23656, loss = 1.44840711\n",
      "Iteration 23657, loss = 1.44840581\n",
      "Iteration 23658, loss = 1.44840450\n",
      "Iteration 23659, loss = 1.44840319\n",
      "Iteration 23660, loss = 1.44840189\n",
      "Iteration 23661, loss = 1.44840058\n",
      "Iteration 23662, loss = 1.44839928\n",
      "Iteration 23663, loss = 1.44839797\n",
      "Iteration 23664, loss = 1.44839667\n",
      "Iteration 23665, loss = 1.44839536\n",
      "Iteration 23666, loss = 1.44839406\n",
      "Iteration 23667, loss = 1.44839275\n",
      "Iteration 23668, loss = 1.44839145\n",
      "Iteration 23669, loss = 1.44839014\n",
      "Iteration 23670, loss = 1.44838884\n",
      "Iteration 23671, loss = 1.44838753\n",
      "Iteration 23672, loss = 1.44838623\n",
      "Iteration 23673, loss = 1.44838492\n",
      "Iteration 23674, loss = 1.44838362\n",
      "Iteration 23675, loss = 1.44838232\n",
      "Iteration 23676, loss = 1.44838101\n",
      "Iteration 23677, loss = 1.44837971\n",
      "Iteration 23678, loss = 1.44837840\n",
      "Iteration 23679, loss = 1.44837710\n",
      "Iteration 23680, loss = 1.44837580\n",
      "Iteration 23681, loss = 1.44837449\n",
      "Iteration 23682, loss = 1.44837319\n",
      "Iteration 23683, loss = 1.44837189\n",
      "Iteration 23684, loss = 1.44837059\n",
      "Iteration 23685, loss = 1.44836928\n",
      "Iteration 23686, loss = 1.44836798\n",
      "Iteration 23687, loss = 1.44836668\n",
      "Iteration 23688, loss = 1.44836538\n",
      "Iteration 23689, loss = 1.44836407\n",
      "Iteration 23690, loss = 1.44836277\n",
      "Iteration 23691, loss = 1.44836147\n",
      "Iteration 23692, loss = 1.44836017\n",
      "Iteration 23693, loss = 1.44835887\n",
      "Iteration 23694, loss = 1.44835757\n",
      "Iteration 23695, loss = 1.44835626\n",
      "Iteration 23696, loss = 1.44835496\n",
      "Iteration 23697, loss = 1.44835366\n",
      "Iteration 23698, loss = 1.44835236\n",
      "Iteration 23699, loss = 1.44835106\n",
      "Iteration 23700, loss = 1.44834976\n",
      "Iteration 23701, loss = 1.44834846\n",
      "Iteration 23702, loss = 1.44834716\n",
      "Iteration 23703, loss = 1.44834586\n",
      "Iteration 23704, loss = 1.44834456\n",
      "Iteration 23705, loss = 1.44834326\n",
      "Iteration 23706, loss = 1.44834196\n",
      "Iteration 23707, loss = 1.44834066\n",
      "Iteration 23708, loss = 1.44833936\n",
      "Iteration 23709, loss = 1.44833806\n",
      "Iteration 23710, loss = 1.44833676\n",
      "Iteration 23711, loss = 1.44833546\n",
      "Iteration 23712, loss = 1.44833416\n",
      "Iteration 23713, loss = 1.44833286\n",
      "Iteration 23714, loss = 1.44833157\n",
      "Iteration 23715, loss = 1.44833027\n",
      "Iteration 23716, loss = 1.44832897\n",
      "Iteration 23717, loss = 1.44832767\n",
      "Iteration 23718, loss = 1.44832637\n",
      "Iteration 23719, loss = 1.44832507\n",
      "Iteration 23720, loss = 1.44832378\n",
      "Iteration 23721, loss = 1.44832248\n",
      "Iteration 23722, loss = 1.44832118\n",
      "Iteration 23723, loss = 1.44831988\n",
      "Iteration 23724, loss = 1.44831859\n",
      "Iteration 23725, loss = 1.44831729\n",
      "Iteration 23726, loss = 1.44831599\n",
      "Iteration 23727, loss = 1.44831469\n",
      "Iteration 23728, loss = 1.44831340\n",
      "Iteration 23729, loss = 1.44831210\n",
      "Iteration 23730, loss = 1.44831080\n",
      "Iteration 23731, loss = 1.44830951\n",
      "Iteration 23732, loss = 1.44830821\n",
      "Iteration 23733, loss = 1.44830691\n",
      "Iteration 23734, loss = 1.44830562\n",
      "Iteration 23735, loss = 1.44830432\n",
      "Iteration 23736, loss = 1.44830303\n",
      "Iteration 23737, loss = 1.44830173\n",
      "Iteration 23738, loss = 1.44830044\n",
      "Iteration 23739, loss = 1.44829914\n",
      "Iteration 23740, loss = 1.44829784\n",
      "Iteration 23741, loss = 1.44829655\n",
      "Iteration 23742, loss = 1.44829525\n",
      "Iteration 23743, loss = 1.44829396\n",
      "Iteration 23744, loss = 1.44829266\n",
      "Iteration 23745, loss = 1.44829137\n",
      "Iteration 23746, loss = 1.44829008\n",
      "Iteration 23747, loss = 1.44828878\n",
      "Iteration 23748, loss = 1.44828749\n",
      "Iteration 23749, loss = 1.44828619\n",
      "Iteration 23750, loss = 1.44828490\n",
      "Iteration 23751, loss = 1.44828360\n",
      "Iteration 23752, loss = 1.44828231\n",
      "Iteration 23753, loss = 1.44828102\n",
      "Iteration 23754, loss = 1.44827972\n",
      "Iteration 23755, loss = 1.44827843\n",
      "Iteration 23756, loss = 1.44827714\n",
      "Iteration 23757, loss = 1.44827584\n",
      "Iteration 23758, loss = 1.44827455\n",
      "Iteration 23759, loss = 1.44827326\n",
      "Iteration 23760, loss = 1.44827197\n",
      "Iteration 23761, loss = 1.44827067\n",
      "Iteration 23762, loss = 1.44826938\n",
      "Iteration 23763, loss = 1.44826809\n",
      "Iteration 23764, loss = 1.44826680\n",
      "Iteration 23765, loss = 1.44826550\n",
      "Iteration 23766, loss = 1.44826421\n",
      "Iteration 23767, loss = 1.44826292\n",
      "Iteration 23768, loss = 1.44826163\n",
      "Iteration 23769, loss = 1.44826034\n",
      "Iteration 23770, loss = 1.44825905\n",
      "Iteration 23771, loss = 1.44825775\n",
      "Iteration 23772, loss = 1.44825646\n",
      "Iteration 23773, loss = 1.44825517\n",
      "Iteration 23774, loss = 1.44825388\n",
      "Iteration 23775, loss = 1.44825259\n",
      "Iteration 23776, loss = 1.44825130\n",
      "Iteration 23777, loss = 1.44825001\n",
      "Iteration 23778, loss = 1.44824872\n",
      "Iteration 23779, loss = 1.44824743\n",
      "Iteration 23780, loss = 1.44824614\n",
      "Iteration 23781, loss = 1.44824485\n",
      "Iteration 23782, loss = 1.44824356\n",
      "Iteration 23783, loss = 1.44824227\n",
      "Iteration 23784, loss = 1.44824098\n",
      "Iteration 23785, loss = 1.44823969\n",
      "Iteration 23786, loss = 1.44823840\n",
      "Iteration 23787, loss = 1.44823711\n",
      "Iteration 23788, loss = 1.44823582\n",
      "Iteration 23789, loss = 1.44823453\n",
      "Iteration 23790, loss = 1.44823325\n",
      "Iteration 23791, loss = 1.44823196\n",
      "Iteration 23792, loss = 1.44823067\n",
      "Iteration 23793, loss = 1.44822938\n",
      "Iteration 23794, loss = 1.44822809\n",
      "Iteration 23795, loss = 1.44822680\n",
      "Iteration 23796, loss = 1.44822552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23797, loss = 1.44822423\n",
      "Iteration 23798, loss = 1.44822294\n",
      "Iteration 23799, loss = 1.44822165\n",
      "Iteration 23800, loss = 1.44822037\n",
      "Iteration 23801, loss = 1.44821908\n",
      "Iteration 23802, loss = 1.44821779\n",
      "Iteration 23803, loss = 1.44821650\n",
      "Iteration 23804, loss = 1.44821522\n",
      "Iteration 23805, loss = 1.44821393\n",
      "Iteration 23806, loss = 1.44821264\n",
      "Iteration 23807, loss = 1.44821136\n",
      "Iteration 23808, loss = 1.44821007\n",
      "Iteration 23809, loss = 1.44820878\n",
      "Iteration 23810, loss = 1.44820750\n",
      "Iteration 23811, loss = 1.44820621\n",
      "Iteration 23812, loss = 1.44820493\n",
      "Iteration 23813, loss = 1.44820364\n",
      "Iteration 23814, loss = 1.44820236\n",
      "Iteration 23815, loss = 1.44820107\n",
      "Iteration 23816, loss = 1.44819978\n",
      "Iteration 23817, loss = 1.44819850\n",
      "Iteration 23818, loss = 1.44819721\n",
      "Iteration 23819, loss = 1.44819593\n",
      "Iteration 23820, loss = 1.44819464\n",
      "Iteration 23821, loss = 1.44819336\n",
      "Iteration 23822, loss = 1.44819207\n",
      "Iteration 23823, loss = 1.44819079\n",
      "Iteration 23824, loss = 1.44818951\n",
      "Iteration 23825, loss = 1.44818822\n",
      "Iteration 23826, loss = 1.44818694\n",
      "Iteration 23827, loss = 1.44818565\n",
      "Iteration 23828, loss = 1.44818437\n",
      "Iteration 23829, loss = 1.44818309\n",
      "Iteration 23830, loss = 1.44818180\n",
      "Iteration 23831, loss = 1.44818052\n",
      "Iteration 23832, loss = 1.44817924\n",
      "Iteration 23833, loss = 1.44817795\n",
      "Iteration 23834, loss = 1.44817667\n",
      "Iteration 23835, loss = 1.44817539\n",
      "Iteration 23836, loss = 1.44817410\n",
      "Iteration 23837, loss = 1.44817282\n",
      "Iteration 23838, loss = 1.44817154\n",
      "Iteration 23839, loss = 1.44817026\n",
      "Iteration 23840, loss = 1.44816897\n",
      "Iteration 23841, loss = 1.44816769\n",
      "Iteration 23842, loss = 1.44816641\n",
      "Iteration 23843, loss = 1.44816513\n",
      "Iteration 23844, loss = 1.44816385\n",
      "Iteration 23845, loss = 1.44816257\n",
      "Iteration 23846, loss = 1.44816128\n",
      "Iteration 23847, loss = 1.44816000\n",
      "Iteration 23848, loss = 1.44815872\n",
      "Iteration 23849, loss = 1.44815744\n",
      "Iteration 23850, loss = 1.44815616\n",
      "Iteration 23851, loss = 1.44815488\n",
      "Iteration 23852, loss = 1.44815360\n",
      "Iteration 23853, loss = 1.44815232\n",
      "Iteration 23854, loss = 1.44815104\n",
      "Iteration 23855, loss = 1.44814976\n",
      "Iteration 23856, loss = 1.44814848\n",
      "Iteration 23857, loss = 1.44814720\n",
      "Iteration 23858, loss = 1.44814592\n",
      "Iteration 23859, loss = 1.44814464\n",
      "Iteration 23860, loss = 1.44814336\n",
      "Iteration 23861, loss = 1.44814208\n",
      "Iteration 23862, loss = 1.44814080\n",
      "Iteration 23863, loss = 1.44813952\n",
      "Iteration 23864, loss = 1.44813824\n",
      "Iteration 23865, loss = 1.44813696\n",
      "Iteration 23866, loss = 1.44813568\n",
      "Iteration 23867, loss = 1.44813440\n",
      "Iteration 23868, loss = 1.44813312\n",
      "Iteration 23869, loss = 1.44813185\n",
      "Iteration 23870, loss = 1.44813057\n",
      "Iteration 23871, loss = 1.44812929\n",
      "Iteration 23872, loss = 1.44812801\n",
      "Iteration 23873, loss = 1.44812673\n",
      "Iteration 23874, loss = 1.44812546\n",
      "Iteration 23875, loss = 1.44812418\n",
      "Iteration 23876, loss = 1.44812290\n",
      "Iteration 23877, loss = 1.44812162\n",
      "Iteration 23878, loss = 1.44812035\n",
      "Iteration 23879, loss = 1.44811907\n",
      "Iteration 23880, loss = 1.44811779\n",
      "Iteration 23881, loss = 1.44811651\n",
      "Iteration 23882, loss = 1.44811524\n",
      "Iteration 23883, loss = 1.44811396\n",
      "Iteration 23884, loss = 1.44811268\n",
      "Iteration 23885, loss = 1.44811141\n",
      "Iteration 23886, loss = 1.44811013\n",
      "Iteration 23887, loss = 1.44810886\n",
      "Iteration 23888, loss = 1.44810758\n",
      "Iteration 23889, loss = 1.44810630\n",
      "Iteration 23890, loss = 1.44810503\n",
      "Iteration 23891, loss = 1.44810375\n",
      "Iteration 23892, loss = 1.44810248\n",
      "Iteration 23893, loss = 1.44810120\n",
      "Iteration 23894, loss = 1.44809993\n",
      "Iteration 23895, loss = 1.44809865\n",
      "Iteration 23896, loss = 1.44809738\n",
      "Iteration 23897, loss = 1.44809610\n",
      "Iteration 23898, loss = 1.44809483\n",
      "Iteration 23899, loss = 1.44809355\n",
      "Iteration 23900, loss = 1.44809228\n",
      "Iteration 23901, loss = 1.44809100\n",
      "Iteration 23902, loss = 1.44808973\n",
      "Iteration 23903, loss = 1.44808846\n",
      "Iteration 23904, loss = 1.44808718\n",
      "Iteration 23905, loss = 1.44808591\n",
      "Iteration 23906, loss = 1.44808463\n",
      "Iteration 23907, loss = 1.44808336\n",
      "Iteration 23908, loss = 1.44808209\n",
      "Iteration 23909, loss = 1.44808081\n",
      "Iteration 23910, loss = 1.44807954\n",
      "Iteration 23911, loss = 1.44807827\n",
      "Iteration 23912, loss = 1.44807699\n",
      "Iteration 23913, loss = 1.44807572\n",
      "Iteration 23914, loss = 1.44807445\n",
      "Iteration 23915, loss = 1.44807318\n",
      "Iteration 23916, loss = 1.44807190\n",
      "Iteration 23917, loss = 1.44807063\n",
      "Iteration 23918, loss = 1.44806936\n",
      "Iteration 23919, loss = 1.44806809\n",
      "Iteration 23920, loss = 1.44806682\n",
      "Iteration 23921, loss = 1.44806554\n",
      "Iteration 23922, loss = 1.44806427\n",
      "Iteration 23923, loss = 1.44806300\n",
      "Iteration 23924, loss = 1.44806173\n",
      "Iteration 23925, loss = 1.44806046\n",
      "Iteration 23926, loss = 1.44805919\n",
      "Iteration 23927, loss = 1.44805792\n",
      "Iteration 23928, loss = 1.44805664\n",
      "Iteration 23929, loss = 1.44805537\n",
      "Iteration 23930, loss = 1.44805410\n",
      "Iteration 23931, loss = 1.44805283\n",
      "Iteration 23932, loss = 1.44805156\n",
      "Iteration 23933, loss = 1.44805029\n",
      "Iteration 23934, loss = 1.44804902\n",
      "Iteration 23935, loss = 1.44804775\n",
      "Iteration 23936, loss = 1.44804648\n",
      "Iteration 23937, loss = 1.44804521\n",
      "Iteration 23938, loss = 1.44804394\n",
      "Iteration 23939, loss = 1.44804267\n",
      "Iteration 23940, loss = 1.44804140\n",
      "Iteration 23941, loss = 1.44804014\n",
      "Iteration 23942, loss = 1.44803887\n",
      "Iteration 23943, loss = 1.44803760\n",
      "Iteration 23944, loss = 1.44803633\n",
      "Iteration 23945, loss = 1.44803506\n",
      "Iteration 23946, loss = 1.44803379\n",
      "Iteration 23947, loss = 1.44803252\n",
      "Iteration 23948, loss = 1.44803125\n",
      "Iteration 23949, loss = 1.44802999\n",
      "Iteration 23950, loss = 1.44802872\n",
      "Iteration 23951, loss = 1.44802745\n",
      "Iteration 23952, loss = 1.44802618\n",
      "Iteration 23953, loss = 1.44802491\n",
      "Iteration 23954, loss = 1.44802365\n",
      "Iteration 23955, loss = 1.44802238\n",
      "Iteration 23956, loss = 1.44802111\n",
      "Iteration 23957, loss = 1.44801985\n",
      "Iteration 23958, loss = 1.44801858\n",
      "Iteration 23959, loss = 1.44801731\n",
      "Iteration 23960, loss = 1.44801604\n",
      "Iteration 23961, loss = 1.44801478\n",
      "Iteration 23962, loss = 1.44801351\n",
      "Iteration 23963, loss = 1.44801225\n",
      "Iteration 23964, loss = 1.44801098\n",
      "Iteration 23965, loss = 1.44800971\n",
      "Iteration 23966, loss = 1.44800845\n",
      "Iteration 23967, loss = 1.44800718\n",
      "Iteration 23968, loss = 1.44800592\n",
      "Iteration 23969, loss = 1.44800465\n",
      "Iteration 23970, loss = 1.44800338\n",
      "Iteration 23971, loss = 1.44800212\n",
      "Iteration 23972, loss = 1.44800085\n",
      "Iteration 23973, loss = 1.44799959\n",
      "Iteration 23974, loss = 1.44799832\n",
      "Iteration 23975, loss = 1.44799706\n",
      "Iteration 23976, loss = 1.44799579\n",
      "Iteration 23977, loss = 1.44799453\n",
      "Iteration 23978, loss = 1.44799327\n",
      "Iteration 23979, loss = 1.44799200\n",
      "Iteration 23980, loss = 1.44799074\n",
      "Iteration 23981, loss = 1.44798947\n",
      "Iteration 23982, loss = 1.44798821\n",
      "Iteration 23983, loss = 1.44798694\n",
      "Iteration 23984, loss = 1.44798568\n",
      "Iteration 23985, loss = 1.44798442\n",
      "Iteration 23986, loss = 1.44798315\n",
      "Iteration 23987, loss = 1.44798189\n",
      "Iteration 23988, loss = 1.44798063\n",
      "Iteration 23989, loss = 1.44797936\n",
      "Iteration 23990, loss = 1.44797810\n",
      "Iteration 23991, loss = 1.44797684\n",
      "Iteration 23992, loss = 1.44797558\n",
      "Iteration 23993, loss = 1.44797431\n",
      "Iteration 23994, loss = 1.44797305\n",
      "Iteration 23995, loss = 1.44797179\n",
      "Iteration 23996, loss = 1.44797053\n",
      "Iteration 23997, loss = 1.44796926\n",
      "Iteration 23998, loss = 1.44796800\n",
      "Iteration 23999, loss = 1.44796674\n",
      "Iteration 24000, loss = 1.44796548\n",
      "Iteration 24001, loss = 1.44796422\n",
      "Iteration 24002, loss = 1.44796296\n",
      "Iteration 24003, loss = 1.44796170\n",
      "Iteration 24004, loss = 1.44796043\n",
      "Iteration 24005, loss = 1.44795917\n",
      "Iteration 24006, loss = 1.44795791\n",
      "Iteration 24007, loss = 1.44795665\n",
      "Iteration 24008, loss = 1.44795539\n",
      "Iteration 24009, loss = 1.44795413\n",
      "Iteration 24010, loss = 1.44795287\n",
      "Iteration 24011, loss = 1.44795161\n",
      "Iteration 24012, loss = 1.44795035\n",
      "Iteration 24013, loss = 1.44794909\n",
      "Iteration 24014, loss = 1.44794783\n",
      "Iteration 24015, loss = 1.44794657\n",
      "Iteration 24016, loss = 1.44794531\n",
      "Iteration 24017, loss = 1.44794405\n",
      "Iteration 24018, loss = 1.44794279\n",
      "Iteration 24019, loss = 1.44794153\n",
      "Iteration 24020, loss = 1.44794027\n",
      "Iteration 24021, loss = 1.44793901\n",
      "Iteration 24022, loss = 1.44793775\n",
      "Iteration 24023, loss = 1.44793650\n",
      "Iteration 24024, loss = 1.44793524\n",
      "Iteration 24025, loss = 1.44793398\n",
      "Iteration 24026, loss = 1.44793272\n",
      "Iteration 24027, loss = 1.44793146\n",
      "Iteration 24028, loss = 1.44793020\n",
      "Iteration 24029, loss = 1.44792895\n",
      "Iteration 24030, loss = 1.44792769\n",
      "Iteration 24031, loss = 1.44792643\n",
      "Iteration 24032, loss = 1.44792517\n",
      "Iteration 24033, loss = 1.44792392\n",
      "Iteration 24034, loss = 1.44792266\n",
      "Iteration 24035, loss = 1.44792140\n",
      "Iteration 24036, loss = 1.44792014\n",
      "Iteration 24037, loss = 1.44791889\n",
      "Iteration 24038, loss = 1.44791763\n",
      "Iteration 24039, loss = 1.44791637\n",
      "Iteration 24040, loss = 1.44791512\n",
      "Iteration 24041, loss = 1.44791386\n",
      "Iteration 24042, loss = 1.44791260\n",
      "Iteration 24043, loss = 1.44791135\n",
      "Iteration 24044, loss = 1.44791009\n",
      "Iteration 24045, loss = 1.44790884\n",
      "Iteration 24046, loss = 1.44790758\n",
      "Iteration 24047, loss = 1.44790632\n",
      "Iteration 24048, loss = 1.44790507\n",
      "Iteration 24049, loss = 1.44790381\n",
      "Iteration 24050, loss = 1.44790256\n",
      "Iteration 24051, loss = 1.44790130\n",
      "Iteration 24052, loss = 1.44790005\n",
      "Iteration 24053, loss = 1.44789879\n",
      "Iteration 24054, loss = 1.44789754\n",
      "Iteration 24055, loss = 1.44789628\n",
      "Iteration 24056, loss = 1.44789503\n",
      "Iteration 24057, loss = 1.44789377\n",
      "Iteration 24058, loss = 1.44789252\n",
      "Iteration 24059, loss = 1.44789127\n",
      "Iteration 24060, loss = 1.44789001\n",
      "Iteration 24061, loss = 1.44788876\n",
      "Iteration 24062, loss = 1.44788750\n",
      "Iteration 24063, loss = 1.44788625\n",
      "Iteration 24064, loss = 1.44788500\n",
      "Iteration 24065, loss = 1.44788374\n",
      "Iteration 24066, loss = 1.44788249\n",
      "Iteration 24067, loss = 1.44788124\n",
      "Iteration 24068, loss = 1.44787998\n",
      "Iteration 24069, loss = 1.44787873\n",
      "Iteration 24070, loss = 1.44787748\n",
      "Iteration 24071, loss = 1.44787623\n",
      "Iteration 24072, loss = 1.44787497\n",
      "Iteration 24073, loss = 1.44787372\n",
      "Iteration 24074, loss = 1.44787247\n",
      "Iteration 24075, loss = 1.44787122\n",
      "Iteration 24076, loss = 1.44786996\n",
      "Iteration 24077, loss = 1.44786871\n",
      "Iteration 24078, loss = 1.44786746\n",
      "Iteration 24079, loss = 1.44786621\n",
      "Iteration 24080, loss = 1.44786496\n",
      "Iteration 24081, loss = 1.44786371\n",
      "Iteration 24082, loss = 1.44786245\n",
      "Iteration 24083, loss = 1.44786120\n",
      "Iteration 24084, loss = 1.44785995\n",
      "Iteration 24085, loss = 1.44785870\n",
      "Iteration 24086, loss = 1.44785745\n",
      "Iteration 24087, loss = 1.44785620\n",
      "Iteration 24088, loss = 1.44785495\n",
      "Iteration 24089, loss = 1.44785370\n",
      "Iteration 24090, loss = 1.44785245\n",
      "Iteration 24091, loss = 1.44785120\n",
      "Iteration 24092, loss = 1.44784995\n",
      "Iteration 24093, loss = 1.44784870\n",
      "Iteration 24094, loss = 1.44784745\n",
      "Iteration 24095, loss = 1.44784620\n",
      "Iteration 24096, loss = 1.44784495\n",
      "Iteration 24097, loss = 1.44784370\n",
      "Iteration 24098, loss = 1.44784245\n",
      "Iteration 24099, loss = 1.44784120\n",
      "Iteration 24100, loss = 1.44783995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24101, loss = 1.44783870\n",
      "Iteration 24102, loss = 1.44783746\n",
      "Iteration 24103, loss = 1.44783621\n",
      "Iteration 24104, loss = 1.44783496\n",
      "Iteration 24105, loss = 1.44783371\n",
      "Iteration 24106, loss = 1.44783246\n",
      "Iteration 24107, loss = 1.44783121\n",
      "Iteration 24108, loss = 1.44782997\n",
      "Iteration 24109, loss = 1.44782872\n",
      "Iteration 24110, loss = 1.44782747\n",
      "Iteration 24111, loss = 1.44782622\n",
      "Iteration 24112, loss = 1.44782498\n",
      "Iteration 24113, loss = 1.44782373\n",
      "Iteration 24114, loss = 1.44782248\n",
      "Iteration 24115, loss = 1.44782123\n",
      "Iteration 24116, loss = 1.44781999\n",
      "Iteration 24117, loss = 1.44781874\n",
      "Iteration 24118, loss = 1.44781749\n",
      "Iteration 24119, loss = 1.44781625\n",
      "Iteration 24120, loss = 1.44781500\n",
      "Iteration 24121, loss = 1.44781375\n",
      "Iteration 24122, loss = 1.44781251\n",
      "Iteration 24123, loss = 1.44781126\n",
      "Iteration 24124, loss = 1.44781001\n",
      "Iteration 24125, loss = 1.44780877\n",
      "Iteration 24126, loss = 1.44780752\n",
      "Iteration 24127, loss = 1.44780628\n",
      "Iteration 24128, loss = 1.44780503\n",
      "Iteration 24129, loss = 1.44780379\n",
      "Iteration 24130, loss = 1.44780254\n",
      "Iteration 24131, loss = 1.44780130\n",
      "Iteration 24132, loss = 1.44780005\n",
      "Iteration 24133, loss = 1.44779881\n",
      "Iteration 24134, loss = 1.44779756\n",
      "Iteration 24135, loss = 1.44779632\n",
      "Iteration 24136, loss = 1.44779507\n",
      "Iteration 24137, loss = 1.44779383\n",
      "Iteration 24138, loss = 1.44779258\n",
      "Iteration 24139, loss = 1.44779134\n",
      "Iteration 24140, loss = 1.44779010\n",
      "Iteration 24141, loss = 1.44778885\n",
      "Iteration 24142, loss = 1.44778761\n",
      "Iteration 24143, loss = 1.44778636\n",
      "Iteration 24144, loss = 1.44778512\n",
      "Iteration 24145, loss = 1.44778388\n",
      "Iteration 24146, loss = 1.44778263\n",
      "Iteration 24147, loss = 1.44778139\n",
      "Iteration 24148, loss = 1.44778015\n",
      "Iteration 24149, loss = 1.44777891\n",
      "Iteration 24150, loss = 1.44777766\n",
      "Iteration 24151, loss = 1.44777642\n",
      "Iteration 24152, loss = 1.44777518\n",
      "Iteration 24153, loss = 1.44777394\n",
      "Iteration 24154, loss = 1.44777269\n",
      "Iteration 24155, loss = 1.44777145\n",
      "Iteration 24156, loss = 1.44777021\n",
      "Iteration 24157, loss = 1.44776897\n",
      "Iteration 24158, loss = 1.44776772\n",
      "Iteration 24159, loss = 1.44776648\n",
      "Iteration 24160, loss = 1.44776524\n",
      "Iteration 24161, loss = 1.44776400\n",
      "Iteration 24162, loss = 1.44776276\n",
      "Iteration 24163, loss = 1.44776152\n",
      "Iteration 24164, loss = 1.44776028\n",
      "Iteration 24165, loss = 1.44775904\n",
      "Iteration 24166, loss = 1.44775780\n",
      "Iteration 24167, loss = 1.44775655\n",
      "Iteration 24168, loss = 1.44775531\n",
      "Iteration 24169, loss = 1.44775407\n",
      "Iteration 24170, loss = 1.44775283\n",
      "Iteration 24171, loss = 1.44775159\n",
      "Iteration 24172, loss = 1.44775035\n",
      "Iteration 24173, loss = 1.44774911\n",
      "Iteration 24174, loss = 1.44774787\n",
      "Iteration 24175, loss = 1.44774663\n",
      "Iteration 24176, loss = 1.44774539\n",
      "Iteration 24177, loss = 1.44774416\n",
      "Iteration 24178, loss = 1.44774292\n",
      "Iteration 24179, loss = 1.44774168\n",
      "Iteration 24180, loss = 1.44774044\n",
      "Iteration 24181, loss = 1.44773920\n",
      "Iteration 24182, loss = 1.44773796\n",
      "Iteration 24183, loss = 1.44773672\n",
      "Iteration 24184, loss = 1.44773548\n",
      "Iteration 24185, loss = 1.44773424\n",
      "Iteration 24186, loss = 1.44773301\n",
      "Iteration 24187, loss = 1.44773177\n",
      "Iteration 24188, loss = 1.44773053\n",
      "Iteration 24189, loss = 1.44772929\n",
      "Iteration 24190, loss = 1.44772805\n",
      "Iteration 24191, loss = 1.44772682\n",
      "Iteration 24192, loss = 1.44772558\n",
      "Iteration 24193, loss = 1.44772434\n",
      "Iteration 24194, loss = 1.44772310\n",
      "Iteration 24195, loss = 1.44772187\n",
      "Iteration 24196, loss = 1.44772063\n",
      "Iteration 24197, loss = 1.44771939\n",
      "Iteration 24198, loss = 1.44771816\n",
      "Iteration 24199, loss = 1.44771692\n",
      "Iteration 24200, loss = 1.44771568\n",
      "Iteration 24201, loss = 1.44771445\n",
      "Iteration 24202, loss = 1.44771321\n",
      "Iteration 24203, loss = 1.44771197\n",
      "Iteration 24204, loss = 1.44771074\n",
      "Iteration 24205, loss = 1.44770950\n",
      "Iteration 24206, loss = 1.44770827\n",
      "Iteration 24207, loss = 1.44770703\n",
      "Iteration 24208, loss = 1.44770580\n",
      "Iteration 24209, loss = 1.44770456\n",
      "Iteration 24210, loss = 1.44770333\n",
      "Iteration 24211, loss = 1.44770209\n",
      "Iteration 24212, loss = 1.44770085\n",
      "Iteration 24213, loss = 1.44769962\n",
      "Iteration 24214, loss = 1.44769839\n",
      "Iteration 24215, loss = 1.44769715\n",
      "Iteration 24216, loss = 1.44769592\n",
      "Iteration 24217, loss = 1.44769468\n",
      "Iteration 24218, loss = 1.44769345\n",
      "Iteration 24219, loss = 1.44769221\n",
      "Iteration 24220, loss = 1.44769098\n",
      "Iteration 24221, loss = 1.44768975\n",
      "Iteration 24222, loss = 1.44768851\n",
      "Iteration 24223, loss = 1.44768728\n",
      "Iteration 24224, loss = 1.44768604\n",
      "Iteration 24225, loss = 1.44768481\n",
      "Iteration 24226, loss = 1.44768358\n",
      "Iteration 24227, loss = 1.44768234\n",
      "Iteration 24228, loss = 1.44768111\n",
      "Iteration 24229, loss = 1.44767988\n",
      "Iteration 24230, loss = 1.44767865\n",
      "Iteration 24231, loss = 1.44767741\n",
      "Iteration 24232, loss = 1.44767618\n",
      "Iteration 24233, loss = 1.44767495\n",
      "Iteration 24234, loss = 1.44767372\n",
      "Iteration 24235, loss = 1.44767248\n",
      "Iteration 24236, loss = 1.44767125\n",
      "Iteration 24237, loss = 1.44767002\n",
      "Iteration 24238, loss = 1.44766879\n",
      "Iteration 24239, loss = 1.44766756\n",
      "Iteration 24240, loss = 1.44766632\n",
      "Iteration 24241, loss = 1.44766509\n",
      "Iteration 24242, loss = 1.44766386\n",
      "Iteration 24243, loss = 1.44766263\n",
      "Iteration 24244, loss = 1.44766140\n",
      "Iteration 24245, loss = 1.44766017\n",
      "Iteration 24246, loss = 1.44765894\n",
      "Iteration 24247, loss = 1.44765771\n",
      "Iteration 24248, loss = 1.44765648\n",
      "Iteration 24249, loss = 1.44765525\n",
      "Iteration 24250, loss = 1.44765401\n",
      "Iteration 24251, loss = 1.44765278\n",
      "Iteration 24252, loss = 1.44765155\n",
      "Iteration 24253, loss = 1.44765032\n",
      "Iteration 24254, loss = 1.44764909\n",
      "Iteration 24255, loss = 1.44764786\n",
      "Iteration 24256, loss = 1.44764664\n",
      "Iteration 24257, loss = 1.44764541\n",
      "Iteration 24258, loss = 1.44764418\n",
      "Iteration 24259, loss = 1.44764295\n",
      "Iteration 24260, loss = 1.44764172\n",
      "Iteration 24261, loss = 1.44764049\n",
      "Iteration 24262, loss = 1.44763926\n",
      "Iteration 24263, loss = 1.44763803\n",
      "Iteration 24264, loss = 1.44763680\n",
      "Iteration 24265, loss = 1.44763557\n",
      "Iteration 24266, loss = 1.44763435\n",
      "Iteration 24267, loss = 1.44763312\n",
      "Iteration 24268, loss = 1.44763189\n",
      "Iteration 24269, loss = 1.44763066\n",
      "Iteration 24270, loss = 1.44762943\n",
      "Iteration 24271, loss = 1.44762821\n",
      "Iteration 24272, loss = 1.44762698\n",
      "Iteration 24273, loss = 1.44762575\n",
      "Iteration 24274, loss = 1.44762452\n",
      "Iteration 24275, loss = 1.44762330\n",
      "Iteration 24276, loss = 1.44762207\n",
      "Iteration 24277, loss = 1.44762084\n",
      "Iteration 24278, loss = 1.44761961\n",
      "Iteration 24279, loss = 1.44761839\n",
      "Iteration 24280, loss = 1.44761716\n",
      "Iteration 24281, loss = 1.44761593\n",
      "Iteration 24282, loss = 1.44761471\n",
      "Iteration 24283, loss = 1.44761348\n",
      "Iteration 24284, loss = 1.44761226\n",
      "Iteration 24285, loss = 1.44761103\n",
      "Iteration 24286, loss = 1.44760980\n",
      "Iteration 24287, loss = 1.44760858\n",
      "Iteration 24288, loss = 1.44760735\n",
      "Iteration 24289, loss = 1.44760613\n",
      "Iteration 24290, loss = 1.44760490\n",
      "Iteration 24291, loss = 1.44760368\n",
      "Iteration 24292, loss = 1.44760245\n",
      "Iteration 24293, loss = 1.44760123\n",
      "Iteration 24294, loss = 1.44760000\n",
      "Iteration 24295, loss = 1.44759878\n",
      "Iteration 24296, loss = 1.44759755\n",
      "Iteration 24297, loss = 1.44759633\n",
      "Iteration 24298, loss = 1.44759510\n",
      "Iteration 24299, loss = 1.44759388\n",
      "Iteration 24300, loss = 1.44759265\n",
      "Iteration 24301, loss = 1.44759143\n",
      "Iteration 24302, loss = 1.44759021\n",
      "Iteration 24303, loss = 1.44758898\n",
      "Iteration 24304, loss = 1.44758776\n",
      "Iteration 24305, loss = 1.44758653\n",
      "Iteration 24306, loss = 1.44758531\n",
      "Iteration 24307, loss = 1.44758409\n",
      "Iteration 24308, loss = 1.44758286\n",
      "Iteration 24309, loss = 1.44758164\n",
      "Iteration 24310, loss = 1.44758042\n",
      "Iteration 24311, loss = 1.44757920\n",
      "Iteration 24312, loss = 1.44757797\n",
      "Iteration 24313, loss = 1.44757675\n",
      "Iteration 24314, loss = 1.44757553\n",
      "Iteration 24315, loss = 1.44757431\n",
      "Iteration 24316, loss = 1.44757308\n",
      "Iteration 24317, loss = 1.44757186\n",
      "Iteration 24318, loss = 1.44757064\n",
      "Iteration 24319, loss = 1.44756942\n",
      "Iteration 24320, loss = 1.44756820\n",
      "Iteration 24321, loss = 1.44756697\n",
      "Iteration 24322, loss = 1.44756575\n",
      "Iteration 24323, loss = 1.44756453\n",
      "Iteration 24324, loss = 1.44756331\n",
      "Iteration 24325, loss = 1.44756209\n",
      "Iteration 24326, loss = 1.44756087\n",
      "Iteration 24327, loss = 1.44755965\n",
      "Iteration 24328, loss = 1.44755843\n",
      "Iteration 24329, loss = 1.44755720\n",
      "Iteration 24330, loss = 1.44755598\n",
      "Iteration 24331, loss = 1.44755476\n",
      "Iteration 24332, loss = 1.44755354\n",
      "Iteration 24333, loss = 1.44755232\n",
      "Iteration 24334, loss = 1.44755110\n",
      "Iteration 24335, loss = 1.44754988\n",
      "Iteration 24336, loss = 1.44754866\n",
      "Iteration 24337, loss = 1.44754744\n",
      "Iteration 24338, loss = 1.44754622\n",
      "Iteration 24339, loss = 1.44754500\n",
      "Iteration 24340, loss = 1.44754379\n",
      "Iteration 24341, loss = 1.44754257\n",
      "Iteration 24342, loss = 1.44754135\n",
      "Iteration 24343, loss = 1.44754013\n",
      "Iteration 24344, loss = 1.44753891\n",
      "Iteration 24345, loss = 1.44753769\n",
      "Iteration 24346, loss = 1.44753647\n",
      "Iteration 24347, loss = 1.44753525\n",
      "Iteration 24348, loss = 1.44753403\n",
      "Iteration 24349, loss = 1.44753282\n",
      "Iteration 24350, loss = 1.44753160\n",
      "Iteration 24351, loss = 1.44753038\n",
      "Iteration 24352, loss = 1.44752916\n",
      "Iteration 24353, loss = 1.44752794\n",
      "Iteration 24354, loss = 1.44752673\n",
      "Iteration 24355, loss = 1.44752551\n",
      "Iteration 24356, loss = 1.44752429\n",
      "Iteration 24357, loss = 1.44752308\n",
      "Iteration 24358, loss = 1.44752186\n",
      "Iteration 24359, loss = 1.44752064\n",
      "Iteration 24360, loss = 1.44751942\n",
      "Iteration 24361, loss = 1.44751821\n",
      "Iteration 24362, loss = 1.44751699\n",
      "Iteration 24363, loss = 1.44751577\n",
      "Iteration 24364, loss = 1.44751456\n",
      "Iteration 24365, loss = 1.44751334\n",
      "Iteration 24366, loss = 1.44751213\n",
      "Iteration 24367, loss = 1.44751091\n",
      "Iteration 24368, loss = 1.44750969\n",
      "Iteration 24369, loss = 1.44750848\n",
      "Iteration 24370, loss = 1.44750726\n",
      "Iteration 24371, loss = 1.44750605\n",
      "Iteration 24372, loss = 1.44750483\n",
      "Iteration 24373, loss = 1.44750362\n",
      "Iteration 24374, loss = 1.44750240\n",
      "Iteration 24375, loss = 1.44750119\n",
      "Iteration 24376, loss = 1.44749997\n",
      "Iteration 24377, loss = 1.44749876\n",
      "Iteration 24378, loss = 1.44749754\n",
      "Iteration 24379, loss = 1.44749633\n",
      "Iteration 24380, loss = 1.44749511\n",
      "Iteration 24381, loss = 1.44749390\n",
      "Iteration 24382, loss = 1.44749268\n",
      "Iteration 24383, loss = 1.44749147\n",
      "Iteration 24384, loss = 1.44749025\n",
      "Iteration 24385, loss = 1.44748904\n",
      "Iteration 24386, loss = 1.44748783\n",
      "Iteration 24387, loss = 1.44748661\n",
      "Iteration 24388, loss = 1.44748540\n",
      "Iteration 24389, loss = 1.44748419\n",
      "Iteration 24390, loss = 1.44748297\n",
      "Iteration 24391, loss = 1.44748176\n",
      "Iteration 24392, loss = 1.44748055\n",
      "Iteration 24393, loss = 1.44747933\n",
      "Iteration 24394, loss = 1.44747812\n",
      "Iteration 24395, loss = 1.44747691\n",
      "Iteration 24396, loss = 1.44747570\n",
      "Iteration 24397, loss = 1.44747448\n",
      "Iteration 24398, loss = 1.44747327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24399, loss = 1.44747206\n",
      "Iteration 24400, loss = 1.44747085\n",
      "Iteration 24401, loss = 1.44746964\n",
      "Iteration 24402, loss = 1.44746842\n",
      "Iteration 24403, loss = 1.44746721\n",
      "Iteration 24404, loss = 1.44746600\n",
      "Iteration 24405, loss = 1.44746479\n",
      "Iteration 24406, loss = 1.44746358\n",
      "Iteration 24407, loss = 1.44746237\n",
      "Iteration 24408, loss = 1.44746116\n",
      "Iteration 24409, loss = 1.44745994\n",
      "Iteration 24410, loss = 1.44745873\n",
      "Iteration 24411, loss = 1.44745752\n",
      "Iteration 24412, loss = 1.44745631\n",
      "Iteration 24413, loss = 1.44745510\n",
      "Iteration 24414, loss = 1.44745389\n",
      "Iteration 24415, loss = 1.44745268\n",
      "Iteration 24416, loss = 1.44745147\n",
      "Iteration 24417, loss = 1.44745026\n",
      "Iteration 24418, loss = 1.44744905\n",
      "Iteration 24419, loss = 1.44744784\n",
      "Iteration 24420, loss = 1.44744663\n",
      "Iteration 24421, loss = 1.44744542\n",
      "Iteration 24422, loss = 1.44744421\n",
      "Iteration 24423, loss = 1.44744300\n",
      "Iteration 24424, loss = 1.44744179\n",
      "Iteration 24425, loss = 1.44744059\n",
      "Iteration 24426, loss = 1.44743938\n",
      "Iteration 24427, loss = 1.44743817\n",
      "Iteration 24428, loss = 1.44743696\n",
      "Iteration 24429, loss = 1.44743575\n",
      "Iteration 24430, loss = 1.44743454\n",
      "Iteration 24431, loss = 1.44743333\n",
      "Iteration 24432, loss = 1.44743213\n",
      "Iteration 24433, loss = 1.44743092\n",
      "Iteration 24434, loss = 1.44742971\n",
      "Iteration 24435, loss = 1.44742850\n",
      "Iteration 24436, loss = 1.44742729\n",
      "Iteration 24437, loss = 1.44742609\n",
      "Iteration 24438, loss = 1.44742488\n",
      "Iteration 24439, loss = 1.44742367\n",
      "Iteration 24440, loss = 1.44742246\n",
      "Iteration 24441, loss = 1.44742126\n",
      "Iteration 24442, loss = 1.44742005\n",
      "Iteration 24443, loss = 1.44741884\n",
      "Iteration 24444, loss = 1.44741764\n",
      "Iteration 24445, loss = 1.44741643\n",
      "Iteration 24446, loss = 1.44741522\n",
      "Iteration 24447, loss = 1.44741402\n",
      "Iteration 24448, loss = 1.44741281\n",
      "Iteration 24449, loss = 1.44741160\n",
      "Iteration 24450, loss = 1.44741040\n",
      "Iteration 24451, loss = 1.44740919\n",
      "Iteration 24452, loss = 1.44740799\n",
      "Iteration 24453, loss = 1.44740678\n",
      "Iteration 24454, loss = 1.44740558\n",
      "Iteration 24455, loss = 1.44740437\n",
      "Iteration 24456, loss = 1.44740316\n",
      "Iteration 24457, loss = 1.44740196\n",
      "Iteration 24458, loss = 1.44740075\n",
      "Iteration 24459, loss = 1.44739955\n",
      "Iteration 24460, loss = 1.44739834\n",
      "Iteration 24461, loss = 1.44739714\n",
      "Iteration 24462, loss = 1.44739594\n",
      "Iteration 24463, loss = 1.44739473\n",
      "Iteration 24464, loss = 1.44739353\n",
      "Iteration 24465, loss = 1.44739232\n",
      "Iteration 24466, loss = 1.44739112\n",
      "Iteration 24467, loss = 1.44738991\n",
      "Iteration 24468, loss = 1.44738871\n",
      "Iteration 24469, loss = 1.44738751\n",
      "Iteration 24470, loss = 1.44738630\n",
      "Iteration 24471, loss = 1.44738510\n",
      "Iteration 24472, loss = 1.44738390\n",
      "Iteration 24473, loss = 1.44738269\n",
      "Iteration 24474, loss = 1.44738149\n",
      "Iteration 24475, loss = 1.44738029\n",
      "Iteration 24476, loss = 1.44737908\n",
      "Iteration 24477, loss = 1.44737788\n",
      "Iteration 24478, loss = 1.44737668\n",
      "Iteration 24479, loss = 1.44737548\n",
      "Iteration 24480, loss = 1.44737427\n",
      "Iteration 24481, loss = 1.44737307\n",
      "Iteration 24482, loss = 1.44737187\n",
      "Iteration 24483, loss = 1.44737067\n",
      "Iteration 24484, loss = 1.44736946\n",
      "Iteration 24485, loss = 1.44736826\n",
      "Iteration 24486, loss = 1.44736706\n",
      "Iteration 24487, loss = 1.44736586\n",
      "Iteration 24488, loss = 1.44736466\n",
      "Iteration 24489, loss = 1.44736346\n",
      "Iteration 24490, loss = 1.44736226\n",
      "Iteration 24491, loss = 1.44736105\n",
      "Iteration 24492, loss = 1.44735985\n",
      "Iteration 24493, loss = 1.44735865\n",
      "Iteration 24494, loss = 1.44735745\n",
      "Iteration 24495, loss = 1.44735625\n",
      "Iteration 24496, loss = 1.44735505\n",
      "Iteration 24497, loss = 1.44735385\n",
      "Iteration 24498, loss = 1.44735265\n",
      "Iteration 24499, loss = 1.44735145\n",
      "Iteration 24500, loss = 1.44735025\n",
      "Iteration 24501, loss = 1.44734905\n",
      "Iteration 24502, loss = 1.44734785\n",
      "Iteration 24503, loss = 1.44734665\n",
      "Iteration 24504, loss = 1.44734545\n",
      "Iteration 24505, loss = 1.44734425\n",
      "Iteration 24506, loss = 1.44734305\n",
      "Iteration 24507, loss = 1.44734185\n",
      "Iteration 24508, loss = 1.44734065\n",
      "Iteration 24509, loss = 1.44733945\n",
      "Iteration 24510, loss = 1.44733826\n",
      "Iteration 24511, loss = 1.44733706\n",
      "Iteration 24512, loss = 1.44733586\n",
      "Iteration 24513, loss = 1.44733466\n",
      "Iteration 24514, loss = 1.44733346\n",
      "Iteration 24515, loss = 1.44733226\n",
      "Iteration 24516, loss = 1.44733106\n",
      "Iteration 24517, loss = 1.44732987\n",
      "Iteration 24518, loss = 1.44732867\n",
      "Iteration 24519, loss = 1.44732747\n",
      "Iteration 24520, loss = 1.44732627\n",
      "Iteration 24521, loss = 1.44732508\n",
      "Iteration 24522, loss = 1.44732388\n",
      "Iteration 24523, loss = 1.44732268\n",
      "Iteration 24524, loss = 1.44732148\n",
      "Iteration 24525, loss = 1.44732029\n",
      "Iteration 24526, loss = 1.44731909\n",
      "Iteration 24527, loss = 1.44731789\n",
      "Iteration 24528, loss = 1.44731670\n",
      "Iteration 24529, loss = 1.44731550\n",
      "Iteration 24530, loss = 1.44731430\n",
      "Iteration 24531, loss = 1.44731311\n",
      "Iteration 24532, loss = 1.44731191\n",
      "Iteration 24533, loss = 1.44731071\n",
      "Iteration 24534, loss = 1.44730952\n",
      "Iteration 24535, loss = 1.44730832\n",
      "Iteration 24536, loss = 1.44730713\n",
      "Iteration 24537, loss = 1.44730593\n",
      "Iteration 24538, loss = 1.44730474\n",
      "Iteration 24539, loss = 1.44730354\n",
      "Iteration 24540, loss = 1.44730234\n",
      "Iteration 24541, loss = 1.44730115\n",
      "Iteration 24542, loss = 1.44729995\n",
      "Iteration 24543, loss = 1.44729876\n",
      "Iteration 24544, loss = 1.44729756\n",
      "Iteration 24545, loss = 1.44729637\n",
      "Iteration 24546, loss = 1.44729518\n",
      "Iteration 24547, loss = 1.44729398\n",
      "Iteration 24548, loss = 1.44729279\n",
      "Iteration 24549, loss = 1.44729159\n",
      "Iteration 24550, loss = 1.44729040\n",
      "Iteration 24551, loss = 1.44728920\n",
      "Iteration 24552, loss = 1.44728801\n",
      "Iteration 24553, loss = 1.44728682\n",
      "Iteration 24554, loss = 1.44728562\n",
      "Iteration 24555, loss = 1.44728443\n",
      "Iteration 24556, loss = 1.44728324\n",
      "Iteration 24557, loss = 1.44728204\n",
      "Iteration 24558, loss = 1.44728085\n",
      "Iteration 24559, loss = 1.44727966\n",
      "Iteration 24560, loss = 1.44727846\n",
      "Iteration 24561, loss = 1.44727727\n",
      "Iteration 24562, loss = 1.44727608\n",
      "Iteration 24563, loss = 1.44727489\n",
      "Iteration 24564, loss = 1.44727369\n",
      "Iteration 24565, loss = 1.44727250\n",
      "Iteration 24566, loss = 1.44727131\n",
      "Iteration 24567, loss = 1.44727012\n",
      "Iteration 24568, loss = 1.44726892\n",
      "Iteration 24569, loss = 1.44726773\n",
      "Iteration 24570, loss = 1.44726654\n",
      "Iteration 24571, loss = 1.44726535\n",
      "Iteration 24572, loss = 1.44726416\n",
      "Iteration 24573, loss = 1.44726297\n",
      "Iteration 24574, loss = 1.44726178\n",
      "Iteration 24575, loss = 1.44726058\n",
      "Iteration 24576, loss = 1.44725939\n",
      "Iteration 24577, loss = 1.44725820\n",
      "Iteration 24578, loss = 1.44725701\n",
      "Iteration 24579, loss = 1.44725582\n",
      "Iteration 24580, loss = 1.44725463\n",
      "Iteration 24581, loss = 1.44725344\n",
      "Iteration 24582, loss = 1.44725225\n",
      "Iteration 24583, loss = 1.44725106\n",
      "Iteration 24584, loss = 1.44724987\n",
      "Iteration 24585, loss = 1.44724868\n",
      "Iteration 24586, loss = 1.44724749\n",
      "Iteration 24587, loss = 1.44724630\n",
      "Iteration 24588, loss = 1.44724511\n",
      "Iteration 24589, loss = 1.44724392\n",
      "Iteration 24590, loss = 1.44724273\n",
      "Iteration 24591, loss = 1.44724154\n",
      "Iteration 24592, loss = 1.44724035\n",
      "Iteration 24593, loss = 1.44723916\n",
      "Iteration 24594, loss = 1.44723797\n",
      "Iteration 24595, loss = 1.44723679\n",
      "Iteration 24596, loss = 1.44723560\n",
      "Iteration 24597, loss = 1.44723441\n",
      "Iteration 24598, loss = 1.44723322\n",
      "Iteration 24599, loss = 1.44723203\n",
      "Iteration 24600, loss = 1.44723084\n",
      "Iteration 24601, loss = 1.44722966\n",
      "Iteration 24602, loss = 1.44722847\n",
      "Iteration 24603, loss = 1.44722728\n",
      "Iteration 24604, loss = 1.44722609\n",
      "Iteration 24605, loss = 1.44722490\n",
      "Iteration 24606, loss = 1.44722372\n",
      "Iteration 24607, loss = 1.44722253\n",
      "Iteration 24608, loss = 1.44722134\n",
      "Iteration 24609, loss = 1.44722015\n",
      "Iteration 24610, loss = 1.44721897\n",
      "Iteration 24611, loss = 1.44721778\n",
      "Iteration 24612, loss = 1.44721659\n",
      "Iteration 24613, loss = 1.44721541\n",
      "Iteration 24614, loss = 1.44721422\n",
      "Iteration 24615, loss = 1.44721303\n",
      "Iteration 24616, loss = 1.44721185\n",
      "Iteration 24617, loss = 1.44721066\n",
      "Iteration 24618, loss = 1.44720948\n",
      "Iteration 24619, loss = 1.44720829\n",
      "Iteration 24620, loss = 1.44720710\n",
      "Iteration 24621, loss = 1.44720592\n",
      "Iteration 24622, loss = 1.44720473\n",
      "Iteration 24623, loss = 1.44720355\n",
      "Iteration 24624, loss = 1.44720236\n",
      "Iteration 24625, loss = 1.44720118\n",
      "Iteration 24626, loss = 1.44719999\n",
      "Iteration 24627, loss = 1.44719881\n",
      "Iteration 24628, loss = 1.44719762\n",
      "Iteration 24629, loss = 1.44719644\n",
      "Iteration 24630, loss = 1.44719525\n",
      "Iteration 24631, loss = 1.44719407\n",
      "Iteration 24632, loss = 1.44719288\n",
      "Iteration 24633, loss = 1.44719170\n",
      "Iteration 24634, loss = 1.44719051\n",
      "Iteration 24635, loss = 1.44718933\n",
      "Iteration 24636, loss = 1.44718815\n",
      "Iteration 24637, loss = 1.44718696\n",
      "Iteration 24638, loss = 1.44718578\n",
      "Iteration 24639, loss = 1.44718460\n",
      "Iteration 24640, loss = 1.44718341\n",
      "Iteration 24641, loss = 1.44718223\n",
      "Iteration 24642, loss = 1.44718104\n",
      "Iteration 24643, loss = 1.44717986\n",
      "Iteration 24644, loss = 1.44717868\n",
      "Iteration 24645, loss = 1.44717750\n",
      "Iteration 24646, loss = 1.44717631\n",
      "Iteration 24647, loss = 1.44717513\n",
      "Iteration 24648, loss = 1.44717395\n",
      "Iteration 24649, loss = 1.44717277\n",
      "Iteration 24650, loss = 1.44717158\n",
      "Iteration 24651, loss = 1.44717040\n",
      "Iteration 24652, loss = 1.44716922\n",
      "Iteration 24653, loss = 1.44716804\n",
      "Iteration 24654, loss = 1.44716685\n",
      "Iteration 24655, loss = 1.44716567\n",
      "Iteration 24656, loss = 1.44716449\n",
      "Iteration 24657, loss = 1.44716331\n",
      "Iteration 24658, loss = 1.44716213\n",
      "Iteration 24659, loss = 1.44716095\n",
      "Iteration 24660, loss = 1.44715977\n",
      "Iteration 24661, loss = 1.44715858\n",
      "Iteration 24662, loss = 1.44715740\n",
      "Iteration 24663, loss = 1.44715622\n",
      "Iteration 24664, loss = 1.44715504\n",
      "Iteration 24665, loss = 1.44715386\n",
      "Iteration 24666, loss = 1.44715268\n",
      "Iteration 24667, loss = 1.44715150\n",
      "Iteration 24668, loss = 1.44715032\n",
      "Iteration 24669, loss = 1.44714914\n",
      "Iteration 24670, loss = 1.44714796\n",
      "Iteration 24671, loss = 1.44714678\n",
      "Iteration 24672, loss = 1.44714560\n",
      "Iteration 24673, loss = 1.44714442\n",
      "Iteration 24674, loss = 1.44714324\n",
      "Iteration 24675, loss = 1.44714206\n",
      "Iteration 24676, loss = 1.44714088\n",
      "Iteration 24677, loss = 1.44713970\n",
      "Iteration 24678, loss = 1.44713852\n",
      "Iteration 24679, loss = 1.44713735\n",
      "Iteration 24680, loss = 1.44713617\n",
      "Iteration 24681, loss = 1.44713499\n",
      "Iteration 24682, loss = 1.44713381\n",
      "Iteration 24683, loss = 1.44713263\n",
      "Iteration 24684, loss = 1.44713145\n",
      "Iteration 24685, loss = 1.44713027\n",
      "Iteration 24686, loss = 1.44712910\n",
      "Iteration 24687, loss = 1.44712792\n",
      "Iteration 24688, loss = 1.44712674\n",
      "Iteration 24689, loss = 1.44712556\n",
      "Iteration 24690, loss = 1.44712439\n",
      "Iteration 24691, loss = 1.44712321\n",
      "Iteration 24692, loss = 1.44712203\n",
      "Iteration 24693, loss = 1.44712085\n",
      "Iteration 24694, loss = 1.44711968\n",
      "Iteration 24695, loss = 1.44711850\n",
      "Iteration 24696, loss = 1.44711732\n",
      "Iteration 24697, loss = 1.44711614\n",
      "Iteration 24698, loss = 1.44711497\n",
      "Iteration 24699, loss = 1.44711379\n",
      "Iteration 24700, loss = 1.44711261\n",
      "Iteration 24701, loss = 1.44711144\n",
      "Iteration 24702, loss = 1.44711026\n",
      "Iteration 24703, loss = 1.44710909\n",
      "Iteration 24704, loss = 1.44710791\n",
      "Iteration 24705, loss = 1.44710673\n",
      "Iteration 24706, loss = 1.44710556\n",
      "Iteration 24707, loss = 1.44710438\n",
      "Iteration 24708, loss = 1.44710321\n",
      "Iteration 24709, loss = 1.44710203\n",
      "Iteration 24710, loss = 1.44710086\n",
      "Iteration 24711, loss = 1.44709968\n",
      "Iteration 24712, loss = 1.44709851\n",
      "Iteration 24713, loss = 1.44709733\n",
      "Iteration 24714, loss = 1.44709616\n",
      "Iteration 24715, loss = 1.44709498\n",
      "Iteration 24716, loss = 1.44709381\n",
      "Iteration 24717, loss = 1.44709263\n",
      "Iteration 24718, loss = 1.44709146\n",
      "Iteration 24719, loss = 1.44709028\n",
      "Iteration 24720, loss = 1.44708911\n",
      "Iteration 24721, loss = 1.44708793\n",
      "Iteration 24722, loss = 1.44708676\n",
      "Iteration 24723, loss = 1.44708559\n",
      "Iteration 24724, loss = 1.44708441\n",
      "Iteration 24725, loss = 1.44708324\n",
      "Iteration 24726, loss = 1.44708207\n",
      "Iteration 24727, loss = 1.44708089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24728, loss = 1.44707972\n",
      "Iteration 24729, loss = 1.44707855\n",
      "Iteration 24730, loss = 1.44707737\n",
      "Iteration 24731, loss = 1.44707620\n",
      "Iteration 24732, loss = 1.44707503\n",
      "Iteration 24733, loss = 1.44707386\n",
      "Iteration 24734, loss = 1.44707268\n",
      "Iteration 24735, loss = 1.44707151\n",
      "Iteration 24736, loss = 1.44707034\n",
      "Iteration 24737, loss = 1.44706917\n",
      "Iteration 24738, loss = 1.44706799\n",
      "Iteration 24739, loss = 1.44706682\n",
      "Iteration 24740, loss = 1.44706565\n",
      "Iteration 24741, loss = 1.44706448\n",
      "Iteration 24742, loss = 1.44706331\n",
      "Iteration 24743, loss = 1.44706213\n",
      "Iteration 24744, loss = 1.44706096\n",
      "Iteration 24745, loss = 1.44705979\n",
      "Iteration 24746, loss = 1.44705862\n",
      "Iteration 24747, loss = 1.44705745\n",
      "Iteration 24748, loss = 1.44705628\n",
      "Iteration 24749, loss = 1.44705511\n",
      "Iteration 24750, loss = 1.44705394\n",
      "Iteration 24751, loss = 1.44705277\n",
      "Iteration 24752, loss = 1.44705160\n",
      "Iteration 24753, loss = 1.44705043\n",
      "Iteration 24754, loss = 1.44704926\n",
      "Iteration 24755, loss = 1.44704809\n",
      "Iteration 24756, loss = 1.44704692\n",
      "Iteration 24757, loss = 1.44704575\n",
      "Iteration 24758, loss = 1.44704458\n",
      "Iteration 24759, loss = 1.44704341\n",
      "Iteration 24760, loss = 1.44704224\n",
      "Iteration 24761, loss = 1.44704107\n",
      "Iteration 24762, loss = 1.44703990\n",
      "Iteration 24763, loss = 1.44703873\n",
      "Iteration 24764, loss = 1.44703756\n",
      "Iteration 24765, loss = 1.44703639\n",
      "Iteration 24766, loss = 1.44703522\n",
      "Iteration 24767, loss = 1.44703405\n",
      "Iteration 24768, loss = 1.44703288\n",
      "Iteration 24769, loss = 1.44703172\n",
      "Iteration 24770, loss = 1.44703055\n",
      "Iteration 24771, loss = 1.44702938\n",
      "Iteration 24772, loss = 1.44702821\n",
      "Iteration 24773, loss = 1.44702704\n",
      "Iteration 24774, loss = 1.44702587\n",
      "Iteration 24775, loss = 1.44702471\n",
      "Iteration 24776, loss = 1.44702354\n",
      "Iteration 24777, loss = 1.44702237\n",
      "Iteration 24778, loss = 1.44702120\n",
      "Iteration 24779, loss = 1.44702004\n",
      "Iteration 24780, loss = 1.44701887\n",
      "Iteration 24781, loss = 1.44701770\n",
      "Iteration 24782, loss = 1.44701654\n",
      "Iteration 24783, loss = 1.44701537\n",
      "Iteration 24784, loss = 1.44701420\n",
      "Iteration 24785, loss = 1.44701303\n",
      "Iteration 24786, loss = 1.44701187\n",
      "Iteration 24787, loss = 1.44701070\n",
      "Iteration 24788, loss = 1.44700954\n",
      "Iteration 24789, loss = 1.44700837\n",
      "Iteration 24790, loss = 1.44700720\n",
      "Iteration 24791, loss = 1.44700604\n",
      "Iteration 24792, loss = 1.44700487\n",
      "Iteration 24793, loss = 1.44700371\n",
      "Iteration 24794, loss = 1.44700254\n",
      "Iteration 24795, loss = 1.44700137\n",
      "Iteration 24796, loss = 1.44700021\n",
      "Iteration 24797, loss = 1.44699904\n",
      "Iteration 24798, loss = 1.44699788\n",
      "Iteration 24799, loss = 1.44699671\n",
      "Iteration 24800, loss = 1.44699555\n",
      "Iteration 24801, loss = 1.44699438\n",
      "Iteration 24802, loss = 1.44699322\n",
      "Iteration 24803, loss = 1.44699205\n",
      "Iteration 24804, loss = 1.44699089\n",
      "Iteration 24805, loss = 1.44698973\n",
      "Iteration 24806, loss = 1.44698856\n",
      "Iteration 24807, loss = 1.44698740\n",
      "Iteration 24808, loss = 1.44698623\n",
      "Iteration 24809, loss = 1.44698507\n",
      "Iteration 24810, loss = 1.44698391\n",
      "Iteration 24811, loss = 1.44698274\n",
      "Iteration 24812, loss = 1.44698158\n",
      "Iteration 24813, loss = 1.44698041\n",
      "Iteration 24814, loss = 1.44697925\n",
      "Iteration 24815, loss = 1.44697809\n",
      "Iteration 24816, loss = 1.44697692\n",
      "Iteration 24817, loss = 1.44697576\n",
      "Iteration 24818, loss = 1.44697460\n",
      "Iteration 24819, loss = 1.44697344\n",
      "Iteration 24820, loss = 1.44697227\n",
      "Iteration 24821, loss = 1.44697111\n",
      "Iteration 24822, loss = 1.44696995\n",
      "Iteration 24823, loss = 1.44696879\n",
      "Iteration 24824, loss = 1.44696762\n",
      "Iteration 24825, loss = 1.44696646\n",
      "Iteration 24826, loss = 1.44696530\n",
      "Iteration 24827, loss = 1.44696414\n",
      "Iteration 24828, loss = 1.44696298\n",
      "Iteration 24829, loss = 1.44696181\n",
      "Iteration 24830, loss = 1.44696065\n",
      "Iteration 24831, loss = 1.44695949\n",
      "Iteration 24832, loss = 1.44695833\n",
      "Iteration 24833, loss = 1.44695717\n",
      "Iteration 24834, loss = 1.44695601\n",
      "Iteration 24835, loss = 1.44695485\n",
      "Iteration 24836, loss = 1.44695369\n",
      "Iteration 24837, loss = 1.44695253\n",
      "Iteration 24838, loss = 1.44695136\n",
      "Iteration 24839, loss = 1.44695020\n",
      "Iteration 24840, loss = 1.44694904\n",
      "Iteration 24841, loss = 1.44694788\n",
      "Iteration 24842, loss = 1.44694672\n",
      "Iteration 24843, loss = 1.44694556\n",
      "Iteration 24844, loss = 1.44694440\n",
      "Iteration 24845, loss = 1.44694324\n",
      "Iteration 24846, loss = 1.44694208\n",
      "Iteration 24847, loss = 1.44694092\n",
      "Iteration 24848, loss = 1.44693977\n",
      "Iteration 24849, loss = 1.44693861\n",
      "Iteration 24850, loss = 1.44693745\n",
      "Iteration 24851, loss = 1.44693629\n",
      "Iteration 24852, loss = 1.44693513\n",
      "Iteration 24853, loss = 1.44693397\n",
      "Iteration 24854, loss = 1.44693281\n",
      "Iteration 24855, loss = 1.44693165\n",
      "Iteration 24856, loss = 1.44693049\n",
      "Iteration 24857, loss = 1.44692933\n",
      "Iteration 24858, loss = 1.44692818\n",
      "Iteration 24859, loss = 1.44692702\n",
      "Iteration 24860, loss = 1.44692586\n",
      "Iteration 24861, loss = 1.44692470\n",
      "Iteration 24862, loss = 1.44692354\n",
      "Iteration 24863, loss = 1.44692239\n",
      "Iteration 24864, loss = 1.44692123\n",
      "Iteration 24865, loss = 1.44692007\n",
      "Iteration 24866, loss = 1.44691891\n",
      "Iteration 24867, loss = 1.44691776\n",
      "Iteration 24868, loss = 1.44691660\n",
      "Iteration 24869, loss = 1.44691544\n",
      "Iteration 24870, loss = 1.44691429\n",
      "Iteration 24871, loss = 1.44691313\n",
      "Iteration 24872, loss = 1.44691197\n",
      "Iteration 24873, loss = 1.44691082\n",
      "Iteration 24874, loss = 1.44690966\n",
      "Iteration 24875, loss = 1.44690850\n",
      "Iteration 24876, loss = 1.44690735\n",
      "Iteration 24877, loss = 1.44690619\n",
      "Iteration 24878, loss = 1.44690503\n",
      "Iteration 24879, loss = 1.44690388\n",
      "Iteration 24880, loss = 1.44690272\n",
      "Iteration 24881, loss = 1.44690157\n",
      "Iteration 24882, loss = 1.44690041\n",
      "Iteration 24883, loss = 1.44689926\n",
      "Iteration 24884, loss = 1.44689810\n",
      "Iteration 24885, loss = 1.44689694\n",
      "Iteration 24886, loss = 1.44689579\n",
      "Iteration 24887, loss = 1.44689463\n",
      "Iteration 24888, loss = 1.44689348\n",
      "Iteration 24889, loss = 1.44689232\n",
      "Iteration 24890, loss = 1.44689117\n",
      "Iteration 24891, loss = 1.44689002\n",
      "Iteration 24892, loss = 1.44688886\n",
      "Iteration 24893, loss = 1.44688771\n",
      "Iteration 24894, loss = 1.44688655\n",
      "Iteration 24895, loss = 1.44688540\n",
      "Iteration 24896, loss = 1.44688424\n",
      "Iteration 24897, loss = 1.44688309\n",
      "Iteration 24898, loss = 1.44688194\n",
      "Iteration 24899, loss = 1.44688078\n",
      "Iteration 24900, loss = 1.44687963\n",
      "Iteration 24901, loss = 1.44687848\n",
      "Iteration 24902, loss = 1.44687732\n",
      "Iteration 24903, loss = 1.44687617\n",
      "Iteration 24904, loss = 1.44687502\n",
      "Iteration 24905, loss = 1.44687386\n",
      "Iteration 24906, loss = 1.44687271\n",
      "Iteration 24907, loss = 1.44687156\n",
      "Iteration 24908, loss = 1.44687041\n",
      "Iteration 24909, loss = 1.44686925\n",
      "Iteration 24910, loss = 1.44686810\n",
      "Iteration 24911, loss = 1.44686695\n",
      "Iteration 24912, loss = 1.44686580\n",
      "Iteration 24913, loss = 1.44686464\n",
      "Iteration 24914, loss = 1.44686349\n",
      "Iteration 24915, loss = 1.44686234\n",
      "Iteration 24916, loss = 1.44686119\n",
      "Iteration 24917, loss = 1.44686004\n",
      "Iteration 24918, loss = 1.44685888\n",
      "Iteration 24919, loss = 1.44685773\n",
      "Iteration 24920, loss = 1.44685658\n",
      "Iteration 24921, loss = 1.44685543\n",
      "Iteration 24922, loss = 1.44685428\n",
      "Iteration 24923, loss = 1.44685313\n",
      "Iteration 24924, loss = 1.44685198\n",
      "Iteration 24925, loss = 1.44685083\n",
      "Iteration 24926, loss = 1.44684968\n",
      "Iteration 24927, loss = 1.44684853\n",
      "Iteration 24928, loss = 1.44684738\n",
      "Iteration 24929, loss = 1.44684623\n",
      "Iteration 24930, loss = 1.44684508\n",
      "Iteration 24931, loss = 1.44684393\n",
      "Iteration 24932, loss = 1.44684278\n",
      "Iteration 24933, loss = 1.44684163\n",
      "Iteration 24934, loss = 1.44684048\n",
      "Iteration 24935, loss = 1.44683933\n",
      "Iteration 24936, loss = 1.44683818\n",
      "Iteration 24937, loss = 1.44683703\n",
      "Iteration 24938, loss = 1.44683588\n",
      "Iteration 24939, loss = 1.44683473\n",
      "Iteration 24940, loss = 1.44683358\n",
      "Iteration 24941, loss = 1.44683243\n",
      "Iteration 24942, loss = 1.44683128\n",
      "Iteration 24943, loss = 1.44683013\n",
      "Iteration 24944, loss = 1.44682898\n",
      "Iteration 24945, loss = 1.44682784\n",
      "Iteration 24946, loss = 1.44682669\n",
      "Iteration 24947, loss = 1.44682554\n",
      "Iteration 24948, loss = 1.44682439\n",
      "Iteration 24949, loss = 1.44682324\n",
      "Iteration 24950, loss = 1.44682210\n",
      "Iteration 24951, loss = 1.44682095\n",
      "Iteration 24952, loss = 1.44681980\n",
      "Iteration 24953, loss = 1.44681865\n",
      "Iteration 24954, loss = 1.44681751\n",
      "Iteration 24955, loss = 1.44681636\n",
      "Iteration 24956, loss = 1.44681521\n",
      "Iteration 24957, loss = 1.44681406\n",
      "Iteration 24958, loss = 1.44681292\n",
      "Iteration 24959, loss = 1.44681177\n",
      "Iteration 24960, loss = 1.44681062\n",
      "Iteration 24961, loss = 1.44680948\n",
      "Iteration 24962, loss = 1.44680833\n",
      "Iteration 24963, loss = 1.44680718\n",
      "Iteration 24964, loss = 1.44680604\n",
      "Iteration 24965, loss = 1.44680489\n",
      "Iteration 24966, loss = 1.44680374\n",
      "Iteration 24967, loss = 1.44680260\n",
      "Iteration 24968, loss = 1.44680145\n",
      "Iteration 24969, loss = 1.44680031\n",
      "Iteration 24970, loss = 1.44679916\n",
      "Iteration 24971, loss = 1.44679802\n",
      "Iteration 24972, loss = 1.44679687\n",
      "Iteration 24973, loss = 1.44679572\n",
      "Iteration 24974, loss = 1.44679458\n",
      "Iteration 24975, loss = 1.44679343\n",
      "Iteration 24976, loss = 1.44679229\n",
      "Iteration 24977, loss = 1.44679114\n",
      "Iteration 24978, loss = 1.44679000\n",
      "Iteration 24979, loss = 1.44678886\n",
      "Iteration 24980, loss = 1.44678771\n",
      "Iteration 24981, loss = 1.44678657\n",
      "Iteration 24982, loss = 1.44678542\n",
      "Iteration 24983, loss = 1.44678428\n",
      "Iteration 24984, loss = 1.44678313\n",
      "Iteration 24985, loss = 1.44678199\n",
      "Iteration 24986, loss = 1.44678085\n",
      "Iteration 24987, loss = 1.44677970\n",
      "Iteration 24988, loss = 1.44677856\n",
      "Iteration 24989, loss = 1.44677741\n",
      "Iteration 24990, loss = 1.44677627\n",
      "Iteration 24991, loss = 1.44677513\n",
      "Iteration 24992, loss = 1.44677399\n",
      "Iteration 24993, loss = 1.44677284\n",
      "Iteration 24994, loss = 1.44677170\n",
      "Iteration 24995, loss = 1.44677056\n",
      "Iteration 24996, loss = 1.44676941\n",
      "Iteration 24997, loss = 1.44676827\n",
      "Iteration 24998, loss = 1.44676713\n",
      "Iteration 24999, loss = 1.44676599\n",
      "Iteration 25000, loss = 1.44676484\n",
      "Iteration 25001, loss = 1.44676370\n",
      "Iteration 25002, loss = 1.44676256\n",
      "Iteration 25003, loss = 1.44676142\n",
      "Iteration 25004, loss = 1.44676028\n",
      "Iteration 25005, loss = 1.44675913\n",
      "Iteration 25006, loss = 1.44675799\n",
      "Iteration 25007, loss = 1.44675685\n",
      "Iteration 25008, loss = 1.44675571\n",
      "Iteration 25009, loss = 1.44675457\n",
      "Iteration 25010, loss = 1.44675343\n",
      "Iteration 25011, loss = 1.44675229\n",
      "Iteration 25012, loss = 1.44675114\n",
      "Iteration 25013, loss = 1.44675000\n",
      "Iteration 25014, loss = 1.44674886\n",
      "Iteration 25015, loss = 1.44674772\n",
      "Iteration 25016, loss = 1.44674658\n",
      "Iteration 25017, loss = 1.44674544\n",
      "Iteration 25018, loss = 1.44674430\n",
      "Iteration 25019, loss = 1.44674316\n",
      "Iteration 25020, loss = 1.44674202\n",
      "Iteration 25021, loss = 1.44674088\n",
      "Iteration 25022, loss = 1.44673974\n",
      "Iteration 25023, loss = 1.44673860\n",
      "Iteration 25024, loss = 1.44673746\n",
      "Iteration 25025, loss = 1.44673632\n",
      "Iteration 25026, loss = 1.44673518\n",
      "Iteration 25027, loss = 1.44673404\n",
      "Iteration 25028, loss = 1.44673290\n",
      "Iteration 25029, loss = 1.44673177\n",
      "Iteration 25030, loss = 1.44673063\n",
      "Iteration 25031, loss = 1.44672949\n",
      "Iteration 25032, loss = 1.44672835\n",
      "Iteration 25033, loss = 1.44672721\n",
      "Iteration 25034, loss = 1.44672607\n",
      "Iteration 25035, loss = 1.44672493\n",
      "Iteration 25036, loss = 1.44672380\n",
      "Iteration 25037, loss = 1.44672266\n",
      "Iteration 25038, loss = 1.44672152\n",
      "Iteration 25039, loss = 1.44672038\n",
      "Iteration 25040, loss = 1.44671924\n",
      "Iteration 25041, loss = 1.44671811\n",
      "Iteration 25042, loss = 1.44671697\n",
      "Iteration 25043, loss = 1.44671583\n",
      "Iteration 25044, loss = 1.44671469\n",
      "Iteration 25045, loss = 1.44671356\n",
      "Iteration 25046, loss = 1.44671242\n",
      "Iteration 25047, loss = 1.44671128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25048, loss = 1.44671014\n",
      "Iteration 25049, loss = 1.44670901\n",
      "Iteration 25050, loss = 1.44670787\n",
      "Iteration 25051, loss = 1.44670673\n",
      "Iteration 25052, loss = 1.44670560\n",
      "Iteration 25053, loss = 1.44670446\n",
      "Iteration 25054, loss = 1.44670333\n",
      "Iteration 25055, loss = 1.44670219\n",
      "Iteration 25056, loss = 1.44670105\n",
      "Iteration 25057, loss = 1.44669992\n",
      "Iteration 25058, loss = 1.44669878\n",
      "Iteration 25059, loss = 1.44669765\n",
      "Iteration 25060, loss = 1.44669651\n",
      "Iteration 25061, loss = 1.44669537\n",
      "Iteration 25062, loss = 1.44669424\n",
      "Iteration 25063, loss = 1.44669310\n",
      "Iteration 25064, loss = 1.44669197\n",
      "Iteration 25065, loss = 1.44669083\n",
      "Iteration 25066, loss = 1.44668970\n",
      "Iteration 25067, loss = 1.44668856\n",
      "Iteration 25068, loss = 1.44668743\n",
      "Iteration 25069, loss = 1.44668629\n",
      "Iteration 25070, loss = 1.44668516\n",
      "Iteration 25071, loss = 1.44668403\n",
      "Iteration 25072, loss = 1.44668289\n",
      "Iteration 25073, loss = 1.44668176\n",
      "Iteration 25074, loss = 1.44668062\n",
      "Iteration 25075, loss = 1.44667949\n",
      "Iteration 25076, loss = 1.44667836\n",
      "Iteration 25077, loss = 1.44667722\n",
      "Iteration 25078, loss = 1.44667609\n",
      "Iteration 25079, loss = 1.44667495\n",
      "Iteration 25080, loss = 1.44667382\n",
      "Iteration 25081, loss = 1.44667269\n",
      "Iteration 25082, loss = 1.44667155\n",
      "Iteration 25083, loss = 1.44667042\n",
      "Iteration 25084, loss = 1.44666929\n",
      "Iteration 25085, loss = 1.44666816\n",
      "Iteration 25086, loss = 1.44666702\n",
      "Iteration 25087, loss = 1.44666589\n",
      "Iteration 25088, loss = 1.44666476\n",
      "Iteration 25089, loss = 1.44666363\n",
      "Iteration 25090, loss = 1.44666249\n",
      "Iteration 25091, loss = 1.44666136\n",
      "Iteration 25092, loss = 1.44666023\n",
      "Iteration 25093, loss = 1.44665910\n",
      "Iteration 25094, loss = 1.44665796\n",
      "Iteration 25095, loss = 1.44665683\n",
      "Iteration 25096, loss = 1.44665570\n",
      "Iteration 25097, loss = 1.44665457\n",
      "Iteration 25098, loss = 1.44665344\n",
      "Iteration 25099, loss = 1.44665231\n",
      "Iteration 25100, loss = 1.44665118\n",
      "Iteration 25101, loss = 1.44665005\n",
      "Iteration 25102, loss = 1.44664891\n",
      "Iteration 25103, loss = 1.44664778\n",
      "Iteration 25104, loss = 1.44664665\n",
      "Iteration 25105, loss = 1.44664552\n",
      "Iteration 25106, loss = 1.44664439\n",
      "Iteration 25107, loss = 1.44664326\n",
      "Iteration 25108, loss = 1.44664213\n",
      "Iteration 25109, loss = 1.44664100\n",
      "Iteration 25110, loss = 1.44663987\n",
      "Iteration 25111, loss = 1.44663874\n",
      "Iteration 25112, loss = 1.44663761\n",
      "Iteration 25113, loss = 1.44663648\n",
      "Iteration 25114, loss = 1.44663535\n",
      "Iteration 25115, loss = 1.44663422\n",
      "Iteration 25116, loss = 1.44663309\n",
      "Iteration 25117, loss = 1.44663196\n",
      "Iteration 25118, loss = 1.44663083\n",
      "Iteration 25119, loss = 1.44662970\n",
      "Iteration 25120, loss = 1.44662858\n",
      "Iteration 25121, loss = 1.44662745\n",
      "Iteration 25122, loss = 1.44662632\n",
      "Iteration 25123, loss = 1.44662519\n",
      "Iteration 25124, loss = 1.44662406\n",
      "Iteration 25125, loss = 1.44662293\n",
      "Iteration 25126, loss = 1.44662180\n",
      "Iteration 25127, loss = 1.44662068\n",
      "Iteration 25128, loss = 1.44661955\n",
      "Iteration 25129, loss = 1.44661842\n",
      "Iteration 25130, loss = 1.44661729\n",
      "Iteration 25131, loss = 1.44661616\n",
      "Iteration 25132, loss = 1.44661504\n",
      "Iteration 25133, loss = 1.44661391\n",
      "Iteration 25134, loss = 1.44661278\n",
      "Iteration 25135, loss = 1.44661165\n",
      "Iteration 25136, loss = 1.44661053\n",
      "Iteration 25137, loss = 1.44660940\n",
      "Iteration 25138, loss = 1.44660827\n",
      "Iteration 25139, loss = 1.44660715\n",
      "Iteration 25140, loss = 1.44660602\n",
      "Iteration 25141, loss = 1.44660489\n",
      "Iteration 25142, loss = 1.44660377\n",
      "Iteration 25143, loss = 1.44660264\n",
      "Iteration 25144, loss = 1.44660151\n",
      "Iteration 25145, loss = 1.44660039\n",
      "Iteration 25146, loss = 1.44659926\n",
      "Iteration 25147, loss = 1.44659813\n",
      "Iteration 25148, loss = 1.44659701\n",
      "Iteration 25149, loss = 1.44659588\n",
      "Iteration 25150, loss = 1.44659476\n",
      "Iteration 25151, loss = 1.44659363\n",
      "Iteration 25152, loss = 1.44659251\n",
      "Iteration 25153, loss = 1.44659138\n",
      "Iteration 25154, loss = 1.44659026\n",
      "Iteration 25155, loss = 1.44658913\n",
      "Iteration 25156, loss = 1.44658801\n",
      "Iteration 25157, loss = 1.44658688\n",
      "Iteration 25158, loss = 1.44658576\n",
      "Iteration 25159, loss = 1.44658463\n",
      "Iteration 25160, loss = 1.44658351\n",
      "Iteration 25161, loss = 1.44658238\n",
      "Iteration 25162, loss = 1.44658126\n",
      "Iteration 25163, loss = 1.44658013\n",
      "Iteration 25164, loss = 1.44657901\n",
      "Iteration 25165, loss = 1.44657789\n",
      "Iteration 25166, loss = 1.44657676\n",
      "Iteration 25167, loss = 1.44657564\n",
      "Iteration 25168, loss = 1.44657451\n",
      "Iteration 25169, loss = 1.44657339\n",
      "Iteration 25170, loss = 1.44657227\n",
      "Iteration 25171, loss = 1.44657114\n",
      "Iteration 25172, loss = 1.44657002\n",
      "Iteration 25173, loss = 1.44656890\n",
      "Iteration 25174, loss = 1.44656777\n",
      "Iteration 25175, loss = 1.44656665\n",
      "Iteration 25176, loss = 1.44656553\n",
      "Iteration 25177, loss = 1.44656441\n",
      "Iteration 25178, loss = 1.44656328\n",
      "Iteration 25179, loss = 1.44656216\n",
      "Iteration 25180, loss = 1.44656104\n",
      "Iteration 25181, loss = 1.44655992\n",
      "Iteration 25182, loss = 1.44655879\n",
      "Iteration 25183, loss = 1.44655767\n",
      "Iteration 25184, loss = 1.44655655\n",
      "Iteration 25185, loss = 1.44655543\n",
      "Iteration 25186, loss = 1.44655431\n",
      "Iteration 25187, loss = 1.44655318\n",
      "Iteration 25188, loss = 1.44655206\n",
      "Iteration 25189, loss = 1.44655094\n",
      "Iteration 25190, loss = 1.44654982\n",
      "Iteration 25191, loss = 1.44654870\n",
      "Iteration 25192, loss = 1.44654758\n",
      "Iteration 25193, loss = 1.44654646\n",
      "Iteration 25194, loss = 1.44654534\n",
      "Iteration 25195, loss = 1.44654422\n",
      "Iteration 25196, loss = 1.44654309\n",
      "Iteration 25197, loss = 1.44654197\n",
      "Iteration 25198, loss = 1.44654085\n",
      "Iteration 25199, loss = 1.44653973\n",
      "Iteration 25200, loss = 1.44653861\n",
      "Iteration 25201, loss = 1.44653749\n",
      "Iteration 25202, loss = 1.44653637\n",
      "Iteration 25203, loss = 1.44653525\n",
      "Iteration 25204, loss = 1.44653413\n",
      "Iteration 25205, loss = 1.44653301\n",
      "Iteration 25206, loss = 1.44653189\n",
      "Iteration 25207, loss = 1.44653078\n",
      "Iteration 25208, loss = 1.44652966\n",
      "Iteration 25209, loss = 1.44652854\n",
      "Iteration 25210, loss = 1.44652742\n",
      "Iteration 25211, loss = 1.44652630\n",
      "Iteration 25212, loss = 1.44652518\n",
      "Iteration 25213, loss = 1.44652406\n",
      "Iteration 25214, loss = 1.44652294\n",
      "Iteration 25215, loss = 1.44652182\n",
      "Iteration 25216, loss = 1.44652070\n",
      "Iteration 25217, loss = 1.44651959\n",
      "Iteration 25218, loss = 1.44651847\n",
      "Iteration 25219, loss = 1.44651735\n",
      "Iteration 25220, loss = 1.44651623\n",
      "Iteration 25221, loss = 1.44651511\n",
      "Iteration 25222, loss = 1.44651400\n",
      "Iteration 25223, loss = 1.44651288\n",
      "Iteration 25224, loss = 1.44651176\n",
      "Iteration 25225, loss = 1.44651064\n",
      "Iteration 25226, loss = 1.44650953\n",
      "Iteration 25227, loss = 1.44650841\n",
      "Iteration 25228, loss = 1.44650729\n",
      "Iteration 25229, loss = 1.44650617\n",
      "Iteration 25230, loss = 1.44650506\n",
      "Iteration 25231, loss = 1.44650394\n",
      "Iteration 25232, loss = 1.44650282\n",
      "Iteration 25233, loss = 1.44650171\n",
      "Iteration 25234, loss = 1.44650059\n",
      "Iteration 25235, loss = 1.44649947\n",
      "Iteration 25236, loss = 1.44649836\n",
      "Iteration 25237, loss = 1.44649724\n",
      "Iteration 25238, loss = 1.44649613\n",
      "Iteration 25239, loss = 1.44649501\n",
      "Iteration 25240, loss = 1.44649389\n",
      "Iteration 25241, loss = 1.44649278\n",
      "Iteration 25242, loss = 1.44649166\n",
      "Iteration 25243, loss = 1.44649055\n",
      "Iteration 25244, loss = 1.44648943\n",
      "Iteration 25245, loss = 1.44648832\n",
      "Iteration 25246, loss = 1.44648720\n",
      "Iteration 25247, loss = 1.44648609\n",
      "Iteration 25248, loss = 1.44648497\n",
      "Iteration 25249, loss = 1.44648386\n",
      "Iteration 25250, loss = 1.44648274\n",
      "Iteration 25251, loss = 1.44648163\n",
      "Iteration 25252, loss = 1.44648051\n",
      "Iteration 25253, loss = 1.44647940\n",
      "Iteration 25254, loss = 1.44647828\n",
      "Iteration 25255, loss = 1.44647717\n",
      "Iteration 25256, loss = 1.44647606\n",
      "Iteration 25257, loss = 1.44647494\n",
      "Iteration 25258, loss = 1.44647383\n",
      "Iteration 25259, loss = 1.44647271\n",
      "Iteration 25260, loss = 1.44647160\n",
      "Iteration 25261, loss = 1.44647049\n",
      "Iteration 25262, loss = 1.44646937\n",
      "Iteration 25263, loss = 1.44646826\n",
      "Iteration 25264, loss = 1.44646715\n",
      "Iteration 25265, loss = 1.44646603\n",
      "Iteration 25266, loss = 1.44646492\n",
      "Iteration 25267, loss = 1.44646381\n",
      "Iteration 25268, loss = 1.44646269\n",
      "Iteration 25269, loss = 1.44646158\n",
      "Iteration 25270, loss = 1.44646047\n",
      "Iteration 25271, loss = 1.44645936\n",
      "Iteration 25272, loss = 1.44645824\n",
      "Iteration 25273, loss = 1.44645713\n",
      "Iteration 25274, loss = 1.44645602\n",
      "Iteration 25275, loss = 1.44645491\n",
      "Iteration 25276, loss = 1.44645380\n",
      "Iteration 25277, loss = 1.44645268\n",
      "Iteration 25278, loss = 1.44645157\n",
      "Iteration 25279, loss = 1.44645046\n",
      "Iteration 25280, loss = 1.44644935\n",
      "Iteration 25281, loss = 1.44644824\n",
      "Iteration 25282, loss = 1.44644713\n",
      "Iteration 25283, loss = 1.44644602\n",
      "Iteration 25284, loss = 1.44644490\n",
      "Iteration 25285, loss = 1.44644379\n",
      "Iteration 25286, loss = 1.44644268\n",
      "Iteration 25287, loss = 1.44644157\n",
      "Iteration 25288, loss = 1.44644046\n",
      "Iteration 25289, loss = 1.44643935\n",
      "Iteration 25290, loss = 1.44643824\n",
      "Iteration 25291, loss = 1.44643713\n",
      "Iteration 25292, loss = 1.44643602\n",
      "Iteration 25293, loss = 1.44643491\n",
      "Iteration 25294, loss = 1.44643380\n",
      "Iteration 25295, loss = 1.44643269\n",
      "Iteration 25296, loss = 1.44643158\n",
      "Iteration 25297, loss = 1.44643047\n",
      "Iteration 25298, loss = 1.44642936\n",
      "Iteration 25299, loss = 1.44642825\n",
      "Iteration 25300, loss = 1.44642714\n",
      "Iteration 25301, loss = 1.44642603\n",
      "Iteration 25302, loss = 1.44642492\n",
      "Iteration 25303, loss = 1.44642381\n",
      "Iteration 25304, loss = 1.44642271\n",
      "Iteration 25305, loss = 1.44642160\n",
      "Iteration 25306, loss = 1.44642049\n",
      "Iteration 25307, loss = 1.44641938\n",
      "Iteration 25308, loss = 1.44641827\n",
      "Iteration 25309, loss = 1.44641716\n",
      "Iteration 25310, loss = 1.44641605\n",
      "Iteration 25311, loss = 1.44641495\n",
      "Iteration 25312, loss = 1.44641384\n",
      "Iteration 25313, loss = 1.44641273\n",
      "Iteration 25314, loss = 1.44641162\n",
      "Iteration 25315, loss = 1.44641051\n",
      "Iteration 25316, loss = 1.44640941\n",
      "Iteration 25317, loss = 1.44640830\n",
      "Iteration 25318, loss = 1.44640719\n",
      "Iteration 25319, loss = 1.44640608\n",
      "Iteration 25320, loss = 1.44640498\n",
      "Iteration 25321, loss = 1.44640387\n",
      "Iteration 25322, loss = 1.44640276\n",
      "Iteration 25323, loss = 1.44640166\n",
      "Iteration 25324, loss = 1.44640055\n",
      "Iteration 25325, loss = 1.44639944\n",
      "Iteration 25326, loss = 1.44639834\n",
      "Iteration 25327, loss = 1.44639723\n",
      "Iteration 25328, loss = 1.44639612\n",
      "Iteration 25329, loss = 1.44639502\n",
      "Iteration 25330, loss = 1.44639391\n",
      "Iteration 25331, loss = 1.44639280\n",
      "Iteration 25332, loss = 1.44639170\n",
      "Iteration 25333, loss = 1.44639059\n",
      "Iteration 25334, loss = 1.44638949\n",
      "Iteration 25335, loss = 1.44638838\n",
      "Iteration 25336, loss = 1.44638728\n",
      "Iteration 25337, loss = 1.44638617\n",
      "Iteration 25338, loss = 1.44638507\n",
      "Iteration 25339, loss = 1.44638396\n",
      "Iteration 25340, loss = 1.44638286\n",
      "Iteration 25341, loss = 1.44638175\n",
      "Iteration 25342, loss = 1.44638065\n",
      "Iteration 25343, loss = 1.44637954\n",
      "Iteration 25344, loss = 1.44637844\n",
      "Iteration 25345, loss = 1.44637733\n",
      "Iteration 25346, loss = 1.44637623\n",
      "Iteration 25347, loss = 1.44637512\n",
      "Iteration 25348, loss = 1.44637402\n",
      "Iteration 25349, loss = 1.44637291\n",
      "Iteration 25350, loss = 1.44637181\n",
      "Iteration 25351, loss = 1.44637071\n",
      "Iteration 25352, loss = 1.44636960\n",
      "Iteration 25353, loss = 1.44636850\n",
      "Iteration 25354, loss = 1.44636740\n",
      "Iteration 25355, loss = 1.44636629\n",
      "Iteration 25356, loss = 1.44636519\n",
      "Iteration 25357, loss = 1.44636409\n",
      "Iteration 25358, loss = 1.44636298\n",
      "Iteration 25359, loss = 1.44636188\n",
      "Iteration 25360, loss = 1.44636078\n",
      "Iteration 25361, loss = 1.44635967\n",
      "Iteration 25362, loss = 1.44635857\n",
      "Iteration 25363, loss = 1.44635747\n",
      "Iteration 25364, loss = 1.44635637\n",
      "Iteration 25365, loss = 1.44635526\n",
      "Iteration 25366, loss = 1.44635416\n",
      "Iteration 25367, loss = 1.44635306\n",
      "Iteration 25368, loss = 1.44635196\n",
      "Iteration 25369, loss = 1.44635085\n",
      "Iteration 25370, loss = 1.44634975\n",
      "Iteration 25371, loss = 1.44634865\n",
      "Iteration 25372, loss = 1.44634755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25373, loss = 1.44634645\n",
      "Iteration 25374, loss = 1.44634535\n",
      "Iteration 25375, loss = 1.44634425\n",
      "Iteration 25376, loss = 1.44634314\n",
      "Iteration 25377, loss = 1.44634204\n",
      "Iteration 25378, loss = 1.44634094\n",
      "Iteration 25379, loss = 1.44633984\n",
      "Iteration 25380, loss = 1.44633874\n",
      "Iteration 25381, loss = 1.44633764\n",
      "Iteration 25382, loss = 1.44633654\n",
      "Iteration 25383, loss = 1.44633544\n",
      "Iteration 25384, loss = 1.44633434\n",
      "Iteration 25385, loss = 1.44633324\n",
      "Iteration 25386, loss = 1.44633214\n",
      "Iteration 25387, loss = 1.44633104\n",
      "Iteration 25388, loss = 1.44632994\n",
      "Iteration 25389, loss = 1.44632884\n",
      "Iteration 25390, loss = 1.44632774\n",
      "Iteration 25391, loss = 1.44632664\n",
      "Iteration 25392, loss = 1.44632554\n",
      "Iteration 25393, loss = 1.44632444\n",
      "Iteration 25394, loss = 1.44632334\n",
      "Iteration 25395, loss = 1.44632224\n",
      "Iteration 25396, loss = 1.44632114\n",
      "Iteration 25397, loss = 1.44632004\n",
      "Iteration 25398, loss = 1.44631894\n",
      "Iteration 25399, loss = 1.44631785\n",
      "Iteration 25400, loss = 1.44631675\n",
      "Iteration 25401, loss = 1.44631565\n",
      "Iteration 25402, loss = 1.44631455\n",
      "Iteration 25403, loss = 1.44631345\n",
      "Iteration 25404, loss = 1.44631235\n",
      "Iteration 25405, loss = 1.44631125\n",
      "Iteration 25406, loss = 1.44631016\n",
      "Iteration 25407, loss = 1.44630906\n",
      "Iteration 25408, loss = 1.44630796\n",
      "Iteration 25409, loss = 1.44630686\n",
      "Iteration 25410, loss = 1.44630577\n",
      "Iteration 25411, loss = 1.44630467\n",
      "Iteration 25412, loss = 1.44630357\n",
      "Iteration 25413, loss = 1.44630247\n",
      "Iteration 25414, loss = 1.44630138\n",
      "Iteration 25415, loss = 1.44630028\n",
      "Iteration 25416, loss = 1.44629918\n",
      "Iteration 25417, loss = 1.44629809\n",
      "Iteration 25418, loss = 1.44629699\n",
      "Iteration 25419, loss = 1.44629589\n",
      "Iteration 25420, loss = 1.44629480\n",
      "Iteration 25421, loss = 1.44629370\n",
      "Iteration 25422, loss = 1.44629260\n",
      "Iteration 25423, loss = 1.44629151\n",
      "Iteration 25424, loss = 1.44629041\n",
      "Iteration 25425, loss = 1.44628931\n",
      "Iteration 25426, loss = 1.44628822\n",
      "Iteration 25427, loss = 1.44628712\n",
      "Iteration 25428, loss = 1.44628603\n",
      "Iteration 25429, loss = 1.44628493\n",
      "Iteration 25430, loss = 1.44628384\n",
      "Iteration 25431, loss = 1.44628274\n",
      "Iteration 25432, loss = 1.44628165\n",
      "Iteration 25433, loss = 1.44628055\n",
      "Iteration 25434, loss = 1.44627946\n",
      "Iteration 25435, loss = 1.44627836\n",
      "Iteration 25436, loss = 1.44627727\n",
      "Iteration 25437, loss = 1.44627617\n",
      "Iteration 25438, loss = 1.44627508\n",
      "Iteration 25439, loss = 1.44627398\n",
      "Iteration 25440, loss = 1.44627289\n",
      "Iteration 25441, loss = 1.44627179\n",
      "Iteration 25442, loss = 1.44627070\n",
      "Iteration 25443, loss = 1.44626960\n",
      "Iteration 25444, loss = 1.44626851\n",
      "Iteration 25445, loss = 1.44626742\n",
      "Iteration 25446, loss = 1.44626632\n",
      "Iteration 25447, loss = 1.44626523\n",
      "Iteration 25448, loss = 1.44626414\n",
      "Iteration 25449, loss = 1.44626304\n",
      "Iteration 25450, loss = 1.44626195\n",
      "Iteration 25451, loss = 1.44626086\n",
      "Iteration 25452, loss = 1.44625976\n",
      "Iteration 25453, loss = 1.44625867\n",
      "Iteration 25454, loss = 1.44625758\n",
      "Iteration 25455, loss = 1.44625648\n",
      "Iteration 25456, loss = 1.44625539\n",
      "Iteration 25457, loss = 1.44625430\n",
      "Iteration 25458, loss = 1.44625321\n",
      "Iteration 25459, loss = 1.44625211\n",
      "Iteration 25460, loss = 1.44625102\n",
      "Iteration 25461, loss = 1.44624993\n",
      "Iteration 25462, loss = 1.44624884\n",
      "Iteration 25463, loss = 1.44624775\n",
      "Iteration 25464, loss = 1.44624665\n",
      "Iteration 25465, loss = 1.44624556\n",
      "Iteration 25466, loss = 1.44624447\n",
      "Iteration 25467, loss = 1.44624338\n",
      "Iteration 25468, loss = 1.44624229\n",
      "Iteration 25469, loss = 1.44624120\n",
      "Iteration 25470, loss = 1.44624010\n",
      "Iteration 25471, loss = 1.44623901\n",
      "Iteration 25472, loss = 1.44623792\n",
      "Iteration 25473, loss = 1.44623683\n",
      "Iteration 25474, loss = 1.44623574\n",
      "Iteration 25475, loss = 1.44623465\n",
      "Iteration 25476, loss = 1.44623356\n",
      "Iteration 25477, loss = 1.44623247\n",
      "Iteration 25478, loss = 1.44623138\n",
      "Iteration 25479, loss = 1.44623029\n",
      "Iteration 25480, loss = 1.44622920\n",
      "Iteration 25481, loss = 1.44622811\n",
      "Iteration 25482, loss = 1.44622702\n",
      "Iteration 25483, loss = 1.44622593\n",
      "Iteration 25484, loss = 1.44622484\n",
      "Iteration 25485, loss = 1.44622375\n",
      "Iteration 25486, loss = 1.44622266\n",
      "Iteration 25487, loss = 1.44622157\n",
      "Iteration 25488, loss = 1.44622048\n",
      "Iteration 25489, loss = 1.44621939\n",
      "Iteration 25490, loss = 1.44621830\n",
      "Iteration 25491, loss = 1.44621721\n",
      "Iteration 25492, loss = 1.44621612\n",
      "Iteration 25493, loss = 1.44621503\n",
      "Iteration 25494, loss = 1.44621395\n",
      "Iteration 25495, loss = 1.44621286\n",
      "Iteration 25496, loss = 1.44621177\n",
      "Iteration 25497, loss = 1.44621068\n",
      "Iteration 25498, loss = 1.44620959\n",
      "Iteration 25499, loss = 1.44620850\n",
      "Iteration 25500, loss = 1.44620742\n",
      "Iteration 25501, loss = 1.44620633\n",
      "Iteration 25502, loss = 1.44620524\n",
      "Iteration 25503, loss = 1.44620415\n",
      "Iteration 25504, loss = 1.44620306\n",
      "Iteration 25505, loss = 1.44620198\n",
      "Iteration 25506, loss = 1.44620089\n",
      "Iteration 25507, loss = 1.44619980\n",
      "Iteration 25508, loss = 1.44619871\n",
      "Iteration 25509, loss = 1.44619763\n",
      "Iteration 25510, loss = 1.44619654\n",
      "Iteration 25511, loss = 1.44619545\n",
      "Iteration 25512, loss = 1.44619437\n",
      "Iteration 25513, loss = 1.44619328\n",
      "Iteration 25514, loss = 1.44619219\n",
      "Iteration 25515, loss = 1.44619111\n",
      "Iteration 25516, loss = 1.44619002\n",
      "Iteration 25517, loss = 1.44618893\n",
      "Iteration 25518, loss = 1.44618785\n",
      "Iteration 25519, loss = 1.44618676\n",
      "Iteration 25520, loss = 1.44618568\n",
      "Iteration 25521, loss = 1.44618459\n",
      "Iteration 25522, loss = 1.44618350\n",
      "Iteration 25523, loss = 1.44618242\n",
      "Iteration 25524, loss = 1.44618133\n",
      "Iteration 25525, loss = 1.44618025\n",
      "Iteration 25526, loss = 1.44617916\n",
      "Iteration 25527, loss = 1.44617808\n",
      "Iteration 25528, loss = 1.44617699\n",
      "Iteration 25529, loss = 1.44617591\n",
      "Iteration 25530, loss = 1.44617482\n",
      "Iteration 25531, loss = 1.44617374\n",
      "Iteration 25532, loss = 1.44617265\n",
      "Iteration 25533, loss = 1.44617157\n",
      "Iteration 25534, loss = 1.44617048\n",
      "Iteration 25535, loss = 1.44616940\n",
      "Iteration 25536, loss = 1.44616831\n",
      "Iteration 25537, loss = 1.44616723\n",
      "Iteration 25538, loss = 1.44616615\n",
      "Iteration 25539, loss = 1.44616506\n",
      "Iteration 25540, loss = 1.44616398\n",
      "Iteration 25541, loss = 1.44616289\n",
      "Iteration 25542, loss = 1.44616181\n",
      "Iteration 25543, loss = 1.44616073\n",
      "Iteration 25544, loss = 1.44615964\n",
      "Iteration 25545, loss = 1.44615856\n",
      "Iteration 25546, loss = 1.44615748\n",
      "Iteration 25547, loss = 1.44615639\n",
      "Iteration 25548, loss = 1.44615531\n",
      "Iteration 25549, loss = 1.44615423\n",
      "Iteration 25550, loss = 1.44615314\n",
      "Iteration 25551, loss = 1.44615206\n",
      "Iteration 25552, loss = 1.44615098\n",
      "Iteration 25553, loss = 1.44614990\n",
      "Iteration 25554, loss = 1.44614881\n",
      "Iteration 25555, loss = 1.44614773\n",
      "Iteration 25556, loss = 1.44614665\n",
      "Iteration 25557, loss = 1.44614557\n",
      "Iteration 25558, loss = 1.44614449\n",
      "Iteration 25559, loss = 1.44614340\n",
      "Iteration 25560, loss = 1.44614232\n",
      "Iteration 25561, loss = 1.44614124\n",
      "Iteration 25562, loss = 1.44614016\n",
      "Iteration 25563, loss = 1.44613908\n",
      "Iteration 25564, loss = 1.44613800\n",
      "Iteration 25565, loss = 1.44613691\n",
      "Iteration 25566, loss = 1.44613583\n",
      "Iteration 25567, loss = 1.44613475\n",
      "Iteration 25568, loss = 1.44613367\n",
      "Iteration 25569, loss = 1.44613259\n",
      "Iteration 25570, loss = 1.44613151\n",
      "Iteration 25571, loss = 1.44613043\n",
      "Iteration 25572, loss = 1.44612935\n",
      "Iteration 25573, loss = 1.44612827\n",
      "Iteration 25574, loss = 1.44612719\n",
      "Iteration 25575, loss = 1.44612611\n",
      "Iteration 25576, loss = 1.44612503\n",
      "Iteration 25577, loss = 1.44612395\n",
      "Iteration 25578, loss = 1.44612287\n",
      "Iteration 25579, loss = 1.44612179\n",
      "Iteration 25580, loss = 1.44612071\n",
      "Iteration 25581, loss = 1.44611963\n",
      "Iteration 25582, loss = 1.44611855\n",
      "Iteration 25583, loss = 1.44611747\n",
      "Iteration 25584, loss = 1.44611639\n",
      "Iteration 25585, loss = 1.44611531\n",
      "Iteration 25586, loss = 1.44611423\n",
      "Iteration 25587, loss = 1.44611315\n",
      "Iteration 25588, loss = 1.44611207\n",
      "Iteration 25589, loss = 1.44611100\n",
      "Iteration 25590, loss = 1.44610992\n",
      "Iteration 25591, loss = 1.44610884\n",
      "Iteration 25592, loss = 1.44610776\n",
      "Iteration 25593, loss = 1.44610668\n",
      "Iteration 25594, loss = 1.44610560\n",
      "Iteration 25595, loss = 1.44610452\n",
      "Iteration 25596, loss = 1.44610345\n",
      "Iteration 25597, loss = 1.44610237\n",
      "Iteration 25598, loss = 1.44610129\n",
      "Iteration 25599, loss = 1.44610021\n",
      "Iteration 25600, loss = 1.44609914\n",
      "Iteration 25601, loss = 1.44609806\n",
      "Iteration 25602, loss = 1.44609698\n",
      "Iteration 25603, loss = 1.44609590\n",
      "Iteration 25604, loss = 1.44609483\n",
      "Iteration 25605, loss = 1.44609375\n",
      "Iteration 25606, loss = 1.44609267\n",
      "Iteration 25607, loss = 1.44609159\n",
      "Iteration 25608, loss = 1.44609052\n",
      "Iteration 25609, loss = 1.44608944\n",
      "Iteration 25610, loss = 1.44608836\n",
      "Iteration 25611, loss = 1.44608729\n",
      "Iteration 25612, loss = 1.44608621\n",
      "Iteration 25613, loss = 1.44608514\n",
      "Iteration 25614, loss = 1.44608406\n",
      "Iteration 25615, loss = 1.44608298\n",
      "Iteration 25616, loss = 1.44608191\n",
      "Iteration 25617, loss = 1.44608083\n",
      "Iteration 25618, loss = 1.44607976\n",
      "Iteration 25619, loss = 1.44607868\n",
      "Iteration 25620, loss = 1.44607760\n",
      "Iteration 25621, loss = 1.44607653\n",
      "Iteration 25622, loss = 1.44607545\n",
      "Iteration 25623, loss = 1.44607438\n",
      "Iteration 25624, loss = 1.44607330\n",
      "Iteration 25625, loss = 1.44607223\n",
      "Iteration 25626, loss = 1.44607115\n",
      "Iteration 25627, loss = 1.44607008\n",
      "Iteration 25628, loss = 1.44606900\n",
      "Iteration 25629, loss = 1.44606793\n",
      "Iteration 25630, loss = 1.44606685\n",
      "Iteration 25631, loss = 1.44606578\n",
      "Iteration 25632, loss = 1.44606471\n",
      "Iteration 25633, loss = 1.44606363\n",
      "Iteration 25634, loss = 1.44606256\n",
      "Iteration 25635, loss = 1.44606148\n",
      "Iteration 25636, loss = 1.44606041\n",
      "Iteration 25637, loss = 1.44605934\n",
      "Iteration 25638, loss = 1.44605826\n",
      "Iteration 25639, loss = 1.44605719\n",
      "Iteration 25640, loss = 1.44605611\n",
      "Iteration 25641, loss = 1.44605504\n",
      "Iteration 25642, loss = 1.44605397\n",
      "Iteration 25643, loss = 1.44605289\n",
      "Iteration 25644, loss = 1.44605182\n",
      "Iteration 25645, loss = 1.44605075\n",
      "Iteration 25646, loss = 1.44604968\n",
      "Iteration 25647, loss = 1.44604860\n",
      "Iteration 25648, loss = 1.44604753\n",
      "Iteration 25649, loss = 1.44604646\n",
      "Iteration 25650, loss = 1.44604539\n",
      "Iteration 25651, loss = 1.44604431\n",
      "Iteration 25652, loss = 1.44604324\n",
      "Iteration 25653, loss = 1.44604217\n",
      "Iteration 25654, loss = 1.44604110\n",
      "Iteration 25655, loss = 1.44604002\n",
      "Iteration 25656, loss = 1.44603895\n",
      "Iteration 25657, loss = 1.44603788\n",
      "Iteration 25658, loss = 1.44603681\n",
      "Iteration 25659, loss = 1.44603574\n",
      "Iteration 25660, loss = 1.44603467\n",
      "Iteration 25661, loss = 1.44603359\n",
      "Iteration 25662, loss = 1.44603252\n",
      "Iteration 25663, loss = 1.44603145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25664, loss = 1.44603038\n",
      "Iteration 25665, loss = 1.44602931\n",
      "Iteration 25666, loss = 1.44602824\n",
      "Iteration 25667, loss = 1.44602717\n",
      "Iteration 25668, loss = 1.44602610\n",
      "Iteration 25669, loss = 1.44602503\n",
      "Iteration 25670, loss = 1.44602396\n",
      "Iteration 25671, loss = 1.44602289\n",
      "Iteration 25672, loss = 1.44602182\n",
      "Iteration 25673, loss = 1.44602075\n",
      "Iteration 25674, loss = 1.44601968\n",
      "Iteration 25675, loss = 1.44601861\n",
      "Iteration 25676, loss = 1.44601754\n",
      "Iteration 25677, loss = 1.44601647\n",
      "Iteration 25678, loss = 1.44601540\n",
      "Iteration 25679, loss = 1.44601433\n",
      "Iteration 25680, loss = 1.44601326\n",
      "Iteration 25681, loss = 1.44601219\n",
      "Iteration 25682, loss = 1.44601112\n",
      "Iteration 25683, loss = 1.44601005\n",
      "Iteration 25684, loss = 1.44600898\n",
      "Iteration 25685, loss = 1.44600791\n",
      "Iteration 25686, loss = 1.44600684\n",
      "Iteration 25687, loss = 1.44600578\n",
      "Iteration 25688, loss = 1.44600471\n",
      "Iteration 25689, loss = 1.44600364\n",
      "Iteration 25690, loss = 1.44600257\n",
      "Iteration 25691, loss = 1.44600150\n",
      "Iteration 25692, loss = 1.44600043\n",
      "Iteration 25693, loss = 1.44599937\n",
      "Iteration 25694, loss = 1.44599830\n",
      "Iteration 25695, loss = 1.44599723\n",
      "Iteration 25696, loss = 1.44599616\n",
      "Iteration 25697, loss = 1.44599509\n",
      "Iteration 25698, loss = 1.44599403\n",
      "Iteration 25699, loss = 1.44599296\n",
      "Iteration 25700, loss = 1.44599189\n",
      "Iteration 25701, loss = 1.44599083\n",
      "Iteration 25702, loss = 1.44598976\n",
      "Iteration 25703, loss = 1.44598869\n",
      "Iteration 25704, loss = 1.44598762\n",
      "Iteration 25705, loss = 1.44598656\n",
      "Iteration 25706, loss = 1.44598549\n",
      "Iteration 25707, loss = 1.44598442\n",
      "Iteration 25708, loss = 1.44598336\n",
      "Iteration 25709, loss = 1.44598229\n",
      "Iteration 25710, loss = 1.44598122\n",
      "Iteration 25711, loss = 1.44598016\n",
      "Iteration 25712, loss = 1.44597909\n",
      "Iteration 25713, loss = 1.44597803\n",
      "Iteration 25714, loss = 1.44597696\n",
      "Iteration 25715, loss = 1.44597589\n",
      "Iteration 25716, loss = 1.44597483\n",
      "Iteration 25717, loss = 1.44597376\n",
      "Iteration 25718, loss = 1.44597270\n",
      "Iteration 25719, loss = 1.44597163\n",
      "Iteration 25720, loss = 1.44597057\n",
      "Iteration 25721, loss = 1.44596950\n",
      "Iteration 25722, loss = 1.44596844\n",
      "Iteration 25723, loss = 1.44596737\n",
      "Iteration 25724, loss = 1.44596631\n",
      "Iteration 25725, loss = 1.44596524\n",
      "Iteration 25726, loss = 1.44596418\n",
      "Iteration 25727, loss = 1.44596311\n",
      "Iteration 25728, loss = 1.44596205\n",
      "Iteration 25729, loss = 1.44596098\n",
      "Iteration 25730, loss = 1.44595992\n",
      "Iteration 25731, loss = 1.44595886\n",
      "Iteration 25732, loss = 1.44595779\n",
      "Iteration 25733, loss = 1.44595673\n",
      "Iteration 25734, loss = 1.44595566\n",
      "Iteration 25735, loss = 1.44595460\n",
      "Iteration 25736, loss = 1.44595354\n",
      "Iteration 25737, loss = 1.44595247\n",
      "Iteration 25738, loss = 1.44595141\n",
      "Iteration 25739, loss = 1.44595035\n",
      "Iteration 25740, loss = 1.44594928\n",
      "Iteration 25741, loss = 1.44594822\n",
      "Iteration 25742, loss = 1.44594716\n",
      "Iteration 25743, loss = 1.44594609\n",
      "Iteration 25744, loss = 1.44594503\n",
      "Iteration 25745, loss = 1.44594397\n",
      "Iteration 25746, loss = 1.44594291\n",
      "Iteration 25747, loss = 1.44594184\n",
      "Iteration 25748, loss = 1.44594078\n",
      "Iteration 25749, loss = 1.44593972\n",
      "Iteration 25750, loss = 1.44593866\n",
      "Iteration 25751, loss = 1.44593759\n",
      "Iteration 25752, loss = 1.44593653\n",
      "Iteration 25753, loss = 1.44593547\n",
      "Iteration 25754, loss = 1.44593441\n",
      "Iteration 25755, loss = 1.44593335\n",
      "Iteration 25756, loss = 1.44593229\n",
      "Iteration 25757, loss = 1.44593122\n",
      "Iteration 25758, loss = 1.44593016\n",
      "Iteration 25759, loss = 1.44592910\n",
      "Iteration 25760, loss = 1.44592804\n",
      "Iteration 25761, loss = 1.44592698\n",
      "Iteration 25762, loss = 1.44592592\n",
      "Iteration 25763, loss = 1.44592486\n",
      "Iteration 25764, loss = 1.44592380\n",
      "Iteration 25765, loss = 1.44592274\n",
      "Iteration 25766, loss = 1.44592167\n",
      "Iteration 25767, loss = 1.44592061\n",
      "Iteration 25768, loss = 1.44591955\n",
      "Iteration 25769, loss = 1.44591849\n",
      "Iteration 25770, loss = 1.44591743\n",
      "Iteration 25771, loss = 1.44591637\n",
      "Iteration 25772, loss = 1.44591531\n",
      "Iteration 25773, loss = 1.44591425\n",
      "Iteration 25774, loss = 1.44591319\n",
      "Iteration 25775, loss = 1.44591213\n",
      "Iteration 25776, loss = 1.44591107\n",
      "Iteration 25777, loss = 1.44591001\n",
      "Iteration 25778, loss = 1.44590896\n",
      "Iteration 25779, loss = 1.44590790\n",
      "Iteration 25780, loss = 1.44590684\n",
      "Iteration 25781, loss = 1.44590578\n",
      "Iteration 25782, loss = 1.44590472\n",
      "Iteration 25783, loss = 1.44590366\n",
      "Iteration 25784, loss = 1.44590260\n",
      "Iteration 25785, loss = 1.44590154\n",
      "Iteration 25786, loss = 1.44590048\n",
      "Iteration 25787, loss = 1.44589943\n",
      "Iteration 25788, loss = 1.44589837\n",
      "Iteration 25789, loss = 1.44589731\n",
      "Iteration 25790, loss = 1.44589625\n",
      "Iteration 25791, loss = 1.44589519\n",
      "Iteration 25792, loss = 1.44589413\n",
      "Iteration 25793, loss = 1.44589308\n",
      "Iteration 25794, loss = 1.44589202\n",
      "Iteration 25795, loss = 1.44589096\n",
      "Iteration 25796, loss = 1.44588990\n",
      "Iteration 25797, loss = 1.44588885\n",
      "Iteration 25798, loss = 1.44588779\n",
      "Iteration 25799, loss = 1.44588673\n",
      "Iteration 25800, loss = 1.44588567\n",
      "Iteration 25801, loss = 1.44588462\n",
      "Iteration 25802, loss = 1.44588356\n",
      "Iteration 25803, loss = 1.44588250\n",
      "Iteration 25804, loss = 1.44588145\n",
      "Iteration 25805, loss = 1.44588039\n",
      "Iteration 25806, loss = 1.44587933\n",
      "Iteration 25807, loss = 1.44587828\n",
      "Iteration 25808, loss = 1.44587722\n",
      "Iteration 25809, loss = 1.44587616\n",
      "Iteration 25810, loss = 1.44587511\n",
      "Iteration 25811, loss = 1.44587405\n",
      "Iteration 25812, loss = 1.44587300\n",
      "Iteration 25813, loss = 1.44587194\n",
      "Iteration 25814, loss = 1.44587089\n",
      "Iteration 25815, loss = 1.44586983\n",
      "Iteration 25816, loss = 1.44586877\n",
      "Iteration 25817, loss = 1.44586772\n",
      "Iteration 25818, loss = 1.44586666\n",
      "Iteration 25819, loss = 1.44586561\n",
      "Iteration 25820, loss = 1.44586455\n",
      "Iteration 25821, loss = 1.44586350\n",
      "Iteration 25822, loss = 1.44586244\n",
      "Iteration 25823, loss = 1.44586139\n",
      "Iteration 25824, loss = 1.44586033\n",
      "Iteration 25825, loss = 1.44585928\n",
      "Iteration 25826, loss = 1.44585822\n",
      "Iteration 25827, loss = 1.44585717\n",
      "Iteration 25828, loss = 1.44585612\n",
      "Iteration 25829, loss = 1.44585506\n",
      "Iteration 25830, loss = 1.44585401\n",
      "Iteration 25831, loss = 1.44585295\n",
      "Iteration 25832, loss = 1.44585190\n",
      "Iteration 25833, loss = 1.44585085\n",
      "Iteration 25834, loss = 1.44584979\n",
      "Iteration 25835, loss = 1.44584874\n",
      "Iteration 25836, loss = 1.44584768\n",
      "Iteration 25837, loss = 1.44584663\n",
      "Iteration 25838, loss = 1.44584558\n",
      "Iteration 25839, loss = 1.44584452\n",
      "Iteration 25840, loss = 1.44584347\n",
      "Iteration 25841, loss = 1.44584242\n",
      "Iteration 25842, loss = 1.44584137\n",
      "Iteration 25843, loss = 1.44584031\n",
      "Iteration 25844, loss = 1.44583926\n",
      "Iteration 25845, loss = 1.44583821\n",
      "Iteration 25846, loss = 1.44583716\n",
      "Iteration 25847, loss = 1.44583610\n",
      "Iteration 25848, loss = 1.44583505\n",
      "Iteration 25849, loss = 1.44583400\n",
      "Iteration 25850, loss = 1.44583295\n",
      "Iteration 25851, loss = 1.44583189\n",
      "Iteration 25852, loss = 1.44583084\n",
      "Iteration 25853, loss = 1.44582979\n",
      "Iteration 25854, loss = 1.44582874\n",
      "Iteration 25855, loss = 1.44582769\n",
      "Iteration 25856, loss = 1.44582664\n",
      "Iteration 25857, loss = 1.44582558\n",
      "Iteration 25858, loss = 1.44582453\n",
      "Iteration 25859, loss = 1.44582348\n",
      "Iteration 25860, loss = 1.44582243\n",
      "Iteration 25861, loss = 1.44582138\n",
      "Iteration 25862, loss = 1.44582033\n",
      "Iteration 25863, loss = 1.44581928\n",
      "Iteration 25864, loss = 1.44581823\n",
      "Iteration 25865, loss = 1.44581718\n",
      "Iteration 25866, loss = 1.44581613\n",
      "Iteration 25867, loss = 1.44581508\n",
      "Iteration 25868, loss = 1.44581403\n",
      "Iteration 25869, loss = 1.44581298\n",
      "Iteration 25870, loss = 1.44581193\n",
      "Iteration 25871, loss = 1.44581088\n",
      "Iteration 25872, loss = 1.44580983\n",
      "Iteration 25873, loss = 1.44580878\n",
      "Iteration 25874, loss = 1.44580773\n",
      "Iteration 25875, loss = 1.44580668\n",
      "Iteration 25876, loss = 1.44580563\n",
      "Iteration 25877, loss = 1.44580458\n",
      "Iteration 25878, loss = 1.44580353\n",
      "Iteration 25879, loss = 1.44580248\n",
      "Iteration 25880, loss = 1.44580143\n",
      "Iteration 25881, loss = 1.44580038\n",
      "Iteration 25882, loss = 1.44579933\n",
      "Iteration 25883, loss = 1.44579828\n",
      "Iteration 25884, loss = 1.44579723\n",
      "Iteration 25885, loss = 1.44579619\n",
      "Iteration 25886, loss = 1.44579514\n",
      "Iteration 25887, loss = 1.44579409\n",
      "Iteration 25888, loss = 1.44579304\n",
      "Iteration 25889, loss = 1.44579199\n",
      "Iteration 25890, loss = 1.44579094\n",
      "Iteration 25891, loss = 1.44578990\n",
      "Iteration 25892, loss = 1.44578885\n",
      "Iteration 25893, loss = 1.44578780\n",
      "Iteration 25894, loss = 1.44578675\n",
      "Iteration 25895, loss = 1.44578571\n",
      "Iteration 25896, loss = 1.44578466\n",
      "Iteration 25897, loss = 1.44578361\n",
      "Iteration 25898, loss = 1.44578256\n",
      "Iteration 25899, loss = 1.44578152\n",
      "Iteration 25900, loss = 1.44578047\n",
      "Iteration 25901, loss = 1.44577942\n",
      "Iteration 25902, loss = 1.44577838\n",
      "Iteration 25903, loss = 1.44577733\n",
      "Iteration 25904, loss = 1.44577628\n",
      "Iteration 25905, loss = 1.44577524\n",
      "Iteration 25906, loss = 1.44577419\n",
      "Iteration 25907, loss = 1.44577314\n",
      "Iteration 25908, loss = 1.44577210\n",
      "Iteration 25909, loss = 1.44577105\n",
      "Iteration 25910, loss = 1.44577000\n",
      "Iteration 25911, loss = 1.44576896\n",
      "Iteration 25912, loss = 1.44576791\n",
      "Iteration 25913, loss = 1.44576687\n",
      "Iteration 25914, loss = 1.44576582\n",
      "Iteration 25915, loss = 1.44576477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25916, loss = 1.44576373\n",
      "Iteration 25917, loss = 1.44576268\n",
      "Iteration 25918, loss = 1.44576164\n",
      "Iteration 25919, loss = 1.44576059\n",
      "Iteration 25920, loss = 1.44575955\n",
      "Iteration 25921, loss = 1.44575850\n",
      "Iteration 25922, loss = 1.44575746\n",
      "Iteration 25923, loss = 1.44575641\n",
      "Iteration 25924, loss = 1.44575537\n",
      "Iteration 25925, loss = 1.44575432\n",
      "Iteration 25926, loss = 1.44575328\n",
      "Iteration 25927, loss = 1.44575224\n",
      "Iteration 25928, loss = 1.44575119\n",
      "Iteration 25929, loss = 1.44575015\n",
      "Iteration 25930, loss = 1.44574910\n",
      "Iteration 25931, loss = 1.44574806\n",
      "Iteration 25932, loss = 1.44574702\n",
      "Iteration 25933, loss = 1.44574597\n",
      "Iteration 25934, loss = 1.44574493\n",
      "Iteration 25935, loss = 1.44574388\n",
      "Iteration 25936, loss = 1.44574284\n",
      "Iteration 25937, loss = 1.44574180\n",
      "Iteration 25938, loss = 1.44574075\n",
      "Iteration 25939, loss = 1.44573971\n",
      "Iteration 25940, loss = 1.44573867\n",
      "Iteration 25941, loss = 1.44573762\n",
      "Iteration 25942, loss = 1.44573658\n",
      "Iteration 25943, loss = 1.44573554\n",
      "Iteration 25944, loss = 1.44573450\n",
      "Iteration 25945, loss = 1.44573345\n",
      "Iteration 25946, loss = 1.44573241\n",
      "Iteration 25947, loss = 1.44573137\n",
      "Iteration 25948, loss = 1.44573033\n",
      "Iteration 25949, loss = 1.44572928\n",
      "Iteration 25950, loss = 1.44572824\n",
      "Iteration 25951, loss = 1.44572720\n",
      "Iteration 25952, loss = 1.44572616\n",
      "Iteration 25953, loss = 1.44572512\n",
      "Iteration 25954, loss = 1.44572407\n",
      "Iteration 25955, loss = 1.44572303\n",
      "Iteration 25956, loss = 1.44572199\n",
      "Iteration 25957, loss = 1.44572095\n",
      "Iteration 25958, loss = 1.44571991\n",
      "Iteration 25959, loss = 1.44571887\n",
      "Iteration 25960, loss = 1.44571783\n",
      "Iteration 25961, loss = 1.44571679\n",
      "Iteration 25962, loss = 1.44571574\n",
      "Iteration 25963, loss = 1.44571470\n",
      "Iteration 25964, loss = 1.44571366\n",
      "Iteration 25965, loss = 1.44571262\n",
      "Iteration 25966, loss = 1.44571158\n",
      "Iteration 25967, loss = 1.44571054\n",
      "Iteration 25968, loss = 1.44570950\n",
      "Iteration 25969, loss = 1.44570846\n",
      "Iteration 25970, loss = 1.44570742\n",
      "Iteration 25971, loss = 1.44570638\n",
      "Iteration 25972, loss = 1.44570534\n",
      "Iteration 25973, loss = 1.44570430\n",
      "Iteration 25974, loss = 1.44570326\n",
      "Iteration 25975, loss = 1.44570222\n",
      "Iteration 25976, loss = 1.44570118\n",
      "Iteration 25977, loss = 1.44570014\n",
      "Iteration 25978, loss = 1.44569910\n",
      "Iteration 25979, loss = 1.44569806\n",
      "Iteration 25980, loss = 1.44569703\n",
      "Iteration 25981, loss = 1.44569599\n",
      "Iteration 25982, loss = 1.44569495\n",
      "Iteration 25983, loss = 1.44569391\n",
      "Iteration 25984, loss = 1.44569287\n",
      "Iteration 25985, loss = 1.44569183\n",
      "Iteration 25986, loss = 1.44569079\n",
      "Iteration 25987, loss = 1.44568975\n",
      "Iteration 25988, loss = 1.44568872\n",
      "Iteration 25989, loss = 1.44568768\n",
      "Iteration 25990, loss = 1.44568664\n",
      "Iteration 25991, loss = 1.44568560\n",
      "Iteration 25992, loss = 1.44568456\n",
      "Iteration 25993, loss = 1.44568353\n",
      "Iteration 25994, loss = 1.44568249\n",
      "Iteration 25995, loss = 1.44568145\n",
      "Iteration 25996, loss = 1.44568041\n",
      "Iteration 25997, loss = 1.44567938\n",
      "Iteration 25998, loss = 1.44567834\n",
      "Iteration 25999, loss = 1.44567730\n",
      "Iteration 26000, loss = 1.44567626\n",
      "Iteration 26001, loss = 1.44567523\n",
      "Iteration 26002, loss = 1.44567419\n",
      "Iteration 26003, loss = 1.44567315\n",
      "Iteration 26004, loss = 1.44567212\n",
      "Iteration 26005, loss = 1.44567108\n",
      "Iteration 26006, loss = 1.44567004\n",
      "Iteration 26007, loss = 1.44566901\n",
      "Iteration 26008, loss = 1.44566797\n",
      "Iteration 26009, loss = 1.44566693\n",
      "Iteration 26010, loss = 1.44566590\n",
      "Iteration 26011, loss = 1.44566486\n",
      "Iteration 26012, loss = 1.44566383\n",
      "Iteration 26013, loss = 1.44566279\n",
      "Iteration 26014, loss = 1.44566175\n",
      "Iteration 26015, loss = 1.44566072\n",
      "Iteration 26016, loss = 1.44565968\n",
      "Iteration 26017, loss = 1.44565865\n",
      "Iteration 26018, loss = 1.44565761\n",
      "Iteration 26019, loss = 1.44565658\n",
      "Iteration 26020, loss = 1.44565554\n",
      "Iteration 26021, loss = 1.44565451\n",
      "Iteration 26022, loss = 1.44565347\n",
      "Iteration 26023, loss = 1.44565244\n",
      "Iteration 26024, loss = 1.44565140\n",
      "Iteration 26025, loss = 1.44565037\n",
      "Iteration 26026, loss = 1.44564933\n",
      "Iteration 26027, loss = 1.44564830\n",
      "Iteration 26028, loss = 1.44564726\n",
      "Iteration 26029, loss = 1.44564623\n",
      "Iteration 26030, loss = 1.44564520\n",
      "Iteration 26031, loss = 1.44564416\n",
      "Iteration 26032, loss = 1.44564313\n",
      "Iteration 26033, loss = 1.44564209\n",
      "Iteration 26034, loss = 1.44564106\n",
      "Iteration 26035, loss = 1.44564003\n",
      "Iteration 26036, loss = 1.44563899\n",
      "Iteration 26037, loss = 1.44563796\n",
      "Iteration 26038, loss = 1.44563693\n",
      "Iteration 26039, loss = 1.44563589\n",
      "Iteration 26040, loss = 1.44563486\n",
      "Iteration 26041, loss = 1.44563383\n",
      "Iteration 26042, loss = 1.44563279\n",
      "Iteration 26043, loss = 1.44563176\n",
      "Iteration 26044, loss = 1.44563073\n",
      "Iteration 26045, loss = 1.44562969\n",
      "Iteration 26046, loss = 1.44562866\n",
      "Iteration 26047, loss = 1.44562763\n",
      "Iteration 26048, loss = 1.44562660\n",
      "Iteration 26049, loss = 1.44562556\n",
      "Iteration 26050, loss = 1.44562453\n",
      "Iteration 26051, loss = 1.44562350\n",
      "Iteration 26052, loss = 1.44562247\n",
      "Iteration 26053, loss = 1.44562144\n",
      "Iteration 26054, loss = 1.44562040\n",
      "Iteration 26055, loss = 1.44561937\n",
      "Iteration 26056, loss = 1.44561834\n",
      "Iteration 26057, loss = 1.44561731\n",
      "Iteration 26058, loss = 1.44561628\n",
      "Iteration 26059, loss = 1.44561525\n",
      "Iteration 26060, loss = 1.44561422\n",
      "Iteration 26061, loss = 1.44561318\n",
      "Iteration 26062, loss = 1.44561215\n",
      "Iteration 26063, loss = 1.44561112\n",
      "Iteration 26064, loss = 1.44561009\n",
      "Iteration 26065, loss = 1.44560906\n",
      "Iteration 26066, loss = 1.44560803\n",
      "Iteration 26067, loss = 1.44560700\n",
      "Iteration 26068, loss = 1.44560597\n",
      "Iteration 26069, loss = 1.44560494\n",
      "Iteration 26070, loss = 1.44560391\n",
      "Iteration 26071, loss = 1.44560288\n",
      "Iteration 26072, loss = 1.44560185\n",
      "Iteration 26073, loss = 1.44560082\n",
      "Iteration 26074, loss = 1.44559979\n",
      "Iteration 26075, loss = 1.44559876\n",
      "Iteration 26076, loss = 1.44559773\n",
      "Iteration 26077, loss = 1.44559670\n",
      "Iteration 26078, loss = 1.44559567\n",
      "Iteration 26079, loss = 1.44559464\n",
      "Iteration 26080, loss = 1.44559361\n",
      "Iteration 26081, loss = 1.44559258\n",
      "Iteration 26082, loss = 1.44559155\n",
      "Iteration 26083, loss = 1.44559052\n",
      "Iteration 26084, loss = 1.44558949\n",
      "Iteration 26085, loss = 1.44558847\n",
      "Iteration 26086, loss = 1.44558744\n",
      "Iteration 26087, loss = 1.44558641\n",
      "Iteration 26088, loss = 1.44558538\n",
      "Iteration 26089, loss = 1.44558435\n",
      "Iteration 26090, loss = 1.44558332\n",
      "Iteration 26091, loss = 1.44558230\n",
      "Iteration 26092, loss = 1.44558127\n",
      "Iteration 26093, loss = 1.44558024\n",
      "Iteration 26094, loss = 1.44557921\n",
      "Iteration 26095, loss = 1.44557818\n",
      "Iteration 26096, loss = 1.44557716\n",
      "Iteration 26097, loss = 1.44557613\n",
      "Iteration 26098, loss = 1.44557510\n",
      "Iteration 26099, loss = 1.44557407\n",
      "Iteration 26100, loss = 1.44557305\n",
      "Iteration 26101, loss = 1.44557202\n",
      "Iteration 26102, loss = 1.44557099\n",
      "Iteration 26103, loss = 1.44556996\n",
      "Iteration 26104, loss = 1.44556894\n",
      "Iteration 26105, loss = 1.44556791\n",
      "Iteration 26106, loss = 1.44556688\n",
      "Iteration 26107, loss = 1.44556586\n",
      "Iteration 26108, loss = 1.44556483\n",
      "Iteration 26109, loss = 1.44556380\n",
      "Iteration 26110, loss = 1.44556278\n",
      "Iteration 26111, loss = 1.44556175\n",
      "Iteration 26112, loss = 1.44556073\n",
      "Iteration 26113, loss = 1.44555970\n",
      "Iteration 26114, loss = 1.44555867\n",
      "Iteration 26115, loss = 1.44555765\n",
      "Iteration 26116, loss = 1.44555662\n",
      "Iteration 26117, loss = 1.44555560\n",
      "Iteration 26118, loss = 1.44555457\n",
      "Iteration 26119, loss = 1.44555355\n",
      "Iteration 26120, loss = 1.44555252\n",
      "Iteration 26121, loss = 1.44555149\n",
      "Iteration 26122, loss = 1.44555047\n",
      "Iteration 26123, loss = 1.44554944\n",
      "Iteration 26124, loss = 1.44554842\n",
      "Iteration 26125, loss = 1.44554739\n",
      "Iteration 26126, loss = 1.44554637\n",
      "Iteration 26127, loss = 1.44554534\n",
      "Iteration 26128, loss = 1.44554432\n",
      "Iteration 26129, loss = 1.44554330\n",
      "Iteration 26130, loss = 1.44554227\n",
      "Iteration 26131, loss = 1.44554125\n",
      "Iteration 26132, loss = 1.44554022\n",
      "Iteration 26133, loss = 1.44553920\n",
      "Iteration 26134, loss = 1.44553817\n",
      "Iteration 26135, loss = 1.44553715\n",
      "Iteration 26136, loss = 1.44553613\n",
      "Iteration 26137, loss = 1.44553510\n",
      "Iteration 26138, loss = 1.44553408\n",
      "Iteration 26139, loss = 1.44553306\n",
      "Iteration 26140, loss = 1.44553203\n",
      "Iteration 26141, loss = 1.44553101\n",
      "Iteration 26142, loss = 1.44552999\n",
      "Iteration 26143, loss = 1.44552896\n",
      "Iteration 26144, loss = 1.44552794\n",
      "Iteration 26145, loss = 1.44552692\n",
      "Iteration 26146, loss = 1.44552589\n",
      "Iteration 26147, loss = 1.44552487\n",
      "Iteration 26148, loss = 1.44552385\n",
      "Iteration 26149, loss = 1.44552283\n",
      "Iteration 26150, loss = 1.44552180\n",
      "Iteration 26151, loss = 1.44552078\n",
      "Iteration 26152, loss = 1.44551976\n",
      "Iteration 26153, loss = 1.44551874\n",
      "Iteration 26154, loss = 1.44551771\n",
      "Iteration 26155, loss = 1.44551669\n",
      "Iteration 26156, loss = 1.44551567\n",
      "Iteration 26157, loss = 1.44551465\n",
      "Iteration 26158, loss = 1.44551363\n",
      "Iteration 26159, loss = 1.44551261\n",
      "Iteration 26160, loss = 1.44551158\n",
      "Iteration 26161, loss = 1.44551056\n",
      "Iteration 26162, loss = 1.44550954\n",
      "Iteration 26163, loss = 1.44550852\n",
      "Iteration 26164, loss = 1.44550750\n",
      "Iteration 26165, loss = 1.44550648\n",
      "Iteration 26166, loss = 1.44550546\n",
      "Iteration 26167, loss = 1.44550444\n",
      "Iteration 26168, loss = 1.44550342\n",
      "Iteration 26169, loss = 1.44550240\n",
      "Iteration 26170, loss = 1.44550137\n",
      "Iteration 26171, loss = 1.44550035\n",
      "Iteration 26172, loss = 1.44549933\n",
      "Iteration 26173, loss = 1.44549831\n",
      "Iteration 26174, loss = 1.44549729\n",
      "Iteration 26175, loss = 1.44549627\n",
      "Iteration 26176, loss = 1.44549525\n",
      "Iteration 26177, loss = 1.44549423\n",
      "Iteration 26178, loss = 1.44549321\n",
      "Iteration 26179, loss = 1.44549219\n",
      "Iteration 26180, loss = 1.44549117\n",
      "Iteration 26181, loss = 1.44549016\n",
      "Iteration 26182, loss = 1.44548914\n",
      "Iteration 26183, loss = 1.44548812\n",
      "Iteration 26184, loss = 1.44548710\n",
      "Iteration 26185, loss = 1.44548608\n",
      "Iteration 26186, loss = 1.44548506\n",
      "Iteration 26187, loss = 1.44548404\n",
      "Iteration 26188, loss = 1.44548302\n",
      "Iteration 26189, loss = 1.44548200\n",
      "Iteration 26190, loss = 1.44548098\n",
      "Iteration 26191, loss = 1.44547997\n",
      "Iteration 26192, loss = 1.44547895\n",
      "Iteration 26193, loss = 1.44547793\n",
      "Iteration 26194, loss = 1.44547691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26195, loss = 1.44547589\n",
      "Iteration 26196, loss = 1.44547488\n",
      "Iteration 26197, loss = 1.44547386\n",
      "Iteration 26198, loss = 1.44547284\n",
      "Iteration 26199, loss = 1.44547182\n",
      "Iteration 26200, loss = 1.44547080\n",
      "Iteration 26201, loss = 1.44546979\n",
      "Iteration 26202, loss = 1.44546877\n",
      "Iteration 26203, loss = 1.44546775\n",
      "Iteration 26204, loss = 1.44546673\n",
      "Iteration 26205, loss = 1.44546572\n",
      "Iteration 26206, loss = 1.44546470\n",
      "Iteration 26207, loss = 1.44546368\n",
      "Iteration 26208, loss = 1.44546267\n",
      "Iteration 26209, loss = 1.44546165\n",
      "Iteration 26210, loss = 1.44546063\n",
      "Iteration 26211, loss = 1.44545962\n",
      "Iteration 26212, loss = 1.44545860\n",
      "Iteration 26213, loss = 1.44545758\n",
      "Iteration 26214, loss = 1.44545657\n",
      "Iteration 26215, loss = 1.44545555\n",
      "Iteration 26216, loss = 1.44545454\n",
      "Iteration 26217, loss = 1.44545352\n",
      "Iteration 26218, loss = 1.44545250\n",
      "Iteration 26219, loss = 1.44545149\n",
      "Iteration 26220, loss = 1.44545047\n",
      "Iteration 26221, loss = 1.44544946\n",
      "Iteration 26222, loss = 1.44544844\n",
      "Iteration 26223, loss = 1.44544743\n",
      "Iteration 26224, loss = 1.44544641\n",
      "Iteration 26225, loss = 1.44544539\n",
      "Iteration 26226, loss = 1.44544438\n",
      "Iteration 26227, loss = 1.44544336\n",
      "Iteration 26228, loss = 1.44544235\n",
      "Iteration 26229, loss = 1.44544133\n",
      "Iteration 26230, loss = 1.44544032\n",
      "Iteration 26231, loss = 1.44543931\n",
      "Iteration 26232, loss = 1.44543829\n",
      "Iteration 26233, loss = 1.44543728\n",
      "Iteration 26234, loss = 1.44543626\n",
      "Iteration 26235, loss = 1.44543525\n",
      "Iteration 26236, loss = 1.44543423\n",
      "Iteration 26237, loss = 1.44543322\n",
      "Iteration 26238, loss = 1.44543221\n",
      "Iteration 26239, loss = 1.44543119\n",
      "Iteration 26240, loss = 1.44543018\n",
      "Iteration 26241, loss = 1.44542916\n",
      "Iteration 26242, loss = 1.44542815\n",
      "Iteration 26243, loss = 1.44542714\n",
      "Iteration 26244, loss = 1.44542612\n",
      "Iteration 26245, loss = 1.44542511\n",
      "Iteration 26246, loss = 1.44542410\n",
      "Iteration 26247, loss = 1.44542308\n",
      "Iteration 26248, loss = 1.44542207\n",
      "Iteration 26249, loss = 1.44542106\n",
      "Iteration 26250, loss = 1.44542005\n",
      "Iteration 26251, loss = 1.44541903\n",
      "Iteration 26252, loss = 1.44541802\n",
      "Iteration 26253, loss = 1.44541701\n",
      "Iteration 26254, loss = 1.44541600\n",
      "Iteration 26255, loss = 1.44541498\n",
      "Iteration 26256, loss = 1.44541397\n",
      "Iteration 26257, loss = 1.44541296\n",
      "Iteration 26258, loss = 1.44541195\n",
      "Iteration 26259, loss = 1.44541093\n",
      "Iteration 26260, loss = 1.44540992\n",
      "Iteration 26261, loss = 1.44540891\n",
      "Iteration 26262, loss = 1.44540790\n",
      "Iteration 26263, loss = 1.44540689\n",
      "Iteration 26264, loss = 1.44540588\n",
      "Iteration 26265, loss = 1.44540486\n",
      "Iteration 26266, loss = 1.44540385\n",
      "Iteration 26267, loss = 1.44540284\n",
      "Iteration 26268, loss = 1.44540183\n",
      "Iteration 26269, loss = 1.44540082\n",
      "Iteration 26270, loss = 1.44539981\n",
      "Iteration 26271, loss = 1.44539880\n",
      "Iteration 26272, loss = 1.44539779\n",
      "Iteration 26273, loss = 1.44539678\n",
      "Iteration 26274, loss = 1.44539577\n",
      "Iteration 26275, loss = 1.44539476\n",
      "Iteration 26276, loss = 1.44539375\n",
      "Iteration 26277, loss = 1.44539274\n",
      "Iteration 26278, loss = 1.44539173\n",
      "Iteration 26279, loss = 1.44539072\n",
      "Iteration 26280, loss = 1.44538971\n",
      "Iteration 26281, loss = 1.44538870\n",
      "Iteration 26282, loss = 1.44538769\n",
      "Iteration 26283, loss = 1.44538668\n",
      "Iteration 26284, loss = 1.44538567\n",
      "Iteration 26285, loss = 1.44538466\n",
      "Iteration 26286, loss = 1.44538365\n",
      "Iteration 26287, loss = 1.44538264\n",
      "Iteration 26288, loss = 1.44538163\n",
      "Iteration 26289, loss = 1.44538062\n",
      "Iteration 26290, loss = 1.44537961\n",
      "Iteration 26291, loss = 1.44537860\n",
      "Iteration 26292, loss = 1.44537759\n",
      "Iteration 26293, loss = 1.44537658\n",
      "Iteration 26294, loss = 1.44537558\n",
      "Iteration 26295, loss = 1.44537457\n",
      "Iteration 26296, loss = 1.44537356\n",
      "Iteration 26297, loss = 1.44537255\n",
      "Iteration 26298, loss = 1.44537154\n",
      "Iteration 26299, loss = 1.44537053\n",
      "Iteration 26300, loss = 1.44536953\n",
      "Iteration 26301, loss = 1.44536852\n",
      "Iteration 26302, loss = 1.44536751\n",
      "Iteration 26303, loss = 1.44536650\n",
      "Iteration 26304, loss = 1.44536549\n",
      "Iteration 26305, loss = 1.44536449\n",
      "Iteration 26306, loss = 1.44536348\n",
      "Iteration 26307, loss = 1.44536247\n",
      "Iteration 26308, loss = 1.44536147\n",
      "Iteration 26309, loss = 1.44536046\n",
      "Iteration 26310, loss = 1.44535945\n",
      "Iteration 26311, loss = 1.44535844\n",
      "Iteration 26312, loss = 1.44535744\n",
      "Iteration 26313, loss = 1.44535643\n",
      "Iteration 26314, loss = 1.44535542\n",
      "Iteration 26315, loss = 1.44535442\n",
      "Iteration 26316, loss = 1.44535341\n",
      "Iteration 26317, loss = 1.44535240\n",
      "Iteration 26318, loss = 1.44535140\n",
      "Iteration 26319, loss = 1.44535039\n",
      "Iteration 26320, loss = 1.44534939\n",
      "Iteration 26321, loss = 1.44534838\n",
      "Iteration 26322, loss = 1.44534737\n",
      "Iteration 26323, loss = 1.44534637\n",
      "Iteration 26324, loss = 1.44534536\n",
      "Iteration 26325, loss = 1.44534436\n",
      "Iteration 26326, loss = 1.44534335\n",
      "Iteration 26327, loss = 1.44534234\n",
      "Iteration 26328, loss = 1.44534134\n",
      "Iteration 26329, loss = 1.44534033\n",
      "Iteration 26330, loss = 1.44533933\n",
      "Iteration 26331, loss = 1.44533832\n",
      "Iteration 26332, loss = 1.44533732\n",
      "Iteration 26333, loss = 1.44533631\n",
      "Iteration 26334, loss = 1.44533531\n",
      "Iteration 26335, loss = 1.44533430\n",
      "Iteration 26336, loss = 1.44533330\n",
      "Iteration 26337, loss = 1.44533230\n",
      "Iteration 26338, loss = 1.44533129\n",
      "Iteration 26339, loss = 1.44533029\n",
      "Iteration 26340, loss = 1.44532928\n",
      "Iteration 26341, loss = 1.44532828\n",
      "Iteration 26342, loss = 1.44532727\n",
      "Iteration 26343, loss = 1.44532627\n",
      "Iteration 26344, loss = 1.44532527\n",
      "Iteration 26345, loss = 1.44532426\n",
      "Iteration 26346, loss = 1.44532326\n",
      "Iteration 26347, loss = 1.44532226\n",
      "Iteration 26348, loss = 1.44532125\n",
      "Iteration 26349, loss = 1.44532025\n",
      "Iteration 26350, loss = 1.44531925\n",
      "Iteration 26351, loss = 1.44531824\n",
      "Iteration 26352, loss = 1.44531724\n",
      "Iteration 26353, loss = 1.44531624\n",
      "Iteration 26354, loss = 1.44531523\n",
      "Iteration 26355, loss = 1.44531423\n",
      "Iteration 26356, loss = 1.44531323\n",
      "Iteration 26357, loss = 1.44531222\n",
      "Iteration 26358, loss = 1.44531122\n",
      "Iteration 26359, loss = 1.44531022\n",
      "Iteration 26360, loss = 1.44530922\n",
      "Iteration 26361, loss = 1.44530822\n",
      "Iteration 26362, loss = 1.44530721\n",
      "Iteration 26363, loss = 1.44530621\n",
      "Iteration 26364, loss = 1.44530521\n",
      "Iteration 26365, loss = 1.44530421\n",
      "Iteration 26366, loss = 1.44530321\n",
      "Iteration 26367, loss = 1.44530220\n",
      "Iteration 26368, loss = 1.44530120\n",
      "Iteration 26369, loss = 1.44530020\n",
      "Iteration 26370, loss = 1.44529920\n",
      "Iteration 26371, loss = 1.44529820\n",
      "Iteration 26372, loss = 1.44529720\n",
      "Iteration 26373, loss = 1.44529620\n",
      "Iteration 26374, loss = 1.44529519\n",
      "Iteration 26375, loss = 1.44529419\n",
      "Iteration 26376, loss = 1.44529319\n",
      "Iteration 26377, loss = 1.44529219\n",
      "Iteration 26378, loss = 1.44529119\n",
      "Iteration 26379, loss = 1.44529019\n",
      "Iteration 26380, loss = 1.44528919\n",
      "Iteration 26381, loss = 1.44528819\n",
      "Iteration 26382, loss = 1.44528719\n",
      "Iteration 26383, loss = 1.44528619\n",
      "Iteration 26384, loss = 1.44528519\n",
      "Iteration 26385, loss = 1.44528419\n",
      "Iteration 26386, loss = 1.44528319\n",
      "Iteration 26387, loss = 1.44528219\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000020\n",
      "Iteration 26388, loss = 1.44528119\n",
      "Iteration 26389, loss = 1.44528034\n",
      "Iteration 26390, loss = 1.44527956\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000004\n",
      "Iteration 26391, loss = 1.44527883\n",
      "Iteration 26392, loss = 1.44527819\n",
      "Iteration 26393, loss = 1.44527761\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 26394, loss = 1.44527708\n",
      "Iteration 26395, loss = 1.44527661\n",
      "Iteration 26396, loss = 1.44527619\n",
      "Training loss did not improve more than tol=0.000001 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 2), learning_rate='adaptive',\n",
       "       learning_rate_init=0.0001, max_iter=100000, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='sgd', tol=1e-06, validation_fraction=0.1, verbose=True,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Modelling\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf1 = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "                   early_stopping=False, hidden_layer_sizes=(100, 2), learning_rate='adaptive',\n",
    "                   learning_rate_init=0.0001, max_iter=100000, random_state=1, shuffle=True,\n",
    "                   solver='lbfgs', tol = 0.000001, validation_fraction=0.1, verbose=True,\n",
    "                   warm_start=False)\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "clf2 =  MLPClassifier(activation='relu', alpha=1e-06, batch_size='auto',\n",
    "                     beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "                     epsilon=1e-08, hidden_layer_sizes=(100, 2), learning_rate='adaptive',\n",
    "                     learning_rate_init=0.0001, max_iter=100000,random_state=1, shuffle=True,\n",
    "                     solver='adam', tol = 0.000001, validation_fraction=0.1, verbose=True,\n",
    "                     warm_start=False)\n",
    "\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "clf3 = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "                       early_stopping=False,hidden_layer_sizes=(100, 2), learning_rate='adaptive',\n",
    "                       learning_rate_init=0.0001, max_iter=100000, momentum=0.9,\n",
    "                       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
    "                       solver='sgd',tol = 0.000001, validation_fraction=0.1, verbose=True,\n",
    "                       warm_start=False)\n",
    "\n",
    "clf3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrance\tCroatia\n",
      "LBFGS\t   1\t   1\n",
      "Adam\t   1\t   1\n",
      "SGD\t   5\t   5\n"
     ]
    }
   ],
   "source": [
    "#Limited-memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (LFBGS)\n",
    "score1 = clf1.predict(data_predict)\n",
    "#Adam\n",
    "score2 = clf2.predict(data_predict)\n",
    "#Stochastic Gradient Descent\n",
    "score3 = clf3.predict(data_predict)\n",
    "\n",
    "print('\\tFrance\\tCroatia')\n",
    "print('LBFGS\\t   ' + str(score1[0])+ '\\t   '+ str(score1[1]))\n",
    "print('Adam\\t   '+ str(score2[0])+ '\\t   '+ str(score2[1]))\n",
    "print('SGD\\t   '+ str(score3[0])+ '\\t   '+ str(score3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Limited-memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (LFBGS)\n",
      "[[0 1 0]\n",
      " [0 6 0]\n",
      " [0 1 0]]\n",
      "accuracy : 0.75\n",
      "\n",
      "\n",
      "Adam\n",
      "[[0 1 0]\n",
      " [0 6 0]\n",
      " [0 1 0]]\n",
      "accuracy : 0.75\n",
      "\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "[[0 1 0]\n",
      " [0 6 0]\n",
      " [0 1 0]]\n",
      "accuracy : 0.75\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "y_pred1 = clf1.predict(X_test)\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "y_pred3 = clf3.predict(X_test)\n",
    "\n",
    "total = y_test.size\n",
    "cm1 = confusion_matrix(y_test, y_pred1)\n",
    "cm2 = confusion_matrix(y_test, y_pred2)\n",
    "cm3 = confusion_matrix(y_test, y_pred3)\n",
    "print(total)\n",
    "\n",
    "#Accuracy\n",
    "def cal_accuracy(cm):\n",
    "    cm = np.array(cm)\n",
    "    row,col = cm.shape\n",
    "    TC =0\n",
    "    for i in range (0,row):\n",
    "        #TrueClassified\n",
    "        TC += cm[i,i]\n",
    "        \n",
    "    return TC/total\n",
    "\n",
    "#labels = range of goals/results\n",
    "print('Limited-memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (LFBGS)')\n",
    "print(cm1)\n",
    "print('accuracy : '  + str(cal_accuracy(cm1)))\n",
    "print('\\n')\n",
    "\n",
    "print('Adam')\n",
    "print(cm2)\n",
    "print('accuracy : '  + str(cal_accuracy(cm2)))\n",
    "print('\\n')\n",
    "\n",
    "print('Stochastic Gradient Descent')\n",
    "print(cm3)\n",
    "print('accuracy : '  + str(cal_accuracy(cm3)))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "LFBGS  : 0.75\n",
      "Adam : 0.75\n",
      "SGD : 0.75\n"
     ]
    }
   ],
   "source": [
    "#Accuracy matrix\n",
    "acc1 = clf1.score(X_test,y_test)\n",
    "acc2 = clf2.score(X_test,y_test)\n",
    "acc3 = clf3.score(X_test,y_test)\n",
    "\n",
    "print('Accuracy')\n",
    "\n",
    "print('LFBGS  : ' + str(acc1))\n",
    "print('Adam : ' + str(acc2))\n",
    "print('SGD : ' + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
